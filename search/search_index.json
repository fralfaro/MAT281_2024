{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Bienvenidos a MAT281!","text":""},{"location":"#contenidos-del-curso","title":"Contenidos del Curso","text":"<p>Toolkit</p><p>Python, Git/GitHub</p> <p>Manipulaci\u00f3n Datos</p><p>Numpy, Pandas</p> <p>Visualizaci\u00f3n</p><p>Matplotlib, Seaborn</p> <p>Machine Learning</p><p>Sklearn, XGBoost</p>"},{"location":"#evaluaciones-del-curso","title":"Evaluaciones del Curso","text":"<p>Laboratorios</p><p>Labs del curso</p> <p>Tareas</p><p>Tareas del curso</p> <p>Proyecto</p><p>Proyecto del curso</p>"},{"location":"__init__/","title":"init","text":""},{"location":"homeworks/hw_01/","title":"Tarea-01","text":"<p>Para efectos pr\u00e1cticos del curso, es posible representar cada pixel de una imagen con un array de 3 dimensiones, cada valor representa a una de las capas RGB. Por lo tanto, una imagen de $n \\times m$ pixeles se representa como un arreglo de dimension $(n, m , 3)$ En <code>Python</code> una de las librer\u00edas de procesamiento de im\u00e1genes m\u00e1s utilizada es <code>Pillow</code>.</p> <p>Abrir una imagen es tan f\u00e1cil como:</p> In\u00a0[\u00a0]: Copied! <pre># librerias\n\nimport numpy as np\nfrom PIL import Image\n</pre> # librerias  import numpy as np from PIL import Image In\u00a0[\u00a0]: Copied! <pre>gatito = Image.open(\"images/gatito.png\")\n</pre> gatito = Image.open(\"images/gatito.png\") <p>Notar que la variable anterior es de una clase espec\u00edfica de la librer\u00eda.</p> In\u00a0[\u00a0]: Copied! <pre>type(gatito)\n</pre> type(gatito) <p>Para ver la imagen en Jupyter puedes utilizar la misma t\u00e9cnica que con los <code>pd.DataFrames</code>, es decir:</p> In\u00a0[\u00a0]: Copied! <pre>gatito\n</pre> gatito <p>Para tener su representaci\u00f3n en un array podemos utilizar el constructor <code>np.array</code> con argumento la imagen.</p> In\u00a0[\u00a0]: Copied! <pre>gatito_np = np.array(gatito)\nprint(f\"Dimension de la imagen gatito: {gatito_np.shape}.\\n\")\nprint(f\"Al convertir a np.ndarry el tipo de elementos es {gatito_np.dtype}.\\n\")\nprint(gatito_np)\n</pre> gatito_np = np.array(gatito) print(f\"Dimension de la imagen gatito: {gatito_np.shape}.\\n\") print(f\"Al convertir a np.ndarry el tipo de elementos es {gatito_np.dtype}.\\n\") print(gatito_np) <p>1.1 Crear una lista vac\u00eda declarada como <code>secret_list</code>.</p> In\u00a0[\u00a0]: Copied! <pre>secret_list = ## FIX ME ##\n</pre> secret_list = ## FIX ME ## <p>1.2 Iterar por cada uno de los canales RGB (<code>gatito_np.shape[2]</code>) y en cada iteraci\u00f3n:</p> <ul> <li>Crear un arreglo temporal llamado <code>secret_aux</code> de dos dimensiones, de la misma dimension de pixeles de la imagen <code>gatito</code> y que tenga valores enteros, <code>0</code> si el valor de la capa de <code>gatito_np</code> es par y <code>1</code> si es impar.</li> </ul> <ul> <li>No iterar por filas y columnas.</li> <li>Utilizar la operaci\u00f3n m\u00f3dulo <code>%</code>.</li> <li>En la i-\u00e9sima iteraci\u00f3n de los canales la capa de <code>gatito_np</code> es <code>gatito_np[:, :, i]</code>.</li> </ul> <ul> <li><p>Escalar <code>secret_aux</code> a valores 0 y 255.</p> </li> <li><p>Cambiar el <code>dtype</code> de <code>secret_aux</code> a <code>np.uint8</code> (utilize el m\u00e9teodo <code>astype()</code>).</p> </li> <li><p>Agregue <code>secret_aux</code> a <code>secret_list</code>.</p> </li> </ul> <p>Al final de la iteraci\u00f3n <code>secret_list</code> debe tener solo tres arreglos.</p> <p>Observaci\u00f3n: recuerde que puede aplicar operaciones directo a un arreglo de numpy.</p> In\u00a0[\u00a0]: Copied! <pre>for channel in ## FIX ME ##:\n    secret_aux = ## FIX ME ##\n    secret_aux = ## FIX ME ##\n    secret_aux = ## FIX ME ##\n    secret_list.append(secret_aux)\n</pre> for channel in ## FIX ME ##:     secret_aux = ## FIX ME ##     secret_aux = ## FIX ME ##     secret_aux = ## FIX ME ##     secret_list.append(secret_aux) In\u00a0[\u00a0]: Copied! <pre>print(f\"secret_list tiene {len(secret_list)} elementos\")\n</pre> print(f\"secret_list tiene {len(secret_list)} elementos\") <p>1.3 Crear la variable <code>secret_np</code> concatenando horizontalmente los elementos de <code>secret_list</code>.</p> In\u00a0[\u00a0]: Copied! <pre>secret_np = ## FIX ME ##\nsecret_np.shape\n</pre> secret_np = ## FIX ME ## secret_np.shape <p>1.4 Crear el objeto <code>secret_img</code> utilizando el arreglo <code>secret_np</code>, asegurar que los valores est\u00e9n entre 0 y 255, y que el dtype sea <code>np.uint8</code>, con el m\u00e9todo <code>Image.fromarray</code> con argumento <code>mode=\"L\"</code></p> In\u00a0[\u00a0]: Copied! <pre>np.unique(secret_np)\n</pre> np.unique(secret_np) In\u00a0[\u00a0]: Copied! <pre>secret_np.dtype\n</pre> secret_np.dtype In\u00a0[\u00a0]: Copied! <pre>secret_img = Image.fromarray(## FIX ME ##)\n</pre> secret_img = Image.fromarray(## FIX ME ##) <p>Ahora puedes ver el resultado!</p> In\u00a0[\u00a0]: Copied! <pre>secret_img\n</pre> secret_img <p>2.1 Selecciona una imagen de 2160 x 3840 pixeles (a.k.a resoluci\u00f3n 4k), lo importante es que sea solo en blanco y negro, en la carpeta <code>images</code> se disponibiliza como ejemplo la imagen <code>black_and_white_example.jpg</code> y crea una variable llamada <code>my_img</code> leyendo la imagen seleccionada con <code>Image.open()</code>.</p> In\u00a0[\u00a0]: Copied! <pre>my_img = Image.open(## FIX ME ##)\n</pre> my_img = Image.open(## FIX ME ##) <p>2.2 Crea un arreglo llamado <code>my_img_np</code> utilizando <code>my_img</code> y el m\u00e9todo <code>np.array()</code>. * Es importante que <code>my_img_np.shape</code> sea <code>(2160, 3840)</code>, es decir, que solo sea de dos dimensiones. Esto porque es una imagen en blanco y negro, no necesitando el modelo RGB.</p> In\u00a0[\u00a0]: Copied! <pre>my_img_np = np.array(my_img)\nprint(my_img_np.shape)\n</pre> my_img_np = np.array(my_img) print(my_img_np.shape) <p>2.3 Crear la variable <code>my_img_np_aux</code> utilizando un umbral con tal de que: - 1: Si el valor del pixel es mayor  al umbral. - 0: Si el valor del pixel es menor o igual al umbral. - El <code>dtype</code> debe ser <code>np.uint8</code>. - Para <code>black_and_white_example.jpg</code> un umbral adecuado es <code>20</code>.</p> In\u00a0[\u00a0]: Copied! <pre>umbral = 20\nmy_img_np_aux = ## FIX ME ##\n</pre> umbral = 20 my_img_np_aux = ## FIX ME ## <p>Puedes probar que tan bien qued\u00f3 la imagen con la siguiente linea. Si crees que no se ve bien, puedes cambiar el umbral.</p> In\u00a0[\u00a0]: Copied! <pre>Image.fromarray(my_img_np_aux * 255)\n</pre> Image.fromarray(my_img_np_aux * 255) <p>2.4 Dividir la imagen en tres arreglos de tama\u00f1o (2160, 1280) y guardarlos en una lista con el nombre <code>my_img_split</code>. Hint: Revisa en la documentaci\u00f3n de <code>numpy</code>.</p> In\u00a0[\u00a0]: Copied! <pre>my_img_split = ## FIX ME ##\n</pre> my_img_split = ## FIX ME ## <p>Revisa utilizando la siguiente iteraci\u00f3n.</p> In\u00a0[\u00a0]: Copied! <pre>for img_array in my_img_split:\n    print(img_array.shape)\n</pre> for img_array in my_img_split:     print(img_array.shape) <p>2.5 La imagen donde se esconder\u00e1 tu imagen selecionada est\u00e1 en la carpeta <code>images</code> con el nombre <code>gatito_original.png</code>, que sospechosamente es de 2160 x 1280 pixeles. Carga la imagen en la variable <code>cat</code> y luego crea arreglo <code>cat_np</code> utilizando <code>cat</code>. Verifica que <code>cat_np.shape = (2160, 1280, 3)</code>.</p> In\u00a0[\u00a0]: Copied! <pre>cat = Image.open(## FIX ME ##)\ncat_np = ## FIX ME ##\nprint(cat_np.shape)\n</pre> cat = Image.open(## FIX ME ##) cat_np = ## FIX ME ## print(cat_np.shape) <p>2.6 Convierte todos los valores de <code>cat_np</code> a valores pares. Esto lo puedes hacer sumando 1 a cada valor de arreglo si es impar</p> In\u00a0[\u00a0]: Copied! <pre>cat_np ## FIX ME ##\n</pre> cat_np ## FIX ME ## <p>2.7 Itera por canal RGB de <code>cat_np</code> y en cada capa suma los valores de uno de los arreglos de <code>my_img_split</code>.</p> In\u00a0[\u00a0]: Copied! <pre>for channel in ## FIX ME ##:\n    cat_np[## FIX ME ##] += ## FIX ME ##\n</pre> for channel in ## FIX ME ##:     cat_np[## FIX ME ##] += ## FIX ME ## <p>2.8 Crea una variable llamada <code>cat_secret_im</code> con <code>Image.fromarray</code> y la variable <code>cat_np</code> (que ya ha sido modificada). Luego guarda la imagen en la carpeta <code>images</code> con el nombre <code>my_secret.png</code>.</p> In\u00a0[\u00a0]: Copied! <pre>cat_secret_im = Image.fromarray(## FIX ME ##)\ncat_secret_im.save(## FIX ME ##)\n</pre> cat_secret_im = Image.fromarray(## FIX ME ##) cat_secret_im.save(## FIX ME ##) <p>2.9 Crea una funci\u00f3n llamada <code>imagenception()</code> que como argumento tenga la ruta de la imagen que quieres descifrar y que descifre la imagen secreta recientemente creada. Hint: Utiliza todos los pasos de la primera parte.</p> In\u00a0[\u00a0]: Copied! <pre>def imagenception(filepath):\n   ## FIX ME ##\n    return secret_img\n</pre> def imagenception(filepath):    ## FIX ME ##     return secret_img In\u00a0[\u00a0]: Copied! <pre>my_secret_img = imagenception(\"images/my_secret.png\")\nmy_secret_img\n</pre> my_secret_img = imagenception(\"images/my_secret.png\") my_secret_img In\u00a0[\u00a0]: Copied! <pre># libraries\nimport pandas as pd\npd.set_option(\"display.max_columns\", 999)  # Permite mostrar hasta 999 columnas de un DataFrame en Jupyter.\n</pre> # libraries import pandas as pd pd.set_option(\"display.max_columns\", 999)  # Permite mostrar hasta 999 columnas de un DataFrame en Jupyter. <p>En la carpeta <code>data/world-happiness</code> se disponen de tres archivos, uno por cada reporte anual (a\u00f1os 2015, 2016 y 2017). No es de sorprender que env\u00eden un archivo por a\u00f1o (podr\u00eda ser mensual, semestral, etc.), lo imortante es ser capaces de leer una cantidad variable de archivos al mismo tiempo. Una buena pr\u00e1ctica es crear un diccionario de dataframes.</p> In\u00a0[\u00a0]: Copied! <pre># Comprehension dictionary\ndf_dict = {\n    year: pd.read_csv(f\"data/world-happiness/{year}.csv\").assign(Year=year)\n    for year in [2015, 2016, 2017]\n}\n</pre> # Comprehension dictionary df_dict = {     year: pd.read_csv(f\"data/world-happiness/{year}.csv\").assign(Year=year)     for year in [2015, 2016, 2017] } <p>Por ejemplo, se puede acceder al DataFrame asociado al archivo <code>data/world-happiness/2016.csv</code> de la siguiente manera:</p> <p>Una peque\u00f1a descripci\u00f3n de las columnas</p> <ul> <li><code>Country</code> Name of the country.</li> <li><code>Region</code> Region the country belongs to.</li> <li><code>Happiness Rank</code> Rank of the country based on the Happiness Score.</li> <li><code>Happiness Score</code> A metric measured in 2015 by asking the sampled people the question: \"How would you rate your happiness on a scale of 0 to 10 where 10 is the happiest.\"</li> <li><code>Standard Error</code> The standard error of the happiness score.</li> <li><code>Economy (GDP per Capita)</code> The extent to which GDP contributes to the calculation of the Happiness Score.</li> <li><code>Family</code> The extent to which Family contributes to the calculation of the Happiness Score</li> <li><code>Health (Life Expectancy)</code> The extent to which Life expectancy contributed to the calculation of the Happiness Score</li> <li><code>Freedom</code> The extent to which Freedom contributed to the calculation of the Happiness Score.</li> <li><code>Trust (Government Corruption)</code> The extent to which Perception of Corruption contributes to Happiness Score.</li> <li><code>Generosity</code> The extent to which Generosity contributed to the calculation of the Happiness Score.</li> <li><code>Dystopia Residual</code> The extent to which Dystopia Residual contributed to the calculation of the Happiness Score.</li> </ul> <p>Notar que los conjuntos de datos no poseen las mismas columnas, por lo tanto, solo se trabajar\u00e1n con las columnas en com\u00fan y posteriormente agregaremos el a\u00f1o con tal de concatenar los tres conjuntos.</p> In\u00a0[\u00a0]: Copied! <pre>from functools import reduce\nintersection_columns = reduce(np.intersect1d, [df_i.columns.values for df_i in df_dict.values()]).tolist() \nprint(intersection_columns)\n</pre> from functools import reduce intersection_columns = reduce(np.intersect1d, [df_i.columns.values for df_i in df_dict.values()]).tolist()  print(intersection_columns) <p>Explica con tus palabras las operaciones que se realizaron para obtener la variable <code>intersection_columns</code>.</p> <p>Respuesta: &lt; RESPONDER AQU\u00cd &gt;</p> <p>Define el DataFrame <code>happiness</code> tal que:</p> <ul> <li>Sea la concatenaci\u00f3n de los dataframes de <code>df_dict</code></li> <li>Resetea los \u00edndices.</li> <li>Selecciona solo las columnas de la lista <code>intersection_columns</code>.</li> <li>Los nombres de las columnas deben estar en min\u00edsculas, reemplazar espacios por guiones bajos (<code>_</code>) y elimina los par\u00e9ntesis.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>happiness = (\n    pd.concat(## FIX ME ##)\n    .droplevel(## FIX ME ##)\n    .## FIX ME ##\n    .## FIX ME ##\n    .## FIX ME ##\n)\nhappiness.head()\n</pre> happiness = (     pd.concat(## FIX ME ##)     .droplevel(## FIX ME ##)     .## FIX ME ##     .## FIX ME ##     .## FIX ME ## ) happiness.head() <p>Como siempre, partimos con un an\u00e1lisis descriptivo simple.</p> In\u00a0[\u00a0]: Copied! <pre>happiness.describe(include=\"all\").fillna(\"\").T\n</pre> happiness.describe(include=\"all\").fillna(\"\").T <p>\u00bfCu\u00e1ntos pa\u00edses no tienen mediciones de felicidad en los tres a\u00f1os del estudio? \u00bfCu\u00e1les son?</p> In\u00a0[\u00a0]: Copied! <pre>## FIX ME ##\n</pre> ## FIX ME ## In\u00a0[\u00a0]: Copied! <pre>## FIX ME ##\n</pre> ## FIX ME ## <p>Respuesta: &lt; RESPONDER AQU\u00cd &gt;</p> <p>Note que la lista de pa\u00edses proveniente de la pregunta anterior tiene errores de consistencia, por ejemplo est\u00e1n los registros de <code>Hong Kong</code> y <code>Hong Kong S.A.R., China</code> que escencialmente son el mismo. Lo mismo ocurre con <code>Taiwan</code> y <code>Somaliland Region</code>.</p> <p>Modifique la columna <code>country</code> del dataframe <code>happiness</code> con tal de reparar los errores de <code>Hong Kong</code>, <code>Taiwan</code> y <code>Somaliland Region</code>.</p> In\u00a0[\u00a0]: Copied! <pre>bad_country_names_dict = {\"Hong Kong S.A.R., China\": \"Hong Kong\", ## FIX ME ##}\nhappiness = happiness.assign(## FIX ME ##)\n</pre> bad_country_names_dict = {\"Hong Kong S.A.R., China\": \"Hong Kong\", ## FIX ME ##} happiness = happiness.assign(## FIX ME ##) <p>Luego de la modificaci\u00f3n, \u00bfCu\u00e1ntos pa\u00edses no tienen mediciones en los tres a\u00f1os de estudio?</p> In\u00a0[\u00a0]: Copied! <pre>## FIX ME ##\n</pre> ## FIX ME ## <p>Pivotea el dataframe <code>happines</code> tal que los \u00edndices sean los a\u00f1os, las columnas los pa\u00edses y el valor su <code>happiness_score</code>. LLena los valores nulos con un string vac\u00edo <code>\"\"</code>. Un pa\u00eds no puede tener m\u00e1s de un registro por a\u00f1o, por lo que puedes utilizar directamente el m\u00e9doto <code>pd.DataFrame.pivot()</code>.</p> In\u00a0[\u00a0]: Copied! <pre>## FIX ME ##\n</pre> ## FIX ME ## <p>\u00bfQu\u00e9 informaci\u00f3n podr\u00edas sacar r\u00e1pidamente de esta tabla pivoteada? \u00bfPodr\u00edas decir que siempre es \u00fatil pivotear una tabla?</p> <p>Respuesta:  &lt; RESPONDER AQU\u00cd &gt;</p> <p>En promedio, \u00bfCu\u00e1les son los tres pa\u00edses con el mayor score de felicidad?</p> In\u00a0[\u00a0]: Copied! <pre>## FIX ME ##\n</pre> ## FIX ME ## <p>Respuesta:  &lt; RESPONDER AQU\u00cd &gt;</p> <p>Calcula el promedio anual de todas las columnas factores de felicidad, es decir, todas las variables num\u00e9ricas excepto <code>happiness_score</code> y <code>happiness_rank</code>.</p> In\u00a0[\u00a0]: Copied! <pre>hap_mean_factors = ## FIX ME ##\nhap_mean_factors\n</pre> hap_mean_factors = ## FIX ME ## hap_mean_factors <p>Respecto al c\u00e1lculo anterior, para cada uno de los a\u00f1os, \u00bfCu\u00e1l es el factor que m\u00e1s contribuye (en promedio) al score de la felicidad y en qu\u00e9 medida?</p> In\u00a0[\u00a0]: Copied! <pre>## FIX ME ##\n</pre> ## FIX ME ## <p>Respuesta:  &lt; RESPONDER AQU\u00cd &gt;</p> <p>A continuaci\u00f3n, agregaremos un nuevo conjunto de datos, el que contiene estad\u00edsticas de suicidio por a\u00f1os, pa\u00edses y rangos et\u00e1reos. Se encuentra disponible en el siguiente link.</p> In\u00a0[\u00a0]: Copied! <pre>suicide = pd.read_csv(\"data/suicide_rates.csv\")\nsuicide.head()\n</pre> suicide = pd.read_csv(\"data/suicide_rates.csv\") suicide.head() <p>La mayor\u00eda de las columnas son autoexplicativas.</p> <ul> <li><code>country</code></li> <li><code>year</code></li> <li><code>sex</code></li> <li><code>age</code></li> <li><code>suicides_no</code></li> <li><code>population</code></li> <li><code>suicides/100k pop</code></li> <li><code>country-year</code></li> <li><code>HDI for year</code> Human Development Index</li> <li><code>gdp_for_year ($)</code> Gross Domestic Product</li> <li><code>gdp_per_capita ($)</code></li> <li><code>generation</code> based on age grouping average</li> </ul> <p>Un poco de estad\u00edstica descriptiva.</p> In\u00a0[\u00a0]: Copied! <pre>suicide.describe(include=\"all\").fillna(\"\").T\n</pre> suicide.describe(include=\"all\").fillna(\"\").T <p>Crea un nuevo DataFrame llamado suicide_agg siguiendo las siguientes instrucciones:</p> <ul> <li>Agrupa por pa\u00eds y a\u00f1o.</li> <li>Suma la poblaci\u00f3n y el n\u00famero de suicidios.</li> <li>Resetea los \u00edndices.</li> <li>Agrega una nueva columna llamada <code>suicides_ratio_100k</code> formada por la divisi\u00f3n de <code>suicides_no</code> y <code>population</code>, para posteriormente muliplicarla por 100,000.</li> <li>Agrega una nuevale columna llamada <code>suicides_rank</code> similar a <code>happiness_rank</code>, es decir, que asigne un orden por a\u00f1o a cada pa\u00eds seg\u00fan la columna <code>suicides_ratio_100k</code> tal que el rank 1 corresponda al que tenga mayor <code>suicides_ratio_100k</code>. Hint: Usa el m\u00e9todo <code>rank()</code>.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># Es posible hacer todas las operaciones encadenadas!\nsuicides_agg = (\n    suicide.groupby(## FIX ME ##])\n    .agg(\n      ## FIX ME ##\n    )\n    .## FIX ME ##\n    .assign(\n        suicides_ratio_100k=## FIX ME ##,\n        suicides_rank=## FIX ME ##\n    )\n)\n</pre> # Es posible hacer todas las operaciones encadenadas! suicides_agg = (     suicide.groupby(## FIX ME ##])     .agg(       ## FIX ME ##     )     .## FIX ME ##     .assign(         suicides_ratio_100k=## FIX ME ##,         suicides_rank=## FIX ME ##     ) ) <p>Crea un nuevo DataFrame con el nombre <code>hap_sui</code> al unir <code>happiness</code> y <code>suicides_agg</code> tal que coincidan pa\u00eds y a\u00f1o, qu\u00e9date con solo los registros que coincidan en ambas DataFrames.</p> In\u00a0[\u00a0]: Copied! <pre>hap_sui = ## FIX ME ##\nhap_sui.head()\n</pre> hap_sui = ## FIX ME ## hap_sui.head() <p>\u00bfQu\u00e9 tipo de correlaci\u00f3n lineal hay entre las variables <code>happiness_rank</code> y <code>suicides_rank</code>?</p> In\u00a0[\u00a0]: Copied! <pre>hap_sui.loc[:, [\"happiness_rank\", \"suicides_rank\"]].## FIX ME ##\n</pre> hap_sui.loc[:, [\"happiness_rank\", \"suicides_rank\"]].## FIX ME ## <p>\u00bfQu\u00e9 tipo de correlaci\u00f3n lineal hay entre las variables <code>happiness_rank</code> y <code>suicides_rank</code> por cada a\u00f1o?</p> In\u00a0[\u00a0]: Copied! <pre>## FIX ME ##\n</pre> ## FIX ME ## <p>Respuesta:  &lt; RESPONDER AQU\u00cd &gt;</p> <p>\u00bfLa respuesta de las dos preguntas anteriores cambia si se utilizan las variables <code>happiness_score</code> y <code>suicides_ratio_100k</code>?</p> In\u00a0[\u00a0]: Copied! <pre>## FIX ME ##\n</pre> ## FIX ME ## <p>Respuesta:  &lt; RESPONDER AQU\u00cd &gt;</p> <p>Estos \u00edndices est\u00e1n ajustados a la Ciudad de Nueva York (NYC). Lo que significa que para la Ciudad de Nueva York, cada \u00edndice deber\u00eda marcar 100(%). Si otra ciudad tiene, por ejemplo, un \u00edndice de alquiler de 120, significa que en esa ciudad se paga de media por el alquiler un 20% m\u00e1s que en Nueva York. Si una ciudad tiene un \u00edndice de alquiler de 70, significa que en esa ciudad los alquileres son de media un 30% m\u00e1s baratos que en Nueva York.</p> <ul> <li><p>El \u00cdndice de Costo de Vida (Sin Alquiler) es un indicador relativo de los precios de bienes de consumo, incluyendo comestibles, restaurantes, transporte y servicios. El \u00cdndice de Costo de Vida no incluye gastos de residencia como alquileres o hipotecas. Si una ciudad tiene un Costo de Vida de 120, significa que Numbeo estima que es un 20% m\u00e1s cara que Nueva York (sin contar alquiler).</p> </li> <li><p>El \u00cdndice de Alquiler es una estimaci\u00f3n de precios de alquiler de apartamentos de una ciudad comparada con Nueva York. Si el \u00cdndice de Alquiler es 80, Numbeo estima que el precio de los alquileres en esa ciudad es de media un 20% m\u00e1s barato que en Nueva York.</p> </li> <li><p>El \u00cdndice de Comestibles es una estimaci\u00f3n de los precios de la compra de una ciudad en comparaci\u00f3n con Nueva York. Para calcular esta secci\u00f3n, Numbeo utiliza el peso de los art\u00edculos en la secci\u00f3n \"Mercados\" por cada ciudad.</p> </li> <li><p>El \u00cdndice de Restaurantes es una comparaci\u00f3n de precios de comidas y bebidas en bares y restaurantes en comparaci\u00f3n con NY.</p> </li> <li><p>El \u00cdndice de Costo de Vida m\u00e1s Alquiler es una estimaci\u00f3n de precios de consumo incluyendo alquiler en comparaci\u00f3n con la Ciudad de Nueva York.</p> </li> <li><p>El Poder Adquisitivo Local muestra la capacidad adquisitiva relativa a la hora de comprar bienes y servicios en una ciudad determinada, con relaci\u00f3n al salario medio de la ciudad. Si el poder adquisitivo dom\u00e9stico es 40, significa que los habitantes de dicha ciudad con salario medio pueden permitirse comprar una media de 60% menos bienes y servicios que los habitantes de Nueva York con salario medio.</p> </li> </ul> <p>Para m\u00e1s informaci\u00f3n sobre los pesos utilizados (f\u00f3rmula completa) puedes visitar: motivaci\u00f3n y metodolog\u00eda.</p> <p>Para comenzar es necesario instalar el paquete <code>lxml</code> en tu entorno virtual de conda para poder descargar los datos. Basta con ejecutar:</p> In\u00a0[\u00a0]: Copied! <pre># instalar lxml\n!pip install lxml\n</pre> # instalar lxml !pip install lxml <p>Se disponibiliza a continuaci\u00f3n la carga de datos de un dataframe.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\npd.set_option('display.max_columns', 999)\n%matplotlib inline\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt  pd.set_option('display.max_columns', 999) %matplotlib inline In\u00a0[\u00a0]: Copied! <pre>years = [2015, 2016, 2017, 2018, 2019, 2020]\nlife_cost = (\n    pd.concat(\n        {\n            year: (\n                pd.read_html(f\"https://www.numbeo.com/cost-of-living/rankings.jsp?title={year}\")[1]\n                .rename(columns=lambda x: x.lower().replace(\" \", \"_\"))\n                .assign(rank=lambda x: x.index + 1)\n                .set_index(\"rank\")\n            ) for year in years\n        }\n    )\n    .rename_axis([\"year\", \"rank\"])\n    .reset_index()\n)\nlife_cost.head()\n</pre> years = [2015, 2016, 2017, 2018, 2019, 2020] life_cost = (     pd.concat(         {             year: (                 pd.read_html(f\"https://www.numbeo.com/cost-of-living/rankings.jsp?title={year}\")[1]                 .rename(columns=lambda x: x.lower().replace(\" \", \"_\"))                 .assign(rank=lambda x: x.index + 1)                 .set_index(\"rank\")             ) for year in years         }     )     .rename_axis([\"year\", \"rank\"])     .reset_index() ) life_cost.head() In\u00a0[\u00a0]: Copied! <pre>## FIX ME PLEASE ##\n</pre> ## FIX ME PLEASE ## In\u00a0[\u00a0]: Copied! <pre>rol_seed = 201110002  # Escribe tu rol UTFSM sin n\u00famero verificador\nmy_cities = life_cost[\"city\"].drop_duplicates().sample(n=10, random_state=rol_seed).values\n</pre> rol_seed = 201110002  # Escribe tu rol UTFSM sin n\u00famero verificador my_cities = life_cost[\"city\"].drop_duplicates().sample(n=10, random_state=rol_seed).values In\u00a0[\u00a0]: Copied! <pre>## FIX ME PLEASE ##\n</pre> ## FIX ME PLEASE ##"},{"location":"homeworks/hw_01/#tarea-01","title":"Tarea-01\u00b6","text":""},{"location":"homeworks/hw_01/#instrucciones","title":"Instrucciones\u00b6","text":"<p>1.- Completa tus datos personales (nombre y rol USM) en siguiente celda.</p> <ul> <li><p>Nombre:</p> </li> <li><p>Rol:</p> </li> </ul> <p>2.- Debes subir este archivo con tus cambios a tu repositorio personal del curso, incluyendo datos, im\u00e1genes, scripts, etc.</p> <p>3.- Se evaluar\u00e1:</p> <ul> <li>Soluciones</li> <li>C\u00f3digo</li> <li>Al presionar  <code>Kernel -&gt; Restart Kernel and Run All Cells</code> deben ejecutarse todas las celdas sin error.</li> </ul>"},{"location":"homeworks/hw_01/#i-imagenception","title":"I.-  Imagenception\u00b6","text":"<p>Desde Wikipedia, RGB (sigla en ingl\u00e9s de red, green, blue) es un modelo de color basado en la s\u00edntesis aditiva, con el que es posible representar un color mediante la mezcla por adici\u00f3n de los tres colores de luz primarios. El modelo de color RGB no define por s\u00ed mismo lo que significa exactamente rojo, verde o azul, por lo que los mismos valores RGB pueden mostrar colores notablemente diferentes en distintos dispositivos que usen este modelo de color. Aunque utilicen un mismo modelo de color, sus espacios de color pueden variar considerablemente.</p> <p>Para indicar con qu\u00e9 proporci\u00f3n es mezclado cada color, se asigna un valor a cada uno de los colores primarios, de manera que el valor \"0\" significa que no interviene en la mezcla y, a medida que ese valor aumenta, se entiende que aporta m\u00e1s intensidad a la mezcla. Aunque el intervalo de valores podr\u00eda ser cualquiera (valores reales entre 0 y 1, valores enteros entre 0 y 37, etc.), es frecuente que cada color primario se codifique con un byte (8 bits).</p> <p>As\u00ed, de manera usual, la intensidad de cada una de las componentes se mide seg\u00fan una escala que va del 0 al 255 y cada color es definido por un conjunto de valores escritos entre par\u00e9ntesis (correspondientes a valores \"R\", \"G\" y \"B\") y separados por comas.</p> <p>El conjunto de todos los colores tambi\u00e9n se puede representar en forma de cubo. Cada color es un punto de la superficie o del interior de \u00e9ste. La escala de grises estar\u00eda situada en la diagonal que une al color blanco con el negro.</p> <p></p>"},{"location":"homeworks/hw_01/#1-encontrando-la-imagen-oculta","title":"1.- Encontrando la imagen oculta\u00b6","text":"<p>La imagen anterior tiene una imagen oculta, el ejercicio corresponde en descifrarlo. Las instrucciones son las siguientes:</p>"},{"location":"homeworks/hw_01/#2-escondiendo-una-nueva-imagen","title":"2.- Escondiendo una nueva imagen\u00b6","text":"<p>Es tu turno, ahora tu esconder\u00e1s una imagen. Las instrucciones son las siguientes:</p>"},{"location":"homeworks/hw_01/#ii-analizando-la-felicidad","title":"II.- Analizando la Felicidad\u00b6","text":"<p>Este ejercicio es netamente an\u00e1lisis de datos, tratando de abarcar problemas t\u00edpicos como la lectura de datos, correcci\u00f3n de errores, m\u00e9tricas agrupadas, uni\u00f3n de datos, etc. Utilizaremos un conjunto de datos llamado World Happiness Report disponible en el siguiente link, de donde se puede obtener informaci\u00f3n al respecto.</p>"},{"location":"homeworks/hw_01/#context","title":"Context\u00b6","text":"<p>The World Happiness Report is a landmark survey of the state of global happiness. The first report was published in 2012, the second in 2013, the third in 2015, and the fourth in the 2016 Update. The World Happiness 2017, which ranks 155 countries by their happiness levels, was released at the United Nations at an event celebrating International Day of Happiness on March 20th. The report continues to gain global recognition as governments, organizations and civil society increasingly use happiness indicators to inform their policy-making decisions. Leading experts across fields \u2013 economics, psychology, survey analysis, national statistics, health, public policy and more \u2013 describe how measurements of well-being can be used effectively to assess the progress of nations. The reports review the state of happiness in the world today and show how the new science of happiness explains personal and national variations in happiness.</p>"},{"location":"homeworks/hw_01/#content","title":"Content\u00b6","text":"<p>The happiness scores and rankings use data from the Gallup World Poll. The scores are based on answers to the main life evaluation question asked in the poll. This question, known as the Cantril ladder, asks respondents to think of a ladder with the best possible life for them being a 10 and the worst possible life being a 0 and to rate their own current lives on that scale. The scores are from nationally representative samples for the years 2013-2016 and use the Gallup weights to make the estimates representative. The columns following the happiness score estimate the extent to which each of six factors \u2013 economic production, social support, life expectancy, freedom, absence of corruption, and generosity \u2013 contribute to making life evaluations higher in each country than they are in Dystopia, a hypothetical country that has values equal to the world\u2019s lowest national averages for each of the six factors. They have no impact on the total score reported for each country, but they do explain why some countries rank higher than others.</p>"},{"location":"homeworks/hw_01/#21-lectura-de-datos","title":"2.1 Lectura de datos\u00b6","text":""},{"location":"homeworks/hw_01/#22-concatenacion-y-procesado","title":"2.2 Concatenaci\u00f3n y procesado\u00b6","text":""},{"location":"homeworks/hw_01/#23-analisis","title":"2.3 An\u00e1lisis\u00b6","text":""},{"location":"homeworks/hw_01/#24-agregando-mas-datos","title":"2.4 Agregando m\u00e1s datos\u00b6","text":""},{"location":"homeworks/hw_01/#iii-indices-de-costos-de-vida","title":"III.- \u00cdndices de Costos de Vida\u00b6","text":""},{"location":"homeworks/hw_01/#ejercicio-31","title":"Ejercicio 3.1\u00b6","text":"<p>Explique lo que se hizo en la celda anterior detalladamente.</p>"},{"location":"homeworks/hw_01/#ejercicio-32","title":"Ejercicio 3.2\u00b6","text":"<p>Genera un histograma del \u00edndice del costo de vida (sin alquiler) para cada a\u00f1o (es decir, 6 histogramas).</p> <p>\u00bfQu\u00e9 conclusi\u00f3n puedes sacar de estos gr\u00e1ficos?</p>"},{"location":"homeworks/hw_01/#ejercicio-33","title":"Ejercicio 3.3\u00b6","text":"<p>Grafica el \u00edndice de restaurantes a trav\u00e9s de los a\u00f1os para diez ciudades escogidas pseudo-aleatoriamente (variable <code>my_cities</code> de la celda siguiente) en un mismo gr\u00e1fico. Recuerda escoger el tipo de gr\u00e1fico adecuadamente.</p> <p>\u00bfVes alguna relaci\u00f3n? \u00bfQu\u00e9 podr\u00edas decir del gr\u00e1fico? \u00bfPor qu\u00e9 no graficar todas las ciudades en lugar de solo escoger algunas?</p>"},{"location":"homeworks/hw_02/","title":"Tarea-02","text":"In\u00a0[\u00a0]: Copied! <pre>## FIX ME PLEASE ##\n</pre> ## FIX ME PLEASE ## In\u00a0[\u00a0]: Copied! <pre>## FIX ME PLEASE ##\n</pre> ## FIX ME PLEASE ##"},{"location":"homeworks/hw_02/#tarea-02","title":"Tarea-02\u00b6","text":""},{"location":"homeworks/hw_02/#instrucciones","title":"Instrucciones\u00b6","text":"<p>1.- Completa tus datos personales (nombre y rol USM) en siguiente celda.</p> <ul> <li><p>Nombre:</p> </li> <li><p>Rol:</p> </li> </ul> <p>2.- Debes subir este archivo con tus cambios a tu repositorio personal del curso, incluyendo datos, im\u00e1genes, scripts, etc.</p> <p>3.- Se evaluar\u00e1:</p> <ul> <li>Soluciones</li> <li>C\u00f3digo</li> <li>Al presionar  <code>Kernel -&gt; Restart Kernel and Run All Cells</code> deben ejecutarse todas las celdas sin error.</li> </ul> <p>4.- Esta Tarea debe ser entregada en Dos Jupyter Notebooks Distinto.</p> <ul> <li>Ejemplo: <code>hw_02_part_01.ipynb</code>, <code>hw_02_part_02.ipynb</code>.</li> </ul>"},{"location":"homeworks/hw_02/#i-learnplatform","title":"I.- LearnPlatform\u00b6","text":""},{"location":"homeworks/hw_02/#introduccion","title":"Introducci\u00f3n\u00b6","text":"<p>Nelson Mandela cre\u00eda que la educaci\u00f3n era el arma m\u00e1s poderosa para cambiar el mundo. Pero no todos los estudiantes tienen las mismas oportunidades de aprender. Es necesario promulgar pol\u00edticas y planes efectivos para que la educaci\u00f3n sea m\u00e1s equitativa, y tal vez su innovador an\u00e1lisis de datos ayude a revelar la soluci\u00f3n.</p> <p>La investigaci\u00f3n actual muestra que los resultados educativos est\u00e1n lejos de ser equitativos. El desequilibrio se vio agravado por la pandemia de COVID-19. Existe una necesidad urgente de comprender y medir mejor el alcance y el impacto de la pandemia en estas inequidades.</p> <p>La empresa de tecnolog\u00eda educativa LearnPlatform se fund\u00f3 en 2014 con la misi\u00f3n de ampliar el acceso equitativo a la tecnolog\u00eda educativa para todos los estudiantes y profesores. Los distritos y estados utilizan el sistema integral de efectividad de la tecnolog\u00eda educativa de LearnPlatform para mejorar continuamente la seguridad, la equidad y la efectividad de su tecnolog\u00eda educativa. LearnPlatform lo hace generando una base de evidencia de lo que est\u00e1 funcionando y promulg\u00e1ndola en beneficio de los estudiantes, los profesores y los presupuestos.</p> <p>En esta competencia de an\u00e1lisis, trabajar\u00e1 para descubrir tendencias en el aprendizaje digital. Logre esto con un an\u00e1lisis de datos sobre c\u00f3mo el compromiso con el aprendizaje digital se relaciona con factores como la demograf\u00eda del distrito, el acceso a la banda ancha y las pol\u00edticas y eventos a nivel estatal / nacional. Luego, env\u00ede un notebook de Kaggle para proponer su mejor soluci\u00f3n a estas desigualdades educativas.</p> <p>Sus presentaciones informar\u00e1n las pol\u00edticas y pr\u00e1cticas que cierran la brecha digital. Con una mejor comprensi\u00f3n de las tendencias de aprendizaje digital, puede ayudar a revertir la p\u00e9rdida de aprendizaje a largo plazo entre los m\u00e1s vulnerables de Estados Unidos, haciendo que la educaci\u00f3n sea m\u00e1s equitativa.</p>"},{"location":"homeworks/hw_02/#planteamiento-del-problema","title":"Planteamiento del problema\u00b6","text":"<p>La pandemia COVID-19 ha interrumpido el aprendizaje de m\u00e1s de 56 millones de estudiantes en los Estados Unidos. En la primavera de 2020, la mayor\u00eda de los gobiernos estatales y locales de los EE. UU. Cerraron las instituciones educativas para detener la propagaci\u00f3n del virus. En respuesta, las escuelas y los maestros han intentado llegar a los estudiantes de forma remota a trav\u00e9s de herramientas de aprendizaje a distancia y plataformas digitales. Hasta el d\u00eda de hoy, las preocupaciones sobre la exacerbaci\u00f3n de la brecha digital y la p\u00e9rdida de aprendizaje a largo plazo entre los estudiantes m\u00e1s vulnerables de Estados Unidos contin\u00faan creciendo.</p>"},{"location":"homeworks/hw_02/#desafio","title":"Desaf\u00edo\u00b6","text":"<p>Los estudiantes deben explorar (1) el estado del aprendizaje digital en 2020 y (2) c\u00f3mo la participaci\u00f3n del aprendizaje digital se relaciona con factores como la demograf\u00eda del distrito, el acceso a banda ancha y las pol\u00edticas y eventos a nivel estatal/nacional.</p> <p>Le recomendamos que oriente el an\u00e1lisis con preguntas relacionadas con los temas descritos anteriormente (en negrita). A continuaci\u00f3n se muestran algunos ejemplos de preguntas que se relacionan con el planteamiento de nuestro problema:</p> <ul> <li>\u00bfCu\u00e1l es el panorama de la conectividad y el compromiso digitales en 2020?</li> <li>\u00bfCu\u00e1l es el efecto de la pandemia de COVID-19 en el aprendizaje en l\u00ednea y a distancia, y c\u00f3mo podr\u00eda evolucionar tambi\u00e9n en el futuro?</li> <li>\u00bfC\u00f3mo cambia la participaci\u00f3n de los estudiantes con los diferentes tipos de tecnolog\u00eda educativa durante el transcurso de la pandemia?</li> <li>\u00bfC\u00f3mo se relaciona la participaci\u00f3n de los estudiantes con las plataformas de aprendizaje en l\u00ednea con las diferentes geograf\u00edas? \u00bfContexto demogr\u00e1fico (por ejemplo, raza/etnia, ESL, discapacidad de aprendizaje)? Contexto de aprendizaje? \u00bfEstatus socioecon\u00f3mico?</li> <li>\u00bfSe correlacionan ciertas intervenciones, pr\u00e1cticas o pol\u00edticas estatales (por ejemplo, est\u00edmulo, reapertura, moratoria de desalojo) con el aumento o la disminuci\u00f3n de la participaci\u00f3n en l\u00ednea?</li> </ul>"},{"location":"homeworks/hw_02/#evaluacion","title":"Evaluaci\u00f3n\u00b6","text":""},{"location":"homeworks/hw_02/#claridad","title":"Claridad\u00b6","text":"<ul> <li>\u00bfEl autor present\u00f3 un hilo claro de preguntas o temas que motivaron su an\u00e1lisis?</li> <li>\u00bfEl autor document\u00f3 por qu\u00e9/c\u00f3mo se eligi\u00f3 y utiliz\u00f3 un conjunto de m\u00e9todos para su an\u00e1lisis?</li> <li>\u00bfEst\u00e1 documentado el notebook de una manera que sea f\u00e1cilmente reproducible (p. Ej., C\u00f3digo, fuentes de datos adicionales, citas)?</li> <li>\u00bfEl notebook contiene visualizaciones de datos claras que ayuden a comunicar de manera eficaz los hallazgos del autor tanto a expertos como a no expertos?</li> </ul>"},{"location":"homeworks/hw_02/#precision","title":"Precisi\u00f3n\u00b6","text":"<ul> <li>\u00bfEl autor proces\u00f3 los datos (por ejemplo, fusionando) y/o fuentes de datos adicionales con precisi\u00f3n?</li> <li>\u00bfLa metodolog\u00eda utilizada en el an\u00e1lisis es apropiada y razonable?</li> <li>\u00bfSon razonables y convincentes las interpretaciones basadas en el an\u00e1lisis y la visualizaci\u00f3n?</li> </ul>"},{"location":"homeworks/hw_02/#creatividad","title":"Creatividad\u00b6","text":"<ul> <li>\u00bfEl notebook ayuda al lector a aprender algo nuevo o lo desaf\u00eda a pensar de una manera nueva?</li> <li>\u00bfEl notebook aprovecha m\u00e9todos novedosos y/o visualizaciones que ayudan a revelar informaci\u00f3n a partir de datos y/o comunicar hallazgos?</li> <li>\u00bfEl autor utiliz\u00f3 fuentes de datos p\u00fablicas adicionales en su an\u00e1lisis?</li> </ul>"},{"location":"homeworks/hw_02/#hints","title":"Hints\u00b6","text":"<ul> <li>Esto corresponde a un desafio de Kaggle (link).</li> <li>La informaci\u00f3n respecto a los datos, lo pueden encontrar en el siguiente link.</li> <li>A modo de inspiraci\u00f3n, pueden ocupar algunos gr\u00e1ficos de otros participantes del desaf\u00edo (link).</li> </ul>"},{"location":"homeworks/hw_02/#ii-titanic-machine-learning-from-disaster","title":"II.- Titanic - Machine Learning from Disaster\u00b6","text":""},{"location":"homeworks/hw_02/#ahoy-welcome-to-kaggle-youre-in-the-right-place","title":"\ud83d\udc4b\ud83d\udef3\ufe0f Ahoy, welcome to Kaggle! You\u2019re in the right place.\u00b6","text":"<p>This is the legendary Titanic ML competition \u2013 the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.</p> <p>The competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.</p> <p>Read on or watch the video below to explore more details. Once you\u2019re ready to start competing, click on the \"Join Competition button to create an account and gain access to the competition data. Then check out Alexis Cook\u2019s Titanic Tutorial that walks you through step by step how to make your first submission!</p>"},{"location":"homeworks/hw_02/#the-challenge","title":"The Challenge\u00b6","text":"<p>The sinking of the Titanic is one of the most infamous shipwrecks in history.</p> <p>On April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.</p> <p>While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.</p> <p>In this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).</p>"},{"location":"homeworks/hw_02/#recommended-tutorial","title":"Recommended Tutorial\u00b6","text":"<p>We highly recommend Alexis Cook\u2019s Titanic Tutorial that walks you through making your very first submission step by step and this starter notebook to get started.</p>"},{"location":"homeworks/hw_02/#overview-of-how-kaggles-competitions-work","title":"Overview of How Kaggle\u2019s Competitions Work\u00b6","text":"<ol> <li>Join the Competition Read about the challenge description, accept the Competition Rules and gain access to the competition dataset.</li> <li>Get to Work Download the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.</li> <li>Make a Submission Upload your prediction as a submission on Kaggle and receive an accuracy score.</li> <li>Check the Leaderboard See how your model ranks against other Kagglers on our leaderboard.</li> <li>Improve Your Score Check out the discussion forum to find lots of tutorials and insights from other competitors.</li> </ol>"},{"location":"homeworks/hw_02/#kaggle-lingo-video","title":"Kaggle Lingo Video\u00b6","text":"<p>You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out Dr. Rachael Tatman\u2019s video on Kaggle Lingo to get up to speed!</p>"},{"location":"homeworks/hw_02/#what-data-will-i-use-in-this-competition","title":"What Data Will I Use in This Competition?\u00b6","text":"<p>In this competition, you\u2019ll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled <code>train.csv</code> and the other is titled <code>test.csv</code>.</p> <p><code>Train.csv</code> will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the \u201cground truth\u201d.</p> <p>The <code>test.csv</code> dataset contains similar information but does not disclose the \u201cground truth\u201d for each passenger. It\u2019s your job to predict these outcomes.</p> <p>Using the patterns you find in the <code>train.csv</code> data, predict whether the other 418 passengers on board (found in <code>test.csv</code>) survived.</p> <p>Check out the \u201cData\u201d tab to explore the datasets even further. Once you feel you\u2019ve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.</p>"},{"location":"homeworks/hw_02/#how-to-submit-your-prediction-to-kaggle","title":"How to Submit your Prediction to Kaggle\u00b6","text":"<p>Once you\u2019re ready to make a submission and get on the leaderboard:</p> <ol> <li>Click on the \u201cSubmit Predictions\u201d button </li> <li>Upload a CSV file in the submission file format. You\u2019re able to submit 10 submissions a day. </li> </ol>"},{"location":"homeworks/hw_02/#submission-file-format","title":"Submission File Format:\u00b6","text":"<p>You should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond <code>PassengerId</code> and <code>Survived</code>) or rows.</p> <p>The file should have exactly 2 columns:</p> <ul> <li><code>PassengerId</code> (sorted in any order)</li> <li><code>Survived</code> (contains your binary predictions: 1 for survived, 0 for deceased)</li> </ul>"},{"location":"homeworks/hw_02/#got-it-im-ready-to-get-started-where-do-i-get-help-if-i-need-it","title":"Got it! I\u2019m ready to get started. Where do I get help if I need it?\u00b6","text":"<ul> <li>For Competition Help: Titanic Discussion Forum</li> <li>Technical Help: Kaggle Contact Us Page</li> </ul> <p>Kaggle doesn\u2019t have a dedicated support team so you\u2019ll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!</p>"},{"location":"homeworks/hw_02/#a-last-word-on-kaggle-notebooks","title":"A Last Word on Kaggle Notebooks\u00b6","text":"<p>As we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.</p> <p>In every competition, you\u2019ll find many Notebooks shared with incredible insights. It\u2019s an invaluable resource worth becoming familiar with. Check out this competition\u2019s Notebooks here.</p> <p>\ud83c\udfc3\u200d\u2640Ready to Compete? Join the Competition Here!</p>"},{"location":"homeworks/hw_02/#goal","title":"Goal\u00b6","text":"<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. For each in the test set, you must predict a 0 or 1 value for the variable.</p>"},{"location":"homeworks/hw_02/#metric","title":"Metric\u00b6","text":"<p>Your score is the percentage of passengers you correctly predict. This is known as accuracy.</p>"},{"location":"homeworks/hw_02/#submission-file-format","title":"Submission File Format\u00b6","text":"<p>You should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.</p> <p>The file should have exactly 2 columns:</p> <ul> <li>PassengerId (sorted in any order)</li> <li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li> </ul> <p>PassengerId,Survived</p> <pre><code>892,0  \n893,1  \n894,0  \nEtc.\n</code></pre> <p>You can download an example submission file (gender_submission.csv) on the Data page.</p>"},{"location":"homeworks/hw_02/#hints","title":"Hints\u00b6","text":"<ul> <li>Esto corresponde a un desafio de Kaggle (link).</li> <li>La informaci\u00f3n respecto a los datos, lo pueden encontrar en el siguiente link.</li> <li>A modo de inspiraci\u00f3n, pueden ocupar algunos gr\u00e1ficos de otros participantes del desaf\u00edo (link).</li> </ul>"},{"location":"labs/lab_01/","title":"Lab-01","text":"In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME   In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME"},{"location":"labs/lab_01/#lab-01","title":"Lab-01\u00b6","text":""},{"location":"labs/lab_01/#problema-01","title":"Problema 01\u00b6","text":"<p>En los siglos XVII y XVIII, James Gregory y Gottfried Leibniz descubrieron una serie infinita que sirve para calcular $\\pi$:</p> <p>$$\\displaystyle \\pi = 4 \\sum_{k=1}^{\\infty}\\dfrac{(-1)^{k+1}}{2k-1} = 4(1-\\dfrac{1}{3}+\\dfrac{1}{5}-\\dfrac{1}{7} + ...) $$</p> <p>Desarolle un programa para estimar el valor de $\\pi$ ocupando el m\u00e9todo de Leibniz, donde la entrada del programa debe ser un n\u00famero entero $n$ que indique cu\u00e1ntos t\u00e9rminos de la suma se utilizar\u00e1.</p> <ul> <li>Ejemplo:<ul> <li>calcular_pi(3) = 3.466666666666667</li> <li>calcular_pi(1000) = 3.140592653839794</li> </ul> </li> </ul>"},{"location":"labs/lab_01/#problema-02","title":"Problema 02\u00b6","text":"<p>Euler realiz\u00f3 varios aportes en relaci\u00f3n a $e$, pero no fue hasta 1748 cuando public\u00f3 su Introductio in analysin infinitorum que dio un tratamiento definitivo a las ideas sobre $e$. All\u00ed mostr\u00f3 que:</p> <p>En los siglos XVII y XVIII, James Gregory y Gottfried Leibniz descubrieron una serie infinita que sirve para calcular \u03c0:</p> <p>$$\\displaystyle e = \\sum_{k=0}^{\\infty}\\dfrac{1}{k!} = 1+\\dfrac{1}{2!}+\\dfrac{1}{3!}+\\dfrac{1}{4!} + ... $$</p> <p>Desarolle un programa para estimar el valor de $e$ ocupando el m\u00e9todo de Euler, donde la entrada del programa debe ser un n\u00famero entero $n$ que indique cu\u00e1ntos t\u00e9rminos de la suma se utilizar\u00e1.</p> <p>Para esto:</p> <ul> <li><p>a) Defina la funci\u00f3n <code>factorial</code>, donde la entrada sea un n\u00famero natural  $n$ y la salida sea el factorial de dicho n\u00famero.</p> <ul> <li>Ejemplo: factorial(3) =3, factorial(5) = 120</li> </ul> </li> <li><p>b) Ocupe la funci\u00f3n <code>factorial</code> dentro de la funci\u00f3n <code>calcular_e</code>.</p> <ul> <li>Ejemplo: calcular_e(3) = 2.6666666666666665, calcular_e(1000) = 2.7182818284590455</li> </ul> </li> </ul>"},{"location":"labs/lab_01/#problema-03","title":"Problema 03\u00b6","text":"<p>Sea $\\sigma(n)$ definido como la suma de los divisores propios de $n$ (n\u00fameros menores que n que se dividen en $n$).</p> <p>Los n\u00fameros amigos son  enteros positivos $n_1$ y $n_2$ tales que la suma de los divisores propios de uno es igual al otro n\u00famero y viceversa, es decir, $\\sigma(n_1)=\\sigma(n_2)$ y $\\sigma(n_2)=\\sigma(n_1)$.</p> <p>Por ejemplo, los n\u00fameros 220 y 284 son n\u00fameros amigos.</p> <ul> <li>los divisores propios de 220 son 1, 2, 4, 5, 10, 11, 20, 22, 44, 55 y 110; por lo tanto $\\sigma(220) = 284$.</li> <li>los divisores propios de 284 son 1, 2, 4, 71 y 142; entonces $\\sigma(284) = 220$.</li> </ul> <p>Implemente una funci\u00f3n llamada <code>amigos</code> cuyo input sean dos n\u00fameros naturales $n_1$ y $n_2$, cuyo output sea verifique si los n\u00fameros son amigos o no.</p> <p>Para esto:</p> <ul> <li><p>a) Defina la funci\u00f3n <code>divisores_propios</code>, donde la entrada sea un n\u00famero natural $n$ y la salida sea una lista con los divisores propios de dicho n\u00famero.</p> <ul> <li>Ejemplo: divisores_propios(220) = [1, 2, 4, 5, 10, 11, 20, 22, 44, 55 y 110], divisores_propios(284) = [1, 2, 4, 71 y 142]</li> </ul> </li> <li><p>b) Ocupe la funci\u00f3n <code>divisores_propios</code> dentro de la funci\u00f3n <code>amigos</code>.</p> <ul> <li>Ejemplo: amigos(220,284) = True, amigos(6,5) = False</li> </ul> </li> </ul>"},{"location":"labs/lab_01/#problema-04","title":"Problema 04\u00b6","text":"<p>La conjetura de Collatz, conocida tambi\u00e9n como conjetura $3n+1$ o conjetura de Ulam (entre otros nombres), fue enunciada por el matem\u00e1tico Lothar Collatz en 1937, y a la fecha no se ha resuelto.</p> <p>Sea la siguiente operaci\u00f3n, aplicable a cualquier n\u00famero entero positivo:</p> <ul> <li>Si el n\u00famero es par, se divide entre 2.</li> <li>Si el n\u00famero es impar, se multiplica por 3 y se suma 1.</li> </ul> <p>La conjetura dice que siempre alcanzaremos el 1 (y por tanto el ciclo 4, 2, 1) para cualquier n\u00famero con el que comencemos.</p> <p>Implemente una funci\u00f3n llamada <code>collatz</code> cuyo input sea un n\u00famero natural positivo $N$ y como output devulva la secuencia de n\u00fameros hasta llegar a 1.</p> <ul> <li>Ejemplo: collatz(9) = [9, 28, 14, 7, 22, 11, 34, 17, 52, 26, 13, 40, 20, 10, 5, 16, 8, 4, 2, 1]</li> </ul>"},{"location":"labs/lab_01/#problema-05","title":"Problema 05\u00b6","text":"<p>La conjetura de Goldbach es uno de los problemas abiertos m\u00e1s antiguos en matem\u00e1ticas. Concretamente, G.H. Hardy, en 1921, en su famoso discurso pronunciado en la Sociedad Matem\u00e1tica de Copenhague, coment\u00f3 que probablemente la conjetura de Goldbach no es solo uno de los problemas no resueltos m\u00e1s dif\u00edciles de la teor\u00eda de n\u00fameros, sino de todas las matem\u00e1ticas. Su enunciado es el siguiente:</p> <p>Todo n\u00famero par mayor que 2 puede escribirse como suma de dos n\u00fameros primos - Christian Goldbach (1742)</p> <p>Implemente una funci\u00f3n llamada <code>goldbach</code> cuyo input sea un n\u00famero natural positivo $n$ y como output devuelva la suma de dos primos ($n_1$ y $n_2$) tal que: $n_1+n_2=n$.</p> <p>Para esto:</p> <ul> <li><p>a) Defina la funci\u00f3n <code>es_primo</code>, donde la entrada sea un n\u00famero natural $n$ y la salida sea True si el n\u00famero es primo y False en otro caso.</p> <ul> <li>Ejemplo: es_primo(3) = True, es_primo(4) = False</li> </ul> </li> <li><p>b)  Defina la funci\u00f3n <code>lista_de_primos</code>, donde la entrada sea un n\u00famero natural par $n$ mayor que dos y la salida sea una lista con todos los n\u00famero primos entre 2 y $n$.</p> <ul> <li>Ejemplo: lista_de_primos(4) = [2,3], lista_de_primos(6) = [2,3,5], lista_de_primos(8) = [2,3,5,7]</li> </ul> </li> <li><p>c) Ocupe la funci\u00f3n <code>lista_de_primos</code> dentro de la funci\u00f3n <code>goldbash</code>.</p> </li> <li><p>Ejemplo: goldbash(4) = (2,2), goldbash(6) = (3,3) , goldbash(8) = (3,5)</p> </li> </ul>"},{"location":"labs/lab_02/","title":"Lab-02","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nYearsExperience = np.array([\n 1.1,1.3,1.5,2.0,2.2,\n 2.9,3.0,3.2,3.2,3.7,\n 3.9,4.0,4.0,4.1,4.5,\n 4.9,5.1,5.3,5.9,6.0,\n 6.8,7.1,7.9,8.2,8.7,\n 9.0,9.5,9.6,10.3,10.5\n ])\n\nSalary =  np.array([\n 39343.0,46205.0,37731.0,43525.0,39891.0,\n 56642.0,60150.0,54445.0,64445.0,57189.0,\n 63218.0,55794.0,56957.0,57081.0,61111.0,\n 67938.0,66029.0,83088.0,81363.0,93940.0,\n 91738.0,98273.0,101302.0,113812.0,109431.0,\n 105582.0,116969.0,112635.0,122391.0,121872.0\n])\n</pre> import numpy as np import matplotlib.pyplot as plt  YearsExperience = np.array([  1.1,1.3,1.5,2.0,2.2,  2.9,3.0,3.2,3.2,3.7,  3.9,4.0,4.0,4.1,4.5,  4.9,5.1,5.3,5.9,6.0,  6.8,7.1,7.9,8.2,8.7,  9.0,9.5,9.6,10.3,10.5  ])  Salary =  np.array([  39343.0,46205.0,37731.0,43525.0,39891.0,  56642.0,60150.0,54445.0,64445.0,57189.0,  63218.0,55794.0,56957.0,57081.0,61111.0,  67938.0,66029.0,83088.0,81363.0,93940.0,  91738.0,98273.0,101302.0,113812.0,109431.0,  105582.0,116969.0,112635.0,122391.0,121872.0 ])  <p>Buscamos encontrar la regresi\u00f3n lineal simple:</p> <p>$$Salary_i=\\beta_0+\\beta_1YearsExperience_i+\\epsilon_i$$</p> <p>Para esto debe resolver las siguientes preguntas:</p> <ol> <li>Defina la funci\u00f3n <code>estimate_coef(x,y)</code> para encontrar los coeficientes de regresi\u00f3n lineal $b = (b_0,b_1)$.</li> </ol> In\u00a0[\u00a0]: Copied! <pre>def estimate_coef(x, y):\n    \"\"\"\n    Encontrar los coeficientes del modelo de \n    regresion lineal: beta = (beta_0,beta_1)\n    \"\"\"\n    beta_0 = 0\n    beta_1 = 0\n    \n    return (beta_0, beta_1)\n</pre> def estimate_coef(x, y):     \"\"\"     Encontrar los coeficientes del modelo de      regresion lineal: beta = (beta_0,beta_1)     \"\"\"     beta_0 = 0     beta_1 = 0          return (beta_0, beta_1) In\u00a0[\u00a0]: Copied! <pre># imprimir valores del beta estimado\nbeta_estimado = estimate_coef(YearsExperience, Salary)\nprint(f\"Coeficientes estimados:\\nb_0 = {beta_estimado[0]} \\nb_1 = {beta_estimado[1]}\")\n</pre> # imprimir valores del beta estimado beta_estimado = estimate_coef(YearsExperience, Salary) print(f\"Coeficientes estimados:\\nb_0 = {beta_estimado[0]} \\nb_1 = {beta_estimado[1]}\") <ol> <li>Grafique su soluci\u00f3n ocupando la funci\u00f3n <code>plot_regression_line(x,y,yhat)</code>.</li> </ol> In\u00a0[\u00a0]: Copied! <pre>def plot_regression_line(x, y, yhat):\n    plt.figure(figsize=(10,4))\n    # plotting the actual points as scatter plot\n    plt.scatter(x, y, color = \"m\",marker = \"o\", s = 30)\n\n\n    # plotting the regression line\n    plt.plot(x, yhat, color = \"g\")\n\n    # putting labels\n    plt.xlabel('YearsExperience')\n    plt.ylabel('Salary')\n    plt.title(\"Plot YearsExperience vs Salary\")\n    \n    # function to show plot\n    plt.show()\n</pre> def plot_regression_line(x, y, yhat):     plt.figure(figsize=(10,4))     # plotting the actual points as scatter plot     plt.scatter(x, y, color = \"m\",marker = \"o\", s = 30)       # plotting the regression line     plt.plot(x, yhat, color = \"g\")      # putting labels     plt.xlabel('YearsExperience')     plt.ylabel('Salary')     plt.title(\"Plot YearsExperience vs Salary\")          # function to show plot     plt.show() In\u00a0[\u00a0]: Copied! <pre># mostrar resultados del ajuste lineal\nprediccion = beta_estimado[0] + beta_estimado[1]*YearsExperience\nplot_regression_line(YearsExperience, Salary, prediccion)\n</pre> # mostrar resultados del ajuste lineal prediccion = beta_estimado[0] + beta_estimado[1]*YearsExperience plot_regression_line(YearsExperience, Salary, prediccion) <ol> <li>Calcule el estad\u00edstico r-cuadrado ($r^2$) y las siguientes m\u00e9tricas de error:<ul> <li>mae</li> <li>rmse</li> <li>mape</li> <li>smape</li> </ul> </li> </ol> In\u00a0[\u00a0]: Copied! <pre>def mae(y, yhat) -&gt; float:\n    \"\"\"\n    Calculo de la metrica: mean absolute error (MAE)\n    \"\"\"\n    # agregar codigo \n    metrica = 0\n    return metrica\n</pre> def mae(y, yhat) -&gt; float:     \"\"\"     Calculo de la metrica: mean absolute error (MAE)     \"\"\"     # agregar codigo      metrica = 0     return metrica In\u00a0[\u00a0]: Copied! <pre>def rmse(y, yhat) -&gt; float:\n    \"\"\"\n    Calculo de la metrica: root mean squared error (RMSE)\n    \"\"\"\n    # agregar codigo \n    metrica = 0\n    return metrica\n</pre> def rmse(y, yhat) -&gt; float:     \"\"\"     Calculo de la metrica: root mean squared error (RMSE)     \"\"\"     # agregar codigo      metrica = 0     return metrica In\u00a0[\u00a0]: Copied! <pre>def mape(y, yhat) -&gt; float:\n    \"\"\"\n    Calculo de la metrica: mean absolute percentage error (MAPE)\n    \"\"\"\n    # agregar codigo \n    metrica = 0\n    return metrica\n</pre> def mape(y, yhat) -&gt; float:     \"\"\"     Calculo de la metrica: mean absolute percentage error (MAPE)     \"\"\"     # agregar codigo      metrica = 0     return metrica In\u00a0[\u00a0]: Copied! <pre>def smape(y, yhat) -&gt; float:\n    \"\"\"\n    Calculo de la metrica: symmetric mean absolute percentage error (SMAPE)\n    \"\"\"\n    # agregar codigo \n    metrica = 0\n    return metrica\n</pre> def smape(y, yhat) -&gt; float:     \"\"\"     Calculo de la metrica: symmetric mean absolute percentage error (SMAPE)     \"\"\"     # agregar codigo      metrica = 0     return metrica In\u00a0[\u00a0]: Copied! <pre>def rsquared(y, yhat) -&gt; float:\n    \"\"\"\n    Calculo del r-cuadrado\n    \"\"\"\n    # agregar codigo \n    metrica = 0\n    return metrica\n</pre> def rsquared(y, yhat) -&gt; float:     \"\"\"     Calculo del r-cuadrado     \"\"\"     # agregar codigo      metrica = 0     return metrica In\u00a0[\u00a0]: Copied! <pre># calcular resultados\ncalcular_mae = round(mae(Salary,prediccion), 4)\ncalcular_rmse = round(rmse(Salary,prediccion), 4)\ncalcular_mape = round(mape(Salary,prediccion), 4)\ncalcular_smape = round(smape(Salary,prediccion), 4)\ncalcular_rsquared = round(rsquared(Salary,prediccion), 4)\n</pre> # calcular resultados calcular_mae = round(mae(Salary,prediccion), 4) calcular_rmse = round(rmse(Salary,prediccion), 4) calcular_mape = round(mape(Salary,prediccion), 4) calcular_smape = round(smape(Salary,prediccion), 4) calcular_rsquared = round(rsquared(Salary,prediccion), 4) In\u00a0[\u00a0]: Copied! <pre># imprimir resultados\nprint(f\"mae:   {calcular_mae}\")\nprint(f\"rmse:  {calcular_rmse}\")\nprint(f\"mape:  {calcular_mape}\")\nprint(f\"smape: {calcular_smape}\")\nprint(f\"r^2:   {calcular_rsquared}\")\n</pre> # imprimir resultados print(f\"mae:   {calcular_mae}\") print(f\"rmse:  {calcular_rmse}\") print(f\"mape:  {calcular_mape}\") print(f\"smape: {calcular_smape}\") print(f\"r^2:   {calcular_rsquared}\") <ol> <li>Conclusiones del caso de estudio (evaluar si la regresi\u00f3n lineal se ajusta correctamente a los datos o no).</li> </ol> <p>Respuesta:</p>"},{"location":"labs/lab_02/#lab-02","title":"Lab-02\u00b6","text":""},{"location":"labs/lab_02/#problema-01","title":"Problema 01\u00b6","text":"<p>El objetivo de este laboratorio es aplicar un modelo de regresi\u00f3n lineal simple.</p>"},{"location":"labs/lab_02/#definicion","title":"Definici\u00f3n\u00b6","text":"<p>El modelo de regresio\u0301n lineal general o modelo de regresi\u00f3n multiple,  supone que, $\\boldsymbol{Y} =  \\boldsymbol{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon},$ donde:</p> <ul> <li>$\\boldsymbol{X} = (x_1,...,x_n)^{T}$: variable explicativa</li> <li>$\\boldsymbol{Y} = (y_1,...,y_n)^{T}$: variable respuesta</li> <li>$\\boldsymbol{\\epsilon} = (\\epsilon_1,...,\\epsilon_n)^{T}$: error se asume un ruido blanco, es decir, $\\epsilon \\sim \\mathcal{N}( \\boldsymbol{0},\\sigma^2I)$</li> <li>$\\boldsymbol{\\beta} = (\\beta_1,...,\\beta_n)^{T}$: coeficientes de regresio\u0301n.</li> </ul> <p>La idea es tratar de establecer la relaci\u00f3n entre las variables independientes y dependientes por medio de ajustar el mejor hyper plano con respecto a los puntos.</p> <p>Por ejemplo, para el caso de la regresi\u00f3n lineal simple, se tiene la siguiente estructura: $y_i=\\beta_0+\\beta_1x_i+\\epsilon_i.$ En este caso, la regresi\u00f3n lineal corresponder\u00e1 a la recta que mejor pasa por los puntos observados.</p> <p></p> <p>Existen algunas situaciones donde los modelos lineales no son apropiados:</p> <ul> <li>El rango de valores de $Y$ est\u00e1 restringido (ejemplo: datos binarios o de conteos).</li> <li>La varianza de $Y$ depende de la media.</li> </ul>"},{"location":"labs/lab_02/#mejores-paremetros-metodo-de-minimos-cudrados","title":"Mejores par\u00e9metros: M\u00e9todo de minimos cudrados\u00b6","text":"<p>El m\u00e9todo de m\u00ednimos cudrados es un m\u00e9todo de optimizaci\u00f3n que busca encontrar la mejor aproximaci\u00f3n mediante la minimizaci\u00f3n de los residuos al cuadrado, es decir, se buscar encontrar:</p> <p>$$(P)\\ \\min \\sum_{i=1}^n e_{i}^2 =\\sum_{i=1}^n (y_{i}-f_{i}(x;\\beta))^2   $$</p> <p>Para el caso de la regresi\u00f3n lineal simple, se busca una funci\u00f3n $$f(x;\\beta) = \\beta_{0} + \\beta_{1}x,$$</p> <p>por lo tanto el problema que se debe resolver es el siguiente:</p> <p>$$(P)\\ \\min \\sum_{i=1}^n e_{i}^2 =\\dfrac{1}{n}\\sum_{i=1}^{n}\\left ( y_{i}-(\\beta_{0} + \\beta_{1}x_{i})\\right )^2$$</p> <p>Lo que significa, que para este problema, se debe encontrar $\\beta = (\\beta_{0},\\beta_{1})$ que minimicen el problema de optimizaci\u00f3n. En este caso la soluci\u00f3n viene dada por:</p> <p>$$\\hat{\\beta}_{1} = \\dfrac{\\sum(x-\\bar{x})(y-\\bar{y})}{\\sum(x-\\bar{x})^2} = \\rho (x,y)\\ ; \\  \\hat{\\beta}_{0} = \\bar{y}-\\hat{\\beta}_{1} \\bar{x} $$</p>"},{"location":"labs/lab_02/#seleccion-de-modelos","title":"Selecci\u00f3n de modelos\u00b6","text":"<p>R-cuadrado</p> <p>El coeficiente de determinaci\u00f3n o R-cuadrado ($r^2$ ) , es un estad\u00edstico usado en el contexto de un modelo estad\u00edstico cuyo principal prop\u00f3sito es predecir futuros resultados o probar una hip\u00f3tesis. El coeficiente determina la calidad del modelo para replicar los resultados, y la proporci\u00f3n de variaci\u00f3n de los resultados que puede explicarse por el modelo.</p> <p>El valor del $r^2$ habitualmente entre 0 y 1, donde 0 significa una mala calidad de ajuste en el modelo y 1 corresponde a un ajuste lineal perfecto. A menudo, este estad\u00edstico es ocupado para modelos lineales.</p> <p>Se define por la f\u00f3rmula:</p> <p>$$r^2 = \\dfrac{SS_{reg}}{SS_{tot}} = 1 - \\dfrac{SS_{res}}{SS_{tot}},$$</p> <p>donde:</p> <ul> <li><p>$SS_{reg}$ ( suma explicada de cuadrados (ESS)): $\\sum_{i}(\\hat{y}-\\bar{y})^2$</p> </li> <li><p>$SS_{res}$: ( suma residual de cuadrados (RSS)): $\\sum_{i}(y_{i}-\\hat{y})^2 = \\sum_{i}e_{i}^2$</p> </li> <li><p>$SS_{tot}$: ( varianza): $\\sum_{i}(y_{i}-\\bar{y})$, donde: $SS_{tot}=SS_{reg}+SS_{res}$</p> </li> </ul> <p>En una forma general, se puede ver que $r^2$ est\u00e1 relacionado con la fracci\u00f3n de varianza inexplicada (FVU), ya que el segundo t\u00e9rmino compara la varianza inexplicada (varianza de los errores del modelo) con la varianza total (de los datos).</p> <p></p> <ul> <li><p>Las \u00e1reas de los cuadrados azules representan los residuos cuadrados con respecto a la regresi\u00f3n lineal ($SS_{tot}$).</p> </li> <li><p>Las \u00e1reas de los cuadrados rojos representan los residuos al cuadrado con respecto al valor promedio ($SS_{res}$).</p> </li> </ul>"},{"location":"labs/lab_02/#error-de-un-modelo","title":"Error de un modelo\u00b6","text":""},{"location":"labs/lab_02/#definicion","title":"Definici\u00f3n\u00b6","text":"<p>El error corresponde a la diferencia entre el valor original y el valor predicho,es decir:</p> <p>$$e_{i}=y_{i}-\\hat{y}_{i} $$</p> <p></p>"},{"location":"labs/lab_02/#formas-de-medir-el-error-de-un-modelo","title":"Formas de medir el error de un modelo\u00b6","text":"<p>Para medir el ajuste de un modelo se ocupan las denominadas funciones de distancias o m\u00e9tricas. Existen varias m\u00e9tricas, dentro de las cuales encontramos:</p> <ol> <li><p>M\u00e9tricas absolutas: Las m\u00e9tricas absolutas o no escalada miden el error sin escalar los valores. Las m\u00e9trica absolutas m\u00e1s ocupadas son:</p> <ul> <li>Mean Absolute Error (MAE)</li> </ul> <p>$$\\textrm{MAE}(y,\\hat{y}) = \\dfrac{1}{n}\\sum_{t=1}^{n}\\left | y_{t}-\\hat{y}_{t}\\right |$$</p> <ul> <li>Mean squared error (MSE):</li> </ul> <p>$$\\textrm{MSE}(y,\\hat{y}) =\\dfrac{1}{n}\\sum_{t=1}^{n}\\left ( y_{t}-\\hat{y}_{t}\\right )^2$$</p> </li> <li><p>M\u00e9tricas Porcentuales: Las m\u00e9tricas porcentuales o escaladas miden el error de manera escalada, es decir, se busca acotar el error entre valores de 0 a 1, donde 0 significa que el ajuste es perfecto, mientras que 1 ser\u00eda un mal ajuste. Cabe destacar que muchas veces las m\u00e9tricas porcentuales puden tener valores mayores a 1.Las m\u00e9trica Porcentuales m\u00e1s ocupadas son:</p> <ul> <li>Mean absolute percentage error (MAPE):</li> </ul> <p>$$\\textrm{MAPE}(y,\\hat{y}) = \\dfrac{1}{n}\\sum_{t=1}^{n}\\left | \\frac{y_{t}-\\hat{y}_{t}}{y_{t}} \\right |$$</p> <ul> <li>Symmetric mean absolute percentage error (sMAPE):</li> </ul> <p>$$\\textrm{sMAPE}(y,\\hat{y}) = \\dfrac{1}{n}\\sum_{t=1}^{n} \\frac{\\left |y_{t}-\\hat{y}_{t}\\right |}{(\\left | y_{t} \\right |^2+\\left | \\hat{y}_{t} \\right |^2)/2}$$</p> </li> </ol>"},{"location":"labs/lab_02/#problema-a-resolver","title":"Problema a resolver\u00b6","text":"<p>En este art\u00edculo, utilizaremos un conjunto de datos de salarios. Nuestro conjunto de datos tendr\u00e1 2 columnas:</p> <ul> <li>a\u00f1os de experiencia (YearsExperience) - variable explicativa</li> <li>salario (Salary) - variable de respuesta</li> </ul> <p>A coninuaci\u00f3n, mostramos expl\u00edcitamente el conjunto de datos.</p>"},{"location":"labs/lab_031/","title":"Lab-031","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\n</pre> import pandas as pd In\u00a0[2]: Copied! <pre># load data\nurl='https://drive.google.com/file/d/1SW1_vWBLd50COj4FbQdZenqAwA0nLDqC/view?usp=drive_link'\nurl='https://drive.google.com/uc?id=' + url.split('/')[-2]\n\ndf = pd.read_csv(url, sep=\"|\" )\ndf.head()\n</pre> # load data url='https://drive.google.com/file/d/1SW1_vWBLd50COj4FbQdZenqAwA0nLDqC/view?usp=drive_link' url='https://drive.google.com/uc?id=' + url.split('/')[-2]  df = pd.read_csv(url, sep=\"|\" ) df.head() Out[2]: user_id age gender occupation zip_code 0 1 24 M technician 85711 1 2 53 F other 94043 2 3 23 M writer 32067 3 4 24 M technician 43537 4 5 33 F other 15213 <p>El objetivo es tratar de obtener la mayor informaci\u00f3n posible de este conjunto de datos. Para cumplir este objetivo debe resolver las siguientes problem\u00e1ticas:</p> <p>1.- \u00bfCu\u00e1l es el n\u00famero de observaciones en el conjunto de datos?</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>2.- \u00bfCu\u00e1l es el n\u00famero de columnas en el conjunto de datos?</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>3.- Imprime el nombre de todas las columnas</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>4.- Imprima el \u00edndice del dataframe</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>5.- \u00bfCu\u00e1l es el tipo de datos de cada columna?</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>6.- Describir el conjunto de datos (hint: .describe())</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>7.- Imprimir solo la columna de occupation.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>8.- \u00bfCu\u00e1ntas ocupaciones diferentes hay en este conjunto de datos?</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>9.- \u00bfCu\u00e1l es la ocupaci\u00f3n m\u00e1s frecuente?</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>10.- \u00bfCu\u00e1l es la edad media de los usuarios?</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>11.- \u00bfCu\u00e1l es la edad con menos ocurrencia?</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"labs/lab_031/#lab-031","title":"Lab-031\u00b6","text":""},{"location":"labs/lab_031/#problema-01","title":"Problema 01\u00b6","text":"<p>EL conjunto de datos se denomina <code>ocupation.csv</code>, el cual contiene informaci\u00f3n de distintos usuarios (edad ,sexo, profesi\u00f3n, etc.).</p> <p>Lo primero es cargar el conjunto de datos y ver las primeras filas que lo componen:</p>"},{"location":"labs/lab_032/","title":"Lab-032","text":"<p>Esta semana revisaremos datos del \u00cdndice de Libertad de Prensa que confecciona cada a\u00f1o la asociaci\u00f3n de Reporteros Sin Fronteras.</p> <p>Nota: el conjunto a utilizar lo encuentra en el siguiente link.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np \nimport pandas as pd\n\nfrom os import listdir\nfrom os.path import isfile, join\n</pre> import numpy as np  import pandas as pd  from os import listdir from os.path import isfile, join In\u00a0[\u00a0]: Copied! <pre>path = \"data/\"\n\narchivos_anio = [path + f for f in listdir(path) if 'libertad_prensa_codigo' not in f ] \ndf_codigos = pd.read_csv(path + 'libertad_prensa_codigo.csv')\n</pre> path = \"data/\"  archivos_anio = [path + f for f in listdir(path) if 'libertad_prensa_codigo' not in f ]  df_codigos = pd.read_csv(path + 'libertad_prensa_codigo.csv') <p>El objetivo es tratar de obtener la mayor informaci\u00f3n posible de este conjunto de datos. Para cumplir este objetivo debe resolver las siguientes problem\u00e1ticas:</p> <ol> <li>Lo primero ser\u00e1 juntar toda la informaci\u00f3n en un solo archivo, para ello necesitamos seguir los siguientes pasos:</li> </ol> <ul> <li>a) Crear el archivo df_anio, que contenga la informaci\u00f3n de libertad_prensa_anio.csv para cada a\u00f1o. Luego, normalice el nombre de las columnas a min\u00fascula.</li> <li>b) Encuentre y elimine el dato que esta duplicado en el archivo df_codigo.</li> <li>c) Crear el archivo df que junte la informaci\u00f3n del archivo df_anio con df_codigo por la columna codigo_iso.</li> </ul> <p>Hint: Para juntar por anio ocupe la funci\u00f3n pd.concat. Para juntar informaci\u00f3n por columna ocupe pd.merge.</p> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME <ol> <li>Encontrar:<ul> <li>\u00bfCu\u00e1l es el n\u00famero de observaciones en el conjunto de datos?</li> <li>\u00bfCu\u00e1l es el n\u00famero de columnas en el conjunto de datos?</li> <li>Imprime el nombre de todas las columnas</li> <li>\u00bfCu\u00e1l es el tipo de datos de cada columna?</li> <li>Describir el conjunto de datos (hint: .describe())</li> </ul> </li> </ol> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME <ol> <li>Desarrolle una funci\u00f3n <code>resumen_df(df)</code> para encontrar el total de elementos distintos y vac\u00edos por columnas.</li> </ol> In\u00a0[\u00a0]: Copied! <pre># respuesta\n\ndef resumen_df(df):\n    \"\"\"\n    funcion resumen con elementos distintos y vacios\n    por columnas\n    \"\"\"\n    nombres = df.columns\n    \n    result = pd.DataFrame({'nombres': nombres})\n    result['elementos_distintos'] = 0\n    result['elementos_vacios'] = 0\n    \n    return result\n</pre> # respuesta  def resumen_df(df):     \"\"\"     funcion resumen con elementos distintos y vacios     por columnas     \"\"\"     nombres = df.columns          result = pd.DataFrame({'nombres': nombres})     result['elementos_distintos'] = 0     result['elementos_vacios'] = 0          return result In\u00a0[\u00a0]: Copied! <pre># retornar \nresumen_df(df)\n</pre> # retornar  resumen_df(df) <ol> <li>Para los paises latinoamericano, encuentre por a\u00f1o  el pa\u00eds con mayor y menor <code>indice</code>.</li> </ol> <ul> <li>a) Mediante un ciclo for.</li> <li>b) Mediante un  groupby.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># respuesta\n\namerica = ['ARG', 'ATG', 'BLZ', 'BOL', 'BRA', 'CAN', 'CHL', 'COL', 'CRI',\n       'CUB', 'DOM', 'ECU', 'GRD', 'GTM', 'GUY', 'HND', 'HTI', 'JAM',\n       'MEX', 'NIC', 'PAN', 'PER', 'PRY', 'SLV', 'SUR', 'TTO', 'URY',\n       'USA', 'VEN']\n\ndf_america = # FIX ME\n</pre> # respuesta  america = ['ARG', 'ATG', 'BLZ', 'BOL', 'BRA', 'CAN', 'CHL', 'COL', 'CRI',        'CUB', 'DOM', 'ECU', 'GRD', 'GTM', 'GUY', 'HND', 'HTI', 'JAM',        'MEX', 'NIC', 'PAN', 'PER', 'PRY', 'SLV', 'SUR', 'TTO', 'URY',        'USA', 'VEN']  df_america = # FIX ME <ol> <li>Para cada pa\u00eds, muestre el indice m\u00e1ximo que alcanzo por anio. Para los datos nulos, rellene con el valor 0.</li> </ol> <p>Ejemplo:</p> <p></p> <p>Hint: Utilice la funci\u00f3n pd.pivot_table.</p> In\u00a0[\u00a0]: Copied! <pre># FIX ME\n</pre> # FIX ME"},{"location":"labs/lab_032/#lab-032","title":"Lab-032\u00b6","text":""},{"location":"labs/lab_032/#diccionario-de-datos","title":"Diccionario de datos\u00b6","text":"Variable Clase Descripci\u00f3n codigo_iso caracter C\u00f3digo ISO del pa\u00eds pais caracter Pa\u00eds anio entero A\u00f1o del resultado indice entero Puntaje \u00cdndice Libertad de Prensa (menor puntaje = mayor libertad de prensa) ranking entero Ranking Libertad de Prensa"},{"location":"labs/lab_032/#fuente-original-y-adaptacion","title":"Fuente original y adaptaci\u00f3n\u00b6","text":"<p>Los datos fueron extra\u00eddos de The World Bank. La fuente original es Reporteros sin Fronteras.</p> <p>Por otro lado, estos archivos han sido modificado intencionalmente para ocupar todo lo aprendido en clases. A continuaci\u00f3n, una breve descripci\u00f3n de cada uno de los data frames:</p> <ul> <li>libertad_prensa_codigo.csv: contiene la informaci\u00f3n codigo_iso/pais. Existe un c\u00f3digo que tiene dos valores.</li> <li>libertad_prensa_anio.csv: contiene la informaci\u00f3n pais/anio/indice/ranking. Los nombres de las columnas estan en may\u00fascula.</li> </ul>"},{"location":"labs/lab_04/","title":"Lab-04","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set(rc={'figure.figsize':(10,8)})\n</pre> import pandas as pd import seaborn as sns import matplotlib.pyplot as plt  sns.set(rc={'figure.figsize':(10,8)}) In\u00a0[\u00a0]: Copied! <pre># cargar datos\ndf = pd.read_csv(\"https://raw.githubusercontent.com/fralfaro/MAT281_2022/main/labs/lab_04/data/company_sales_data.csv\")\ndf.head()\n</pre> # cargar datos df = pd.read_csv(\"https://raw.githubusercontent.com/fralfaro/MAT281_2022/main/labs/lab_04/data/company_sales_data.csv\") df.head() <p>El objetivo es tratar de obtener la mayor informaci\u00f3n posible de este conjunto de datos. Para cumplir este objetivo debe resolver las siguientes problem\u00e1ticas:</p> <p>Observaci\u00f3n.- Puedes ocupar las librer\u00edas de Matplolib o Seaborn.</p> <ol> <li>Lea el \"total_profit\" de todos los meses, mu\u00e9strelo usando un gr\u00e1fico lineal y un gr\u00e1fico de dispersi\u00f3n.</li> </ol> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME <ol> <li>Lea todos los datos de ventas de productos y mu\u00e9strelos utilizando un gr\u00e1fico multil\u00ednea.</li> </ol> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME <ol> <li>Lea los datos de ventas de productos de \"facecream\" y \"facewash\" y mu\u00e9strelos usando el gr\u00e1fico de barras.</li> </ol> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME <ol> <li>Lea todos los datos de ventas de productos y mu\u00e9strelos utilizando un gr\u00e1fico box-plot.</li> </ol> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME <ol> <li>Calcule los datos de ventas totales del a\u00f1o pasado para cada producto y mu\u00e9strelos usando un gr\u00e1fico circular</li> </ol> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME"},{"location":"labs/lab_04/#lab-04","title":"Lab-04\u00b6","text":""},{"location":"labs/lab_04/#problema-01","title":"Problema 01\u00b6","text":"<p>EL conjunto de datos se denomina <code>company_sales_data.csv</code>, el cual contiene informaci\u00f3n tal como: n\u00famero del mes, unidades, precio, etc.</p> <p>Lo primero es cargar el conjunto de datos y ver las primeras filas que lo componen:</p>"},{"location":"labs/lab_05/","title":"Lab-05","text":"<p>El Iris dataset es un conjunto de datos que contine una  muestras de tres especies de Iris (Iris setosa, Iris virginica e Iris versicolor). Se midi\u00f3 cuatro rasgos de cada muestra: el largo y ancho del s\u00e9palo y p\u00e9talo, en cent\u00edmetros.</p> <p>Lo primero es cargar el conjunto de datos y ver las primeras filas que lo componen:</p> In\u00a0[\u00a0]: Copied! <pre># librerias\n \nimport os\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \npd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes\n\n\n# Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab\n%matplotlib inline\n</pre> # librerias   import os import numpy as np import pandas as pd  import matplotlib.pyplot as plt import seaborn as sns  pd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes   # Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab %matplotlib inline In\u00a0[\u00a0]: Copied! <pre># cargar datos\ndata = pd.read_csv(\"https://raw.githubusercontent.com/fralfaro/MAT281_2022/main/labs/lab_05/data/iris_contaminados.csv\")\ndata.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n\ndata.head()\n</pre> # cargar datos data = pd.read_csv(\"https://raw.githubusercontent.com/fralfaro/MAT281_2022/main/labs/lab_05/data/iris_contaminados.csv\") data.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']  data.head()  <p>Su objetivo es realizar un correcto E.D.A., para esto debe seguir las siguientes intrucciones:</p> <ol> <li>Realizar un conteo de elementos de la columna species y corregir seg\u00fan su criterio. Reemplace por \"default\" los valores <code>nan</code>.</li> </ol> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME <ol> <li>Realizar un gr\u00e1fico de box-plot sobre el largo y ancho de los petalos y s\u00e9palos. Reemplace por 0 los valores nan.</li> </ol> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME <ol> <li>Anteriormente se define un rango de valores v\u00e1lidos para los valores del largo y ancho de los petalos y s\u00e9palos. Agregue una columna denominada label  que identifique cu\u00e1l de estos valores esta fuera del rango de valores v\u00e1lidos.</li> </ol> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME <ol> <li>Realice un gr\u00e1fico de sepal_length vs petal_length y otro de sepal_width vs petal_width categorizados por la etiqueta label. Concluya sus resultados.</li> </ol> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME <ol> <li>Filtre los datos v\u00e1lidos y realice un gr\u00e1fico de sepal_length vs petal_length categorizados por la etiqueta species.</li> </ol> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME"},{"location":"labs/lab_05/#lab-05","title":"Lab-05\u00b6","text":""},{"location":"labs/lab_05/#problema-01","title":"Problema 01\u00b6","text":""},{"location":"labs/lab_05/#bases-del-experimento","title":"Bases del experimento\u00b6","text":"<p>Lo primero es identificar las variables que influyen en el estudio y la naturaleza de esta.</p> <ul> <li>species:<ul> <li>Descripci\u00f3n: Nombre de la especie de Iris.</li> <li>Tipo de dato: string</li> <li>Limitantes: solo existen tres tipos (setosa, virginia y versicolor).</li> </ul> </li> <li>sepalLength:<ul> <li>Descripci\u00f3n: largo del s\u00e9palo.</li> <li>Tipo de dato: float.</li> <li>Limitantes: los valores se encuentran entre 4.0 y 7.0 cm.</li> </ul> </li> <li>sepalWidth:<ul> <li>Descripci\u00f3n: ancho del s\u00e9palo.</li> <li>Tipo de dato: float.</li> <li>Limitantes: los valores se encuentran entre 2.0 y 4.5 cm.</li> </ul> </li> <li>petalLength:<ul> <li>Descripci\u00f3n: largo del p\u00e9talo.</li> <li>Tipo de dato: float.</li> <li>Limitantes: los valores se encuentran entre 1.0 y 7.0 cm.</li> </ul> </li> <li>petalWidth:<ul> <li>Descripci\u00f3n: ancho del p\u00e9palo.</li> <li>Tipo de dato: float.</li> <li>Limitantes: los valores se encuentran entre 0.1 y 2.5 cm.</li> </ul> </li> </ul>"},{"location":"labs/lab_06/","title":"Lab-06","text":"<p>El cuarteto de Anscombe comprende cuatro conjuntos de datos que tienen las mismas propiedades estad\u00edsticas, pero que evidentemente son distintas al inspeccionar sus gr\u00e1ficos respectivos.</p> <p>Cada conjunto consiste de once puntos (x, y) y fueron construidos por el estad\u00edstico F. J. Anscombe. El cuarteto es una demostraci\u00f3n de la importancia de mirar gr\u00e1ficamente un conjunto de datos antes de analizarlos.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n\n\n%matplotlib inline\nsns.set_palette(\"deep\", desat=.6)\nsns.set(rc={'figure.figsize':(11.7,8.27)})\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error, r2_score    %matplotlib inline sns.set_palette(\"deep\", desat=.6) sns.set(rc={'figure.figsize':(11.7,8.27)}) In\u00a0[\u00a0]: Copied! <pre># Cargar los datos del cuarteto de Anscombe\ndata = sns.load_dataset(\"anscombe\")\ndata.head()\n</pre> # Cargar los datos del cuarteto de Anscombe data = sns.load_dataset(\"anscombe\") data.head() <p>Basado en la informaci\u00f3n presentada responda las siguientes preguntas:</p> <ol> <li>Gr\u00e1fique mediante un gr\u00e1fico tipo scatter cada grupo. A simple vista, \u00bf los grupos son muy distintos entre si?.</li> </ol> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME <ol> <li>Realice un resumen de las medidas estad\u00edsticas m\u00e1s significativas ocuapando el comando <code>describe</code> para cada grupo. Interprete.</li> </ol> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME <ol> <li>Realice un ajuste lineal (con sklearn) y calcule los resultados de las m\u00e9tricas para cada grupo. Adem\u00e1s, grafique los resultados de la regresi\u00f3n lineal para cada grupo. Interprete.</li> </ol> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME <ol> <li>Es claro que el ajuste lineal para algunos grupos no es el correcto. Existen varias formas de solucionar este problema (eliminar outliers, otros modelos, etc.). Identifique una estrategia para que el modelo de regresi\u00f3n lineal ajuste de mejor manera e implemente otros modelos en los casos que encuentre necesario.</li> </ol> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME"},{"location":"labs/lab_06/#lab-06","title":"Lab-06\u00b6","text":""},{"location":"labs/lab_06/#i-problema-01","title":"I.- Problema 01\u00b6","text":""},{"location":"labs/lab_071/","title":"Lab-071","text":"<p>Los datos se refieren a las casas encontradas en un distrito determinado de California y algunas estad\u00edsticas resumidas sobre ellas basadas en los datos del censo de 1990. Tenga en cuenta que los datos no se limpian, por lo que se requieren algunos pasos de procesamiento previo.</p> <p>Las columnas son las siguientes, sus nombres se explican por s\u00ed mismos:</p> <ul> <li>longitude</li> <li>latitude</li> <li>housingmedianage</li> <li>total_rooms</li> <li>total_bedrooms</li> <li>population</li> <li>households</li> <li>median_income</li> <li>medianhousevalue</li> <li>ocean_proximity</li> </ul> <p>El objetivo es poder predecir el valor promedio de cada propiedad. Para poder completar correctamente este laboratorio, es necesario seguir la siguiente r\u00fabrica de trabajo:</p> <ol> <li>Definici\u00f3n del problema</li> <li>Estad\u00edstica descriptiva</li> <li>Visualizaci\u00f3n descriptiva</li> <li>Preprocesamiento</li> <li>Selecci\u00f3n de modelo (Por lo menos debe comparar cuatro modelos)</li> <li>M\u00e9tricas y an\u00e1lisis de resultados</li> <li>Visualizaciones del modelo</li> <li>Conclusiones</li> </ol> <p>Observaci\u00f3n: El alumno tiene la libertad de desarrollar un an\u00e1lisis m\u00e1s completo del problema. Puede tomar como referencia el siguiente link.</p> In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns In\u00a0[2]: Copied! <pre>from sklearn.metrics import mean_absolute_error, mean_squared_error\n\ndef mean_absolute_percentage_error(y_true, y_pred):\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\ndef regression_metrics(df):\n    \"\"\"\n    Aplicar las distintas m\u00e9tricas definidas\n    :param df: DataFrame con las columnas: ['y', 'yhat']\n    :return: DataFrame con las m\u00e9tricas especificadas\n    \"\"\"\n    df_result = pd.DataFrame()\n\n    y_true = df['y']\n    y_pred = df['yhat']\n\n    df_result['mae'] = [round(mean_absolute_error(y_true, y_pred), 4)]\n    df_result['mse'] = [round(mean_squared_error(y_true, y_pred), 4)]\n    df_result['rmse'] = [round(np.sqrt(mean_squared_error(y_true, y_pred)), 4)]\n    df_result['mape'] = [round(mean_absolute_percentage_error(y_true, y_pred), 4)]\n    df_result['smape'] = [round(2 * mean_absolute_percentage_error(y_true, y_pred) / (mean_absolute_percentage_error(y_true, y_pred) + 100), 4)]\n\n    return df_result\n</pre> from sklearn.metrics import mean_absolute_error, mean_squared_error  def mean_absolute_percentage_error(y_true, y_pred):     return np.mean(np.abs((y_true - y_pred) / y_true)) * 100  def regression_metrics(df):     \"\"\"     Aplicar las distintas m\u00e9tricas definidas     :param df: DataFrame con las columnas: ['y', 'yhat']     :return: DataFrame con las m\u00e9tricas especificadas     \"\"\"     df_result = pd.DataFrame()      y_true = df['y']     y_pred = df['yhat']      df_result['mae'] = [round(mean_absolute_error(y_true, y_pred), 4)]     df_result['mse'] = [round(mean_squared_error(y_true, y_pred), 4)]     df_result['rmse'] = [round(np.sqrt(mean_squared_error(y_true, y_pred)), 4)]     df_result['mape'] = [round(mean_absolute_percentage_error(y_true, y_pred), 4)]     df_result['smape'] = [round(2 * mean_absolute_percentage_error(y_true, y_pred) / (mean_absolute_percentage_error(y_true, y_pred) + 100), 4)]      return df_result In\u00a0[3]: Copied! <pre>from sklearn.datasets import fetch_california_housing\n\n# Cargar los datos de housing\nhousing_data = fetch_california_housing(as_frame=True)\n\n# Convertir los datos en un DataFrame de pandas\nhousing = housing_data['data']\nhousing['target'] = housing_data['target']\n\n# Visualizar las primeras filas del DataFrame\nhousing.head()\n</pre> from sklearn.datasets import fetch_california_housing  # Cargar los datos de housing housing_data = fetch_california_housing(as_frame=True)  # Convertir los datos en un DataFrame de pandas housing = housing_data['data'] housing['target'] = housing_data['target']  # Visualizar las primeras filas del DataFrame housing.head() Out[3]: MedInc HouseAge AveRooms AveBedrms Population AveOccup Latitude Longitude target 0 8.3252 41.0 6.984127 1.023810 322.0 2.555556 37.88 -122.23 4.526 1 8.3014 21.0 6.238137 0.971880 2401.0 2.109842 37.86 -122.22 3.585 2 7.2574 52.0 8.288136 1.073446 496.0 2.802260 37.85 -122.24 3.521 3 5.6431 52.0 5.817352 1.073059 558.0 2.547945 37.85 -122.25 3.413 4 3.8462 52.0 6.281853 1.081081 565.0 2.181467 37.85 -122.25 3.422 In\u00a0[4]: Copied! <pre># FIXME\n</pre> # FIXME"},{"location":"labs/lab_071/#lab-071","title":"Lab-071\u00b6","text":""},{"location":"labs/lab_071/#i-problema-01","title":"I.- Problema 01\u00b6","text":""},{"location":"labs/lab_072/","title":"Lab-072","text":"<p>El objetivo es a partir de los datos, hacer la mejor predicci\u00f3n de cada imagen. Para ellos es necesario realizar los pasos cl\u00e1sicos de un proyecto de Machine Learning, como estad\u00edstica descriptiva, visualizaci\u00f3n y preprocesamiento.</p> <ul> <li><p>Se solicita ajustar al menos tres modelos de clasificaci\u00f3n:</p> <ul> <li>Regresi\u00f3n log\u00edstica</li> <li>K-Nearest Neighbours</li> <li>Uno o m\u00e1s algoritmos a su elecci\u00f3n link.</li> </ul> </li> <li><p>Realizar una predicci\u00f3n con cada uno de los tres modelos con los datos test y obtener el score.</p> </li> <li><p>Analizar sus m\u00e9tricas de error (accuracy, precision, recall, f-score)</p> </li> </ul> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\nfrom sklearn import datasets\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n</pre> import numpy as np import pandas as pd from sklearn import datasets import matplotlib.pyplot as plt  %matplotlib inline In\u00a0[\u00a0]: Copied! <pre>digits_dict = datasets.load_digits()\nprint(digits_dict[\"DESCR\"])\n</pre> digits_dict = datasets.load_digits() print(digits_dict[\"DESCR\"]) In\u00a0[\u00a0]: Copied! <pre># informacion de las columnas\ndigits_dict.keys()\n</pre> # informacion de las columnas digits_dict.keys() In\u00a0[\u00a0]: Copied! <pre># informacion del target\ndigits_dict[\"target\"]\n</pre> # informacion del target digits_dict[\"target\"] <p>A continuaci\u00f3n se crea dataframe declarado como <code>digits</code> con los datos de <code>digits_dict</code> tal que tenga 65 columnas, las 6 primeras a la representaci\u00f3n de la imagen en escala de grises (0-blanco, 255-negro) y la \u00faltima correspondiente al d\u00edgito (<code>target</code>) con el nombre target.</p> In\u00a0[\u00a0]: Copied! <pre># leer datos\ndigits = (\n    pd.DataFrame(\n        digits_dict[\"data\"],\n    )\n    .rename(columns=lambda x: f\"c{x:02d}\")\n    .assign(target=digits_dict[\"target\"])\n    .astype(int)\n)\n\ndigits.head()\n</pre> # leer datos digits = (     pd.DataFrame(         digits_dict[\"data\"],     )     .rename(columns=lambda x: f\"c{x:02d}\")     .assign(target=digits_dict[\"target\"])     .astype(int) )  digits.head() In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME In\u00a0[\u00a0]: Copied! <pre>digits_dict[\"images\"][0]\n</pre> digits_dict[\"images\"][0] <p>Visualiza im\u00e1genes de los d\u00edgitos utilizando la llave <code>images</code> de <code>digits_dict</code>.</p> <p>Sugerencia: Utiliza <code>plt.subplots</code> y el m\u00e9todo <code>imshow</code>. Puedes hacer una grilla de varias im\u00e1genes al mismo tiempo!</p> In\u00a0[\u00a0]: Copied! <pre>nx, ny = 5, 5\nfig, axs = plt.subplots(nx, ny, figsize=(12, 12))\n## FIXME\n</pre> nx, ny = 5, 5 fig, axs = plt.subplots(nx, ny, figsize=(12, 12)) ## FIXME  In\u00a0[\u00a0]: Copied! <pre># features, target\n\nX = digits.drop(columns=\"target\").values\ny = digits[\"target\"].values\n</pre> # features, target  X = digits.drop(columns=\"target\").values y = digits[\"target\"].values In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME In\u00a0[\u00a0]: Copied! <pre>def mostrar_resultados(digits, model, nx=5, ny=5, label=\"correctos\"):\n    \"\"\"\n    Muestra los resultados de las predicciones de un modelo de clasificaci\u00f3n en particular. \n    Se toman aleatoriamente los valores de los resultados.\n    \n    - label == 'correctos': muestra los valores en los que el modelo acierta.\n    - label == 'incorrectos': muestra los valores en los que el modelo no acierta.\n\n    Observaci\u00f3n: El modelo que se recibe como argumento no debe estar entrenado.\n    \n    :param digits: dataset 'digits'\n    :param model: modelo de sklearn\n    :param nx: n\u00famero de filas (subplots)\n    :param ny: n\u00famero de columnas (subplots)\n    :param label: 'correctos' o 'incorrectos'\n    :return: gr\u00e1ficos matplotlib\n    \"\"\"\n    \n    X = digits.drop(columns=\"target\").values\n    y = digits[\"target\"].values\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) \n    model.fit(X_train, y_train)  # Ajustar el modelo\n    y_pred = model.predict(X_test)\n\n    # Mostrar los datos correctos\n    if label == \"correctos\":\n        mask = (y_pred == y_test)\n        color = \"green\"\n        \n    # Mostrar los datos incorrectos\n    elif label == \"incorrectos\":\n        mask = (y_pred != y_test)\n        color = \"red\"\n    \n    else:\n        raise ValueError(\"Valor incorrecto\")\n        \n    X_aux = X_test[mask]\n    y_aux_true = y_test[mask]\n    y_aux_pred = y_pred[mask]\n\n    # Mostrar los resultados\n    n_samples = min(nx * ny, len(X_aux))\n    indices = np.random.choice(len(X_aux), n_samples, replace=False)\n    fig, ax = plt.subplots(nx, ny, figsize=(12, 12))\n    \n    for i, index in enumerate(indices):\n        data = X_aux[index, :].reshape(8, 8)\n        label_pred = str(int(y_aux_pred[index]))\n        label_true = str(int(y_aux_true[index]))\n        row = i // ny\n        col = i % ny\n        ax[row, col].imshow(data, interpolation='nearest', cmap='gray_r')\n        ax[row, col].text(0, 0, label_pred, horizontalalignment='center', verticalalignment='center', fontsize=10, color=color)\n        ax[row, col].text(7, 0, label_true, horizontalalignment='center', verticalalignment='center', fontsize=10, color='blue')\n        ax[row, col].get_xaxis().set_visible(False)\n        ax[row, col].get_yaxis().set_visible(False)\n    \n    plt.show()\n</pre> def mostrar_resultados(digits, model, nx=5, ny=5, label=\"correctos\"):     \"\"\"     Muestra los resultados de las predicciones de un modelo de clasificaci\u00f3n en particular.      Se toman aleatoriamente los valores de los resultados.          - label == 'correctos': muestra los valores en los que el modelo acierta.     - label == 'incorrectos': muestra los valores en los que el modelo no acierta.      Observaci\u00f3n: El modelo que se recibe como argumento no debe estar entrenado.          :param digits: dataset 'digits'     :param model: modelo de sklearn     :param nx: n\u00famero de filas (subplots)     :param ny: n\u00famero de columnas (subplots)     :param label: 'correctos' o 'incorrectos'     :return: gr\u00e1ficos matplotlib     \"\"\"          X = digits.drop(columns=\"target\").values     y = digits[\"target\"].values     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)      model.fit(X_train, y_train)  # Ajustar el modelo     y_pred = model.predict(X_test)      # Mostrar los datos correctos     if label == \"correctos\":         mask = (y_pred == y_test)         color = \"green\"              # Mostrar los datos incorrectos     elif label == \"incorrectos\":         mask = (y_pred != y_test)         color = \"red\"          else:         raise ValueError(\"Valor incorrecto\")              X_aux = X_test[mask]     y_aux_true = y_test[mask]     y_aux_pred = y_pred[mask]      # Mostrar los resultados     n_samples = min(nx * ny, len(X_aux))     indices = np.random.choice(len(X_aux), n_samples, replace=False)     fig, ax = plt.subplots(nx, ny, figsize=(12, 12))          for i, index in enumerate(indices):         data = X_aux[index, :].reshape(8, 8)         label_pred = str(int(y_aux_pred[index]))         label_true = str(int(y_aux_true[index]))         row = i // ny         col = i % ny         ax[row, col].imshow(data, interpolation='nearest', cmap='gray_r')         ax[row, col].text(0, 0, label_pred, horizontalalignment='center', verticalalignment='center', fontsize=10, color=color)         ax[row, col].text(7, 0, label_true, horizontalalignment='center', verticalalignment='center', fontsize=10, color='blue')         ax[row, col].get_xaxis().set_visible(False)         ax[row, col].get_yaxis().set_visible(False)          plt.show() <p>Pregunta</p> <ul> <li><p>Tomando en cuenta el mejor modelo entontrado en el <code>Ejercicio 3</code>, grafique los resultados cuando:</p> </li> <li><p>el valor predicho y original son iguales</p> </li> <li><p>el valor predicho y original son distintos</p> </li> <li><p>Cuando el valor predicho y original son distintos ,  \u00bfPor qu\u00e9 ocurren estas fallas?</p> </li> </ul> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME"},{"location":"labs/lab_072/#lab-072","title":"Lab-072\u00b6","text":""},{"location":"labs/lab_072/#i-problema-01","title":"I.- Problema 01\u00b6","text":""},{"location":"labs/lab_072/#exploracion-de-los-datos","title":"Exploraci\u00f3n de los datos\u00b6","text":"<p>A continuaci\u00f3n se carga el conjunto de datos a utilizar, a trav\u00e9s del sub-m\u00f3dulo <code>datasets</code> de <code>sklearn</code>.</p>"},{"location":"labs/lab_072/#ejercicio-1","title":"Ejercicio 1\u00b6","text":"<p>An\u00e1lisis exploratorio: Realiza tu an\u00e1lisis exploratorio, no debes olvidar nada! Recuerda, cada an\u00e1lisis debe responder una pregunta.</p> <p>Algunas sugerencias:</p> <ul> <li>\u00bfC\u00f3mo se distribuyen los datos?</li> <li>\u00bfCu\u00e1nta memoria estoy utilizando?</li> <li>\u00bfQu\u00e9 tipo de datos son?</li> <li>\u00bfCu\u00e1ntos registros por clase hay?</li> <li>\u00bfHay registros que no se correspondan con tu conocimiento previo de los datos?</li> </ul>"},{"location":"labs/lab_072/#ejercicio-2","title":"Ejercicio 2\u00b6","text":"<p>Visualizaci\u00f3n: Para visualizar los datos utilizaremos el m\u00e9todo <code>imshow</code> de <code>matplotlib</code>. Resulta necesario convertir el arreglo desde las dimensiones (1,64)  a (8,8) para que la imagen sea cuadrada y pueda distinguirse el d\u00edgito. Superpondremos adem\u00e1s el label correspondiente al d\u00edgito, mediante el m\u00e9todo <code>text</code>. Esto nos permitir\u00e1 comparar la imagen generada con la etiqueta asociada a los valores. Realizaremos lo anterior para los primeros 25 datos del archivo.</p>"},{"location":"labs/lab_072/#ejercicio-3","title":"Ejercicio 3\u00b6","text":"<p>Machine Learning: En esta parte usted debe entrenar los distintos modelos escogidos desde la librer\u00eda de <code>skelearn</code>. Para cada modelo, debe realizar los siguientes pasos:</p> <ul> <li><p>train-test</p> <ul> <li>Crear conjunto de entrenamiento y testeo (usted determine las proporciones adecuadas).</li> <li>Imprimir por pantalla el largo del conjunto de entrenamiento y de testeo.</li> </ul> </li> <li><p>modelo:</p> <ul> <li>Instanciar el modelo objetivo desde la librer\u00eda sklearn.</li> </ul> </li> <li><p>M\u00e9tricas:</p> <ul> <li>Graficar matriz de confusi\u00f3n.</li> <li>Analizar m\u00e9tricas de error.</li> </ul> </li> </ul> <p>Preguntas a responder:</p> <ul> <li>\u00bfCu\u00e1l modelo es mejor basado en sus m\u00e9tricas?</li> <li>\u00bfCu\u00e1l modelo demora menos tiempo en ajustarse?</li> <li>\u00bfQu\u00e9 modelo escoges?</li> </ul>"},{"location":"labs/lab_072/#ejercicio-4","title":"Ejercicio 4\u00b6","text":"<p>Comprensi\u00f3n del modelo: Tomando en cuenta el mejor modelo entontrado en el <code>Ejercicio 3</code>, debe comprender e interpretar minuciosamente los resultados y gr\u00e1ficos asocados al modelo en estudio, para ello debe resolver los siguientes puntos:</p> <ul> <li>Curva AUC\u2013ROC: Replica el ejemplo del siguiente  link pero con el modelo, par\u00e1metros y m\u00e9trica adecuada. Saque conclusiones del gr\u00e1fico.</li> </ul>"},{"location":"labs/lab_072/#ejercicio-5","title":"Ejercicio 5\u00b6","text":"<p>Visualizando Resultados: A continuaci\u00f3n se provee c\u00f3digo para comparar las etiquetas predichas vs las etiquetas reales del conjunto de test.</p>"},{"location":"labs/lab_072/#ejercicio-6","title":"Ejercicio 6\u00b6","text":"<p>Conclusiones: Entrega tu veredicto, responde las preguntas iniciales, visualizaciones, trabajos futuros, dificultades, etc.</p>"},{"location":"labs/lab_08/","title":"Lab-08","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.cluster import KMeans\n\n\n%matplotlib inline\nsns.set_palette(\"deep\", desat=.6)\nsns.set(rc={'figure.figsize':(11.7,8.27)})\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns  from sklearn.preprocessing import MinMaxScaler from sklearn.dummy import DummyClassifier from sklearn.cluster import KMeans   %matplotlib inline sns.set_palette(\"deep\", desat=.6) sns.set(rc={'figure.figsize':(11.7,8.27)}) In\u00a0[\u00a0]: Copied! <pre># cargar datos\ndf = pd.read_csv(\"https://raw.githubusercontent.com/fralfaro/MAT281_2022/main/labs/lab_08/data/vehiculos_procesado_con_grupos.csv\", sep=\",\")\\\n       .drop(\n            [\"fabricante\", \n             \"modelo\",\n             \"transmision\", \n             \"traccion\", \n             \"clase\", \n             \"combustible\",\n             \"consumo\"], \n    \n          axis=1)\n\ndf.head()\n</pre> # cargar datos df = pd.read_csv(\"https://raw.githubusercontent.com/fralfaro/MAT281_2022/main/labs/lab_08/data/vehiculos_procesado_con_grupos.csv\", sep=\",\")\\        .drop(             [\"fabricante\",               \"modelo\",              \"transmision\",               \"traccion\",               \"clase\",               \"combustible\",              \"consumo\"],                 axis=1)  df.head() <p>En este caso, no solo se tienen datos num\u00e9ricos, sino que tambi\u00e9n categ\u00f3ricos. Adem\u00e1s, tenemos problemas de datos vac\u00edos (Nan). As\u00ed que para resolver este problema, seguiremos varios pasos:</p> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME <p>Al observar el gr\u00e1fico resultante, se pueden obtener conclusiones sobre el n\u00famero apropiado de clusters. La regla del codo sugiere elegir el n\u00famero de clusters donde la reducci\u00f3n en la inercia se estabiliza significativamente. En otras palabras, se busca el punto en el gr\u00e1fico donde la curva de inercia comienza a aplanarse o forma un codo.</p> In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns\n</pre> import pandas as pd from sklearn.datasets import load_iris from sklearn.decomposition import PCA from sklearn.manifold import TSNE  import matplotlib.pyplot as plt from matplotlib.colors import ListedColormap from mpl_toolkits.mplot3d import Axes3D import seaborn as sns In\u00a0[\u00a0]: Copied! <pre>dataset = load_iris()\n\nfeatures = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\ntarget = 'species'\niris = pd.DataFrame(\n    dataset.data,\n    columns=features)\n\niris['species'] = dataset.target\niris.head()\n</pre> dataset = load_iris()  features = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width'] target = 'species' iris = pd.DataFrame(     dataset.data,     columns=features)  iris['species'] = dataset.target iris.head() <p>El objetivo es aplicar ambos algoritmos de la siguiente manera:</p> <ul> <li>An\u00e1lisis detallado algoritma PCA (tablas, gr\u00e1ficos, etc.)</li> <li>An\u00e1lisis detallado algoritma TSNE (tablas, gr\u00e1ficos, etc.)</li> <li>Comparar ambos algoritmos (conclusiones del caso)</li> </ul> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME"},{"location":"labs/lab_08/#lab-08","title":"Lab-08\u00b6","text":""},{"location":"labs/lab_08/#i-problema-01","title":"I.- Problema 01\u00b6","text":"<p>El conjunto de datos se denomina <code>vehiculos_procesado_con_grupos.csv</code>, el cual contine algunas de las caracter\u00edsticas m\u00e1s importante de un veh\u00edculo.</p> <p>En este ejercicio se tiene como objetivo, es poder clasificar los distintos veh\u00edculos basados en las cracter\u00edsticas que se presentan a continuaci\u00f3n. La dificultad de este ejercicio rad\u00edca en que ahora tenemos variables num\u00e9ricas y variables categ\u00f3ricas.</p> <p>Lo primero ser\u00e1 cargar el conjunto de datos:</p>"},{"location":"labs/lab_08/#1-normalizar-datos","title":"1.- Normalizar datos\u00b6","text":"<ol> <li>Cree un conjunto de datos con las variables num\u00e9ricas, adem\u00e1s, para cada dato vac\u00eda, rellene con el promedio asociado a esa columna. Finalmente, normalize los datos mediante el procesamiento MinMaxScaler de sklearn.</li> </ol> <p>2.-  Cree un conjunto de datos con las variables categ\u00f3ricas , adem\u00e1s, transforme de variables categoricas a numericas ocupando el comando get_dummies de pandas (referencia). Explique a grande rasgo como se realiza la codificaci\u00f3n de variables num\u00e9ricas a categ\u00f3ricas.</p> <p>3.- Junte ambos dataset en uno, llamado df_procesado.</p>"},{"location":"labs/lab_08/#2-realizar-ajuste-mediante-kmeans","title":"2.- Realizar ajuste mediante kmeans\u00b6","text":"<p>Una vez depurado el conjunto de datos, es momento de aplicar el algoritmo de kmeans.</p> <ol> <li>Ajuste el modelo de kmeans sobre el conjunto de datos, con un total de 8 clusters.</li> <li>Asociar a cada individuo el correspondiente cluster y calcular valor de los centroides de cada cluster.</li> <li>Realizar un resumen de las principales cualidades de cada cluster. Para  esto debe calcular (para cluster) las siguientes medidas de resumen:<ul> <li>Valor promedio de las variables num\u00e9rica</li> <li>Moda para las variables numericas</li> </ul> </li> </ol>"},{"location":"labs/lab_08/#3-elegir-numero-de-cluster","title":"3.- Elegir N\u00famero de cluster\u00b6","text":"<p>Estime mediante la regla del codo, el n\u00famero de cluster apropiados para el caso. Para efectos pr\u00e1cticos, eliga la siguiente secuencia como n\u00famero de clusters a comparar:</p> <p>$$[5, 10, 20, 30, 50, 75, 100, 200, 300]$$</p> <p>Una vez realizado el gr\u00e1fico, saque sus propias conclusiones del caso.</p>"},{"location":"labs/lab_08/#ii-problema-02","title":"II.- Problema 02\u00b6","text":"<p>Para el conjunto de datos de Iris, se pide realizar una reducci\u00f3n de dimensionalidad ocupando las t\u00e9cnicas de PCA y TSNE (vistas en clases).</p>"},{"location":"labs/lab_09/","title":"Lab-09","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n\n\n%matplotlib inline\nsns.set_palette(\"deep\", desat=.6)\nsns.set(rc={'figure.figsize':(11.7,8.27)})\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns  from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import GridSearchCV from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score    %matplotlib inline sns.set_palette(\"deep\", desat=.6) sns.set(rc={'figure.figsize':(11.7,8.27)}) In\u00a0[2]: Copied! <pre># cargar datos\ndf = pd.read_csv(\"https://raw.githubusercontent.com/fralfaro/MAT281_2022/main/labs/lab_09/data/BC.csv\", sep=\",\")\ndf = df.set_index('id')\ndf['diagnosis'] = df['diagnosis'].replace({'M':1,'B':0}) # target \ndf.head()\n</pre> # cargar datos df = pd.read_csv(\"https://raw.githubusercontent.com/fralfaro/MAT281_2022/main/labs/lab_09/data/BC.csv\", sep=\",\") df = df.set_index('id') df['diagnosis'] = df['diagnosis'].replace({'M':1,'B':0}) # target  df.head() Out[2]: diagnosis radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean concave points_mean symmetry_mean ... radius_worst texture_worst perimeter_worst area_worst smoothness_worst compactness_worst concavity_worst concave points_worst symmetry_worst fractal_dimension_worst id 842302 1 17.99 10.38 122.80 1001.0 0.11840 0.27760 0.3001 0.14710 0.2419 ... 25.38 17.33 184.60 2019.0 0.1622 0.6656 0.7119 0.2654 0.4601 0.11890 842517 1 20.57 17.77 132.90 1326.0 0.08474 0.07864 0.0869 0.07017 0.1812 ... 24.99 23.41 158.80 1956.0 0.1238 0.1866 0.2416 0.1860 0.2750 0.08902 84300903 1 19.69 21.25 130.00 1203.0 0.10960 0.15990 0.1974 0.12790 0.2069 ... 23.57 25.53 152.50 1709.0 0.1444 0.4245 0.4504 0.2430 0.3613 0.08758 84348301 1 11.42 20.38 77.58 386.1 0.14250 0.28390 0.2414 0.10520 0.2597 ... 14.91 26.50 98.87 567.7 0.2098 0.8663 0.6869 0.2575 0.6638 0.17300 84358402 1 20.29 14.34 135.10 1297.0 0.10030 0.13280 0.1980 0.10430 0.1809 ... 22.54 16.67 152.20 1575.0 0.1374 0.2050 0.4000 0.1625 0.2364 0.07678 <p>5 rows \u00d7 31 columns</p> <p>Basado en la informaci\u00f3n presentada responda las siguientes preguntas:</p> <ol> <li>Realice un an\u00e1lisis exploratorio del conjunto de datos.</li> </ol> In\u00a0[1]: Copied! <pre># FIXME\n</pre> # FIXME <ol> <li>Normalizar las variables num\u00e9ricas con el m\u00e9todo StandardScaler.</li> </ol> In\u00a0[6]: Copied! <pre># FIXME\n</pre> # FIXME <ol> <li>Realizar un m\u00e9todo de reducci\u00f3n de dimensionalidad visto en clases.</li> </ol> In\u00a0[7]: Copied! <pre># FIXME\n</pre> # FIXME <ol> <li>Aplique al menos tres modelos de clasificaci\u00f3n distintos. Para cada uno de los modelos escogidos, realice una optimizaci\u00f3n de los hiperpar\u00e1metros. adem\u00e1s, calcule las respectivas m\u00e9tricas. Concluya.</li> </ol> In\u00a0[8]: Copied! <pre># FIXME\n</pre> # FIXME"},{"location":"labs/lab_09/#lab-09","title":"Lab-09\u00b6","text":""},{"location":"labs/lab_09/#i-problema-01","title":"I.- Problema 01\u00b6","text":"<p>El c\u00e1ncer de mama  es una proliferaci\u00f3n maligna de las c\u00e9lulas epiteliales que revisten los conductos o lobulillos mamarios. Es una enfermedad clonal; donde una c\u00e9lula individual producto de una serie de mutaciones som\u00e1ticas o de l\u00ednea germinal adquiere la capacidad de dividirse sin control ni orden, haciendo que se reproduzca hasta formar un tumor. El tumor resultante, que comienza como anomal\u00eda leve, pasa a ser grave, invade tejidos vecinos y, finalmente, se propaga a otras partes del cuerpo.</p> <p>El conjunto de datos se denomina <code>BC.csv</code>, el cual contine la informaci\u00f3n de distintos pacientes con tumosres (benignos o malignos) y algunas caracter\u00edsticas del mismo.</p> <p>Las caracter\u00edsticas se calculan a partir de una imagen digitalizada de un aspirado con aguja fina (FNA) de una masa mamaria. Describen las caracter\u00edsticas de los n\u00facleos celulares presentes en la imagen. Los detalles se puede encontrar en [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34].</p> <p>Lo primero ser\u00e1 cargar el conjunto de datos:</p>"},{"location":"labs/lab_10/","title":"Lab-10","text":"In\u00a0[\u00a0]: Copied! <pre># librerias \nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns \nimport statsmodels.api as sm\nfrom prophet import Prophet\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\n# graficos incrustados\nsns.set_style('whitegrid')\n%matplotlib inline\n\n# parametros esteticos de seaborn\nsns.set_palette(\"deep\", desat=.6)\nsns.set_context(rc={\"figure.figsize\": (12, 4)})\n</pre> # librerias  import pandas as pd import numpy as np  import matplotlib.pyplot as plt  import seaborn as sns  import statsmodels.api as sm from prophet import Prophet from statsmodels.tsa.statespace.sarimax import SARIMAX  # graficos incrustados sns.set_style('whitegrid') %matplotlib inline  # parametros esteticos de seaborn sns.set_palette(\"deep\", desat=.6) sns.set_context(rc={\"figure.figsize\": (12, 4)}) In\u00a0[\u00a0]: Copied! <pre>from sklearn.metrics import mean_absolute_error, mean_squared_error\n\ndef mean_absolute_percentage_error(y_true, y_pred):\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\ndef regression_metrics(df):\n    \"\"\"\n    Aplicar las distintas m\u00e9tricas definidas\n    :param df: DataFrame con las columnas: ['y', 'yhat']\n    :return: DataFrame con las m\u00e9tricas especificadas\n    \"\"\"\n    df_result = pd.DataFrame()\n\n    y_true = df['y']\n    y_pred = df['yhat']\n\n    df_result['mae'] = [round(mean_absolute_error(y_true, y_pred), 4)]\n    df_result['mse'] = [round(mean_squared_error(y_true, y_pred), 4)]\n    df_result['rmse'] = [round(np.sqrt(mean_squared_error(y_true, y_pred)), 4)]\n    df_result['mape'] = [round(mean_absolute_percentage_error(y_true, y_pred), 4)]\n    df_result['smape'] = [round(2 * mean_absolute_percentage_error(y_true, y_pred) / (mean_absolute_percentage_error(y_true, y_pred) + 100), 4)]\n\n    return df_result\n</pre> from sklearn.metrics import mean_absolute_error, mean_squared_error  def mean_absolute_percentage_error(y_true, y_pred):     return np.mean(np.abs((y_true - y_pred) / y_true)) * 100  def regression_metrics(df):     \"\"\"     Aplicar las distintas m\u00e9tricas definidas     :param df: DataFrame con las columnas: ['y', 'yhat']     :return: DataFrame con las m\u00e9tricas especificadas     \"\"\"     df_result = pd.DataFrame()      y_true = df['y']     y_pred = df['yhat']      df_result['mae'] = [round(mean_absolute_error(y_true, y_pred), 4)]     df_result['mse'] = [round(mean_squared_error(y_true, y_pred), 4)]     df_result['rmse'] = [round(np.sqrt(mean_squared_error(y_true, y_pred)), 4)]     df_result['mape'] = [round(mean_absolute_percentage_error(y_true, y_pred), 4)]     df_result['smape'] = [round(2 * mean_absolute_percentage_error(y_true, y_pred) / (mean_absolute_percentage_error(y_true, y_pred) + 100), 4)]      return df_result In\u00a0[\u00a0]: Copied! <pre># read data\n\nvalidate_categorie = [\n  'Introduction', 'M\u00e9fait','Vol dans / sur v\u00e9hicule \u00e0 moteur', 'Vol de v\u00e9hicule \u00e0 moteur',\n]\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/fralfaro/MAT281_2022/main/labs/lab_10/data/interventionscitoyendo.csv\", sep=\",\", encoding='latin-1')\ndf.columns = df.columns.str.lower()\ndf['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n\ndf = df.loc[lambda x: x['categorie'].isin(validate_categorie)]\ndf = df.sort_values(['categorie','date'])\ndf.head()\n</pre> # read data  validate_categorie = [   'Introduction', 'M\u00e9fait','Vol dans / sur v\u00e9hicule \u00e0 moteur', 'Vol de v\u00e9hicule \u00e0 moteur', ]  df = pd.read_csv(\"https://raw.githubusercontent.com/fralfaro/MAT281_2022/main/labs/lab_10/data/interventionscitoyendo.csv\", sep=\",\", encoding='latin-1') df.columns = df.columns.str.lower() df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')  df = df.loc[lambda x: x['categorie'].isin(validate_categorie)] df = df.sort_values(['categorie','date']) df.head() <p>Como tenemos muchos datos por categor\u00eda a nivel de d\u00eda, agruparemos a nivel de semanas y separaremos cada serie temporal.</p> In\u00a0[\u00a0]: Copied! <pre>cols = ['date','pdq']\ny_s1 = df.loc[lambda x: x.categorie == validate_categorie[0] ][cols].set_index('date').resample('W').mean()\ny_s2 = df.loc[lambda x: x.categorie == validate_categorie[1] ][cols].set_index('date').resample('W').mean()\ny_s3 = df.loc[lambda x: x.categorie == validate_categorie[2] ][cols].set_index('date').resample('W').mean()\ny_s4 = df.loc[lambda x: x.categorie == validate_categorie[3] ][cols].set_index('date').resample('W').mean()\n</pre> cols = ['date','pdq'] y_s1 = df.loc[lambda x: x.categorie == validate_categorie[0] ][cols].set_index('date').resample('W').mean() y_s2 = df.loc[lambda x: x.categorie == validate_categorie[1] ][cols].set_index('date').resample('W').mean() y_s3 = df.loc[lambda x: x.categorie == validate_categorie[2] ][cols].set_index('date').resample('W').mean() y_s4 = df.loc[lambda x: x.categorie == validate_categorie[3] ][cols].set_index('date').resample('W').mean() <p>El objetivo de este laboratorio es poder realizar un an\u00e1lisis completo del conjunto de datos en estudio, para eso debe responder las siguientes preguntas:</p> <ol> <li>Realizar un gr\u00e1fico para cada serie temporal $y\\_{si}, i =1,2,3,4$.</li> </ol> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME <ol> <li>Escoger alguna serie temporal $y\\_{si}, i =1,2,3,4$. Luego:</li> </ol> <ul> <li>Realizar un an\u00e1lisis exploratorio de la serie temporal escogida</li> <li>Aplicar el modelo de pron\u00f3stico $SARIMA(p,d,q)x(P,D,Q,S)$, probando varias configuraciones de los hiperpar\u00e1metros. Encuentre la mejor configuraci\u00f3n. Concluya.</li> <li>Para el mejor modelo encontrado, verificar si el residuo corresponde a un ruido blanco.</li> </ul> <p>Hint: Tome como <code>target_date</code> =  '2021-01-01'. Recuerde considerar que su columna de valores se llama <code>pdq</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Creando la clase SarimaModels\nclass SarimaModels:\n    def __init__(self, params):\n        self.params = params\n\n    @property\n    def name_model(self):\n        return f\"SARIMA_{self.params[0]}X{self.params[1]}\".replace(' ', '')\n\n    @staticmethod\n    def test_train_model(y, date):\n        mask_ds = y.index &lt; date\n        y_train = y[mask_ds]\n        y_test = y[~mask_ds]\n        return y_train, y_test\n\n    def fit_model(self, y, date):\n        y_train, y_test = self.test_train_model(y, date)\n        model = SARIMAX(y_train,\n                        order=self.params[0],\n                        seasonal_order=self.params[1],\n                        enforce_stationarity=False,\n                        enforce_invertibility=False)\n        model_fit = model.fit(disp=0)\n        return model_fit\n\n    def df_testing(self, y, date):\n        y_train, y_test = self.test_train_model(y, date)\n        model = SARIMAX(y_train,\n                        order=self.params[0],\n                        seasonal_order=self.params[1],\n                        enforce_stationarity=False,\n                        enforce_invertibility=False)\n        model_fit = model.fit(disp=0)\n        start_index = y_test.index.min()\n        end_index = y_test.index.max()\n        preds = model_fit.get_prediction(start=start_index, end=end_index, dynamic=False)\n        df_temp = pd.DataFrame({\n            'y': y_test['pdq'],\n            'yhat': preds.predicted_mean\n        })\n        return df_temp\n\n    def metrics(self, y, date):\n        df_temp = self.df_testing(y, date)\n        df_metrics = regression_metrics(df_temp)\n        df_metrics['model'] = self.name_model\n        return df_metrics\n\n# Definir par\u00e1metros\nimport itertools\np = d = q = range(0, 2)\npdq = list(itertools.product(p, d, q))\nseasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\nparams = list(itertools.product(pdq, seasonal_pdq))\ntarget_date = '2021-01-01'\n</pre> # Creando la clase SarimaModels class SarimaModels:     def __init__(self, params):         self.params = params      @property     def name_model(self):         return f\"SARIMA_{self.params[0]}X{self.params[1]}\".replace(' ', '')      @staticmethod     def test_train_model(y, date):         mask_ds = y.index &lt; date         y_train = y[mask_ds]         y_test = y[~mask_ds]         return y_train, y_test      def fit_model(self, y, date):         y_train, y_test = self.test_train_model(y, date)         model = SARIMAX(y_train,                         order=self.params[0],                         seasonal_order=self.params[1],                         enforce_stationarity=False,                         enforce_invertibility=False)         model_fit = model.fit(disp=0)         return model_fit      def df_testing(self, y, date):         y_train, y_test = self.test_train_model(y, date)         model = SARIMAX(y_train,                         order=self.params[0],                         seasonal_order=self.params[1],                         enforce_stationarity=False,                         enforce_invertibility=False)         model_fit = model.fit(disp=0)         start_index = y_test.index.min()         end_index = y_test.index.max()         preds = model_fit.get_prediction(start=start_index, end=end_index, dynamic=False)         df_temp = pd.DataFrame({             'y': y_test['pdq'],             'yhat': preds.predicted_mean         })         return df_temp      def metrics(self, y, date):         df_temp = self.df_testing(y, date)         df_metrics = regression_metrics(df_temp)         df_metrics['model'] = self.name_model         return df_metrics  # Definir par\u00e1metros import itertools p = d = q = range(0, 2) pdq = list(itertools.product(p, d, q)) seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))] params = list(itertools.product(pdq, seasonal_pdq)) target_date = '2021-01-01' In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME <ol> <li>Resuelva el ejercicio anterior utilizando la librer\u00eda de <code>Prophet</code>.</li> </ol> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME"},{"location":"labs/lab_10/#lab-10","title":"Lab-10\u00b6","text":""},{"location":"labs/lab_10/#i-problema-01","title":"I.- Problema 01\u00b6","text":"<p>Lista de actos delictivos registrados por el Service de police de la Ville de Montr\u00e9al (SPVM).</p> <p></p> <p>El conjunto de datos en estudio <code>interventionscitoyendo.csv</code> corresponde a  todos los delitos entre 2015 y agosto de 2020en Montreal. Cada delito est\u00e1 asociado en grandes categor\u00edas, y hay informaci\u00f3n sobre la ubicaci\u00f3n, el momento del d\u00eda, etc.</p> <p>Nota: Para m\u00e1s informaci\u00f3n seguir el siguiente el link.</p>"},{"location":"lectures/data_manipulation/eda_01/","title":"Caso Aplicado","text":"<p>EL dataset <code>terremotos.csv</code> contiene la informaci\u00f3n de los terremotos de los pa\u00edses durante el a\u00f1o 2000 al 2011. Debido a que la informaci\u00f3n de este dataset es relativamente f\u00e1cil de trabajar, hemos creado un dataset denominado <code>terremotos_contaminados.csv</code> que posee informaci\u00f3n contaminada en cada una de sus columnas. De esta forma se podr\u00e1 ilustrar los distintos inconvenientes que se puede tener en el an\u00e1lisis exploratorio de datos.</p> <p>\u00bf Por qu\u00e9 no puede ser un terremoto con una intensidad mayor a 9.6?. Esto se debe a que el terremoto con mayor magnitud registrado por la humanidad es de 9.6, ocurrido en Chile (Valdivia) durante el a\u00f1o 1960. Por lo tanto, entre mayor conocimiento se tenga del fen\u00f3meno en estudio, m\u00e1s restrictivo se vulve el an\u00e1lisis exploratorio y m\u00e1s sentido tienen los resultados obtenidos.</p> In\u00a0[1]: Copied! <pre>#!pip install matplotlib==3.4.2\n</pre> #!pip install matplotlib==3.4.2 In\u00a0[2]: Copied! <pre># cargar librerias\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes\n\n# Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab\n%matplotlib inline\n</pre> # cargar librerias  import os import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns  pd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes  # Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab %matplotlib inline In\u00a0[3]: Copied! <pre># cargar datos\nurl='https://drive.google.com/file/d/16LGmX235sIPgdhyIyHfZlNzavU3JlL22/view?usp=drive_link'\nurl='https://drive.google.com/uc?id=' + url.split('/')[-2]\n\nterremotos_data = pd.read_csv(url, sep=\",\")\nterremotos_data.head()\n</pre> # cargar datos url='https://drive.google.com/file/d/16LGmX235sIPgdhyIyHfZlNzavU3JlL22/view?usp=drive_link' url='https://drive.google.com/uc?id=' + url.split('/')[-2]  terremotos_data = pd.read_csv(url, sep=\",\") terremotos_data.head() Out[3]: A\u00f1o Pais Magnitud Informacion 0 2000 Turkey 6 info no valiosa 1 2000 Turkmenistan 7 info no valiosa 2 2000 Azerbaijan 6.5 info no valiosa 3 2000 Azerbaijan 6.8 info no valiosa 4 2000 Papua New Guinea 8 info no valiosa In\u00a0[4]: Copied! <pre># normalizar columnas (minusculas y sin espacios)\n\nterremotos_data.columns = terremotos_data.columns.str.lower().str.strip()\nterremotos_data.head()\n</pre> # normalizar columnas (minusculas y sin espacios)  terremotos_data.columns = terremotos_data.columns.str.lower().str.strip() terremotos_data.head() Out[4]: a\u00f1o pais magnitud informacion 0 2000 Turkey 6 info no valiosa 1 2000 Turkmenistan 7 info no valiosa 2 2000 Azerbaijan 6.5 info no valiosa 3 2000 Azerbaijan 6.8 info no valiosa 4 2000 Papua New Guinea 8 info no valiosa In\u00a0[5]: Copied! <pre>terremotos_data[\"pais\"]\n</pre> terremotos_data[\"pais\"] Out[5]: <pre>0                Turkey\n1          Turkmenistan\n2            Azerbaijan\n3           Azerbaijan \n4      Papua New Guinea\n             ...       \n223              Serbia\n224              Haiti \n225                 NaN\n226                 NaN\n227               arica\nName: pais, Length: 228, dtype: object</pre> In\u00a0[6]: Copied! <pre># resumen de la informacion\n\ndef resumen_por_columna(df,cols):\n    pd_series = df[cols]\n    \n    # elementos distintos \n    l_unique = pd_series.unique()\n    \n    # elementos vacios\n    \n    l_vacios = pd_series[pd_series.isna()]\n    \n    df_info = pd.DataFrame({\n        'columna': [cols],\n        'unicos': [len(l_unique)],\n        'vacios': [len(l_vacios)]\n    })\n    \n    return df_info\n</pre> # resumen de la informacion  def resumen_por_columna(df,cols):     pd_series = df[cols]          # elementos distintos      l_unique = pd_series.unique()          # elementos vacios          l_vacios = pd_series[pd_series.isna()]          df_info = pd.DataFrame({         'columna': [cols],         'unicos': [len(l_unique)],         'vacios': [len(l_vacios)]     })          return df_info In\u00a0[7]: Copied! <pre>frames = []\n\nfor col in terremotos_data.columns:\n    aux_df = resumen_por_columna(terremotos_data,col)\n    frames.append(aux_df)\n    \ndf_info = pd.concat(frames).reset_index(drop=True)\ndf_info['% vacios'] = df_info['vacios']/len(terremotos_data)\ndf_info\n</pre> frames = []  for col in terremotos_data.columns:     aux_df = resumen_por_columna(terremotos_data,col)     frames.append(aux_df)      df_info = pd.concat(frames).reset_index(drop=True) df_info['% vacios'] = df_info['vacios']/len(terremotos_data) df_info Out[7]: columna unicos vacios % vacios 0 a\u00f1o 17 2 0.008772 1 pais 75 2 0.008772 2 magnitud 46 3 0.013158 3 informacion 4 8 0.035088 In\u00a0[8]: Copied! <pre>plt.style.use('default')\nplt.figure(figsize=(7,5))\n\nplotting = sns.barplot(\n    x=\"columna\",\n    y=\"unicos\",\n    data=df_info.sort_values('unicos'),\n    palette=\"Greys_d\",\n    linewidth=3\n)\n\nfor container in plotting.containers:\n    plotting.bar_label(container,fontsize=12)\n\nplt.title(\"Valores Distintos\")\nplt.yticks(fontsize=14)\nplt.xticks(fontsize=14)\n\nplt.show()\n</pre> plt.style.use('default') plt.figure(figsize=(7,5))  plotting = sns.barplot(     x=\"columna\",     y=\"unicos\",     data=df_info.sort_values('unicos'),     palette=\"Greys_d\",     linewidth=3 )  for container in plotting.containers:     plotting.bar_label(container,fontsize=12)  plt.title(\"Valores Distintos\") plt.yticks(fontsize=14) plt.xticks(fontsize=14)  plt.show() <p>\u00bfQu\u00e9 falta en los datos y c\u00f3mo los maneja?</p> <p>En este caso, se tiene la informaci\u00f3n suficiente para poder realizar el experimento, solo falta ver que los datos de la muestra no esten lo suficientemente contaminados.</p> <p>\u00bfQu\u00e9 hacer con los datos faltantes, outliers o informaci\u00f3n mal inputada?</p> <p>Este caso es m\u00e1s interesante, y se necesita ir detallando columna por columna los distintos an\u00e1lisis.</p> In\u00a0[9]: Copied! <pre>terremotos_data['a\u00f1o'].unique()\n</pre> terremotos_data['a\u00f1o'].unique() Out[9]: <pre>array(['2000', '2001', 'dos mil uno', '2002', '2003', '2004', '2005',\n       '2006', '2007', '2008', '2009', '2010', '2011', '1997', '1990',\n       '1999', nan], dtype=object)</pre> In\u00a0[10]: Copied! <pre># bar plot: a\u00f1o\n\nplt.style.use('default')\nplt.figure(figsize=(7,4))\n\nplotting = sns.countplot(\n    y=\"a\u00f1o\",\n    data=terremotos_data,\n    order=terremotos_data['a\u00f1o'].value_counts().index,\n    palette=\"Greys_d\",\n    linewidth=3\n)\n\nfor container in plotting.containers:\n    plotting.bar_label(container,fontsize=12)\n\n\nplt.show()\n</pre> # bar plot: a\u00f1o  plt.style.use('default') plt.figure(figsize=(7,4))  plotting = sns.countplot(     y=\"a\u00f1o\",     data=terremotos_data,     order=terremotos_data['a\u00f1o'].value_counts().index,     palette=\"Greys_d\",     linewidth=3 )  for container in plotting.containers:     plotting.bar_label(container,fontsize=12)   plt.show() <p>Se presentan las siguientes anomalidades:</p> <ul> <li>A\u00f1os sin importancia: Se ha establecido que los a\u00f1os de estudios son desde el a\u00f1o 2000 al 2011.</li> <li>Nombres mal escritos: en este caso sabemos que 'dos mil uno' corresponde a '2001'.</li> <li>Datos vac\u00edo</li> </ul> <p>Ahora la pregunta es, \u00bf qu\u00e9 debemos hacer primero?. Lo primero es corregir la informaci\u00f3n, dar un formato est\u00e1ndar a los datos y luego filtrar.</p> <p>a) Correcci\u00f3n</p> In\u00a0[11]: Copied! <pre>terremotos_data.loc[terremotos_data['a\u00f1o']=='dos mil uno','a\u00f1o'] = '2001'\nterremotos_data.loc[terremotos_data['a\u00f1o'].isnull(),'a\u00f1o'] = '0'\n\nterremotos_data['a\u00f1o'].unique()\n</pre> terremotos_data.loc[terremotos_data['a\u00f1o']=='dos mil uno','a\u00f1o'] = '2001' terremotos_data.loc[terremotos_data['a\u00f1o'].isnull(),'a\u00f1o'] = '0'  terremotos_data['a\u00f1o'].unique() Out[11]: <pre>array(['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007',\n       '2008', '2009', '2010', '2011', '1997', '1990', '1999', '0'],\n      dtype=object)</pre> <p>b) Formato</p> <p>El formato de los a\u00f1os es integer, por lo tanto se le debe dar ese formato.</p> In\u00a0[12]: Copied! <pre>terremotos_data['a\u00f1o'] = terremotos_data['a\u00f1o'].astype(int)\n</pre> terremotos_data['a\u00f1o'] = terremotos_data['a\u00f1o'].astype(int) In\u00a0[13]: Copied! <pre>terremotos_data['a\u00f1o'].unique()\n</pre> terremotos_data['a\u00f1o'].unique() Out[13]: <pre>array([2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010,\n       2011, 1997, 1990, 1999,    0])</pre> <p>c) Filtro</p> <p>Se ha establecido que los a\u00f1os de estudios son desde el a\u00f1o 2000 al 2011, por lo tanto los a\u00f1os en estudios deberian ser:</p> In\u00a0[14]: Copied! <pre>anios_estudio = [x for x in range(2000,2011+1)]\nprint(anios_estudio)\n</pre> anios_estudio = [x for x in range(2000,2011+1)] print(anios_estudio) <pre>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011]\n</pre> <p>Por lo tanto ya tenemo nuestro primer filtro:</p> In\u00a0[15]: Copied! <pre>mask_anio = terremotos_data['a\u00f1o'].isin(anios_estudio)\n</pre> mask_anio = terremotos_data['a\u00f1o'].isin(anios_estudio) In\u00a0[16]: Copied! <pre>set(terremotos_data['pais'].unique())\n</pre> set(terremotos_data['pais'].unique()) Out[16]: <pre>{'Afghanistan',\n 'Afghanistan ',\n 'Algeria',\n 'Algeria ',\n 'Argentina',\n 'Azerbaijan',\n 'Azerbaijan ',\n 'Bangladesh',\n 'Burma ',\n 'Chile',\n 'Chile ',\n 'China',\n 'China ',\n 'Colombia',\n 'Costa Rica',\n 'Costa Rica ',\n 'Democratic Republic of the Congo',\n 'Democratic Republic of the Congo ',\n 'Dominican Republic',\n 'Ecuador',\n 'El Salvador ',\n 'Greece',\n 'Greece ',\n 'Guadeloupe',\n 'Guatemala',\n 'Haiti ',\n 'India',\n 'India ',\n 'Indonesia',\n 'Indonesia ',\n 'Iran',\n 'Iran ',\n 'Iran, 2005 Qeshm earthquake',\n 'Italy',\n 'Italy ',\n 'Japan',\n 'Japan ',\n 'Kazakhstan',\n 'Kyrgyzstan ',\n 'Martinique',\n 'Mexico ',\n 'Morocco',\n 'Morocco ',\n 'Mozambique',\n 'New Zealand',\n 'New Zealand ',\n 'Nicaragua',\n 'Pakistan',\n 'Pakistan ',\n 'Panama',\n 'Papua New Guinea',\n 'Peru',\n 'Peru ',\n 'Philippines',\n 'Russian Federation',\n 'Rwanda',\n 'Samoa ',\n 'Serbia',\n 'Slovenia',\n 'Solomon Islands ',\n 'Taiwan',\n 'Taiwan ',\n 'Tajikistan',\n 'Tajikistan ',\n 'Tanzania',\n 'Tanzania ',\n 'Turkey',\n 'Turkey ',\n 'Turkmenistan',\n 'United States ',\n 'Venezuela',\n 'Vietnam',\n 'arica',\n nan,\n 'shile'}</pre> In\u00a0[17]: Copied! <pre># bar plot: pais\n\nplt.style.use('default')\nplt.figure(figsize=(14,14))\n\nplotting = sns.countplot(\n    y=\"pais\",\n    data=terremotos_data,\n    order=terremotos_data.pais.value_counts().index,\n    palette=\"Greys_d\",\n    linewidth=3\n)\n\nfor container in plotting.containers:\n    plotting.bar_label(container,fontsize=12)\n\n\nplt.show()\n</pre> # bar plot: pais  plt.style.use('default') plt.figure(figsize=(14,14))  plotting = sns.countplot(     y=\"pais\",     data=terremotos_data,     order=terremotos_data.pais.value_counts().index,     palette=\"Greys_d\",     linewidth=3 )  for container in plotting.containers:     plotting.bar_label(container,fontsize=12)   plt.show() <p>Se presentan las siguientes anomalidades:</p> <ul> <li>Formato de los nombres: no se le ha aplicado strip() y lower(), por lo cual tenemos casos como: 'Turkey' y 'Turkey ' como elementos diferentes.</li> <li>Nombres mal escritos: en este caso sabemos que 'shile' corresponde a 'Chile' e 'Iran, 2005 Qeshm earthquake' corrsponde a 'Iran'.</li> <li>Datos vac\u00edo</li> </ul> <p>Se solucionar\u00e1 cada uno de estos inconvenientes:</p> <p>Correcci\u00f3n de los nombres</p> In\u00a0[18]: Copied! <pre>terremotos_data.loc[terremotos_data['pais']=='arica','pais'] = 'chile'\nterremotos_data.loc[terremotos_data['pais']=='shile','pais'] = 'chile'\nterremotos_data.loc[terremotos_data['pais']=='Iran, 2005 Qeshm earthquake','pais'] = 'iran'\nterremotos_data.loc[terremotos_data['pais'].isnull(),'pais'] = 'sin_nombre'\n</pre> terremotos_data.loc[terremotos_data['pais']=='arica','pais'] = 'chile' terremotos_data.loc[terremotos_data['pais']=='shile','pais'] = 'chile' terremotos_data.loc[terremotos_data['pais']=='Iran, 2005 Qeshm earthquake','pais'] = 'iran' terremotos_data.loc[terremotos_data['pais'].isnull(),'pais'] = 'sin_nombre' <p>Formato</p> In\u00a0[19]: Copied! <pre># correccion formato de nombre \nterremotos_data['pais'] = terremotos_data['pais'].str.lower().str.strip()\n</pre> # correccion formato de nombre  terremotos_data['pais'] = terremotos_data['pais'].str.lower().str.strip() <p>Filtro</p> In\u00a0[20]: Copied! <pre>mask_pais = terremotos_data['pais']!='sin_nombre'\n</pre> mask_pais = terremotos_data['pais']!='sin_nombre' In\u00a0[21]: Copied! <pre># bar plot: magnitud\n\nplt.style.use('default')\nplt.figure(figsize=(10,7))\n\nplotting = sns.countplot(\n    y=\"magnitud\",\n    data=terremotos_data,\n    order=terremotos_data['magnitud'].value_counts().index,\n    palette=\"Greys_d\",\n    linewidth=3\n)\n\nfor container in plotting.containers:\n    plotting.bar_label(container,fontsize=10)\n\n\nplt.show()\n</pre> # bar plot: magnitud  plt.style.use('default') plt.figure(figsize=(10,7))  plotting = sns.countplot(     y=\"magnitud\",     data=terremotos_data,     order=terremotos_data['magnitud'].value_counts().index,     palette=\"Greys_d\",     linewidth=3 )  for container in plotting.containers:     plotting.bar_label(container,fontsize=10)   plt.show() <p>Se presentan las siguientes anomalidades:</p> <ul> <li>Magnitudes sin importancia: Se ha establecido que las magnitudes de un terremoto son de 5 a 9.6.</li> <li>Datos con informaci\u00f3n comprimida: Debido a una inputaci\u00f3n incorrecta de los datos o a una mala lectura, la informaci\u00f3n se concentra en una celda.</li> <li>Datos vac\u00edo</li> </ul> <p>Correcci\u00f3n de las magnitudes</p> In\u00a0[22]: Copied! <pre># caso: tradicional\nterremotos_data.loc[terremotos_data['magnitud']=='2002-Tanzania-5.8','magnitud'] = '0'\nterremotos_data.loc[terremotos_data['magnitud']=='2003-japan-8.5','magnitud'] = '0'\nterremotos_data.loc[terremotos_data['magnitud'].isnull(),'magnitud'] = '0'\n</pre> # caso: tradicional terremotos_data.loc[terremotos_data['magnitud']=='2002-Tanzania-5.8','magnitud'] = '0' terremotos_data.loc[terremotos_data['magnitud']=='2003-japan-8.5','magnitud'] = '0' terremotos_data.loc[terremotos_data['magnitud'].isnull(),'magnitud'] = '0' In\u00a0[23]: Copied! <pre>terremotos_data.head()\n</pre> terremotos_data.head() Out[23]: a\u00f1o pais magnitud informacion 0 2000 turkey 6 info no valiosa 1 2000 turkmenistan 7 info no valiosa 2 2000 azerbaijan 6.5 info no valiosa 3 2000 azerbaijan 6.8 info no valiosa 4 2000 papua new guinea 8 info no valiosa In\u00a0[24]: Copied! <pre># caso: informacion comprimida\nterremotos_data.loc[-1] = [2002,'tanzania','5.8','-']\nterremotos_data.loc[-2] = [2003,'japan','8.5','-']\n</pre> # caso: informacion comprimida terremotos_data.loc[-1] = [2002,'tanzania','5.8','-'] terremotos_data.loc[-2] = [2003,'japan','8.5','-'] In\u00a0[25]: Copied! <pre>terremotos_data = terremotos_data.reset_index(drop=True)\nterremotos_data.tail()\n</pre> terremotos_data = terremotos_data.reset_index(drop=True) terremotos_data.tail() Out[25]: a\u00f1o pais magnitud informacion 225 0 sin_nombre 0 NaN 226 0 sin_nombre 0 NaN 227 2005 chile 8 valiosa 228 2002 tanzania 5.8 - 229 2003 japan 8.5 - <p>Correcci\u00f3n formato de las magnitudes</p> In\u00a0[26]: Copied! <pre>terremotos_data['magnitud'].unique()\n</pre> terremotos_data['magnitud'].unique() Out[26]: <pre>array(['6', '7', '6.5', '6.8', '8', '5.7', '6.4', '5.5', '6.3', '5.4',\n       '6.1', '6.7', '7.9', '7.2', '7.5', '5.3', '5.9', '9.7', '5.8',\n       '4.7', '7.6', '8.4', '5', '5.6', '6.6', '6.2', '7.1', '7.3', '5.1',\n       '5.2', '8.3', '6.9', '9.1', '4.9', '7.8', '8.6', '7.7', '7.4',\n       '8.5', '8.1', '8.8', '9', '-10', '0'], dtype=object)</pre> In\u00a0[27]: Copied! <pre>terremotos_data['magnitud'] = terremotos_data['magnitud'].astype(float)\nterremotos_data['magnitud'].unique()\n</pre> terremotos_data['magnitud'] = terremotos_data['magnitud'].astype(float) terremotos_data['magnitud'].unique() Out[27]: <pre>array([  6. ,   7. ,   6.5,   6.8,   8. ,   5.7,   6.4,   5.5,   6.3,\n         5.4,   6.1,   6.7,   7.9,   7.2,   7.5,   5.3,   5.9,   9.7,\n         5.8,   4.7,   7.6,   8.4,   5. ,   5.6,   6.6,   6.2,   7.1,\n         7.3,   5.1,   5.2,   8.3,   6.9,   9.1,   4.9,   7.8,   8.6,\n         7.7,   7.4,   8.5,   8.1,   8.8,   9. , -10. ,   0. ])</pre> <p>c) Filtro de las magnitudes</p> In\u00a0[28]: Copied! <pre>mask_mag_inf =  terremotos_data['magnitud']&gt;=5\nmask_mag_sup =  terremotos_data['magnitud']&lt;=9.6\n\nmask_mag = mask_mag_inf &amp; mask_mag_sup\n</pre> mask_mag_inf =  terremotos_data['magnitud']&gt;=5 mask_mag_sup =  terremotos_data['magnitud']&lt;=9.6  mask_mag = mask_mag_inf &amp; mask_mag_sup In\u00a0[29]: Copied! <pre>terremotos_data['informacion'].unique()\n</pre> terremotos_data['informacion'].unique() Out[29]: <pre>array(['info no valiosa', 'info valiosa', nan, 'valiosa', '-'],\n      dtype=object)</pre> <p>Se observa que esta columna no aporta informaci\u00f3n valiosa al estudio, quedando excluida para cualquier an\u00e1lisis importante.</p> In\u00a0[30]: Copied! <pre>#terremotos_data.drop('informacion', axis=1, inplace=True)\n</pre> #terremotos_data.drop('informacion', axis=1, inplace=True) In\u00a0[31]: Copied! <pre>terremotos_data.head()\n</pre> terremotos_data.head() Out[31]: a\u00f1o pais magnitud informacion 0 2000 turkey 6.0 info no valiosa 1 2000 turkmenistan 7.0 info no valiosa 2 2000 azerbaijan 6.5 info no valiosa 3 2000 azerbaijan 6.8 info no valiosa 4 2000 papua new guinea 8.0 info no valiosa <p>5. \u00bfSe puede sacar m\u00e1s provecho a los datos ?</p> <p>Una vez realizado toda la limpieza de datos, debemos filtrar la informaci\u00f3n que se considere importante.</p> <ul> <li>A\u00f1os: A\u00f1os desde el 2000 al 2011.</li> <li>Pa\u00eds: Paises con nombre distinto de sin_nombre</li> <li>Magnitud: Magnitud entre 5 y 9.6.</li> </ul> In\u00a0[32]: Copied! <pre># aplicar filtros\nterremotos_data_filtrado = terremotos_data[mask_anio &amp; mask_pais &amp; mask_mag]\nterremotos_data_filtrado.head()\n</pre> # aplicar filtros terremotos_data_filtrado = terremotos_data[mask_anio &amp; mask_pais &amp; mask_mag] terremotos_data_filtrado.head() Out[32]: a\u00f1o pais magnitud informacion 0 2000 turkey 6.0 info no valiosa 1 2000 turkmenistan 7.0 info no valiosa 2 2000 azerbaijan 6.5 info no valiosa 3 2000 azerbaijan 6.8 info no valiosa 4 2000 papua new guinea 8.0 info no valiosa <p>Veamos cu\u00e1nta informaci\u00f3n se perdio:</p> In\u00a0[33]: Copied! <pre>print('Cantidad de filas dataset sin filtro:',len(terremotos_data))\nprint('Cantidad de filas dataset con filtro:',len(terremotos_data_filtrado))\n</pre> print('Cantidad de filas dataset sin filtro:',len(terremotos_data)) print('Cantidad de filas dataset con filtro:',len(terremotos_data_filtrado)) <pre>Cantidad de filas dataset sin filtro: 230\nCantidad de filas dataset con filtro: 212\n</pre> <p>Ahora veamos el el resumen de la informaci\u00f3n para el nuevo dataset:</p> In\u00a0[34]: Copied! <pre>frames = []\n\nfor col in terremotos_data_filtrado.columns:\n    aux_df = resumen_por_columna(terremotos_data_filtrado,col)\n    frames.append(aux_df)\n    \ndf_info = pd.concat(frames).reset_index(drop=True)\ndf_info\n</pre> frames = []  for col in terremotos_data_filtrado.columns:     aux_df = resumen_por_columna(terremotos_data_filtrado,col)     frames.append(aux_df)      df_info = pd.concat(frames).reset_index(drop=True) df_info Out[34]: columna unicos vacios 0 a\u00f1o 12 0 1 pais 50 0 2 magnitud 39 0 3 informacion 3 0 <p>Finalmente, podemos responder la pregunta del inicio:</p> In\u00a0[35]: Copied! <pre># formato wide\nterremotos_data_filtrado.pivot_table(index=\"pais\", \n                                     columns=\"a\u00f1o\",\n                                     values=\"magnitud\", \n                                     fill_value='', \n                                     aggfunc=pd.np.max)\n</pre> # formato wide terremotos_data_filtrado.pivot_table(index=\"pais\",                                       columns=\"a\u00f1o\",                                      values=\"magnitud\",                                       fill_value='',                                       aggfunc=pd.np.max) <pre>C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_18544\\1854650910.py:6: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n  aggfunc=pd.np.max)\n</pre> Out[35]: a\u00f1o 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 pais afghanistan 6.3 5.0 7.3 5.8 6.5 6.5 algeria 5.7 6.8 5.2 5.5 argentina 7.2 6.1 azerbaijan 6.8 bangladesh 5.6 burma 6.8 chile 6.3 8.0 7.7 8.8 china 5.9 5.6 5.5 6.3 5.3 5.2 5.0 6.1 7.9 5.7 6.9 5.4 colombia 6.5 5.9 costa rica 6.4 6.1 democratic republic of the congo 6.2 5.9 dominican republic 6.4 ecuador 5.5 el salvador 7.6 greece 6.2 6.4 guadeloupe 6.3 guatemala 6.4 haiti 7.0 india 7.6 6.5 5.1 5.3 5.1 6.9 indonesia 7.9 7.5 6.9 9.1 8.6 7.7 8.5 7.3 7.6 iran 5.3 6.5 6.6 6.3 6.4 6.1 italy 5.9 6.2 japan 6.1 6.8 8.3 6.6 6.6 6.7 6.9 6.4 9.0 kazakhstan 6.0 kyrgyzstan 6.9 martinique 7.4 mexico 7.5 morocco 6.3 mozambique 7.0 new zealand 5.4 6.6 6.3 nicaragua 5.4 pakistan 6.3 5.4 7.6 5.2 6.4 panama 6.5 papua new guinea 8.0 7.6 6.1 peru 8.4 7.5 8.0 philippines 7.5 6.5 7.1 5.3 russian federation 7.3 6.2 rwanda 5.3 samoa 8.1 serbia 5.7 slovenia 5.2 solomon islands 8.1 taiwan 6.4 7.1 5.2 7.0 tajikistan 5.2 5.6 5.2 tanzania 6.4 5.5 6.8 turkey 6.0 6.5 6.3 5.6 5.9 6.1 7.1 turkmenistan 7.0 united states 6.8 6.6 venezuela 5.5 vietnam 5.3 <p>\u00bf Podemos sacar m\u00e1s informaci\u00f3n ?. Por supuesto que se puede, no obstante, siempre se debe ser preciso con la informaci\u00f3n que se</p> <p>Conclusi\u00f3n del caso</p> <ul> <li>El an\u00e1lisis exploratorio de datos (EDA) es una metodolog\u00eda que sirve para asegurarse de la calidad de los datos.</li> <li>A medida que se tiene m\u00e1s expertice en el tema, mejor es el an\u00e1lisis de datos y por tanto, mejor son los resultados obtenidos.</li> <li>No existe un procedimiento est\u00e1ndar para realizar el EDA, pero siempre se debe tener una claridad mental con:<ul> <li>Problema que se quiere resolverlo</li> <li>C\u00f3mo resolver el problema</li> <li>Posibles problemas de la muestra (datos perdidos, ouliers, etc.)</li> </ul> </li> </ul>"},{"location":"lectures/data_manipulation/eda_01/#caso-aplicado","title":"Caso Aplicado\u00b6","text":""},{"location":"lectures/data_manipulation/eda_01/#bases-del-experimento","title":"Bases del experimento\u00b6","text":"<p>Lo primero es identificar las variables que influyen en el estudio y la naturaleza de esta.</p> <ul> <li>Pa\u00eds:<ul> <li>Descripci\u00f3n: Pa\u00eds del evento s\u00edsmico.</li> <li>Tipo de dato: string</li> <li>Limitantes: No pueden haber nombre de ciudades, comunas o pueblos.</li> </ul> </li> <li>A\u00f1o:<ul> <li>Descripci\u00f3n: A\u00f1o del evento s\u00edsmico.</li> <li>Tipo de dato: integer.</li> <li>Limitantes: el a\u00f1o debe estar entre al a\u00f1o 2000 y 2011.</li> </ul> </li> <li>Magnitud:<ul> <li>Descripci\u00f3n: Magnitud del evento s\u00edsmico.</li> <li>Tipo de dato: float.</li> <li>Limitantes: la magnitud puede estar entre 5 y 9.6.</li> </ul> </li> </ul>"},{"location":"lectures/data_manipulation/eda_01/#conjunto-de-datos","title":"Conjunto de datos\u00b6","text":"<p>El conjunto de datos consta de cuatro columnas:</p> <ul> <li>Pa\u00eds</li> <li>A\u00f1o</li> <li>Magnitud</li> <li>Informaci\u00f3n</li> </ul>"},{"location":"lectures/data_manipulation/eda_01/#checklist-del-experimento","title":"Checklist del experimento\u00b6","text":"<p>Dado que conocemos el fen\u00f3meno en estudio, vayamos realizando un checklist de los procesos para hacer un correcto EDA.</p> <p>1. \u00bf Qu\u00e9 pregunta (s) est\u00e1s tratando de resolver (o probar que est\u00e1s equivocado)?</p> <p>El objetivo es encontrar el terremoto de mayor magnitud por pa\u00eds en los distintos a\u00f1os.</p> <p>2. \u00bf Qu\u00e9 tipo de datos tienes ?</p> <p>Los tipos de variables que tiene el conjunto de datos son:</p> <ul> <li>Categ\u00f3ricas: Pa\u00eds, Informaci\u00f3n.</li> <li>Num\u00e9ricas: A\u00f1o, Magnitud.</li> </ul> <p></p>"},{"location":"lectures/data_manipulation/eda_01/#columna-ano","title":"Columna: A\u00f1o\u00b6","text":"<p>Los a\u00f1os distintos en la muestra son:</p>"},{"location":"lectures/data_manipulation/eda_01/#columna-pais","title":"Columna: Pa\u00eds\u00b6","text":"<p>Los paises distintos en la muestra son:</p>"},{"location":"lectures/data_manipulation/eda_01/#columna-magnitud","title":"Columna: Magnitud\u00b6","text":"<p>Las magnitudes distintas en la muestra son:</p>"},{"location":"lectures/data_manipulation/eda_01/#columna-informacion","title":"Columna: Informaci\u00f3n\u00b6","text":"<p>La cantidad de elementos distintos para la columna informaci\u00f3n son:</p>"},{"location":"lectures/data_manipulation/eda_01/#solucion","title":"Soluci\u00f3n\u00b6","text":""},{"location":"lectures/data_manipulation/eda_01/#referencia","title":"Referencia\u00b6","text":"<ol> <li>Detailed exploratory data analysis with python</li> <li>Exploratory Data Analysis (EDA) and Data Visualization with Python</li> </ol>"},{"location":"lectures/data_manipulation/eda_intro/","title":"Introducci\u00f3n","text":"<p>El an\u00e1lisis exploratorio de datos es una forma de analizar datos definido por John W. Tukey (E.D.A.: Exploratory data analysis) es el tratamiento estad\u00edstico al que se someten las muestras recogidas durante un proceso de investigaci\u00f3n en cualquier campo cient\u00edfico. Para mayor rapidez y precisi\u00f3n, todo el proceso suele realizarse por medios inform\u00e1ticos, con aplicaciones espec\u00edficas para el tratamiento estad\u00edstico. </p> <p></p>"},{"location":"lectures/data_manipulation/eda_intro/#diagrama-de-flujo-del-proceso-de-ciencia-de-datos","title":"Diagrama de flujo del proceso de ciencia de datos","text":"<p>John W. Tukey escribi\u00f3 el libro An\u00e1lisis de datos exploratorios en 1977. Tukey sostuvo que se puso demasiado \u00e9nfasis en las estad\u00edsticas en las pruebas de hip\u00f3tesis estad\u00edsticas (an\u00e1lisis de datos confirmatorios); Se necesitaba poner m\u00e1s \u00e9nfasis en el uso de datos para sugerir hip\u00f3tesis para probar. En particular, sostuvo que confundir los dos tipos de an\u00e1lisis y emplearlos en el mismo conjunto de datos puede conducir a un sesgo sistem\u00e1tico debido a los problemas inherentes a las hip\u00f3tesis de prueba sugeridas por los datos.</p> <p>Los objetivos de EDA son:</p> <ul> <li>Sugerir hip\u00f3tesis sobre las causas de los fen\u00f3menos observados.</li> <li>Evaluar los supuestos en los que se basar\u00e1 la inferencia estad\u00edstica</li> <li>Apoyar la selecci\u00f3n de herramientas y t\u00e9cnicas estad\u00edsticas apropiadas.</li> <li>Proporcionar una base para una mayor recopilaci\u00f3n de datos a trav\u00e9s de encuestas o experimentos</li> </ul> <p>Muchas t\u00e9cnicas de EDA se han adoptado en la miner\u00eda de datos. Tambi\u00e9n se les est\u00e1 ense\u00f1ando a los j\u00f3venes estudiantes como una forma de presentarles el pensamiento estad\u00edstico.</p> <p></p>"},{"location":"lectures/data_manipulation/eda_intro/#datos-limpios-clean-dataset","title":"Datos limpios (clean dataset)","text":"<p>Los principios de datos limpios (Tidy Data de Hadley Wickham) proveen una manera est\u00e1ndar de organizar la informaci\u00f3n:</p> <ul> <li>Cada variable forma una columna.</li> <li>Cada observaci\u00f3n forma un rengl\u00f3n.</li> <li>Cada tipo de unidad observacional forma una tabla.</li> </ul> <p>Vale la pena notar que los principios de los datos limpios se pueden ver como teor\u00eda de algebra relacional para estad\u00edsticos, est\u00f3s principios equivalen a la tercera forma normal de Codd con enfoque en una sola tabla de datos en lugar de muchas conectadas en bases de datos relacionales.</p> <p>\u00bf Cu\u00e1l de las sigiuentes tablas se ajusta al principio de datos limpios ?. </p> <p>Caso 01</p> <p></p> <p>Caso 02</p> <p></p>"},{"location":"lectures/data_manipulation/eda_intro/#checklist-en-el-analisis-exploratorio-de-datos","title":"Checklist en el an\u00e1lisis exploratorio de datos","text":"<p>En el contexto de procesamiento de datos, el an\u00e1lisis exploratorio de datos deber\u00eda dar respuestas a las siguientes preguntas:</p> <ol> <li>\u00bfQu\u00e9 pregunta (s) est\u00e1s tratando de resolver (o probar que est\u00e1s equivocado)?</li> <li>\u00bfQu\u00e9 tipo de datos tiene y c\u00f3mo trata los diferentes tipos?</li> <li>\u00bfQu\u00e9 falta en los datos y c\u00f3mo los maneja?</li> <li>\u00bfQu\u00e9 hacer con los datos faltantes, outliers o informaci\u00f3n mal inputada?</li> <li>\u00bfSe puede sacar m\u00e1s provecho a los datos ?</li> </ol>"},{"location":"lectures/data_manipulation/pd_01/","title":"Pandas","text":"<p>Pandas es un paquete de Python que proporciona estructuras de datos r\u00e1pidas, flexibles y expresivas dise\u00f1adas para que trabajar con datos \"relacionales\" o \"etiquetados\" sea f\u00e1cil e intuitivo.</p> <p>Su objetivo es ser el bloque de construcci\u00f3n fundamental de alto nivel para hacer an\u00e1lisis de datos pr\u00e1cticos del mundo real en Python. Adem\u00e1s, tiene el objetivo m\u00e1s amplio de convertirse en la herramienta de an\u00e1lisis/manipulaci\u00f3n de datos de c\u00f3digo abierto m\u00e1s potente y flexible disponible en cualquier idioma. Ya est\u00e1 en camino hacia este objetivo.</p> <p>Series y DataFrames</p> <ul> <li><p>Las series son  arreglos unidimensionales con etiquetas. Se puede pensar como una generalizaci\u00f3n de los diccionarios de Python.</p> </li> <li><p>Los dataframe son arreglos bidimensionales y una extensi\u00f3n natural de las series. Se puede pensar como la generalizaci\u00f3n de un numpy.array.</p> </li> </ul> In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\n</pre> import pandas as pd import numpy as np In\u00a0[2]: Copied! <pre># Crear una serie con \u00edndices de letras\ndata = [10, 20, 30, 40, 50]\nindices = ['A', 'B', 'C', 'D', 'E']\nmy_serie = pd.Series(data, index=indices)\nprint(my_serie)\n</pre> # Crear una serie con \u00edndices de letras data = [10, 20, 30, 40, 50] indices = ['A', 'B', 'C', 'D', 'E'] my_serie = pd.Series(data, index=indices) print(my_serie) <pre>A    10\nB    20\nC    30\nD    40\nE    50\ndtype: int64\n</pre> In\u00a0[3]: Copied! <pre># tipo \nprint(\"type:\")\nprint( type(my_serie) )\n</pre> # tipo  print(\"type:\") print( type(my_serie) ) <pre>type:\n&lt;class 'pandas.core.series.Series'&gt;\n</pre> In\u00a0[4]: Copied! <pre># valores \nprint(\"values:\")\nprint(my_serie.values)\n</pre> # valores  print(\"values:\") print(my_serie.values) <pre>values:\n[10 20 30 40 50]\n</pre> In\u00a0[5]: Copied! <pre># indice\nprint(\"index:\")\nprint(my_serie.index)\n</pre> # indice print(\"index:\") print(my_serie.index) <pre>index:\nIndex(['A', 'B', 'C', 'D', 'E'], dtype='object')\n</pre> <ul> <li>El m\u00e9todo <code>loc</code> se utiliza para acceder y seleccionar datos utilizando etiquetas de \u00edndice.</li> <li>El m\u00e9todo <code>iloc</code> se utiliza para acceder y seleccionar datos utilizando \u00edndices enteros basados en la posici\u00f3n</li> </ul> In\u00a0[6]: Copied! <pre># acceder al valor de la serie: directo\nprint(\"direct:\")\nprint(my_serie['B'])\n</pre> # acceder al valor de la serie: directo print(\"direct:\") print(my_serie['B']) <pre>direct:\n20\n</pre> In\u00a0[7]: Copied! <pre># acceder al valor de la serie: loc\nprint(\"loc:\")\nprint(my_serie.loc['B'])\n</pre> # acceder al valor de la serie: loc print(\"loc:\") print(my_serie.loc['B']) <pre>loc:\n20\n</pre> In\u00a0[8]: Copied! <pre># acceder al valor de la serie: iloc\nprint(\"iloc:\")\nprint(my_serie.iloc[1])\n</pre> # acceder al valor de la serie: iloc print(\"iloc:\") print(my_serie.iloc[1]) <pre>iloc:\n20\n</pre> In\u00a0[9]: Copied! <pre># editar valores\nprint(\"edit:\")\nprint(\"\\nold 'D':\",my_serie.loc['D'] )\n</pre> # editar valores print(\"edit:\") print(\"\\nold 'D':\",my_serie.loc['D'] ) <pre>edit:\n\nold 'D': 40\n</pre> In\u00a0[10]: Copied! <pre>my_serie.loc['D'] = 1000\nprint(\"new 'D':\",my_serie.loc['D'] )\n</pre> my_serie.loc['D'] = 1000 print(\"new 'D':\",my_serie.loc['D'] ) <pre>new 'D': 1000\n</pre> In\u00a0[11]: Copied! <pre># ordenar valores\nprint(\"order:\")\nmy_serie.sort_values()\n</pre> # ordenar valores print(\"order:\") my_serie.sort_values() <pre>order:\n</pre> Out[11]: <pre>A      10\nB      20\nC      30\nE      50\nD    1000\ndtype: int64</pre> In\u00a0[12]: Copied! <pre># ordenar valores\nprint(\"order:\")\nmy_serie.sort_values(ascending  = False)\n</pre> # ordenar valores print(\"order:\") my_serie.sort_values(ascending  = False) <pre>order:\n</pre> Out[12]: <pre>D    1000\nE      50\nC      30\nB      20\nA      10\ndtype: int64</pre> <p>B\u00e1sicas</p> In\u00a0[13]: Copied! <pre>s1 = pd.Series([1,2,3,4,5])\ns2 = pd.Series([1,1,2,2,2])\n</pre> s1 = pd.Series([1,2,3,4,5]) s2 = pd.Series([1,1,2,2,2]) In\u00a0[14]: Copied! <pre># suma\nprint(f\"suma: \\n{s1+s2}\\n\")\n</pre> # suma print(f\"suma: \\n{s1+s2}\\n\") <pre>suma: \n0    2\n1    3\n2    5\n3    6\n4    7\ndtype: int64\n\n</pre> In\u00a0[15]: Copied! <pre># multiplicacion\nprint(f\"multiplicacion: \\n{s1*s2}\")\n</pre> # multiplicacion print(f\"multiplicacion: \\n{s1*s2}\") <pre>multiplicacion: \n0     1\n1     2\n2     6\n3     8\n4    10\ndtype: int64\n</pre> <p>Estad\u00edsticas</p> In\u00a0[16]: Copied! <pre># crear serie\ns1 = pd.Series([1,1,1,2,2,2,3,3,3,4,5,5,5,5])\nprint(f\"max:    {s1.max()}\") # maximo\nprint(f\"min:    {s1.min()}\") # minimo\nprint(f\"mean:   {s1.mean()}\") # promedio\nprint(f\"median: {s1.median()}\") # mediana\n</pre> # crear serie s1 = pd.Series([1,1,1,2,2,2,3,3,3,4,5,5,5,5]) print(f\"max:    {s1.max()}\") # maximo print(f\"min:    {s1.min()}\") # minimo print(f\"mean:   {s1.mean()}\") # promedio print(f\"median: {s1.median()}\") # mediana <pre>max:    5\nmin:    1\nmean:   3.0\nmedian: 3.0\n</pre> In\u00a0[17]: Copied! <pre># funcion describe\ns1.describe()\n</pre> # funcion describe s1.describe() Out[17]: <pre>count    14.000000\nmean      3.000000\nstd       1.568929\nmin       1.000000\n25%       2.000000\n50%       3.000000\n75%       4.750000\nmax       5.000000\ndtype: float64</pre> <p>Conteos</p> In\u00a0[18]: Copied! <pre># funcion count_values\ns2 = pd.Series([\"a\",\"a\",\"b\",\"c\",\"c\",\"c\",\"c\",\"d\",])\nprint(f\"Conteo:\\n{s2.value_counts()}\")\n</pre> # funcion count_values s2 = pd.Series([\"a\",\"a\",\"b\",\"c\",\"c\",\"c\",\"c\",\"d\",]) print(f\"Conteo:\\n{s2.value_counts()}\") <pre>Conteo:\nc    4\na    2\nb    1\nd    1\ndtype: int64\n</pre> In\u00a0[19]: Copied! <pre># 1. definir valor maximo \nn_max = s1.max()\n</pre> # 1. definir valor maximo  n_max = s1.max() In\u00a0[20]: Copied! <pre># 2.- definir \"mask\" que busca el valor objetivo\nmask = (s1 == n_max)\n</pre> # 2.- definir \"mask\" que busca el valor objetivo mask = (s1 == n_max) In\u00a0[21]: Copied! <pre># 3.- aplicar mask sobre la serie\ns1[mask]\n</pre> # 3.- aplicar mask sobre la serie s1[mask] Out[21]: <pre>10    5\n11    5\n12    5\n13    5\ndtype: int64</pre> In\u00a0[22]: Copied! <pre># crear serie\ns_null = pd.Series([1,2,np.nan,4,5,6,7,np.nan,9])\ns_null\n</pre> # crear serie s_null = pd.Series([1,2,np.nan,4,5,6,7,np.nan,9]) s_null Out[22]: <pre>0    1.0\n1    2.0\n2    NaN\n3    4.0\n4    5.0\n5    6.0\n6    7.0\n7    NaN\n8    9.0\ndtype: float64</pre> In\u00a0[23]: Copied! <pre># mask valores nulos\nprint(\"is null?:\\n\")\nprint(s_null.isnull() )\n</pre> # mask valores nulos print(\"is null?:\\n\") print(s_null.isnull() ) <pre>is null?:\n\n0    False\n1    False\n2     True\n3    False\n4    False\n5    False\n6    False\n7     True\n8    False\ndtype: bool\n</pre> In\u00a0[24]: Copied! <pre># filtrar valores nulos\nprint(\"null serie: \\n\")\nprint(s_null[s_null.isnull()] )\n</pre> # filtrar valores nulos print(\"null serie: \\n\") print(s_null[s_null.isnull()] ) <pre>null serie: \n\n2   NaN\n7   NaN\ndtype: float64\n</pre> <p>Encontrar valores no nulos</p> In\u00a0[25]: Copied! <pre># imprimir serie\nprint(\"serie:\")\nprint( s_null )\n</pre> # imprimir serie print(\"serie:\") print( s_null ) <pre>serie:\n0    1.0\n1    2.0\n2    NaN\n3    4.0\n4    5.0\n5    6.0\n6    7.0\n7    NaN\n8    9.0\ndtype: float64\n</pre> In\u00a0[26]: Copied! <pre># mask valores no nulos\nprint(\"\\nis not null?:\")\nprint(s_null.notnull() )\n</pre> # mask valores no nulos print(\"\\nis not null?:\") print(s_null.notnull() ) <pre>\nis not null?:\n0     True\n1     True\n2    False\n3     True\n4     True\n5     True\n6     True\n7    False\n8     True\ndtype: bool\n</pre> In\u00a0[27]: Copied! <pre># filtrar valores no nulos\nprint(\"\\nserie with not null values\")\nprint(s_null[s_null.notnull()] )\n</pre> # filtrar valores no nulos print(\"\\nserie with not null values\") print(s_null[s_null.notnull()] ) <pre>\nserie with not null values\n0    1.0\n1    2.0\n3    4.0\n4    5.0\n5    6.0\n6    7.0\n8    9.0\ndtype: float64\n</pre> <p>La pregunta que nos queda hacer es: \u00bf Qu\u00e9 se debe hacer con los valores nulos ?, la respuesta es depende.</p> <ul> <li>Si tenemos muchos datos, lo m\u00e1s probable es que se puedan eliminar estos datos sin culpa.</li> <li>Si se tienen poco datos, lo m\u00e1s probable es que se necesite inputar un valor por defecto a los valores nulos (ejemplo: el promedio).</li> </ul> <p>Crear Serie de fechas</p> In\u00a0[28]: Copied! <pre># crear serie de fechas\ndate_rng = pd.date_range(start='1/1/2019', end='1/03/2019', freq='4H')\n</pre> # crear serie de fechas date_rng = pd.date_range(start='1/1/2019', end='1/03/2019', freq='4H') In\u00a0[29]: Copied! <pre># imprimir serie\nprint(\"serie:\")\nprint( date_rng )\n</pre> # imprimir serie print(\"serie:\") print( date_rng ) <pre>serie:\nDatetimeIndex(['2019-01-01 00:00:00', '2019-01-01 04:00:00',\n               '2019-01-01 08:00:00', '2019-01-01 12:00:00',\n               '2019-01-01 16:00:00', '2019-01-01 20:00:00',\n               '2019-01-02 00:00:00', '2019-01-02 04:00:00',\n               '2019-01-02 08:00:00', '2019-01-02 12:00:00',\n               '2019-01-02 16:00:00', '2019-01-02 20:00:00',\n               '2019-01-03 00:00:00'],\n              dtype='datetime64[ns]', freq='4H')\n</pre> In\u00a0[30]: Copied! <pre># tipo \nprint(\"type:\\n\")\nprint( type(date_rng) )\n</pre> # tipo  print(\"type:\\n\") print( type(date_rng) ) <pre>type:\n\n&lt;class 'pandas.core.indexes.datetimes.DatetimeIndex'&gt;\n</pre> <p>Atributos de las fechas</p> In\u00a0[31]: Copied! <pre># obtener fechas\nprint(\"date:\\n\")\nprint(date_rng.date)\n\n# obtener tiempo\nprint(\"\\nhour:\\n\")\nprint(date_rng.time)\n</pre> # obtener fechas print(\"date:\\n\") print(date_rng.date)  # obtener tiempo print(\"\\nhour:\\n\") print(date_rng.time) <pre>date:\n\n[datetime.date(2019, 1, 1) datetime.date(2019, 1, 1)\n datetime.date(2019, 1, 1) datetime.date(2019, 1, 1)\n datetime.date(2019, 1, 1) datetime.date(2019, 1, 1)\n datetime.date(2019, 1, 2) datetime.date(2019, 1, 2)\n datetime.date(2019, 1, 2) datetime.date(2019, 1, 2)\n datetime.date(2019, 1, 2) datetime.date(2019, 1, 2)\n datetime.date(2019, 1, 3)]\n\nhour:\n\n[datetime.time(0, 0) datetime.time(4, 0) datetime.time(8, 0)\n datetime.time(12, 0) datetime.time(16, 0) datetime.time(20, 0)\n datetime.time(0, 0) datetime.time(4, 0) datetime.time(8, 0)\n datetime.time(12, 0) datetime.time(16, 0) datetime.time(20, 0)\n datetime.time(0, 0)]\n</pre> <p>Fechas y string</p> In\u00a0[32]: Copied! <pre># elementos de datetime a string \nstring_date_rng = [str(x) for x in date_rng]\n\nprint(\"datetime to string: \\n\")\nprint( np.array(string_date_rng) )\n</pre> # elementos de datetime a string  string_date_rng = [str(x) for x in date_rng]  print(\"datetime to string: \\n\") print( np.array(string_date_rng) ) <pre>datetime to string: \n\n['2019-01-01 00:00:00' '2019-01-01 04:00:00' '2019-01-01 08:00:00'\n '2019-01-01 12:00:00' '2019-01-01 16:00:00' '2019-01-01 20:00:00'\n '2019-01-02 00:00:00' '2019-01-02 04:00:00' '2019-01-02 08:00:00'\n '2019-01-02 12:00:00' '2019-01-02 16:00:00' '2019-01-02 20:00:00'\n '2019-01-03 00:00:00']\n</pre> In\u00a0[33]: Copied! <pre># elementos de string a datetime \ntimestamp_date_rng = pd.to_datetime(string_date_rng, infer_datetime_format=True)\n\nprint(\"string to datetime:\\n\")\nprint( timestamp_date_rng )\n</pre> # elementos de string a datetime  timestamp_date_rng = pd.to_datetime(string_date_rng, infer_datetime_format=True)  print(\"string to datetime:\\n\") print( timestamp_date_rng ) <pre>string to datetime:\n\nDatetimeIndex(['2019-01-01 00:00:00', '2019-01-01 04:00:00',\n               '2019-01-01 08:00:00', '2019-01-01 12:00:00',\n               '2019-01-01 16:00:00', '2019-01-01 20:00:00',\n               '2019-01-02 00:00:00', '2019-01-02 04:00:00',\n               '2019-01-02 08:00:00', '2019-01-02 12:00:00',\n               '2019-01-02 16:00:00', '2019-01-02 20:00:00',\n               '2019-01-03 00:00:00'],\n              dtype='datetime64[ns]', freq=None)\n</pre> In\u00a0[34]: Copied! <pre># empty dataframe\ndf_empty = pd.DataFrame()\ndf_empty\n</pre> # empty dataframe df_empty = pd.DataFrame() df_empty Out[34]: In\u00a0[35]: Copied! <pre># dataframe with list\ndf_list = pd.DataFrame(\n    [\n        [\"nombre_01\", \"apellido_01\", 60],\n        [\"nombre_02\", \"apellido_02\", 14]\n    ],\n    \n    columns = [\"nombre\", \"apellido\", \"edad\"]\n)\ndf_list\n</pre> # dataframe with list df_list = pd.DataFrame(     [         [\"nombre_01\", \"apellido_01\", 60],         [\"nombre_02\", \"apellido_02\", 14]     ],          columns = [\"nombre\", \"apellido\", \"edad\"] ) df_list Out[35]: nombre apellido edad 0 nombre_01 apellido_01 60 1 nombre_02 apellido_02 14 In\u00a0[36]: Copied! <pre># dataframe with dct\ndf_dct =  pd.DataFrame(\n    {\n        \"nombre\": [\"nombre_01\", \"nombre_02\",],\n        \"apellido\": [\"apellido_01\", \"apellido_02\"],\n        \"edad\": np.array([60,14]),\n    }\n)\n\ndf_dct\n</pre> # dataframe with dct df_dct =  pd.DataFrame(     {         \"nombre\": [\"nombre_01\", \"nombre_02\",],         \"apellido\": [\"apellido_01\", \"apellido_02\"],         \"edad\": np.array([60,14]),     } )  df_dct Out[36]: nombre apellido edad 0 nombre_01 apellido_01 60 1 nombre_02 apellido_02 14 In\u00a0[37]: Copied! <pre># load data\n\nurl='https://drive.google.com/file/d/1V7k6JxEn7aJuWHmQH0ZxzbZTNZJeMkvs/view?usp=drive_link'\nurl='https://drive.google.com/uc?id=' + url.split('/')[-2]\n\nplayer_data = pd.read_csv(url, sep=\",\" )\nplayer_data\n</pre> # load data  url='https://drive.google.com/file/d/1V7k6JxEn7aJuWHmQH0ZxzbZTNZJeMkvs/view?usp=drive_link' url='https://drive.google.com/uc?id=' + url.split('/')[-2]  player_data = pd.read_csv(url, sep=\",\" ) player_data Out[37]: name year_start year_end position height weight birth_date college 0 Alaa Abdelnaby 1991 1995 F-C 6-10 240.0 June 24, 1968 Duke University 1 Zaid Abdul-Aziz 1969 1978 C-F 6-9 235.0 April 7, 1946 Iowa State University 2 Kareem Abdul-Jabbar 1970 1989 C 7-2 225.0 April 16, 1947 University of California, Los Angeles 3 Mahmoud Abdul-Rauf 1991 2001 G 6-1 162.0 March 9, 1969 Louisiana State University 4 Tariq Abdul-Wahad 1998 2003 F 6-6 223.0 November 3, 1974 San Jose State University ... ... ... ... ... ... ... ... ... 4545 Ante Zizic 2018 2018 F-C 6-11 250.0 January 4, 1997 NaN 4546 Jim Zoet 1983 1983 C 7-1 240.0 December 20, 1953 Kent State University 4547 Bill Zopf 1971 1971 G 6-1 170.0 June 7, 1948 Duquesne University 4548 Ivica Zubac 2017 2018 C 7-1 265.0 March 18, 1997 NaN 4549 Matt Zunic 1949 1949 G-F 6-3 195.0 December 19, 1919 George Washington University <p>4550 rows \u00d7 8 columns</p> In\u00a0[38]: Copied! <pre># primeras silas\nprint(\"first 5 rows:\")\nplayer_data.head(5)\n</pre> # primeras silas print(\"first 5 rows:\") player_data.head(5) <pre>first 5 rows:\n</pre> Out[38]: name year_start year_end position height weight birth_date college 0 Alaa Abdelnaby 1991 1995 F-C 6-10 240.0 June 24, 1968 Duke University 1 Zaid Abdul-Aziz 1969 1978 C-F 6-9 235.0 April 7, 1946 Iowa State University 2 Kareem Abdul-Jabbar 1970 1989 C 7-2 225.0 April 16, 1947 University of California, Los Angeles 3 Mahmoud Abdul-Rauf 1991 2001 G 6-1 162.0 March 9, 1969 Louisiana State University 4 Tariq Abdul-Wahad 1998 2003 F 6-6 223.0 November 3, 1974 San Jose State University In\u00a0[39]: Copied! <pre># ultimas filas\nprint(\"\\nlast 5 rows:\")\nplayer_data.tail(5)\n</pre> # ultimas filas print(\"\\nlast 5 rows:\") player_data.tail(5) <pre>\nlast 5 rows:\n</pre> Out[39]: name year_start year_end position height weight birth_date college 4545 Ante Zizic 2018 2018 F-C 6-11 250.0 January 4, 1997 NaN 4546 Jim Zoet 1983 1983 C 7-1 240.0 December 20, 1953 Kent State University 4547 Bill Zopf 1971 1971 G 6-1 170.0 June 7, 1948 Duquesne University 4548 Ivica Zubac 2017 2018 C 7-1 265.0 March 18, 1997 NaN 4549 Matt Zunic 1949 1949 G-F 6-3 195.0 December 19, 1919 George Washington University In\u00a0[40]: Copied! <pre># tipo\nprint(\"\\ntype of dataframe:\")\ntype(player_data)\n</pre> # tipo print(\"\\ntype of dataframe:\") type(player_data) <pre>\ntype of dataframe:\n</pre> Out[40]: <pre>pandas.core.frame.DataFrame</pre> In\u00a0[41]: Copied! <pre># tipo por columnas\nprint(\"\\ntype of columns:\")\nplayer_data.dtypes\n</pre> # tipo por columnas print(\"\\ntype of columns:\") player_data.dtypes <pre>\ntype of columns:\n</pre> Out[41]: <pre>name           object\nyear_start      int64\nyear_end        int64\nposition       object\nheight         object\nweight        float64\nbirth_date     object\ncollege        object\ndtype: object</pre> In\u00a0[42]: Copied! <pre># dimension\nprint(\"\\nshape:\")\nplayer_data.shape\n</pre> # dimension print(\"\\nshape:\") player_data.shape <pre>\nshape:\n</pre> Out[42]: <pre>(4550, 8)</pre> In\u00a0[43]: Copied! <pre># nombre de las columnas\nprint(\"\\ncols:\")\nplayer_data.columns\n</pre> # nombre de las columnas print(\"\\ncols:\") player_data.columns <pre>\ncols:\n</pre> Out[43]: <pre>Index(['name', 'year_start', 'year_end', 'position', 'height', 'weight',\n       'birth_date', 'college'],\n      dtype='object')</pre> In\u00a0[44]: Copied! <pre># indice\nprint(\"\\nindex:\")\nplayer_data.index\n</pre> # indice print(\"\\nindex:\") player_data.index <pre>\nindex:\n</pre> Out[44]: <pre>RangeIndex(start=0, stop=4550, step=1)</pre> In\u00a0[45]: Copied! <pre># acceder a la columna posicion\nprint(\"\\ncolumn 'position': \")\nplayer_data['position'].head()\n</pre> # acceder a la columna posicion print(\"\\ncolumn 'position': \") player_data['position'].head() <pre>\ncolumn 'position': \n</pre> Out[45]: <pre>0    F-C\n1    C-F\n2      C\n3      G\n4      F\nName: position, dtype: object</pre> In\u00a0[46]: Copied! <pre># cambiar nombre de una o varias columnas\nplayer_data = player_data.rename(columns={\"birth_date\": \"Birth\", \"college\": \"College\"})\nplayer_data.head()\n</pre> # cambiar nombre de una o varias columnas player_data = player_data.rename(columns={\"birth_date\": \"Birth\", \"college\": \"College\"}) player_data.head() Out[46]: name year_start year_end position height weight Birth College 0 Alaa Abdelnaby 1991 1995 F-C 6-10 240.0 June 24, 1968 Duke University 1 Zaid Abdul-Aziz 1969 1978 C-F 6-9 235.0 April 7, 1946 Iowa State University 2 Kareem Abdul-Jabbar 1970 1989 C 7-2 225.0 April 16, 1947 University of California, Los Angeles 3 Mahmoud Abdul-Rauf 1991 2001 G 6-1 162.0 March 9, 1969 Louisiana State University 4 Tariq Abdul-Wahad 1998 2003 F 6-6 223.0 November 3, 1974 San Jose State University In\u00a0[47]: Copied! <pre># fijar columna especifica como indice\nplayer_data = player_data.set_index([\"name\"])\nplayer_data.head()\n</pre> # fijar columna especifica como indice player_data = player_data.set_index([\"name\"]) player_data.head() Out[47]: year_start year_end position height weight Birth College name Alaa Abdelnaby 1991 1995 F-C 6-10 240.0 June 24, 1968 Duke University Zaid Abdul-Aziz 1969 1978 C-F 6-9 235.0 April 7, 1946 Iowa State University Kareem Abdul-Jabbar 1970 1989 C 7-2 225.0 April 16, 1947 University of California, Los Angeles Mahmoud Abdul-Rauf 1991 2001 G 6-1 162.0 March 9, 1969 Louisiana State University Tariq Abdul-Wahad 1998 2003 F 6-6 223.0 November 3, 1974 San Jose State University In\u00a0[48]: Copied! <pre># ordenar dataframe por columna especifica\nplayer_data = player_data.sort_values(\"weight\")\nplayer_data.head()\n</pre> # ordenar dataframe por columna especifica player_data = player_data.sort_values(\"weight\") player_data.head() Out[48]: year_start year_end position height weight Birth College name Penny Early 1969 1969 G 5-3 114.0 May 30, 1943 NaN Spud Webb 1986 1998 G 5-6 133.0 July 13, 1963 North Carolina State University Earl Boykins 1999 2012 G 5-5 135.0 June 2, 1976 Eastern Michigan University Muggsy Bogues 1988 2001 G 5-3 136.0 January 9, 1965 Wake Forest University Chet Aubuchon 1947 1947 G 5-10 137.0 May 18, 1916 Michigan State University In\u00a0[49]: Copied! <pre># resumen de la informaci\u00f3n \nplayer_data.describe(include='all')# player_data.describe()\n</pre> # resumen de la informaci\u00f3n  player_data.describe(include='all')# player_data.describe() Out[49]: year_start year_end position height weight Birth College count 4550.000000 4550.000000 4549 4549 4544.000000 4519 4248 unique NaN NaN 7 28 NaN 4161 473 top NaN NaN G 6-7 NaN February 23, 1945 University of Kentucky freq NaN NaN 1574 473 NaN 3 99 mean 1985.076264 1989.272527 NaN NaN 208.908011 NaN NaN std 20.974188 21.874761 NaN NaN 26.268662 NaN NaN min 1947.000000 1947.000000 NaN NaN 114.000000 NaN NaN 25% 1969.000000 1973.000000 NaN NaN 190.000000 NaN NaN 50% 1986.000000 1992.000000 NaN NaN 210.000000 NaN NaN 75% 2003.000000 2009.000000 NaN NaN 225.000000 NaN NaN max 2018.000000 2018.000000 NaN NaN 360.000000 NaN NaN <p>a) Determine si el dataframe tiene valores nulos</p> In\u00a0[50]: Copied! <pre># veamos las columnas con datos nulos\nfor col in player_data.columns:\n   \n    temp = player_data[player_data[col].isnull()]\n    print(col)\n    print(f\"valores nulos: {len(temp)}\\n\")\n</pre> # veamos las columnas con datos nulos for col in player_data.columns:         temp = player_data[player_data[col].isnull()]     print(col)     print(f\"valores nulos: {len(temp)}\\n\") <pre>year_start\nvalores nulos: 0\n\nyear_end\nvalores nulos: 0\n\nposition\nvalores nulos: 1\n\nheight\nvalores nulos: 1\n\nweight\nvalores nulos: 6\n\nBirth\nvalores nulos: 31\n\nCollege\nvalores nulos: 302\n\n</pre> In\u00a0[51]: Copied! <pre># ocupar comando .notnull().all(axis=1) para ver todos los valores que NO son nulos para todas las columnas\nplayer_data.notnull().all(axis=1).head(10)\n</pre> # ocupar comando .notnull().all(axis=1) para ver todos los valores que NO son nulos para todas las columnas player_data.notnull().all(axis=1).head(10) Out[51]: <pre>name\nPenny Early        False\nSpud Webb           True\nEarl Boykins        True\nMuggsy Bogues       True\nChet Aubuchon       True\nGreg Grant          True\nAngelo Musi         True\nErnie Calverley     True\nTyler Ulis          True\nLionel Malamed      True\ndtype: bool</pre> <p>b) Elimine los valores nulos del dataframe</p> In\u00a0[52]: Copied! <pre># ocupar masking\nmask = lambda df: df.notnull().all(axis=1)\nplayer_data = player_data[mask]\nplayer_data.head()\n</pre> # ocupar masking mask = lambda df: df.notnull().all(axis=1) player_data = player_data[mask] player_data.head() Out[52]: year_start year_end position height weight Birth College name Spud Webb 1986 1998 G 5-6 133.0 July 13, 1963 North Carolina State University Earl Boykins 1999 2012 G 5-5 135.0 June 2, 1976 Eastern Michigan University Muggsy Bogues 1988 2001 G 5-3 136.0 January 9, 1965 Wake Forest University Chet Aubuchon 1947 1947 G 5-10 137.0 May 18, 1916 Michigan State University Greg Grant 1990 1996 G 5-7 140.0 August 29, 1966 Trenton State University <p>c) Determinar el tiempo (en a\u00f1os) de cada jugador en su posici\u00f3n</p> In\u00a0[53]: Copied! <pre>player_data['duration'] = player_data['year_end'] - player_data['year_start']\nplayer_data.head()\n</pre> player_data['duration'] = player_data['year_end'] - player_data['year_start'] player_data.head() Out[53]: year_start year_end position height weight Birth College duration name Spud Webb 1986 1998 G 5-6 133.0 July 13, 1963 North Carolina State University 12 Earl Boykins 1999 2012 G 5-5 135.0 June 2, 1976 Eastern Michigan University 13 Muggsy Bogues 1988 2001 G 5-3 136.0 January 9, 1965 Wake Forest University 13 Chet Aubuchon 1947 1947 G 5-10 137.0 May 18, 1916 Michigan State University 0 Greg Grant 1990 1996 G 5-7 140.0 August 29, 1966 Trenton State University 6 <p>d) Castear la fecha de str a objeto datetime</p> In\u00a0[54]: Copied! <pre>player_data['birth_date_dt'] = pd.to_datetime(player_data['Birth'], format=\"%B %d, %Y\")\nplayer_data.head()\n</pre> player_data['birth_date_dt'] = pd.to_datetime(player_data['Birth'], format=\"%B %d, %Y\") player_data.head() Out[54]: year_start year_end position height weight Birth College duration birth_date_dt name Spud Webb 1986 1998 G 5-6 133.0 July 13, 1963 North Carolina State University 12 1963-07-13 Earl Boykins 1999 2012 G 5-5 135.0 June 2, 1976 Eastern Michigan University 13 1976-06-02 Muggsy Bogues 1988 2001 G 5-3 136.0 January 9, 1965 Wake Forest University 13 1965-01-09 Chet Aubuchon 1947 1947 G 5-10 137.0 May 18, 1916 Michigan State University 0 1916-05-18 Greg Grant 1990 1996 G 5-7 140.0 August 29, 1966 Trenton State University 6 1966-08-29 <p>e) Determinar todas las posiciones</p> In\u00a0[55]: Copied! <pre>positions = player_data['position'].unique()\npositions\n</pre> positions = player_data['position'].unique() positions Out[55]: <pre>array(['G', 'G-F', 'F-G', 'F', 'F-C', 'C-F', 'C'], dtype=object)</pre> <p>f) Iterar sobre cada posici\u00f3n y encontrar el mayor valor asociado a la columna <code>weight</code></p> In\u00a0[56]: Copied! <pre># Iterar sobre cada posici\u00f3n y encontrar el mayor valor.\nnba_position_duration = dict()\n\n# iterar\nfor position in positions:\n    \n    # filtrar \n    df_aux = player_data.loc[lambda x: x['position'] == position]\n    \n    # encontrar maximo de la columna objetivo\n    max_duration = df_aux['weight'].max()\n    \n    # guardar en pd.Series\n    nba_position_duration[position] = max_duration\n    \n# retornar serie\nnba_position_duration\n</pre> # Iterar sobre cada posici\u00f3n y encontrar el mayor valor. nba_position_duration = dict()  # iterar for position in positions:          # filtrar      df_aux = player_data.loc[lambda x: x['position'] == position]          # encontrar maximo de la columna objetivo     max_duration = df_aux['weight'].max()          # guardar en pd.Series     nba_position_duration[position] = max_duration      # retornar serie nba_position_duration Out[56]: <pre>{'G': 235.0,\n 'G-F': 240.0,\n 'F-G': 245.0,\n 'F': 284.0,\n 'F-C': 290.0,\n 'C-F': 280.0,\n 'C': 360.0}</pre>"},{"location":"lectures/data_manipulation/pd_01/#pandas","title":"Pandas\u00b6","text":""},{"location":"lectures/data_manipulation/pd_01/#pandas-series","title":"Pandas Series\u00b6","text":""},{"location":"lectures/data_manipulation/pd_01/#trabajando-con-pandasseries","title":"Trabajando con  pandas.Series\u00b6","text":""},{"location":"lectures/data_manipulation/pd_01/#operaciones-matematicas","title":"Operaciones matem\u00e1ticas\u00b6","text":"<p>Al igual que numpy, las series de pandas pueden realizar operaciones matem\u00e1ticas similares (mientr\u00e1s los arreglos a operar sean del tipo num\u00e9rico). Por otro lado existen otras funciones de utilidad.</p>"},{"location":"lectures/data_manipulation/pd_01/#filtro","title":"Filtro\u00b6","text":"<p>En Pandas, puedes aplicar filtros o m\u00e1scaras a un DataFrame utilizando operaciones l\u00f3gicas y comparativas. Esto te permite seleccionar y filtrar las filas que cumplan con ciertas condiciones espec\u00edficas.</p>"},{"location":"lectures/data_manipulation/pd_01/#valores-nulos-o-datos-perdidos","title":"Valores Nulos o datos perdidos\u00b6","text":"<p>En algunas ocaciones, los arreglos no tienen informaci\u00f3n en una determinada posici\u00f3n, lo cual puede ser perjudicial si no se tiene control sobre estos valores.</p> <p>Encontrar valores nulos</p>"},{"location":"lectures/data_manipulation/pd_01/#manejo-de-fechas","title":"Manejo de Fechas\u00b6","text":"<p>Pandas tambi\u00e9n trae m\u00f3dulos para trabajar el formato de fechas.</p>"},{"location":"lectures/data_manipulation/pd_01/#pandas-dataframes","title":"Pandas Dataframes\u00b6","text":""},{"location":"lectures/data_manipulation/pd_01/#trabajando-con-pandasdataframes","title":"Trabajando  con pandas.DataFrames\u00b6","text":"<p>Como se mencina anteriormente, los dataframes son arreglos de series, los cuales pueden ser de distintos tipos (num\u00e9ricos, string, etc.). En esta parte mostraremos un ejemplo aplicado de las distintas funcionalidades de los dataframes.</p>"},{"location":"lectures/data_manipulation/pd_01/#creacion-de-dataframes","title":"Creaci\u00f3n de dataframes\u00b6","text":"<p>La creaci\u00f3n se puede hacer de variadas formas con listas, dictionarios , numpy array , entre otros.</p>"},{"location":"lectures/data_manipulation/pd_01/#lectura-de-datos-con-dataframes","title":"Lectura de datos con dataframes\u00b6","text":"<p>En general, cuando se trabajan con datos, estos se almacenan en alg\u00fan lugar y en alg\u00fan tipo de formato, por ejemplo:</p> <ul> <li><code>.txt</code></li> <li><code>.csv</code></li> <li><code>.xlsx</code></li> <li><code>.db</code></li> <li>etc.</li> </ul> <p>Para cada formato, existe un m\u00f3dulo para realizar la lectura de datos. En este caso, se analiza el conjunto de datos 'player_data.csv', el cual muestra informacion b\u00e1sica de algunos jugadores de la NBA.</p> <p></p>"},{"location":"lectures/data_manipulation/pd_01/#modulos-basicos","title":"M\u00f3dulos b\u00e1sicos\u00b6","text":"<p>Existen m\u00f3dulos para comprender r\u00e1pidamente la naturaleza del dataframe.</p>"},{"location":"lectures/data_manipulation/pd_01/#operando-sobre-dataframes","title":"Operando sobre Dataframes\u00b6","text":"<p>Cuando se trabaja con un conjunto de datos, se crea una din\u00e1mica de preguntas y respuestas, en donde a medida que necesito informaci\u00f3n, se va accediendo al dataframe. En algunas ocaciones es directo, basta un simple m\u00f3dulo, aunque en otras ser\u00e1 necesaria realizar operaciones un poco m\u00e1s complejas.</p> <p>Por ejemplo, del conjunto de datos en estudio, se esta interesado en responder las siguientes preguntas:</p>"},{"location":"lectures/data_manipulation/pd_01/#referencias","title":"Referencias\u00b6","text":"<ol> <li>Python Pandas Tutorial: A Complete Introduction for Beginners</li> <li>General functions</li> </ol>"},{"location":"lectures/data_manipulation/pd_02/","title":"Groupby","text":"<p>Groupby es un concepto bastante simple. Podemos crear una agrupaci\u00f3n de categor\u00edas y aplicar una funci\u00f3n a las categor\u00edas.</p> <p>El proceso de groupby se puede resumiren los siguientes pasos:</p> <ul> <li>Divisi\u00f3n: es un proceso en el que dividimos los datos en grupos aplicando algunas condiciones en los conjuntos de datos.</li> <li>Aplicaci\u00f3n: es un proceso en el que aplicamos una funci\u00f3n a cada grupo de forma independiente</li> <li>Combinaci\u00f3n: es un proceso en el que combinamos diferentes conjuntos de datos despu\u00e9s de aplicar groupby y resultados en una estructura de datos</li> </ul> <p></p> <p>Despu\u00e9s de dividir los datos en un grupo, aplicamos una funci\u00f3n a cada grupo para realizar algunas operaciones que son:</p> <ul> <li>Agregaci\u00f3n: es un proceso en el que calculamos una estad\u00edstica resumida (o estad\u00edstica) sobre cada grupo. Por ejemplo, Calcular sumas de grupo o medios</li> <li>Transformaci\u00f3n: es un proceso en el que realizamos algunos c\u00e1lculos espec\u00edficos del grupo y devolvemos un \u00edndice similar. Por ejemplo, llenar NA dentro de grupos con un valor derivado de cada grupo</li> <li>Filtraci\u00f3n: es un proceso en el cual descartamos algunos grupos, de acuerdo con un c\u00e1lculo grupal que eval\u00faa Verdadero o Falso. Por ejemplo, Filtrar datos en funci\u00f3n de la suma o media grupal</li> </ul> <p>Para comprender mejor el concepto de agrupaci\u00f3n de tablas, se realiza un ejercicio simple sobre el conjunto de datos pokemon.csv</p> <p></p> In\u00a0[1]: Copied! <pre># libreria\nimport pandas as pd\nimport numpy as np\nimport os\n</pre> # libreria import pandas as pd import numpy as np import os In\u00a0[2]: Copied! <pre># cargar datos\n\nurl='https://drive.google.com/file/d/1uVJcfTBWemOTfxmjWTqALfUpIMdH3LVV/view?usp=drive_link'\nurl='https://drive.google.com/uc?id=' + url.split('/')[-2]\n\npokemon_data = pd.read_csv(url, sep=\",\")\npokemon_data.head()\n</pre> # cargar datos  url='https://drive.google.com/file/d/1uVJcfTBWemOTfxmjWTqALfUpIMdH3LVV/view?usp=drive_link' url='https://drive.google.com/uc?id=' + url.split('/')[-2]  pokemon_data = pd.read_csv(url, sep=\",\") pokemon_data.head() Out[2]: # Name Type 1 Type 2 HP Attack Defense Sp. Atk Sp. Def Speed Generation Legendary 0 1 Bulbasaur Grass Poison 45 49 49 65 65 45 1 False 1 2 Ivysaur Grass Poison 60 62 63 80 80 60 1 False 2 3 Venusaur Grass Poison 80 82 83 100 100 80 1 False 3 4 Mega Venusaur Grass Poison 80 100 123 122 120 80 1 False 4 5 Charmander Fire NaN 39 52 43 60 50 65 1 False In\u00a0[3]: Copied! <pre># renombrar columnas\npokemon_data.columns = pokemon_data.columns.str.lower().str.replace('.','_').str.replace(' ','_') #change into upper case\npokemon_data.head()\n</pre> # renombrar columnas pokemon_data.columns = pokemon_data.columns.str.lower().str.replace('.','_').str.replace(' ','_') #change into upper case pokemon_data.head() <pre>C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_7740\\2937267274.py:2: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n  pokemon_data.columns = pokemon_data.columns.str.lower().str.replace('.','_').str.replace(' ','_') #change into upper case\n</pre> Out[3]: # name type_1 type_2 hp attack defense sp__atk sp__def speed generation legendary 0 1 Bulbasaur Grass Poison 45 49 49 65 65 45 1 False 1 2 Ivysaur Grass Poison 60 62 63 80 80 60 1 False 2 3 Venusaur Grass Poison 80 82 83 100 100 80 1 False 3 4 Mega Venusaur Grass Poison 80 100 123 122 120 80 1 False 4 5 Charmander Fire NaN 39 52 43 60 50 65 1 False In\u00a0[4]: Copied! <pre>grupo = pokemon_data.groupby('generation')  \ngrupo[['name']].count().reset_index()\n</pre> grupo = pokemon_data.groupby('generation')   grupo[['name']].count().reset_index() Out[4]: generation name 0 1 165 1 2 106 2 3 160 3 4 121 4 5 165 5 6 82 In\u00a0[5]: Copied! <pre>grupo = pokemon_data.groupby(['type_1','type_2']) \ngrupo['name'].count().reset_index()\n</pre> grupo = pokemon_data.groupby(['type_1','type_2'])  grupo['name'].count().reset_index() Out[5]: type_1 type_2 name 0 Bug Electric 2 1 Bug Fighting 2 2 Bug Fire 2 3 Bug Flying 14 4 Bug Ghost 1 ... ... ... ... 131 Water Ice 3 132 Water Poison 3 133 Water Psychic 5 134 Water Rock 4 135 Water Steel 1 <p>136 rows \u00d7 3 columns</p> In\u00a0[6]: Copied! <pre># metodo 01: ocupando el comando agg\ngrupo = pokemon_data.groupby(['legendary'])\ndf_leng = grupo.agg({'hp':[np.mean,sum]}).reset_index()\ndf_leng\n</pre> # metodo 01: ocupando el comando agg grupo = pokemon_data.groupby(['legendary']) df_leng = grupo.agg({'hp':[np.mean,sum]}).reset_index() df_leng Out[6]: legendary hp mean sum 0 False 67.182313 49379 1 True 92.738462 6028 In\u00a0[7]: Copied! <pre># metodo 02: ocupando el comando apply\ndef my_custom_function(x):\n        \"\"\"\n        Funcion que calcula el hp promedio y total\n        \"\"\"\n        names = {\n        'mean': x['hp'].mean(),\n        'sum':x['hp'].sum()}\n        \n        return pd.Series(names, index=['mean', 'sum'])\n</pre> # metodo 02: ocupando el comando apply def my_custom_function(x):         \"\"\"         Funcion que calcula el hp promedio y total         \"\"\"         names = {         'mean': x['hp'].mean(),         'sum':x['hp'].sum()}                  return pd.Series(names, index=['mean', 'sum']) In\u00a0[8]: Copied! <pre>grupo = pokemon_data.groupby(['legendary'])\ndf_leng = grupo.apply(my_custom_function).reset_index()\ndf_leng\n</pre> grupo = pokemon_data.groupby(['legendary']) df_leng = grupo.apply(my_custom_function).reset_index() df_leng Out[8]: legendary mean sum 0 False 67.182313 49379.0 1 True 92.738462 6028.0 In\u00a0[9]: Copied! <pre>cols_statistics = [\n    'generation', 'hp', \n    'attack', 'defense',\n    'sp__atk','sp__def', \n    'speed'\n]\n\ngrupo = pokemon_data[cols_statistics].groupby('generation') \nsc = lambda x: (x - x.mean()) / x.std()\ngrupo.transform(sc)\n</pre> cols_statistics = [     'generation', 'hp',      'attack', 'defense',     'sp__atk','sp__def',      'speed' ]  grupo = pokemon_data[cols_statistics].groupby('generation')  sc = lambda x: (x - x.mean()) / x.std() grupo.transform(sc) Out[9]: hp attack defense sp__atk sp__def speed 0 -0.739479 -0.898969 -0.763283 -0.198010 -0.160373 -0.929521 1 -0.206695 -0.476132 -0.274479 0.237542 0.427740 -0.424060 2 0.503685 0.174386 0.423812 0.818277 1.211892 0.249889 3 0.503685 0.759852 1.820395 1.457086 1.996043 0.249889 4 -0.952593 -0.801391 -0.972770 -0.343193 -0.748487 -0.255573 ... ... ... ... ... ... ... 795 -0.873754 0.829182 2.337149 0.808641 2.496477 -0.639851 796 -0.873754 2.885421 1.062058 2.695980 1.166968 1.695510 797 0.561116 1.171889 -0.531806 2.381423 1.831723 0.138603 798 0.561116 2.885421 -0.531806 3.010536 1.831723 0.527830 799 0.561116 1.171889 1.380831 1.752310 0.502214 0.138603 <p>800 rows \u00d7 6 columns</p> In\u00a0[10]: Copied! <pre>grupo = pokemon_data[['name','generation']].groupby('generation')  \ngrupo.filter(lambda x: len(x['name']) &lt; 150)\n</pre> grupo = pokemon_data[['name','generation']].groupby('generation')   grupo.filter(lambda x: len(x['name']) &lt; 150)  Out[10]: name generation 166 Chikorita 2 167 Bayleef 2 168 Meganium 2 169 Cyndaquil 2 170 Quilava 2 ... ... ... 795 Diancie 6 796 Mega Diancie 6 797 Hoopa Confined 6 798 Hoopa Unbound 6 799 Volcanion 6 <p>309 rows \u00d7 2 columns</p>"},{"location":"lectures/data_manipulation/pd_02/#groupby","title":"Groupby\u00b6","text":""},{"location":"lectures/data_manipulation/pd_02/#aplicacion","title":"Aplicaci\u00f3n\u00b6","text":""},{"location":"lectures/data_manipulation/pd_02/#numero-de-pokemones-por-generacion","title":"N\u00famero de pokemones por generaci\u00f3n\u00b6","text":""},{"location":"lectures/data_manipulation/pd_02/#numero-de-pokemones-por-par-tipo-i-y-tipo-ii","title":"N\u00famero de pokemones  por par Tipo I y Tipo II\u00b6","text":""},{"location":"lectures/data_manipulation/pd_02/#calcular-hp-promedio-y-hp-total-agrupados-si-el-pokemon-es-legendario-o-no","title":"Calcular hp promedio y hp total agrupados si el pokemon es legendario o no\u00b6","text":""},{"location":"lectures/data_manipulation/pd_02/#metodo-01-ocupando-el-comando-agg","title":"m\u00e9todo 01: ocupando el comando agg\u00b6","text":""},{"location":"lectures/data_manipulation/pd_02/#metodo-02-ocupando-el-comando-apply","title":"m\u00e9todo 02: ocupando el comando apply\u00b6","text":""},{"location":"lectures/data_manipulation/pd_02/#normalizar-las-estadisticas-agrupados-por-generacion","title":"Normalizar las estad\u00edsticas agrupados por generaci\u00f3n\u00b6","text":""},{"location":"lectures/data_manipulation/pd_02/#identificar-generaciones-que-tienen-menos-de-100-pokemones","title":"Identificar generaciones  que tienen menos de 100 pokemones\u00b6","text":""},{"location":"lectures/data_manipulation/pd_02/#referencias","title":"Referencias\u00b6","text":"<ol> <li>Groupby</li> </ol>"},{"location":"lectures/data_manipulation/pd_03/","title":"Merge &amp; Concat","text":"<p>En muchas ocasiones nos podemos encontrar con que los conjuntos de datos no se encuentran agregados en una \u00fanica tabla. Cuando esto sucede, existen dos formas para unir la informaci\u00f3n de distintas tablas: merge y concat.</p> In\u00a0[1]: Copied! <pre>import os\nimport numpy as pd\nimport pandas as pd\n</pre> import os import numpy as pd import pandas as pd In\u00a0[2]: Copied! <pre>df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],\n                    'B': ['B0', 'B1', 'B2', 'B3'],\n                    'C': ['C0', 'C1', 'C2', 'C3'],\n                    'D': ['D0', 'D1', 'D2', 'D3']},\n                   index=[0, 1, 2, 3])\n\n\ndf2 = pd.DataFrame({'A': ['A4', 'A5', 'A6', 'A7'],\n                    'B': ['B4', 'B5', 'B6', 'B7'],\n                    'C': ['C4', 'C5', 'C6', 'C7'],\n                    'D': ['D4', 'D5', 'D6', 'D7']},\n                   index=[4, 5, 6, 7])\n\n\ndf3 = pd.DataFrame({'A': ['A8', 'A9', 'A10', 'A11'],\n                    'B': ['B8', 'B9', 'B10', 'B11'],\n                    'C': ['C8', 'C9', 'C10', 'C11'],\n                    'D': ['D8', 'D9', 'D10', 'D11']},\n                   index=[8, 9, 10, 11])\n\n\nframes = [df1, df2, df3]\n\nresult = pd.concat(frames)\n</pre> df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],                     'B': ['B0', 'B1', 'B2', 'B3'],                     'C': ['C0', 'C1', 'C2', 'C3'],                     'D': ['D0', 'D1', 'D2', 'D3']},                    index=[0, 1, 2, 3])   df2 = pd.DataFrame({'A': ['A4', 'A5', 'A6', 'A7'],                     'B': ['B4', 'B5', 'B6', 'B7'],                     'C': ['C4', 'C5', 'C6', 'C7'],                     'D': ['D4', 'D5', 'D6', 'D7']},                    index=[4, 5, 6, 7])   df3 = pd.DataFrame({'A': ['A8', 'A9', 'A10', 'A11'],                     'B': ['B8', 'B9', 'B10', 'B11'],                     'C': ['C8', 'C9', 'C10', 'C11'],                     'D': ['D8', 'D9', 'D10', 'D11']},                    index=[8, 9, 10, 11])   frames = [df1, df2, df3]  result = pd.concat(frames) <p></p> In\u00a0[3]: Copied! <pre>df4 = pd.DataFrame({'B1': ['hola', 'B3', 'B6', 'B7'],\n                       'D': ['D2', 'D3', 'D6', 'D7'],\n                       'F': ['F2', 'F3', 'F6', 'F7']},\n                      index=[2, 3, 6, 7])\n   \n\nresult = pd.concat([df1, df4], axis=0, sort=False)\n</pre> df4 = pd.DataFrame({'B1': ['hola', 'B3', 'B6', 'B7'],                        'D': ['D2', 'D3', 'D6', 'D7'],                        'F': ['F2', 'F3', 'F6', 'F7']},                       index=[2, 3, 6, 7])      result = pd.concat([df1, df4], axis=0, sort=False) <p></p> In\u00a0[4]: Copied! <pre>result = pd.concat([df1, df4], axis=1, sort=False)\n</pre> result = pd.concat([df1, df4], axis=1, sort=False) <p></p> In\u00a0[5]: Copied! <pre>left = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'],\n                     'A': ['A0', 'A1', 'A2', 'A3'],\n                     'B': ['B0', 'B1', 'B2', 'B3']})\n\n\nright = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'],\n                      'C': ['C0', 'C1', 'C2', 'C3'],\n                      'D': ['D0', 'D1', 'D2', 'D3']})\n\n\nresult = pd.merge(left, right, on='key')\n</pre> left = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'],                      'A': ['A0', 'A1', 'A2', 'A3'],                      'B': ['B0', 'B1', 'B2', 'B3']})   right = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'],                       'C': ['C0', 'C1', 'C2', 'C3'],                       'D': ['D0', 'D1', 'D2', 'D3']})   result = pd.merge(left, right, on='key') <p></p> <p>En este ejemplo, se especifica en la opci\u00f3n <code>on</code> las columnas (keys) donde se realizar\u00e1 el cruce de informaci\u00f3n de ambas tablas.</p> <p></p> In\u00a0[6]: Copied! <pre>left = pd.DataFrame({'key1': ['K0', 'K0', 'K1', 'K2'],\n                     'key2': ['K0', 'K1', 'K0', 'K1'],\n                     'A': ['A0', 'A1', 'A2', 'A3'],\n                     'B': ['B0', 'B1', 'B2', 'B3']})\n\n\nright = pd.DataFrame({'key1': ['K0', 'K1', 'K1', 'K2'],\n                      'key2': ['K0', 'K0', 'K0', 'K0'],\n                      'C': ['C0', 'C1', 'C2', 'C3'],\n                      'D': ['D0', 'D1', 'D2', 'D3']})\n</pre> left = pd.DataFrame({'key1': ['K0', 'K0', 'K1', 'K2'],                      'key2': ['K0', 'K1', 'K0', 'K1'],                      'A': ['A0', 'A1', 'A2', 'A3'],                      'B': ['B0', 'B1', 'B2', 'B3']})   right = pd.DataFrame({'key1': ['K0', 'K1', 'K1', 'K2'],                       'key2': ['K0', 'K0', 'K0', 'K0'],                       'C': ['C0', 'C1', 'C2', 'C3'],                       'D': ['D0', 'D1', 'D2', 'D3']}) In\u00a0[7]: Copied! <pre>merge_left = pd.merge(left, right,how= 'left', on=['key1', 'key2'])\n</pre> merge_left = pd.merge(left, right,how= 'left', on=['key1', 'key2']) <p></p> In\u00a0[8]: Copied! <pre>merge_rigth = pd.merge(left, right, how='right', on=['key1', 'key2'])\n</pre> merge_rigth = pd.merge(left, right, how='right', on=['key1', 'key2']) <p></p> In\u00a0[9]: Copied! <pre>merge_outer = pd.merge(left, right, how='outer', on=['key1', 'key2'])\n</pre> merge_outer = pd.merge(left, right, how='outer', on=['key1', 'key2']) <p></p> In\u00a0[10]: Copied! <pre>merge_inner = pd.merge(left, right, how='inner', on=['key1', 'key2'])\n</pre> merge_inner = pd.merge(left, right, how='inner', on=['key1', 'key2']) <p></p> In\u00a0[11]: Copied! <pre>left = pd.DataFrame({'A': [1, 2], 'B': [2, 2]})\nright = pd.DataFrame({'A': [4, 5, 6], 'B': [2, 2, 2]})\n\nresult = pd.merge(left, right, on='B', how='outer')\n</pre> left = pd.DataFrame({'A': [1, 2], 'B': [2, 2]}) right = pd.DataFrame({'A': [4, 5, 6], 'B': [2, 2, 2]})  result = pd.merge(left, right, on='B', how='outer') <p></p>"},{"location":"lectures/data_manipulation/pd_03/#merge-concat","title":"Merge &amp; Concat\u00b6","text":""},{"location":"lectures/data_manipulation/pd_03/#concat","title":"Concat\u00b6","text":"<p>La funci\u00f3n <code>concat()</code> realiza todo el trabajo pesado de realizar operaciones de concatenaci\u00f3n a lo largo de un eje mientras realiza la l\u00f3gica de conjunto opcional (uni\u00f3n o intersecci\u00f3n) de los \u00edndices (si los hay) en los otros ejes. Tenga en cuenta que digo \"si hay alguno\" porque solo hay un \u00fanico eje posible de concatenaci\u00f3n para Series.</p>"},{"location":"lectures/data_manipulation/pd_03/#concatenar-varias-tablas-con-las-mismas-columnas","title":"Concatenar varias tablas con las mismas columnas\u00b6","text":""},{"location":"lectures/data_manipulation/pd_03/#concatenar-varias-tablas-con-distintas-columnas-por-filas","title":"Concatenar varias tablas con distintas columnas (por filas)\u00b6","text":""},{"location":"lectures/data_manipulation/pd_03/#concatenar-varias-tablas-con-distintas-columnas-por-columnas","title":"Concatenar varias tablas con distintas columnas (por columnas)\u00b6","text":""},{"location":"lectures/data_manipulation/pd_03/#merge","title":"Merge\u00b6","text":"<p>La funci\u00f3n <code>merge()</code> se usa para combinar dos (o m\u00e1s) tablas sobre valores de columnas comunes (keys).</p>"},{"location":"lectures/data_manipulation/pd_03/#tipos-de-merge","title":"Tipos de merge\u00b6","text":"<p>La opci\u00f3n how especificica el tipo de cruce que se realizar\u00e1.</p> <ul> <li>left: usa las llaves solo de la tabla izquierda</li> <li>right: usa las llaves solo de la tabla derecha</li> <li>outer: usa las llaves de la uni\u00f3n de  ambas tablas.</li> <li>inner: usa las llaves de la intersecci\u00f3n de  ambas tablas.</li> </ul>"},{"location":"lectures/data_manipulation/pd_03/#merge-left","title":"Merge left\u00b6","text":""},{"location":"lectures/data_manipulation/pd_03/#merge-right","title":"Merge right\u00b6","text":""},{"location":"lectures/data_manipulation/pd_03/#merge-outer","title":"Merge outer\u00b6","text":""},{"location":"lectures/data_manipulation/pd_03/#merge-inner","title":"Merge inner\u00b6","text":""},{"location":"lectures/data_manipulation/pd_03/#problemas-de-llaves-duplicadas","title":"Problemas de llaves duplicadas\u00b6","text":"<p>Cuando se quiere realizar el cruce de dos tablas, pero an ambas tablas existe una columna (key) con el mismo nombre, para diferenciar la informaci\u00f3n entre la columna de una tabla y otra, pandas devulve el nombre de la columna con un gui\u00f3n bajo x (key_x) y otra con un gui\u00f3n bajo y (key_y)</p>"},{"location":"lectures/data_manipulation/pd_03/#referencias","title":"Referencias\u00b6","text":"<ol> <li>Merge, join, and concatenate</li> </ol>"},{"location":"lectures/data_manipulation/pd_04/","title":"Pivot &amp; Melt","text":"<p>Por ejemplo, el conjunto de datos Zoo Data Set presenta las caracter\u00edsticas de diversos animales, de los cuales presentamos las primeras 5 columnas.</p> animal_name hair feathers eggs milk antelope 1 0 0 1 bear 1 0 0 1 buffalo 1 0 0 1 catfish 0 0 1 0 <p>La tabla as\u00ed presentada se encuentra en wide format, es decir, donde los valores se extienden a trav\u00e9s de las columnas.</p> <p>Ser\u00eda posible representar el mismo contenido anterior en long format, es decir, donde los mismos valores se indicaran a trav\u00e9s de las filas:</p> <p>|animal_name|characteristic|value| |-----------|----|--------| |antelope|hair |1| |antelope|feathers|0| |antelope|eggs|0| |antelope|milk|1| |...|...|...|...|..| |catfish|hair |0| |catfish|feathers|0| |catfish|eggs|1| |catfish|milk|0|</p> <p>En python existen maneras de pasar del formato wide al formato long y viceversa.</p> In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport os\n</pre> import pandas as pd import numpy as np import os In\u00a0[2]: Copied! <pre># formato long\n\nurl='https://drive.google.com/file/d/1Knut8vY4wuDFrO3bEjlQVUwq-CaRNJpu/view?usp=drive_link'\nurl='https://drive.google.com/uc?id=' + url.split('/')[-2]\n\ndf = pd.read_csv(url, sep=\",\")\ndf.head()\n</pre> # formato long  url='https://drive.google.com/file/d/1Knut8vY4wuDFrO3bEjlQVUwq-CaRNJpu/view?usp=drive_link' url='https://drive.google.com/uc?id=' + url.split('/')[-2]  df = pd.read_csv(url, sep=\",\") df.head() Out[2]: A\u00f1o Pais Magnitud 0 2011 Turkey 7.1 1 2011 India 6.9 2 2011 Japan 7.1 3 2011 Burma 6.8 4 2011 Japan 9.0 <p>Por ejemplo,  se quiere saber el terremoto de mayor magnitud a nivel de pa\u00eds a\u00f1o. Tenemos dos formas de mostrar la informaci\u00f3n.</p> In\u00a0[3]: Copied! <pre># formato long\ndf.groupby(['Pais','A\u00f1o']).max()\n</pre> # formato long df.groupby(['Pais','A\u00f1o']).max() Out[3]: Magnitud Pais A\u00f1o Afghanistan 2000 6.3 2001 5.0 2002 5.8 2003 5.8 2004 6.5 ... ... ... Turkmenistan 2000 7.0 United States 2001 6.8 2003 6.6 Venezuela 2006 5.5 Vietnam 2005 5.3 <p>154 rows \u00d7 1 columns</p> In\u00a0[4]: Copied! <pre># formato wide\ndf_pivot = df.pivot_table(index=\"Pais\", columns=\"A\u00f1o\", values=\"Magnitud\", fill_value='', aggfunc=np.max)\ndf_pivot\n</pre> # formato wide df_pivot = df.pivot_table(index=\"Pais\", columns=\"A\u00f1o\", values=\"Magnitud\", fill_value='', aggfunc=np.max) df_pivot Out[4]: A\u00f1o 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 Pais Afghanistan 6.3 5.0 5.8 5.8 6.5 Afghanistan 7.3 6.5 Algeria 5.7 5.7 5.2 5.5 Algeria 6.8 Argentina 7.2 6.1 ... ... ... ... ... ... ... ... ... ... ... ... ... Turkey 6.3 6.1 7.1 Turkmenistan 7.0 United States 6.8 6.6 Venezuela 5.5 Vietnam 5.3 <p>72 rows \u00d7 12 columns</p> <p></p> <p>a) El valor indicado para la columna es \u00fanico</p> In\u00a0[5]: Copied! <pre>columns = [\"sala\",\"dia\",\"08:00\",\"09:00\",\"10:00\"]\ndata = [\n        [\"C201\",\"Lu\", \"mat1\",\"mat1\",    \"\"],\n        [\"C201\",\"Ma\", \"\",\"\",\"\"],\n        [\"C202\",\"Lu\", \"\",\"\",\"\"],\n        [\"C202\",\"Ma\", \"mat1\",\"mat1\",    \"\"],\n        [\"C203\",\"Lu\", \"fis1\",\"fis1\",\"fis1\"],\n        [\"C203\",\"Ma\", \"fis1\",\"fis1\",\"fis1\"],\n       ]\ndf = pd.DataFrame(data=data, columns=columns)\ndf\n</pre> columns = [\"sala\",\"dia\",\"08:00\",\"09:00\",\"10:00\"] data = [         [\"C201\",\"Lu\", \"mat1\",\"mat1\",    \"\"],         [\"C201\",\"Ma\", \"\",\"\",\"\"],         [\"C202\",\"Lu\", \"\",\"\",\"\"],         [\"C202\",\"Ma\", \"mat1\",\"mat1\",    \"\"],         [\"C203\",\"Lu\", \"fis1\",\"fis1\",\"fis1\"],         [\"C203\",\"Ma\", \"fis1\",\"fis1\",\"fis1\"],        ] df = pd.DataFrame(data=data, columns=columns) df Out[5]: sala dia 08:00 09:00 10:00 0 C201 Lu mat1 mat1 1 C201 Ma 2 C202 Lu 3 C202 Ma mat1 mat1 4 C203 Lu fis1 fis1 fis1 5 C203 Ma fis1 fis1 fis1 In\u00a0[6]: Copied! <pre># Despivotear incorrectamente la tabla\ndf_melt = df.melt(id_vars=[\"sala\"], var_name=\"hora\", value_name=\"curso\")\ndf_melt\n</pre> # Despivotear incorrectamente la tabla df_melt = df.melt(id_vars=[\"sala\"], var_name=\"hora\", value_name=\"curso\") df_melt Out[6]: sala hora curso 0 C201 dia Lu 1 C201 dia Ma 2 C202 dia Lu 3 C202 dia Ma 4 C203 dia Lu 5 C203 dia Ma 6 C201 08:00 mat1 7 C201 08:00 8 C202 08:00 9 C202 08:00 mat1 10 C203 08:00 fis1 11 C203 08:00 fis1 12 C201 09:00 mat1 13 C201 09:00 14 C202 09:00 15 C202 09:00 mat1 16 C203 09:00 fis1 17 C203 09:00 fis1 18 C201 10:00 19 C201 10:00 20 C202 10:00 21 C202 10:00 22 C203 10:00 fis1 23 C203 10:00 fis1 In\u00a0[7]: Copied! <pre># Despivotear correctamente la tabla\ndf_melt = df.melt(id_vars=[\"sala\", \"dia\"], var_name=\"hora\", value_name=\"curso\")\ndf_melt[df_melt.curso!=\"\"].sort_values([\"sala\",\"dia\",\"hora\"])\n</pre> # Despivotear correctamente la tabla df_melt = df.melt(id_vars=[\"sala\", \"dia\"], var_name=\"hora\", value_name=\"curso\") df_melt[df_melt.curso!=\"\"].sort_values([\"sala\",\"dia\",\"hora\"]) Out[7]: sala dia hora curso 0 C201 Lu 08:00 mat1 6 C201 Lu 09:00 mat1 3 C202 Ma 08:00 mat1 9 C202 Ma 09:00 mat1 4 C203 Lu 08:00 fis1 10 C203 Lu 09:00 fis1 16 C203 Lu 10:00 fis1 5 C203 Ma 08:00 fis1 11 C203 Ma 09:00 fis1 17 C203 Ma 10:00 fis1 <p>b) Relaciones no \u00fanicas</p> In\u00a0[8]: Copied! <pre>columns = [\"sala\",\"curso\",\"Lu\",\"Ma\",\"hora\"]\ndata = [\n        [\"C201\",\"mat1\",\"X\",\"\",\"8:00-10:00\"],\n        [\"C202\",\"mat1\",\"\",\"X\",\"8:00-10:00\"],\n        [\"C203\",\"fis1\",\"X\",\"X\",\"8:00-11:00\"],\n       ]\ndf = pd.DataFrame(data=data, columns=columns)\ndf\n</pre> columns = [\"sala\",\"curso\",\"Lu\",\"Ma\",\"hora\"] data = [         [\"C201\",\"mat1\",\"X\",\"\",\"8:00-10:00\"],         [\"C202\",\"mat1\",\"\",\"X\",\"8:00-10:00\"],         [\"C203\",\"fis1\",\"X\",\"X\",\"8:00-11:00\"],        ] df = pd.DataFrame(data=data, columns=columns) df Out[8]: sala curso Lu Ma hora 0 C201 mat1 X 8:00-10:00 1 C202 mat1 X 8:00-10:00 2 C203 fis1 X X 8:00-11:00 <p>m\u00e9todo 01:  Despivotear manualmente y generar un nuevo dataframe</p> <ul> <li>Ventajas: Si se puede es una soluci\u00f3n directa y r\u00e1pida.</li> <li>Desventaja: requiere programaci\u00f3n expl\u00edcita de la tarea, no es reutilizable.</li> </ul> In\u00a0[9]: Copied! <pre># Obtener el d\u00eda lunes\ndf_Lu = df.loc[df.Lu==\"X\", [\"sala\",\"curso\",\"hora\"]]\ndf_Lu[\"dia\"] = \"Lu\"\ndf_Lu\n</pre> # Obtener el d\u00eda lunes df_Lu = df.loc[df.Lu==\"X\", [\"sala\",\"curso\",\"hora\"]] df_Lu[\"dia\"] = \"Lu\" df_Lu Out[9]: sala curso hora dia 0 C201 mat1 8:00-10:00 Lu 2 C203 fis1 8:00-11:00 Lu In\u00a0[10]: Copied! <pre># Obtener el d\u00eda martes\ndf_Ma = df.loc[df.Ma==\"X\", [\"sala\",\"curso\",\"hora\"]]\ndf_Ma[\"dia\"] = \"Ma\"\ndf_Ma\n</pre> # Obtener el d\u00eda martes df_Ma = df.loc[df.Ma==\"X\", [\"sala\",\"curso\",\"hora\"]] df_Ma[\"dia\"] = \"Ma\" df_Ma Out[10]: sala curso hora dia 1 C202 mat1 8:00-10:00 Ma 2 C203 fis1 8:00-11:00 Ma In\u00a0[11]: Copied! <pre># Juntar\npd.concat([df_Lu,df_Ma])\n</pre> # Juntar pd.concat([df_Lu,df_Ma]) Out[11]: sala curso hora dia 0 C201 mat1 8:00-10:00 Lu 2 C203 fis1 8:00-11:00 Lu 1 C202 mat1 8:00-10:00 Ma 2 C203 fis1 8:00-11:00 Ma <p>m\u00e9todo 02: Iterar sobre las filas y generar contenido para un nuevo dataframe</p> <ul> <li>Ventajas: En general, f\u00e1cil de codificar.</li> <li>Desventaja: puede ser lento, es ineficiente.</li> </ul> In\u00a0[12]: Copied! <pre>my_columns = [\"sala\",\"curso\",\"dia\",\"hora\"]\nmy_data = []\nfor i, df_row in df.iterrows():\n    # Procesar cada fila\n    if df_row.Lu==\"X\":\n        my_row = [df_row.sala, df_row.curso, \"Lu\", df_row.hora]\n        my_data.append(my_row)\n    if df_row.Ma==\"X\":\n        my_row = [df_row.sala, df_row.curso, \"Ma\", df_row.hora]\n        my_data.append(my_row)\nnew_df = pd.DataFrame(data=my_data, columns=my_columns)\nnew_df\n</pre> my_columns = [\"sala\",\"curso\",\"dia\",\"hora\"] my_data = [] for i, df_row in df.iterrows():     # Procesar cada fila     if df_row.Lu==\"X\":         my_row = [df_row.sala, df_row.curso, \"Lu\", df_row.hora]         my_data.append(my_row)     if df_row.Ma==\"X\":         my_row = [df_row.sala, df_row.curso, \"Ma\", df_row.hora]         my_data.append(my_row) new_df = pd.DataFrame(data=my_data, columns=my_columns) new_df Out[12]: sala curso dia hora 0 C201 mat1 Lu 8:00-10:00 1 C202 mat1 Ma 8:00-10:00 2 C203 fis1 Lu 8:00-11:00 3 C203 fis1 Ma 8:00-11:00"},{"location":"lectures/data_manipulation/pd_04/#pivot-melt","title":"Pivot &amp; Melt\u00b6","text":""},{"location":"lectures/data_manipulation/pd_04/#formato-wide-y-formato-long","title":"Formato Wide y Formato Long\u00b6","text":"<p>Dentro del mundo de los dataframe (o datos tabulares) existen dos formas de presentar la naturaleza de los datos: formato wide y formato long.</p>"},{"location":"lectures/data_manipulation/pd_04/#pivotear-y-despivotear-tablas","title":"Pivotear y despivotear tablas\u00b6","text":""},{"location":"lectures/data_manipulation/pd_04/#pivot","title":"Pivot\u00b6","text":"<p>El pivoteo de una tabla corresponde al paso de una tabla desde el formato long al formato wide. T\u00edpicamente esto se realiza para poder comparar los valores que se obtienen para alg\u00fan registro en particular, o para utilizar algunas herramientas de visualizaci\u00f3n b\u00e1sica que requieren dicho formato.</p> <p>Para ejemplificar estos resultados, ocupemos el conjunto de datos terremotos.csv, con contiene los registros de  terremotos de distintos paises desde el a\u00f1o 2000 al 2011.</p> <p></p>"},{"location":"lectures/data_manipulation/pd_04/#despivotear-un-tabla","title":"Despivotear un tabla\u00b6","text":"<p>El despivotear una tabla corresponde al paso de una tabla desde el formato wide al formato long.</p> <p>Se reconocen dos situaciones:</p> <ol> <li>El valor indicado para la columna es \u00fanico, y s\u00f3lo se requiere definir correctamente las columnas.</li> <li>El valor indicado por la columna no es \u00fanico o requiere un procesamiento adicional, y se requiere una iteraci\u00f3n m\u00e1s profunda.</li> </ol> <p>Para ejemplificar esto, crearemos un conjunto de datos con los horarios de los ramos que se tiene que dictar en un determinado d\u00eda, hora y lugar.</p>"},{"location":"lectures/data_manipulation/pd_04/#metodos","title":"M\u00e9todos\u00b6","text":""},{"location":"lectures/data_manipulation/pd_04/#referencias","title":"Referencias\u00b6","text":"<ol> <li>Reshaping and pivot tables</li> </ol>"},{"location":"lectures/data_manipulation/pd_intro/","title":"Introducci\u00f3n","text":"<p>\u00bfEste formato de datos te parece familar?</p>"},{"location":"lectures/data_manipulation/pd_intro/#introduccion","title":"Introducci\u00f3n\u00b6","text":"<p>\u00bfTe imaginas como las grandes compa\u00f1\u00edas o gobiernos almacenan sus datos?. No, no es en un excel gigante en un pendrive.</p> <p>Es importante saber como modificar o crear valor a trav\u00e9s de distintas tablas de datos, por lo que esta clase se centrar\u00e1 en hacer esto, motivando a partir del uso de bases de datos relacionales.</p> <p>Una Base de Datos es un conjunto de datos almacenados en una computadora (generalmente un servidor, m\u00e1quina virtual, etc.) y que poseen una estructura tal que sean de f\u00e1cil acceso.</p>"},{"location":"lectures/data_manipulation/pd_intro/#base-de-datos-relacional","title":"Base de Datos Relacional\u00b6","text":"<p>Es el tipo de base de datos m\u00e1s ampliamente utilizado, aunque existen otros tipos de bases de datos para fines espec\u00edficos. Utiliza una estructura tal que es posible identificar y acceder a datos relacionados entre si. Generalmente una base de datos relacional est\u00e1 organizada en tablas.</p> <p>Las tablas est\u00e1n conformadas de filas y columnas. Cada columna posee un nombre y tiene un tipo de dato espec\u00edfico, mientras que las filas son registros almacenados.</p> <p>Por ejemplo, la siguiente tabla tiene tres columnas y cuatro registros. En particular, la columna <code>age</code> tiene tipo <code>INTEGER</code> y las otras dos tipo <code>STRING</code>.</p> <p></p>"},{"location":"lectures/data_manipulation/pd_intro/#que-es-sql","title":"\u00bfQu\u00e9 es SQL?\u00b6","text":"<p>Sus siglas significan Structured Query Language (Lenguaje de Consulta Estructurada) es un lenguaje de programaci\u00f3n utilizado para comunicarse con datos almacenados en un Sistema de Gesti\u00f3n de Bases de Datos Relacionales (Relational Database Management System o RDBMS). Posee una sintaxis muy similar al idioma ingl\u00e9s, con lo cual se hace relativamente f\u00e1cil de escribir, leer e interpretar.</p> <p>Hay distintos RDBMS entre los cuales la sintaxis de SQL difiere ligeramente. Los m\u00e1s populares son:</p> <ul> <li>SQLite</li> <li>MySQL / MariaDB</li> <li>PostgreSQL</li> <li>Oracle DB</li> <li>SQL Server</li> </ul> <p>En una empresa de tecnolog\u00eda hay cargos especialmente destinados a todo lo que tenga que ver con bases de datos, por ejemplo: creaci\u00f3n, mantenci\u00f3n, actualizaci\u00f3n, obtenci\u00f3n de datos, transformaci\u00f3n, seguridad y un largo etc.</p> <p>Los matem\u00e1ticos en la industria suelen tener cargos como Data Scientist, Data Analyst, Data Statistician, Data X (reemplace X con algo fancy tal de formar un cargo que quede bien en Linkedin), en donde lo importante es otorgar valor a estos datos. Por ende, lo m\u00ednimo que deben satisfacer es:</p> <ul> <li>Entendimiento casi total del modelo de datos (tablas, relaciones, tipos, etc.)</li> <li>Seleccionar datos a medida (queries).</li> </ul>"},{"location":"lectures/data_manipulation/pd_intro/#modelo-de-datos","title":"Modelo de datos\u00b6","text":"<p>Es la forma en que se organizan los datos. En las bases de datos incluso es posible conocer las relaciones entre tablas. A menudo se presentan gr\u00e1ficamente como en la imagen de abajo (esta ser\u00e1 la base de datos que utilizaremos en los ejericios del d\u00eda de</p> <p></p> <p>Esta base de datos se conoce con el nombre de chinook database. La descripci\u00f3n y las im\u00e1genes se pueden encontrar aqu\u00ed.</p> <p>En la figura anterior, existen algunas columnas especiales con una llave al lado de su nombre. \u00bfQu\u00e9 crees que significan?</p> <p>Las 11 tablas se definen de la siguiente forma (en ingl\u00e9s):</p> <ul> <li><code>employees</code> table stores employees data such as employee id, last name, first name, etc. It also has a field named ReportsTo to specify who reports to whom.</li> <li><code>customers</code> table stores customers data.</li> <li><code>invoices</code> &amp; <code>invoice_items</code> tables: these two tables store invoice data. The <code>invoices</code> table stores invoice header data and the <code>invoice_items</code> table stores the invoice line items data.</li> <li><code>artists</code> table stores artists data. It is a simple table that contains only artist id and name.</li> <li><code>albums</code> table stores data about a list of tracks. Each album belongs to one artist. However, one artist may have multiple albums.</li> <li><code>media_types</code> table stores media types such as MPEG audio file, ACC audio file, etc.</li> <li><code>genres</code> table stores music types such as rock, jazz, metal, etc.</li> <li><code>tracks</code> table store the data of songs. Each track belongs to one album.</li> <li><code>playlists</code> &amp; <code>playlist_track tables</code>: <code>playlists</code> table store data about playlists. Each playlist contains a list of tracks. Each track may belong to multiple playlists. The relationship between the <code>playlists</code> table and <code>tracks</code> table is many-to-many. The <code>playlist_track</code> table is used to reflect this relationship.</li> </ul>"},{"location":"lectures/data_manipulation/sc_01/","title":"Numpy","text":"<p>Numpy s una biblioteca de Python que se utiliza para trabajar con matrices y vectores de datos num\u00e9ricos. Proporciona un conjunto de funciones y m\u00e9todos eficientes y optimizados para el procesamiento de datos num\u00e9ricos, incluyendo operaciones matriciales, estad\u00edsticas, \u00e1lgebra lineal, entre otras.</p> <p>NumPy es ampliamente utilizado en \u00e1reas como la ciencia de datos, el aprendizaje autom\u00e1tico, la ingenier\u00eda y la f\u00edsica, entre otras. Es una herramienta fundamental en el ecosistema de Python para el procesamiento y an\u00e1lisis de datos num\u00e9ricos.</p> <p>Algunas de las caracter\u00edsticas principales de NumPy son:</p> <ul> <li>Arrays multidimensionales eficientes y optimizados para operaciones num\u00e9ricas.</li> <li>Funciones y m\u00e9todos para operaciones matem\u00e1ticas y estad\u00edsticas.</li> <li>Integraci\u00f3n con otras bibliotecas de Python para el procesamiento de datos, como Pandas y Matplotlib.</li> <li>Capacidad de procesar grandes conjuntos de datos num\u00e9ricos de manera eficiente y escalable.</li> </ul> <p></p> <ul> <li><p>Los array NumPy ocupan menos espacio. Esto significa que una matriz entera arbitraria de longitud n en necesidades numpy se calcula por:</p> <pre><code>                  96 + n * 8 bytes</code></pre> </li> </ul> <p></p> <p>Veamos un ejemplo:</p> In\u00a0[1]: Copied! <pre>import numpy as np \nimport time\nimport sys\n\ndef arreglo_python(n):\n    t1 = time.time()\n    X = range(n)\n    Y = range(n)\n    Z = [X[i] + Y[i] for i in range(len(X)) ]\n    return (time.time() - t1,sys.getsizeof(Z) )\n\ndef arreglo_numpy(n):\n    t1 = time.time()\n    X = np.arange(n)\n    Y = np.arange(n)\n    Z = X + Y\n    return (time.time() - t1,sys.getsizeof(Z) )\n</pre> import numpy as np  import time import sys  def arreglo_python(n):     t1 = time.time()     X = range(n)     Y = range(n)     Z = [X[i] + Y[i] for i in range(len(X)) ]     return (time.time() - t1,sys.getsizeof(Z) )  def arreglo_numpy(n):     t1 = time.time()     X = np.arange(n)     Y = np.arange(n)     Z = X + Y     return (time.time() - t1,sys.getsizeof(Z) ) In\u00a0[2]: Copied! <pre># generar varios casos\nfor size_of_vec in [10,100,1000,10000,100000,1000000]:\n    print(f\"size of vector: {size_of_vec}\")\n    \n    #list vs numpy\n    t1, size1 = arreglo_python(size_of_vec)\n    t2, size2 = arreglo_numpy(size_of_vec)\n    \n    # resultados\n    print(f\"python list -- time: {round(t1,8)} seg, size: {size1} bytes\")\n    print(f\"numpy array -- time: {round(t2,8)} seg, size: {size2} bytes\\n\")\n</pre> # generar varios casos for size_of_vec in [10,100,1000,10000,100000,1000000]:     print(f\"size of vector: {size_of_vec}\")          #list vs numpy     t1, size1 = arreglo_python(size_of_vec)     t2, size2 = arreglo_numpy(size_of_vec)          # resultados     print(f\"python list -- time: {round(t1,8)} seg, size: {size1} bytes\")     print(f\"numpy array -- time: {round(t2,8)} seg, size: {size2} bytes\\n\") <pre>size of vector: 10\npython list -- time: 0.0 seg, size: 184 bytes\nnumpy array -- time: 0.0 seg, size: 152 bytes\n\nsize of vector: 100\npython list -- time: 0.0 seg, size: 904 bytes\nnumpy array -- time: 0.0 seg, size: 512 bytes\n\nsize of vector: 1000\npython list -- time: 0.0 seg, size: 9016 bytes\nnumpy array -- time: 0.0 seg, size: 4112 bytes\n\nsize of vector: 10000\npython list -- time: 0.00201082 seg, size: 87616 bytes\nnumpy array -- time: 0.0 seg, size: 40112 bytes\n\nsize of vector: 100000\npython list -- time: 0.01404929 seg, size: 824456 bytes\nnumpy array -- time: 0.0 seg, size: 400112 bytes\n\nsize of vector: 1000000\npython list -- time: 0.14597535 seg, size: 8697456 bytes\nnumpy array -- time: 0.00299621 seg, size: 4000112 bytes\n\n</pre> In\u00a0[3]: Copied! <pre># Crear un array 1D de longitud 3\narr_1d = np.array([1, 2, 3])\nprint(arr_1d)\n</pre> # Crear un array 1D de longitud 3 arr_1d = np.array([1, 2, 3]) print(arr_1d) <pre>[1 2 3]\n</pre> In\u00a0[4]: Copied! <pre># Crear un array 2D de tama\u00f1o 2x3\narr_2d = np.array([\n    [1, 2, 3], \n    [4, 5, 6]\n])\nprint(arr_2d)\n</pre> # Crear un array 2D de tama\u00f1o 2x3 arr_2d = np.array([     [1, 2, 3],      [4, 5, 6] ]) print(arr_2d) <pre>[[1 2 3]\n [4 5 6]]\n</pre> In\u00a0[5]: Copied! <pre># Crear un array 3D de tama\u00f1o 2x2x3\narr_3d = np.array([\n    [\n        [1, 2, 3], \n        [4, 5, 6]\n    ], \n    [\n        [7, 8, 9],\n        [10, 11, 12]\n    ]\n])\nprint(arr_3d)\n</pre> # Crear un array 3D de tama\u00f1o 2x2x3 arr_3d = np.array([     [         [1, 2, 3],          [4, 5, 6]     ],      [         [7, 8, 9],         [10, 11, 12]     ] ]) print(arr_3d) <pre>[[[ 1  2  3]\n  [ 4  5  6]]\n\n [[ 7  8  9]\n  [10 11 12]]]\n</pre> <p>Veamos algunos atributos de los arreglos en Numpy:</p> <ul> <li><code>ndarray.shape</code>: devuelve la forma (dimensiones) del array.</li> <li><code>ndarray.ndim</code>: devuelve el n\u00famero de dimensiones del array.</li> <li><code>ndarray.size</code>: devuelve el n\u00famero total de elementos en el array.</li> <li><code>ndarray.dtype</code>: devuelve el tipo de datos de los elementos del array.</li> </ul> In\u00a0[6]: Copied! <pre># crear objeto\nobj_numpy =np.array(\n    [1, 2, 3, 4, 5, 6, 7, 8, 9])\n</pre> # crear objeto obj_numpy =np.array(     [1, 2, 3, 4, 5, 6, 7, 8, 9]) In\u00a0[7]: Copied! <pre># shape\nobj_numpy.shape\n</pre> # shape obj_numpy.shape Out[7]: <pre>(9,)</pre> In\u00a0[8]: Copied! <pre># ndim\nobj_numpy.ndim\n</pre> # ndim obj_numpy.ndim Out[8]: <pre>1</pre> In\u00a0[9]: Copied! <pre># size\nobj_numpy.size\n</pre> # size obj_numpy.size Out[9]: <pre>9</pre> In\u00a0[10]: Copied! <pre># dtype\nobj_numpy.dtype\n</pre> # dtype obj_numpy.dtype Out[10]: <pre>dtype('int32')</pre> In\u00a0[11]: Copied! <pre>import numpy as np\n\n# Crear dos arrays\na = np.array([1, 2, 3])\nb = np.array([4, 5, 6])\n</pre> import numpy as np  # Crear dos arrays a = np.array([1, 2, 3]) b = np.array([4, 5, 6]) In\u00a0[12]: Copied! <pre># Suma de arrays\nc = a + b\nprint(c)\n</pre> # Suma de arrays c = a + b print(c) <pre>[5 7 9]\n</pre> In\u00a0[13]: Copied! <pre># Resta de arrays\nc = a - b\nprint(c)\n</pre> # Resta de arrays c = a - b print(c) <pre>[-3 -3 -3]\n</pre> In\u00a0[14]: Copied! <pre># Multiplicaci\u00f3n de arrays\nc = a * b\nprint(c)\n</pre> # Multiplicaci\u00f3n de arrays c = a * b print(c) <pre>[ 4 10 18]\n</pre> In\u00a0[15]: Copied! <pre># Divisi\u00f3n de arrays\nc = a / b\nprint(c)\n</pre> # Divisi\u00f3n de arrays c = a / b print(c) <pre>[0.25 0.4  0.5 ]\n</pre> In\u00a0[16]: Copied! <pre># Crear dos arrays\na = np.array([1, 2, 3])\nb = np.array([2, 2, 4])\n</pre> # Crear dos arrays a = np.array([1, 2, 3]) b = np.array([2, 2, 4]) In\u00a0[17]: Copied! <pre># Comparaci\u00f3n de arrays\nc = a == b\nprint(c)\n</pre> # Comparaci\u00f3n de arrays c = a == b print(c) <pre>[False  True False]\n</pre> In\u00a0[18]: Copied! <pre># Comparaci\u00f3n de arrays\nc = a &gt; b\nprint(c)\n</pre> # Comparaci\u00f3n de arrays c = a &gt; b print(c) <pre>[False False False]\n</pre> In\u00a0[19]: Copied! <pre># Comparaci\u00f3n de arrays\nc = a &lt;= b\nprint(c)\n</pre> # Comparaci\u00f3n de arrays c = a &lt;= b print(c) <pre>[ True  True  True]\n</pre> In\u00a0[20]: Copied! <pre># Crear un array\na = np.array([1, 2, 3, 4, 5])\n\n# Suma de los elementos en un array\nsum_a = np.sum(a)\nprint(f\"Suma de elementos: {sum_a}\")\n</pre> # Crear un array a = np.array([1, 2, 3, 4, 5])  # Suma de los elementos en un array sum_a = np.sum(a) print(f\"Suma de elementos: {sum_a}\") <pre>Suma de elementos: 15\n</pre> In\u00a0[21]: Copied! <pre># M\u00ednimo y m\u00e1ximo de los elementos en un array\nmin_a = np.min(a)\nmax_a = np.max(a)\n\nprint(f\"minimo: {min_a}\")\nprint(f\"maximo: {max_a}\")\n</pre> # M\u00ednimo y m\u00e1ximo de los elementos en un array min_a = np.min(a) max_a = np.max(a)  print(f\"minimo: {min_a}\") print(f\"maximo: {max_a}\") <pre>minimo: 1\nmaximo: 5\n</pre> In\u00a0[22]: Copied! <pre># Promedio y desviaci\u00f3n est\u00e1ndar de los elementos en un array\nmean_a = np.mean(a)\nstd_a = np.std(a)\n\nprint(f\"promedio: {mean_a}\")\nprint(f\"desviacion estandar: {std_a}\")\n</pre> # Promedio y desviaci\u00f3n est\u00e1ndar de los elementos en un array mean_a = np.mean(a) std_a = np.std(a)  print(f\"promedio: {mean_a}\") print(f\"desviacion estandar: {std_a}\") <pre>promedio: 3.0\ndesviacion estandar: 1.4142135623730951\n</pre> <p>Acceder a elementos individuales de un array unidimensional</p> In\u00a0[23]: Copied! <pre># Crear un array unidimensional\na = np.array([1, 2, 3, 4, 5])\n</pre> # Crear un array unidimensional a = np.array([1, 2, 3, 4, 5]) In\u00a0[24]: Copied! <pre># Acceder al primer elemento\nprint(a[0])\n</pre> # Acceder al primer elemento print(a[0]) <pre>1\n</pre> In\u00a0[25]: Copied! <pre># Acceder al \u00faltimo elemento\nprint(a[-1])\n</pre> # Acceder al \u00faltimo elemento print(a[-1]) <pre>5\n</pre> In\u00a0[26]: Copied! <pre># Acceder al tercer elemento\nprint(a[2])\n</pre> # Acceder al tercer elemento print(a[2]) <pre>3\n</pre> <p>Acceder a elementos individuales de un array multidimensional</p> In\u00a0[27]: Copied! <pre># Crear un array bidimensional\nb = np.array([\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]]\n)\n</pre> # Crear un array bidimensional b = np.array([     [1, 2, 3],     [4, 5, 6],     [7, 8, 9]] ) In\u00a0[28]: Copied! <pre># Acceder al primer elemento\nprint(b[0, 0])\n</pre> # Acceder al primer elemento print(b[0, 0]) <pre>1\n</pre> In\u00a0[29]: Copied! <pre># Acceder al \u00faltimo elemento\nprint(b[-1, -1])\n</pre> # Acceder al \u00faltimo elemento print(b[-1, -1]) <pre>9\n</pre> In\u00a0[30]: Copied! <pre># Acceder al elemento en la segunda fila y tercer columna\nprint(b[1, 2])\n</pre> # Acceder al elemento en la segunda fila y tercer columna print(b[1, 2]) <pre>6\n</pre> <p>Rebanar un array unidimensional</p> In\u00a0[31]: Copied! <pre># Rebanar un array unidimensional\na = np.array([1, 2, 3, 4, 5])\n</pre> # Rebanar un array unidimensional a = np.array([1, 2, 3, 4, 5]) In\u00a0[32]: Copied! <pre># Rebanar los primeros tres elementos\nprint(a[:3])\n</pre> # Rebanar los primeros tres elementos print(a[:3]) <pre>[1 2 3]\n</pre> In\u00a0[33]: Copied! <pre># Rebanar los \u00faltimos dos elementos\nprint(a[-2:])\n</pre> # Rebanar los \u00faltimos dos elementos print(a[-2:]) <pre>[4 5]\n</pre> In\u00a0[34]: Copied! <pre># Rebanar todos los elementos saltando de dos en dos\nprint(a[::2])\n</pre> # Rebanar todos los elementos saltando de dos en dos print(a[::2]) <pre>[1 3 5]\n</pre> <p>Rebanar un array multidimensional</p> In\u00a0[35]: Copied! <pre># Rebanar un array bidimensional\nb = np.array([\n    [1, 2, 3], \n    [4, 5, 6], \n    [7, 8, 9]]\n)\n</pre> # Rebanar un array bidimensional b = np.array([     [1, 2, 3],      [4, 5, 6],      [7, 8, 9]] ) In\u00a0[36]: Copied! <pre># Rebanar la primera y segunda fila\nprint(b[:2, :])\n</pre> # Rebanar la primera y segunda fila print(b[:2, :]) <pre>[[1 2 3]\n [4 5 6]]\n</pre> In\u00a0[37]: Copied! <pre># Rebanar la segunda y tercera columna\nprint(b[:, 1:])\n</pre> # Rebanar la segunda y tercera columna print(b[:, 1:]) <pre>[[2 3]\n [5 6]\n [8 9]]\n</pre> In\u00a0[38]: Copied! <pre># Rebanar la diagonal principal\nprint(b.diagonal())\n</pre> # Rebanar la diagonal principal print(b.diagonal()) <pre>[1 5 9]\n</pre> <p>Vectores/Matrices especializadas</p> In\u00a0[39]: Copied! <pre># Arreglo de ceros: np.zeros(shape)\nprint(\"Zeros:\")\nprint( np.zeros((3,3)) )\n</pre> # Arreglo de ceros: np.zeros(shape) print(\"Zeros:\") print( np.zeros((3,3)) ) <pre>Zeros:\n[[0. 0. 0.]\n [0. 0. 0.]\n [0. 0. 0.]]\n</pre> In\u00a0[40]: Copied! <pre># Arreglos de uno: np.ones(shape)\nprint(\"\\nOnes:\")\nprint( np.ones((3,3)) )\n</pre> # Arreglos de uno: np.ones(shape) print(\"\\nOnes:\") print( np.ones((3,3)) ) <pre>\nOnes:\n[[1. 1. 1.]\n [1. 1. 1.]\n [1. 1. 1.]]\n</pre> In\u00a0[41]: Copied! <pre># Arreglo vacio: np.empty(shape)\nprint(\"\\nEmpty:\")\nprint( np.empty([2, 2]) )\n</pre> # Arreglo vacio: np.empty(shape) print(\"\\nEmpty:\") print( np.empty([2, 2]) ) <pre>\nEmpty:\n[[2.12199579e-314 4.67296746e-307]\n [6.04736351e-321 9.88850039e-312]]\n</pre> In\u00a0[42]: Copied! <pre># Rango de valores: np.range(start, stop, step)\nprint(\"\\nRange:\")\nnp.arange(0., 10., 1.)\n</pre> # Rango de valores: np.range(start, stop, step) print(\"\\nRange:\") np.arange(0., 10., 1.)  <pre>\nRange:\n</pre> Out[42]: <pre>array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])</pre> In\u00a0[43]: Copied! <pre># Grilla de valores: np.linspace(start, end, n_values)\nprint(\"\\nRegular grid:\")\nprint( np.linspace(0., 1., 9) )\n</pre> # Grilla de valores: np.linspace(start, end, n_values) print(\"\\nRegular grid:\") print( np.linspace(0., 1., 9) ) <pre>\nRegular grid:\n[0.    0.125 0.25  0.375 0.5   0.625 0.75  0.875 1.   ]\n</pre> In\u00a0[44]: Copied! <pre># fijar semilla\nnp.random.seed(42)\n\n# Sequencia de valores aleatorios: np.random\nprint(\"\\nRandom sequences:\")\nprint( np.random.uniform(10, size=6) )\n</pre> # fijar semilla np.random.seed(42)  # Sequencia de valores aleatorios: np.random print(\"\\nRandom sequences:\") print( np.random.uniform(10, size=6) ) <pre>\nRandom sequences:\n[6.62913893 1.44357124 3.41205452 4.61207364 8.59583224 8.59604932]\n</pre> <p>Operaciones con Matrices</p> In\u00a0[45]: Copied! <pre># crear matrices\n\nA = np.array([\n    [1,2,3],\n    [4,5,6],\n    [7,8,9]\n])\n\nB = np.array([\n    [9,8,7],\n    [6,5,4],\n    [3,2,1]]\n)\n\nprint(f\"Matrix A: \\n {A}\\n\")\nprint(f\"Matrix B: \\n {B}\")\n</pre> # crear matrices  A = np.array([     [1,2,3],     [4,5,6],     [7,8,9] ])  B = np.array([     [9,8,7],     [6,5,4],     [3,2,1]] )  print(f\"Matrix A: \\n {A}\\n\") print(f\"Matrix B: \\n {B}\") <pre>Matrix A: \n [[1 2 3]\n [4 5 6]\n [7 8 9]]\n\nMatrix B: \n [[9 8 7]\n [6 5 4]\n [3 2 1]]\n</pre> In\u00a0[46]: Copied! <pre># sumar dos matrices\nprint(\"Sum:\")\nprint( A+B )\n</pre> # sumar dos matrices print(\"Sum:\") print( A+B ) <pre>Sum:\n[[10 10 10]\n [10 10 10]\n [10 10 10]]\n</pre> In\u00a0[47]: Copied! <pre># restar dos matrices\nprint(\"\\nSubtraction\")\nprint( A-B )\n</pre> # restar dos matrices print(\"\\nSubtraction\") print( A-B ) <pre>\nSubtraction\n[[-8 -6 -4]\n [-2  0  2]\n [ 4  6  8]]\n</pre> In\u00a0[48]: Copied! <pre># producto uno a uno\nprint(\"\\nProduct\")\nprint( A*B )\n</pre> # producto uno a uno print(\"\\nProduct\") print( A*B ) <pre>\nProduct\n[[ 9 16 21]\n [24 25 24]\n [21 16  9]]\n</pre> In\u00a0[49]: Copied! <pre># producto matricial\nprint(\"\\nMatricial Product\")\nprint( np.dot(A,B) )\n</pre> # producto matricial print(\"\\nMatricial Product\") print( np.dot(A,B) ) <pre>\nMatricial Product\n[[ 30  24  18]\n [ 84  69  54]\n [138 114  90]]\n</pre> In\u00a0[50]: Copied! <pre># potencia\nprint(\"\\n Power\")\nprint( A**2 )\n</pre> # potencia print(\"\\n Power\") print( A**2 ) <pre>\n Power\n[[ 1  4  9]\n [16 25 36]\n [49 64 81]]\n</pre> <p>Algunas funciones especiales</p> In\u00a0[51]: Copied! <pre># crear matriz\nA = np.array([\n    [1,2,3],\n    [4,5,6],\n    [7,8,9]\n])\n</pre> # crear matriz A = np.array([     [1,2,3],     [4,5,6],     [7,8,9] ]) In\u00a0[52]: Copied! <pre>print(\"funcion exponencial\")\nprint( np.exp(A) )\n</pre> print(\"funcion exponencial\") print( np.exp(A) ) <pre>funcion exponencial\n[[2.71828183e+00 7.38905610e+00 2.00855369e+01]\n [5.45981500e+01 1.48413159e+02 4.03428793e+02]\n [1.09663316e+03 2.98095799e+03 8.10308393e+03]]\n</pre> In\u00a0[53]: Copied! <pre>print(\"funcion seno\")\nprint( np.sin(A) )\n</pre> print(\"funcion seno\") print( np.sin(A) ) <pre>funcion seno\n[[ 0.84147098  0.90929743  0.14112001]\n [-0.7568025  -0.95892427 -0.2794155 ]\n [ 0.6569866   0.98935825  0.41211849]]\n</pre> In\u00a0[54]: Copied! <pre>print(\"funcion coseno\")\nprint( np.cos(A))\n</pre> print(\"funcion coseno\") print( np.cos(A)) <pre>funcion coseno\n[[ 0.54030231 -0.41614684 -0.9899925 ]\n [-0.65364362  0.28366219  0.96017029]\n [ 0.75390225 -0.14550003 -0.91113026]]\n</pre> In\u00a0[55]: Copied! <pre>print(\"funcion tangente\")\nprint( np.tan(A) )\n</pre> print(\"funcion tangente\") print( np.tan(A) ) <pre>funcion tangente\n[[ 1.55740772 -2.18503986 -0.14254654]\n [ 1.15782128 -3.38051501 -0.29100619]\n [ 0.87144798 -6.79971146 -0.45231566]]\n</pre> <p>Operaciones \u00e1lgebra l\u00edneal</p> In\u00a0[56]: Copied! <pre># crear matriz\nA = np.array([[1,2],\n              [3,4]])\n</pre> # crear matriz A = np.array([[1,2],               [3,4]]) In\u00a0[57]: Copied! <pre># matriz transpuesta\nprint(\"Transpose: \")\nprint( A.T )\n</pre> # matriz transpuesta print(\"Transpose: \") print( A.T ) <pre>Transpose: \n[[1 3]\n [2 4]]\n</pre> In\u00a0[58]: Copied! <pre># determinante\nprint(\"determinant\")\nprint( round(np.linalg.det(A) ,2))\n</pre> # determinante print(\"determinant\") print( round(np.linalg.det(A) ,2)) <pre>determinant\n-2.0\n</pre> In\u00a0[59]: Copied! <pre># Matriz Inversa\nprint(\"Inverse\")\nprint( np.linalg.inv(A) )\n</pre> # Matriz Inversa print(\"Inverse\") print( np.linalg.inv(A) ) <pre>Inverse\n[[-2.   1. ]\n [ 1.5 -0.5]]\n</pre> In\u00a0[60]: Copied! <pre># traza\nprint(\"Trace\")\nprint( np.trace(A))\n</pre> # traza print(\"Trace\") print( np.trace(A)) <pre>Trace\n5\n</pre> In\u00a0[61]: Copied! <pre># Valores y vectores propios\neigenvalues, eigenvectors = np.linalg.eig(A) \n\nprint(\"eigenvalues\")\nprint( eigenvalues )\nprint(\"\\neigenvectors\")\nprint( eigenvectors )\n</pre> # Valores y vectores propios eigenvalues, eigenvectors = np.linalg.eig(A)   print(\"eigenvalues\") print( eigenvalues ) print(\"\\neigenvectors\") print( eigenvectors ) <pre>eigenvalues\n[-0.37228132  5.37228132]\n\neigenvectors\n[[-0.82456484 -0.41597356]\n [ 0.56576746 -0.90937671]]\n</pre> In\u00a0[62]: Copied! <pre># Valores y vectores propios\neigenvalues, eigenvectors = np.linalg.eig(A) \n\nprint(\"eigenvalues\")\nprint( eigenvalues )\nprint(\"\\neigenvectors\")\nprint( eigenvectors )\n</pre> # Valores y vectores propios eigenvalues, eigenvectors = np.linalg.eig(A)   print(\"eigenvalues\") print( eigenvalues ) print(\"\\neigenvectors\") print( eigenvectors ) <pre>eigenvalues\n[-0.37228132  5.37228132]\n\neigenvectors\n[[-0.82456484 -0.41597356]\n [ 0.56576746 -0.90937671]]\n</pre> <p>Resolver sitema de ecuaciones</p> In\u00a0[63]: Copied! <pre># crear matriz\nA = np.array([[1,2],\n              [3,4]])\n\n# sistemas lineales: Ax = b\nb = np.array([[5.], [7.]])\n</pre> # crear matriz A = np.array([[1,2],               [3,4]])  # sistemas lineales: Ax = b b = np.array([[5.], [7.]]) In\u00a0[64]: Copied! <pre>print(\"linear system: Ax=b\")\nprint(\"\\nx = \")\nprint( np.linalg.solve(A, b) )\n</pre> print(\"linear system: Ax=b\") print(\"\\nx = \") print( np.linalg.solve(A, b) ) <pre>linear system: Ax=b\n\nx = \n[[-3.]\n [ 4.]]\n</pre> <p></p> <p>Broadcasting es un t\u00e9rmino utilizado en NumPy para describir la forma en que las operaciones entre arrays con diferentes formas se manejan autom\u00e1ticamente por NumPy. En otras palabras, cuando se realizan operaciones aritm\u00e9ticas entre dos arrays de diferentes formas, NumPy ajusta autom\u00e1ticamente la forma del array m\u00e1s peque\u00f1o para que coincida con la forma del array m\u00e1s grande antes de realizar la operaci\u00f3n. Esto permite que las operaciones se realicen de manera eficiente sin la necesidad de crear copias adicionales de los datos.</p> <p>Las reglas de broadcasting en NumPy son las siguientes:</p> <ul> <li><p>Si los dos arrays tienen el mismo n\u00famero de dimensiones, pero las formas no son iguales, NumPy agrega 1 a la forma del array m\u00e1s peque\u00f1o para que coincida con la forma del array m\u00e1s grande.</p> </li> <li><p>Si la forma de los dos arrays no es la misma y no tienen el mismo n\u00famero de dimensiones, NumPy agrega 1 a las dimensiones menos y expande las formas de los arrays para que sean iguales en la dimensi\u00f3n m\u00e1s alta.</p> </li> <li><p>Si la forma de los dos arrays no es la misma y alguna dimensi\u00f3n no coincide, NumPy genera un error.</p> </li> </ul> In\u00a0[65]: Copied! <pre># example 01\na = np.arange(3)+ 5\nprint(f\"np.arange(3)+ 5:\\n{a}\" )\n</pre> # example 01 a = np.arange(3)+ 5 print(f\"np.arange(3)+ 5:\\n{a}\" ) <pre>np.arange(3)+ 5:\n[5 6 7]\n</pre> In\u00a0[66]: Copied! <pre># example 02\nb = np.ones((3,3))+np.arange(3)\nprint(f\"np.ones((3,3))+np.arange(3):\\n{b}\" )\n</pre> # example 02 b = np.ones((3,3))+np.arange(3) print(f\"np.ones((3,3))+np.arange(3):\\n{b}\" ) <pre>np.ones((3,3))+np.arange(3):\n[[1. 2. 3.]\n [1. 2. 3.]\n [1. 2. 3.]]\n</pre> In\u00a0[67]: Copied! <pre># example 03\nc = np.arange(3).reshape((3,1)) +  np.arange(3)\nprint(f\"np.arange(3).reshape((3,1)) +  np.arange(3):\\n{c }\" )\n</pre> # example 03 c = np.arange(3).reshape((3,1)) +  np.arange(3) print(f\"np.arange(3).reshape((3,1)) +  np.arange(3):\\n{c }\" ) <pre>np.arange(3).reshape((3,1)) +  np.arange(3):\n[[0 1 2]\n [1 2 3]\n [2 3 4]]\n</pre>"},{"location":"lectures/data_manipulation/sc_01/#numpy","title":"Numpy\u00b6","text":""},{"location":"lectures/data_manipulation/sc_01/#python-lists-vs-numpy-arrays","title":"Python Lists vs Numpy Arrays\u00b6","text":"<p>Las listas de Python y los arrays de NumPy son dos estructuras de datos diferentes que se utilizan para almacenar y manipular conjuntos de datos en Python.</p> <ul> <li><p>Las listas de Python son una colecci\u00f3n de elementos que pueden ser de diferentes tipos de datos, como enteros, flotantes, cadenas, etc. Pueden ser de longitud variable y se pueden modificar en tiempo de ejecuci\u00f3n. Las listas de Python son m\u00e1s flexibles y vers\u00e1tiles que los arrays de NumPy, pero pueden ser m\u00e1s lentas para operaciones matem\u00e1ticas y num\u00e9ricas.</p> </li> <li><p>Los arrays de NumPy son una estructura de datos m\u00e1s especializada que se utiliza para trabajar con datos num\u00e9ricos, como matrices y vectores. Son m\u00e1s r\u00e1pidos y eficientes que las listas de Python para operaciones num\u00e9ricas y matem\u00e1ticas, y proporcionan muchas funciones y m\u00e9todos \u00fatiles para el procesamiento de datos, como operaciones matriciales y de \u00e1lgebra lineal. Los arrays de NumPy tienen una longitud fija y no se pueden modificar una vez creados.</p> </li> </ul> <p>Una pregunta com\u00fan para principiantes es cu\u00e1l es la verdadera diferencia aqu\u00ed. La respuesta es el rendimiento. Las estructuras de datos de Numpy funcionan mejor en:</p> <ul> <li>Tama\u00f1o: las estructuras de datos de Numpy ocupan menos espacio</li> <li>Rendimiento: necesitan velocidad y son m\u00e1s r\u00e1pidos que las listas</li> <li>Funcionalidad: SciPy y NumPy tienen funciones optimizadas, como las operaciones de \u00e1lgebra lineal integradas.</li> </ul>"},{"location":"lectures/data_manipulation/sc_01/#diferencias-tiempo-memoria","title":"Diferencias Tiempo - Memoria\u00b6","text":"<p>Los principales beneficios del uso de matrices NumPy deber\u00edan ser un menor consumo de memoria y un mejor comportamiento en tiempo de ejecuci\u00f3n.</p> <ul> <li><p>Para las listas de Python podemos concluir de esto que para cada elemento nuevo, necesitamos otros ocho bytes para la referencia al nuevo objeto. El nuevo objeto entero en s\u00ed consume 28 bytes. El tama\u00f1o de una lista lst sin el tama\u00f1o de los elementos se puede calcular con:</p> <pre><code>                  64 + 8 * len (lst) + + len (lst) * 28</code></pre> </li> </ul>"},{"location":"lectures/data_manipulation/sc_01/#objetos-en-numpy","title":"Objetos en Numpy\u00b6","text":"<p>En NumPy, el objeto principal es el array multidimensional (llamado <code>ndarray</code>), que es una estructura de datos eficiente para el procesamiento de datos num\u00e9ricos. Los arrays en NumPy son objetos homog\u00e9neos y de tama\u00f1o fijo que contienen elementos del mismo tipo de datos.</p> <p>Aqu\u00ed hay algunos ejemplos de objetos en NumPy:</p>"},{"location":"lectures/data_manipulation/sc_01/#operaciones","title":"Operaciones\u00b6","text":"<p>NumPy proporciona una variedad de operaciones b\u00e1sicas que se pueden realizar en los arrays, como operaciones aritm\u00e9ticas, operaciones de comparaci\u00f3n y operaciones de agregaci\u00f3n. Aqu\u00ed hay algunos ejemplos de operaciones b\u00e1sicas en NumPy:</p>"},{"location":"lectures/data_manipulation/sc_01/#operaciones-aritmeticas","title":"Operaciones aritm\u00e9ticas\u00b6","text":""},{"location":"lectures/data_manipulation/sc_01/#operaciones-de-comparacion","title":"Operaciones de comparaci\u00f3n\u00b6","text":""},{"location":"lectures/data_manipulation/sc_01/#operaciones-de-estadisticas","title":"Operaciones de estad\u00edsticas\u00b6","text":""},{"location":"lectures/data_manipulation/sc_01/#indexacion","title":"Indexaci\u00f3n\u00b6","text":"<p>La indexaci\u00f3n en NumPy es similar a la indexaci\u00f3n en listas de Python, pero se extiende a m\u00faltiples dimensiones. En NumPy, los arrays se pueden indexar y rebanar para acceder a elementos y subarrays espec\u00edficos. Aqu\u00ed hay algunos ejemplos de indexaci\u00f3n en NumPy:</p>"},{"location":"lectures/data_manipulation/sc_01/#algebra-lineal","title":"Algebra Lineal\u00b6","text":"<p>NumPy es una biblioteca popular en Python para el \u00e1lgebra lineal, que es una rama de las matem\u00e1ticas que se enfoca en el estudio de vectores, matrices y sistemas de ecuaciones lineales. NumPy proporciona una gran cantidad de funciones y m\u00e9todos para realizar operaciones de \u00e1lgebra lineal de manera eficiente en Python.</p> <p>Aqu\u00ed hay algunos ejemplos de operaciones de \u00e1lgebra lineal que se pueden realizar con NumPy:</p>"},{"location":"lectures/data_manipulation/sc_01/#broadcasting","title":"Broadcasting\u00b6","text":""},{"location":"lectures/data_manipulation/sc_01/#referencias","title":"Referencias\u00b6","text":"<ol> <li>Quickstart tutorial-numpy</li> <li>Mathematical functions</li> </ol>"},{"location":"lectures/data_manipulation/sc_02/","title":"SciPy","text":"<p><code>SciPy</code> se basa en el marco <code>NumPy</code> de bajo nivel para matrices multidimensionales y proporciona una gran cantidad de algoritmos cient\u00edficos de alto nivel. Algunos de los temas que cubre SciPy son:</p> <ul> <li>Special functions (scipy.special)</li> <li>Integration (scipy.integrate)</li> <li>Optimization (scipy.optimize)</li> <li>Interpolation (scipy.interpolate)</li> <li>Fourier Transforms (scipy.fftpack)</li> <li>Signal Processing (scipy.signal)</li> <li>Sparse Eigenvalue Problems (scipy.sparse)</li> <li>Statistics (scipy.stats)</li> </ul> <p>Cada uno de estos subm\u00f3dulos proporciona una serie de funciones y clases que se pueden utilizar para resolver problemas en sus respectivos temas.</p> <p>En esta lecci\u00f3n veremos c\u00f3mo usar algunos de estos subpaquetes.</p> <p>Para acceder al paquete SciPy en un programa Python, comenzamos importando todo desde el m\u00f3dulo <code>scipy</code>.</p> In\u00a0[1]: Copied! <pre>from scipy import *\n</pre> from scipy import * <p>Si solo necesitamos usar parte del marco SciPy, podemos incluir selectivamente solo aquellos m\u00f3dulos que nos interesan. Por ejemplo, para incluir el paquete de \u00e1lgebra lineal bajo el nombre <code>la</code>, podemos hacer:</p> In\u00a0[2]: Copied! <pre>import scipy.linalg as la\n</pre> import scipy.linalg as la <p>Un gran n\u00famero de funciones matem\u00e1ticas especiales son importantes para muchos problemas de f\u00edsica computacional. SciPy proporciona implementaciones de un conjunto muy extenso de funciones especiales. Para obtener m\u00e1s detalles, consulte la lista de funciones en el documento de referencia en http://docs.scipy.org/doc/scipy/reference/special.html#module-scipy.special.</p> <p>Para demostrar el uso t\u00edpico de funciones especiales, veremos con m\u00e1s detalle las funciones de Bessel:</p> In\u00a0[3]: Copied! <pre>#\n# The scipy.special module includes a large number of Bessel-functions\n# Here we will use the functions jn and yn, which are the Bessel functions \n# of the first and second kind and real-valued order. We also include the \n# function jn_zeros and yn_zeros that gives the zeroes of the functions jn\n# and yn.\n#\nfrom scipy.special import jn, yn, jn_zeros, yn_zeros\nimport numpy as np\nimport matplotlib.pyplot as plt\n</pre> # # The scipy.special module includes a large number of Bessel-functions # Here we will use the functions jn and yn, which are the Bessel functions  # of the first and second kind and real-valued order. We also include the  # function jn_zeros and yn_zeros that gives the zeroes of the functions jn # and yn. # from scipy.special import jn, yn, jn_zeros, yn_zeros import numpy as np import matplotlib.pyplot as plt In\u00a0[4]: Copied! <pre>n = 0    # order\nx = 0.0\n\n# Bessel function of first kind\nprint( f\"J_{n}({x}) = {jn(n, x)}\")\n</pre> n = 0    # order x = 0.0  # Bessel function of first kind print( f\"J_{n}({x}) = {jn(n, x)}\") <pre>J_0(0.0) = 1.0\n</pre> In\u00a0[5]: Copied! <pre>x = 1.0\n# Bessel function of second kind\nprint( f\"Y_{n}({x}) = {yn(n, x)}\")\n</pre> x = 1.0 # Bessel function of second kind print( f\"Y_{n}({x}) = {yn(n, x)}\") <pre>Y_0(1.0) = 0.08825696421567697\n</pre> In\u00a0[6]: Copied! <pre>x = np.linspace(0, 10, 100)\n\nfig, ax = plt.subplots()\nfor n in range(4):\n    ax.plot(x, jn(n, x), label=r\"$J_%d(x)$\" % n)\nax.legend();\nplt.show()\n</pre> x = np.linspace(0, 10, 100)  fig, ax = plt.subplots() for n in range(4):     ax.plot(x, jn(n, x), label=r\"$J_%d(x)$\" % n) ax.legend(); plt.show() In\u00a0[7]: Copied! <pre># zeros of Bessel functions\nn = 0 # order\nm = 4 # number of roots to compute\njn_zeros(n, m)\n</pre> # zeros of Bessel functions n = 0 # order m = 4 # number of roots to compute jn_zeros(n, m) Out[7]: <pre>array([ 2.40482556,  5.52007811,  8.65372791, 11.79153444])</pre> <p>Evaluaci\u00f3n num\u00e9rica de una funci\u00f3n del tipo $\\displaystyle \\int_a^b f(x) dx$</p> <p>se llama cuadratura num\u00e9rica, o simplemente cuadratura. SciPy proporciona una serie de funciones para diferentes tipos de cuadratura, por ejemplo, <code>quad</code>,<code> dblquad</code> y <code>tplquad</code> para integrales simples, dobles y triples, respectivamente.</p> In\u00a0[8]: Copied! <pre>from scipy.integrate import quad, dblquad, tplquad\n</pre> from scipy.integrate import quad, dblquad, tplquad <p>La funci\u00f3n <code>quad</code> toma una gran cantidad de argumentos opcionales, que se pueden usar para ajustar el comportamiento de la funci\u00f3n (prueba con<code>help (quad)</code>para obtener m\u00e1s detalles).</p> <p>El uso b\u00e1sico es el siguiente:</p> In\u00a0[9]: Copied! <pre># define a simple function for the integrand\ndef f(x):\n    return x\n</pre> # define a simple function for the integrand def f(x):     return x In\u00a0[10]: Copied! <pre>x_lower = 0 # the lower limit of x\nx_upper = 1 # the upper limit of x\n\nval, abserr = quad(f, x_lower, x_upper)\n\nprint(f\"integral value = {val} absolute error = {abserr}\" )\n</pre> x_lower = 0 # the lower limit of x x_upper = 1 # the upper limit of x  val, abserr = quad(f, x_lower, x_upper)  print(f\"integral value = {val} absolute error = {abserr}\" ) <pre>integral value = 0.5 absolute error = 5.551115123125783e-15\n</pre> <p>Si necesitamos pasar argumentos adicionales a la funci\u00f3n integrando, podemos usar el argumento de palabra clave <code>args</code>:</p> In\u00a0[11]: Copied! <pre>def integrand(x, n):\n    \"\"\"\n    Bessel function of first kind and order n. \n    \"\"\"\n    return jn(n, x)\n\n\nx_lower = 0  # the lower limit of x\nx_upper = 10 # the upper limit of x\n\nval, abserr = quad(integrand, x_lower, x_upper, args=(3,))\n\nprint(val, abserr )\n</pre> def integrand(x, n):     \"\"\"     Bessel function of first kind and order n.      \"\"\"     return jn(n, x)   x_lower = 0  # the lower limit of x x_upper = 10 # the upper limit of x  val, abserr = quad(integrand, x_lower, x_upper, args=(3,))  print(val, abserr ) <pre>0.7366751370811073 9.389126882496413e-13\n</pre> <p>Para funciones simples, podemos usar una funci\u00f3n lambda (funci\u00f3n sin nombre) en lugar de definir expl\u00edcitamente una funci\u00f3n para el integrando:</p> In\u00a0[12]: Copied! <pre>val, abserr = quad(lambda x: np.exp(-x ** 2), -np.inf, np.inf)\n\nprint( f\"numerical  = {val} {abserr}\")\n</pre> val, abserr = quad(lambda x: np.exp(-x ** 2), -np.inf, np.inf)  print( f\"numerical  = {val} {abserr}\") <pre>numerical  = 1.7724538509055159 1.4202636756659625e-08\n</pre> In\u00a0[13]: Copied! <pre>analytical = np.lib.scimath.sqrt(np.pi)\nprint(f\"analytical = {analytical}\")\n</pre> analytical = np.lib.scimath.sqrt(np.pi) print(f\"analytical = {analytical}\") <pre>analytical = 1.7724538509055159\n</pre> <p>Como se muestra en el ejemplo anterior, tambi\u00e9n podemos usar <code>Inf</code> o <code>-Inf</code> como l\u00edmites integrales.</p> <p>La integraci\u00f3n de dimensiones superiores funciona de la misma manera:</p> In\u00a0[14]: Copied! <pre>def integrand(x, y):\n    return np.exp(-x**2-y**2)\n\nx_lower = 0  \nx_upper = 10\ny_lower = 0\ny_upper = 10\n\nval, abserr = dblquad(integrand, x_lower, x_upper, lambda x : y_lower, lambda x: y_upper)\n\nprint( val, abserr )\n</pre> def integrand(x, y):     return np.exp(-x**2-y**2)  x_lower = 0   x_upper = 10 y_lower = 0 y_upper = 10  val, abserr = dblquad(integrand, x_lower, x_upper, lambda x : y_lower, lambda x: y_upper)  print( val, abserr ) <pre>0.7853981633974476 1.3753098510218537e-08\n</pre> <p>Observe c\u00f3mo tuvimos que pasar funciones lambda para los l\u00edmites de la integraci\u00f3n $y$, ya que estas en general pueden ser funciones de $x$.</p> In\u00a0[15]: Copied! <pre>from scipy.integrate import odeint, ode\n</pre> from scipy.integrate import odeint, ode <p>Un sistema de EDO se suele formular en forma est\u00e1ndar antes de ser atacado num\u00e9ricamente. La forma est\u00e1ndar es:</p> <p>$y' = f(y, t)$, donde:   $y = [y_1(t), y_2(t), ..., y_n(t)]$</p> <p>y $ f $ es alguna funci\u00f3n que da las derivadas de la funci\u00f3n $ y_i (t) $. Para resolver una EDO necesitamos conocer la funci\u00f3n $ f $ y una condici\u00f3n inicial, $ y (0) $.</p> <p>Tenga en cuenta que las EDO de orden superior siempre se pueden escribir de esta forma introduciendo nuevas variables para las derivadas intermedias.</p> <p>Una vez que hemos definido la funci\u00f3n de Python <code>f</code> y la matriz<code> y_0</code> (que es $ f $ y $ y (0) $ en la formulaci\u00f3n matem\u00e1tica), podemos usar la funci\u00f3n <code>odeint</code> como:</p> <pre>y_t = odeint(f, y_0, t)\n</pre> <p>donde <code>t</code> es una matriz con coordenadas de tiempo para resolver el problema de ODE. <code>y_t</code> es una matriz con una fila para cada punto en el tiempo en<code> t</code>, donde cada columna corresponde a una soluci\u00f3n <code>y_i (t)</code> en ese momento.</p> <p>Veremos c\u00f3mo podemos implementar <code>f</code> e<code> y_0</code> en c\u00f3digo Python en los ejemplos siguientes.</p> <p></p> <p>Las ecuaciones de movimiento del p\u00e9ndulo se dan en la p\u00e1gina wiki:</p> <p>${\\dot \\theta_1} = \\frac{6}{m\\ell^2} \\frac{ 2 p_{\\theta_1} - 3 \\cos(\\theta_1-\\theta_2) p_{\\theta_2}}{16 - 9 \\cos^2(\\theta_1-\\theta_2)}$</p> <p>${\\dot \\theta_2} = \\frac{6}{m\\ell^2} \\frac{ 8 p_{\\theta_2} - 3 \\cos(\\theta_1-\\theta_2) p_{\\theta_1}}{16 - 9 \\cos^2(\\theta_1-\\theta_2)}.$</p> <p>${\\dot p_{\\theta_1}} = -\\frac{1}{2} m \\ell^2 \\left [ {\\dot \\theta_1} {\\dot \\theta_2} \\sin (\\theta_1-\\theta_2) + 3 \\frac{g}{\\ell} \\sin \\theta_1 \\right ]$</p> <p>${\\dot p_{\\theta_2}} = -\\frac{1}{2} m \\ell^2 \\left [ -{\\dot \\theta_1} {\\dot \\theta_2} \\sin (\\theta_1-\\theta_2) +  \\frac{g}{\\ell} \\sin \\theta_2 \\right]$</p> <p>Para simplificar el seguimiento del c\u00f3digo Python, introduzcamos nuevos nombres de variables y la notaci\u00f3n vectorial: $x = [\\theta_1, \\theta_2, p_{\\theta_1}, p_{\\theta_2}]$</p> <p>${\\dot x_1} = \\frac{6}{m\\ell^2} \\frac{ 2 x_3 - 3 \\cos(x_1-x_2) x_4}{16 - 9 \\cos^2(x_1-x_2)}$</p> <p>${\\dot x_2} = \\frac{6}{m\\ell^2} \\frac{ 8 x_4 - 3 \\cos(x_1-x_2) x_3}{16 - 9 \\cos^2(x_1-x_2)}$</p> <p>${\\dot x_3} = -\\frac{1}{2} m \\ell^2 \\left [ {\\dot x_1} {\\dot x_2} \\sin (x_1-x_2) + 3 \\frac{g}{\\ell} \\sin x_1 \\right ]$</p> <p>${\\dot x_4} = -\\frac{1}{2} m \\ell^2 \\left [ -{\\dot x_1} {\\dot x_2} \\sin (x_1-x_2) +  \\frac{g}{\\ell} \\sin x_2 \\right]$</p> In\u00a0[16]: Copied! <pre>g = 9.82\nL = 0.5\nm = 0.1\n\ndef dx(x, t):\n    \"\"\"\n    The right-hand side of the pendulum ODE\n    \"\"\"\n    x1, x2, x3, x4 = x[0], x[1], x[2], x[3]\n    \n    dx1 = 6.0/(m*L**2) * (2 * x3 - 3 * np.cos(x1-x2) * x4)/(16 - 9 *  np.cos(x1-x2)**2)\n    dx2 = 6.0/(m*L**2) * (8 * x4 - 3 *  np.cos(x1-x2) * x3)/(16 - 9 *  np.cos(x1-x2)**2)\n    dx3 = -0.5 * m * L**2 * ( dx1 * dx2 *  np.sin(x1-x2) + 3 * (g/L) *  np.sin(x1))\n    dx4 = -0.5 * m * L**2 * (-dx1 * dx2 *  np.sin(x1-x2) + (g/L) *  np.sin(x2))\n    \n    return [dx1, dx2, dx3, dx4]\n</pre> g = 9.82 L = 0.5 m = 0.1  def dx(x, t):     \"\"\"     The right-hand side of the pendulum ODE     \"\"\"     x1, x2, x3, x4 = x[0], x[1], x[2], x[3]          dx1 = 6.0/(m*L**2) * (2 * x3 - 3 * np.cos(x1-x2) * x4)/(16 - 9 *  np.cos(x1-x2)**2)     dx2 = 6.0/(m*L**2) * (8 * x4 - 3 *  np.cos(x1-x2) * x3)/(16 - 9 *  np.cos(x1-x2)**2)     dx3 = -0.5 * m * L**2 * ( dx1 * dx2 *  np.sin(x1-x2) + 3 * (g/L) *  np.sin(x1))     dx4 = -0.5 * m * L**2 * (-dx1 * dx2 *  np.sin(x1-x2) + (g/L) *  np.sin(x2))          return [dx1, dx2, dx3, dx4] In\u00a0[17]: Copied! <pre># choose an initial state\nx0 = [np.pi/4, np.pi/2, 0, 0]\n</pre> # choose an initial state x0 = [np.pi/4, np.pi/2, 0, 0] In\u00a0[18]: Copied! <pre># time coodinate to solve the ODE for: from 0 to 10 seconds\nt = np.linspace(0, 10, 250)\n</pre> # time coodinate to solve the ODE for: from 0 to 10 seconds t = np.linspace(0, 10, 250) In\u00a0[19]: Copied! <pre># solve the ODE problem\nx = odeint(dx, x0, t)\n</pre> # solve the ODE problem x = odeint(dx, x0, t) In\u00a0[20]: Copied! <pre># plot the angles as a function of time\n\nfig, axes = plt.subplots(1,2, figsize=(12,4))\naxes[0].plot(t, x[:, 0], 'r', label=\"theta1\")\naxes[0].plot(t, x[:, 1], 'b', label=\"theta2\")\n\n\nx1 = + L *  np.sin(x[:, 0])\ny1 = - L *  np.cos(x[:, 0])\n\nx2 = x1 + L *  np.sin(x[:, 1])\ny2 = y1 - L *  np.cos(x[:, 1])\n    \naxes[1].plot(x1, y1, 'r', label=\"pendulum1\")\naxes[1].plot(x2, y2, 'b', label=\"pendulum2\")\naxes[1].set_ylim([-1, 0])\naxes[1].set_xlim([1, -1]);\n</pre> # plot the angles as a function of time  fig, axes = plt.subplots(1,2, figsize=(12,4)) axes[0].plot(t, x[:, 0], 'r', label=\"theta1\") axes[0].plot(t, x[:, 1], 'b', label=\"theta2\")   x1 = + L *  np.sin(x[:, 0]) y1 = - L *  np.cos(x[:, 0])  x2 = x1 + L *  np.sin(x[:, 1]) y2 = y1 - L *  np.cos(x[:, 1])      axes[1].plot(x1, y1, 'r', label=\"pendulum1\") axes[1].plot(x2, y2, 'b', label=\"pendulum2\") axes[1].set_ylim([-1, 0]) axes[1].set_xlim([1, -1]); In\u00a0[21]: Copied! <pre>def dy(y, t, zeta, w0):\n    \"\"\"\n    The right-hand side of the damped oscillator ODE\n    \"\"\"\n    x, p = y[0], y[1]\n    \n    dx = p\n    dp = -2 * zeta * w0 * p - w0**2 * x\n\n    return [dx, dp]\n</pre> def dy(y, t, zeta, w0):     \"\"\"     The right-hand side of the damped oscillator ODE     \"\"\"     x, p = y[0], y[1]          dx = p     dp = -2 * zeta * w0 * p - w0**2 * x      return [dx, dp] In\u00a0[22]: Copied! <pre># initial state: \ny0 = [1.0, 0.0]\n</pre> # initial state:  y0 = [1.0, 0.0] In\u00a0[23]: Copied! <pre># time coodinate to solve the ODE for\nt =  np.linspace(0, 10, 1000)\nw0 = 2*np.pi*1.0\n</pre> # time coodinate to solve the ODE for t =  np.linspace(0, 10, 1000) w0 = 2*np.pi*1.0 In\u00a0[24]: Copied! <pre># solve the ODE problem for three different values of the damping ratio\n\ny1 = odeint(dy, y0, t, args=(0.0, w0)) # undamped\ny2 = odeint(dy, y0, t, args=(0.2, w0)) # under damped\ny3 = odeint(dy, y0, t, args=(1.0, w0)) # critial damping\ny4 = odeint(dy, y0, t, args=(5.0, w0)) # over damped\n</pre> # solve the ODE problem for three different values of the damping ratio  y1 = odeint(dy, y0, t, args=(0.0, w0)) # undamped y2 = odeint(dy, y0, t, args=(0.2, w0)) # under damped y3 = odeint(dy, y0, t, args=(1.0, w0)) # critial damping y4 = odeint(dy, y0, t, args=(5.0, w0)) # over damped In\u00a0[25]: Copied! <pre>fig, ax = plt.subplots()\nax.plot(t, y1[:,0], 'k', label=\"undamped\", linewidth=0.25)\nax.plot(t, y2[:,0], 'r', label=\"under damped\")\nax.plot(t, y3[:,0], 'b', label=r\"critical damping\")\nax.plot(t, y4[:,0], 'g', label=\"over damped\")\nax.legend();\n</pre> fig, ax = plt.subplots() ax.plot(t, y1[:,0], 'k', label=\"undamped\", linewidth=0.25) ax.plot(t, y2[:,0], 'r', label=\"under damped\") ax.plot(t, y3[:,0], 'b', label=r\"critical damping\") ax.plot(t, y4[:,0], 'g', label=\"over damped\") ax.legend(); In\u00a0[26]: Copied! <pre>from numpy.fft import fftfreq\nfrom scipy.fftpack import *\n</pre> from numpy.fft import fftfreq from scipy.fftpack import * <p>Para demostrar c\u00f3mo hacer una transformada r\u00e1pida de Fourier con SciPy, veamos la FFT de la soluci\u00f3n al oscilador amortiguado de la secci\u00f3n anterior:</p> In\u00a0[27]: Copied! <pre>N = len(t)\ndt = t[1]-t[0]\n\n# calculate the fast fourier transform\n# y2 is the solution to the under-damped oscillator from the previous section\nF = fft(y2[:,0]) \n\n# calculate the frequencies for the components in F\nw = fftfreq(N, dt)\n</pre> N = len(t) dt = t[1]-t[0]  # calculate the fast fourier transform # y2 is the solution to the under-damped oscillator from the previous section F = fft(y2[:,0])   # calculate the frequencies for the components in F w = fftfreq(N, dt) In\u00a0[28]: Copied! <pre>fig, ax = plt.subplots(figsize=(9,3))\nax.plot(w, abs(F));\n</pre> fig, ax = plt.subplots(figsize=(9,3)) ax.plot(w, abs(F)); <p>Dado que la se\u00f1al es real, el espectro es sim\u00e9trico. Por lo tanto, solo necesitamos trazar la parte que corresponde a las frecuencias positivas. Para extraer esa parte de <code>w</code> y<code> F</code> podemos usar algunos de los trucos de indexaci\u00f3n para matrices NumPy que vimos en la lecci\u00f3n 2:</p> In\u00a0[29]: Copied! <pre>indices =  np.where(w &gt; 0) # select only indices for elements that corresponds to positive frequencies\nw_pos = w[indices]\nF_pos = F[indices]\n</pre> indices =  np.where(w &gt; 0) # select only indices for elements that corresponds to positive frequencies w_pos = w[indices] F_pos = F[indices] In\u00a0[30]: Copied! <pre>fig, ax = plt.subplots(figsize=(9,3))\nax.plot(w_pos, abs(F_pos))\nax.set_xlim(0, 5);\n</pre> fig, ax = plt.subplots(figsize=(9,3)) ax.plot(w_pos, abs(F_pos)) ax.set_xlim(0, 5); <p>Como era de esperar, ahora vemos un pico en el espectro que se centra alrededor de 1, que es la frecuencia que usamos en el ejemplo del oscilador amortiguado.</p> <p>Las matrices dispersas suelen ser \u00fatiles en simulaciones num\u00e9ricas que tratan con sistemas grandes, si el problema se puede describir en forma de matriz donde las matrices o vectores contienen en su mayor\u00eda ceros. Scipy tiene un buen soporte para matrices dispersas, con operaciones b\u00e1sicas de \u00e1lgebra lineal (como resoluci\u00f3n de ecuaciones, c\u00e1lculos de valores propios, etc.).</p> <p>Hay muchas estrategias posibles para almacenar matrices dispersas de manera eficiente. Algunos de los m\u00e1s comunes son el llamado formulario de coordenadas (COO), formulario de lista de lista (LIL) y CSC de columna comprimida y dispersa (y fila, CSR). Cada formato tiene algunas ventajas y desventajas. La mayor\u00eda de los algoritmos computacionales (resoluci\u00f3n de ecuaciones, multiplicaci\u00f3n de matriz-matriz, etc.) se pueden implementar de manera eficiente usando formatos CSR o CSC, pero no son tan intuitivos ni tan f\u00e1ciles de inicializar. Muy a menudo, una matriz dispersa se crea inicialmente en formato COO o LIL (donde podemos agregar elementos de manera eficiente a los datos de la matriz dispersa), y luego se convierte a CSC o CSR antes de usarse en c\u00e1lculos reales.</p> <p>Para obtener m\u00e1s informaci\u00f3n sobre estos formatos dispersos, consulte p. Ej. http://en.wikipedia.org/wiki/Sparse_matrix</p> <p>Cuando creamos una matriz dispersa, tenemos que elegir en qu\u00e9 formato se debe almacenar. Por ejemplo,</p> In\u00a0[31]: Copied! <pre>from scipy.sparse import *\n</pre> from scipy.sparse import * In\u00a0[32]: Copied! <pre># dense matrix\nM = np.array([[1,0,0,0], [0,3,0,0], [0,1,1,0], [1,0,0,1]]); M\n</pre> # dense matrix M = np.array([[1,0,0,0], [0,3,0,0], [0,1,1,0], [1,0,0,1]]); M Out[32]: <pre>array([[1, 0, 0, 0],\n       [0, 3, 0, 0],\n       [0, 1, 1, 0],\n       [1, 0, 0, 1]])</pre> In\u00a0[33]: Copied! <pre># convert from dense to sparse\nA = csr_matrix(M); A\n</pre> # convert from dense to sparse A = csr_matrix(M); A Out[33]: <pre>&lt;4x4 sparse matrix of type '&lt;class 'numpy.intc'&gt;'\n\twith 6 stored elements in Compressed Sparse Row format&gt;</pre> In\u00a0[34]: Copied! <pre># convert from sparse to dense\nA.todense()\n</pre> # convert from sparse to dense A.todense() Out[34]: <pre>matrix([[1, 0, 0, 0],\n        [0, 3, 0, 0],\n        [0, 1, 1, 0],\n        [1, 0, 0, 1]], dtype=int32)</pre> <p>Una forma m\u00e1s eficiente de crear matrices dispersas: cree una matriz vac\u00eda y complete con el uso de indexaci\u00f3n matricial (evita crear una matriz densa potencialmente grande)</p> In\u00a0[35]: Copied! <pre>A = lil_matrix((4,4)) # empty 4x4 sparse matrix\nA[0,0] = 1\nA[1,1] = 3\nA[2,2] = A[2,1] = 1\nA[3,3] = A[3,0] = 1\nA\n</pre> A = lil_matrix((4,4)) # empty 4x4 sparse matrix A[0,0] = 1 A[1,1] = 3 A[2,2] = A[2,1] = 1 A[3,3] = A[3,0] = 1 A Out[35]: <pre>&lt;4x4 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n\twith 6 stored elements in List of Lists format&gt;</pre> In\u00a0[36]: Copied! <pre>A.todense()\n</pre> A.todense() Out[36]: <pre>matrix([[1., 0., 0., 0.],\n        [0., 3., 0., 0.],\n        [0., 1., 1., 0.],\n        [1., 0., 0., 1.]])</pre> <p>Conversi\u00f3n entre diferentes formatos de matriz dispersa:</p> In\u00a0[37]: Copied! <pre>A\n</pre> A Out[37]: <pre>&lt;4x4 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n\twith 6 stored elements in List of Lists format&gt;</pre> In\u00a0[38]: Copied! <pre>A = csr_matrix(A); A\n</pre> A = csr_matrix(A); A Out[38]: <pre>&lt;4x4 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n\twith 6 stored elements in Compressed Sparse Row format&gt;</pre> In\u00a0[39]: Copied! <pre>A = csc_matrix(A); A\n</pre> A = csc_matrix(A); A Out[39]: <pre>&lt;4x4 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n\twith 6 stored elements in Compressed Sparse Column format&gt;</pre> <p>Podemos calcular con matrices dispersas como con matrices densas:</p> In\u00a0[40]: Copied! <pre>A.todense()\n</pre> A.todense() Out[40]: <pre>matrix([[1., 0., 0., 0.],\n        [0., 3., 0., 0.],\n        [0., 1., 1., 0.],\n        [1., 0., 0., 1.]])</pre> In\u00a0[41]: Copied! <pre>(A * A).todense()\n</pre> (A * A).todense() Out[41]: <pre>matrix([[1., 0., 0., 0.],\n        [0., 9., 0., 0.],\n        [0., 4., 1., 0.],\n        [2., 0., 0., 1.]])</pre> In\u00a0[42]: Copied! <pre>A.todense()\n</pre> A.todense() Out[42]: <pre>matrix([[1., 0., 0., 0.],\n        [0., 3., 0., 0.],\n        [0., 1., 1., 0.],\n        [1., 0., 0., 1.]])</pre> In\u00a0[43]: Copied! <pre>A.dot(A).todense()\n</pre> A.dot(A).todense() Out[43]: <pre>matrix([[1., 0., 0., 0.],\n        [0., 9., 0., 0.],\n        [0., 4., 1., 0.],\n        [2., 0., 0., 1.]])</pre> In\u00a0[44]: Copied! <pre>v = np.array([1,2,3,4])[:,np.newaxis]; v\n</pre> v = np.array([1,2,3,4])[:,np.newaxis]; v Out[44]: <pre>array([[1],\n       [2],\n       [3],\n       [4]])</pre> In\u00a0[45]: Copied! <pre># sparse matrix - dense vector multiplication\nA * v\n</pre> # sparse matrix - dense vector multiplication A * v Out[45]: <pre>array([[1.],\n       [6.],\n       [5.],\n       [5.]])</pre> In\u00a0[46]: Copied! <pre># same result with dense matrix - dense vector multiplcation\nA.todense() * v\n</pre> # same result with dense matrix - dense vector multiplcation A.todense() * v Out[46]: <pre>matrix([[1.],\n        [6.],\n        [5.],\n        [5.]])</pre> <p>La optimizaci\u00f3n (encontrar m\u00ednimos o m\u00e1ximos de una funci\u00f3n) es un campo amplio en matem\u00e1ticas, y la optimizaci\u00f3n de funciones complicadas o en muchas variables puede estar bastante involucrada. Aqu\u00ed solo veremos algunos casos muy simples. Para obtener una introducci\u00f3n m\u00e1s detallada a la optimizaci\u00f3n con SciPy, consulte: http://scipy-lectures.github.com/advanced/mathematical_optimization/index.html</p> <p>Para usar el m\u00f3dulo de optimizaci\u00f3n en scipy, primero incluya el m\u00f3dulo <code>optimize</code>:</p> In\u00a0[47]: Copied! <pre>from scipy import optimize\n</pre> from scipy import optimize In\u00a0[48]: Copied! <pre>def f(x):\n    return 4*x**3 + (x-2)**2 + x**4\n</pre> def f(x):     return 4*x**3 + (x-2)**2 + x**4 In\u00a0[49]: Copied! <pre>fig, ax  = plt.subplots()\nx =  np.linspace(-5, 3, 100)\nax.plot(x, f(x));\n</pre> fig, ax  = plt.subplots() x =  np.linspace(-5, 3, 100) ax.plot(x, f(x)); <p>Podemos usar la funci\u00f3n <code>fmin_bfgs</code> para encontrar los m\u00ednimos de una funci\u00f3n:</p> In\u00a0[50]: Copied! <pre>x_min = optimize.fmin_bfgs(f, -2)\nx_min\n</pre> x_min = optimize.fmin_bfgs(f, -2) x_min  <pre>Optimization terminated successfully.\n         Current function value: -3.506641\n         Iterations: 5\n         Function evaluations: 16\n         Gradient evaluations: 8\n</pre> Out[50]: <pre>array([-2.6729815])</pre> In\u00a0[51]: Copied! <pre>optimize.fmin_bfgs(f, 0.5)\n</pre> optimize.fmin_bfgs(f, 0.5)  <pre>Optimization terminated successfully.\n         Current function value: 2.804988\n         Iterations: 3\n         Function evaluations: 10\n         Gradient evaluations: 5\n</pre> Out[51]: <pre>array([0.46961745])</pre> <p>Tambi\u00e9n podemos usar las funciones <code>brent</code> o<code> fminbound</code>. Tienen una sintaxis un poco diferente y usan diferentes algoritmos.</p> In\u00a0[52]: Copied! <pre>optimize.brent(f)\n</pre> optimize.brent(f) Out[52]: <pre>0.46961743402759754</pre> In\u00a0[53]: Copied! <pre>optimize.fminbound(f, -4, 2)\n</pre> optimize.fminbound(f, -4, 2) Out[53]: <pre>-2.6729822917513886</pre> In\u00a0[54]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.optimize as optimize\n\nomega_c = 3.0\n\ndef f(omega):\n    with np.errstate(divide='ignore', invalid='ignore'):  # Evitar advertencias de divisi\u00f3n por cero\n        result = np.tan(2 * np.pi * omega) - omega_c / omega\n        result[np.isinf(result)] = np.nan  # Reemplazar infinitos con NaN\n\n    return result\n\nfig, ax = plt.subplots(figsize=(10, 4))\nx = np.linspace(0, 3, 1000)\ny = f(x)\nmask = np.where(abs(y) &gt; 50)\nx[mask] = y[mask] = np.nan  # Eliminar la l\u00ednea vertical cuando la funci\u00f3n cambia de signo\nax.plot(x, y)\nax.plot([0, 3], [0, 0], 'k')\nax.set_ylim(-5, 5)\nplt.show()\n\nroot1 = optimize.fsolve(f, 0.1)\nroot2 = optimize.fsolve(f, 0.6)\nroot3 = optimize.fsolve(f, 1.1)\n\nprint(\"Root 1:\", root1)\nprint(\"Root 2:\", root2)\nprint(\"Root 3:\", root3)\n</pre> import numpy as np import matplotlib.pyplot as plt import scipy.optimize as optimize  omega_c = 3.0  def f(omega):     with np.errstate(divide='ignore', invalid='ignore'):  # Evitar advertencias de divisi\u00f3n por cero         result = np.tan(2 * np.pi * omega) - omega_c / omega         result[np.isinf(result)] = np.nan  # Reemplazar infinitos con NaN      return result  fig, ax = plt.subplots(figsize=(10, 4)) x = np.linspace(0, 3, 1000) y = f(x) mask = np.where(abs(y) &gt; 50) x[mask] = y[mask] = np.nan  # Eliminar la l\u00ednea vertical cuando la funci\u00f3n cambia de signo ax.plot(x, y) ax.plot([0, 3], [0, 0], 'k') ax.set_ylim(-5, 5) plt.show()  root1 = optimize.fsolve(f, 0.1) root2 = optimize.fsolve(f, 0.6) root3 = optimize.fsolve(f, 1.1)  print(\"Root 1:\", root1) print(\"Root 2:\", root2) print(\"Root 3:\", root3)  <pre>Root 1: [0.23743014]\nRoot 2: [0.71286972]\nRoot 3: [1.18990285]\n</pre> <p>La interpolaci\u00f3n es simple y conveniente en scipy: la funci\u00f3n <code>interp1d</code>, cuando se dan matrices que describen datos $X$ e $Y$, devuelve un objeto que se comporta como una funci\u00f3n que se puede llamar para un valor arbitrario de $x$ (en el rango cubierto por $X$), y devuelve el valor de $y$ interpolado correspondiente:</p> In\u00a0[55]: Copied! <pre>from scipy.interpolate import *\n</pre> from scipy.interpolate import * In\u00a0[56]: Copied! <pre>def f(x):\n    return np.sin(x)\n</pre> def f(x):     return np.sin(x) In\u00a0[57]: Copied! <pre>n = np.arange(0, 10)  \nx = np.linspace(0, 9, 100)\n\ny_meas = f(n) + 0.1 * np.random.randn(len(n)) # simulate measurement with noise\ny_real = f(x)\n\nlinear_interpolation = interp1d(n, y_meas)\ny_interp1 = linear_interpolation(x)\n\ncubic_interpolation = interp1d(n, y_meas, kind='cubic')\ny_interp2 = cubic_interpolation(x)\n</pre> n = np.arange(0, 10)   x = np.linspace(0, 9, 100)  y_meas = f(n) + 0.1 * np.random.randn(len(n)) # simulate measurement with noise y_real = f(x)  linear_interpolation = interp1d(n, y_meas) y_interp1 = linear_interpolation(x)  cubic_interpolation = interp1d(n, y_meas, kind='cubic') y_interp2 = cubic_interpolation(x) In\u00a0[58]: Copied! <pre>fig, ax = plt.subplots(figsize=(10,4))\nax.plot(n, y_meas, 'bs', label='noisy data')\nax.plot(x, y_real, 'k', lw=2, label='true function')\nax.plot(x, y_interp1, 'r', label='linear interp')\nax.plot(x, y_interp2, 'g', label='cubic interp')\nax.legend(loc=3);\n</pre> fig, ax = plt.subplots(figsize=(10,4)) ax.plot(n, y_meas, 'bs', label='noisy data') ax.plot(x, y_real, 'k', lw=2, label='true function') ax.plot(x, y_interp1, 'r', label='linear interp') ax.plot(x, y_interp2, 'g', label='cubic interp') ax.legend(loc=3); <p>El m\u00f3dulo <code>scipy.stats</code> contiene una gran cantidad de distribuciones estad\u00edsticas, funciones estad\u00edsticas y pruebas. Para obtener una documentaci\u00f3n completa de sus funciones, consulte http://docs.scipy.org/doc/scipy/reference/stats.html.</p> <p>Tambi\u00e9n hay un paquete de Python muy poderoso para modelado estad\u00edstico llamado statsmodels. Consulte http://statsmodels.sourceforge.net para obtener m\u00e1s detalles.</p> In\u00a0[59]: Copied! <pre>from scipy import stats\n</pre> from scipy import stats In\u00a0[60]: Copied! <pre># create a (discreet) random variable with poissionian distribution\n\nX = stats.poisson(3.5) # photon distribution for a coherent state with n=3.5 photons\n</pre> # create a (discreet) random variable with poissionian distribution  X = stats.poisson(3.5) # photon distribution for a coherent state with n=3.5 photons In\u00a0[61]: Copied! <pre>n = np.arange(0,15)\n\nfig, axes = plt.subplots(3,1, sharex=True)\n\n# plot the probability mass function (PMF)\naxes[0].step(n, X.pmf(n))\n\n# plot the commulative distribution function (CDF)\naxes[1].step(n, X.cdf(n))\n\n# plot histogram of 1000 random realizations of the stochastic variable X\naxes[2].hist(X.rvs(size=1000));\n</pre> n = np.arange(0,15)  fig, axes = plt.subplots(3,1, sharex=True)  # plot the probability mass function (PMF) axes[0].step(n, X.pmf(n))  # plot the commulative distribution function (CDF) axes[1].step(n, X.cdf(n))  # plot histogram of 1000 random realizations of the stochastic variable X axes[2].hist(X.rvs(size=1000)); In\u00a0[62]: Copied! <pre># create a (continous) random variable with normal distribution\nY = stats.norm()\n</pre> # create a (continous) random variable with normal distribution Y = stats.norm() In\u00a0[63]: Copied! <pre>x = np.linspace(-5,5,100)\n\nfig, axes = plt.subplots(3,1, sharex=True)\n\n# plot the probability distribution function (PDF)\naxes[0].plot(x, Y.pdf(x))\n\n# plot the commulative distributin function (CDF)\naxes[1].plot(x, Y.cdf(x));\n\n# plot histogram of 1000 random realizations of the stochastic variable Y\naxes[2].hist(Y.rvs(size=1000), bins=50);\n</pre> x = np.linspace(-5,5,100)  fig, axes = plt.subplots(3,1, sharex=True)  # plot the probability distribution function (PDF) axes[0].plot(x, Y.pdf(x))  # plot the commulative distributin function (CDF) axes[1].plot(x, Y.cdf(x));  # plot histogram of 1000 random realizations of the stochastic variable Y axes[2].hist(Y.rvs(size=1000), bins=50); <p>Estad\u00edsticas</p> In\u00a0[64]: Copied! <pre>X.mean(), X.std(), X.var() # poission distribution\n</pre> X.mean(), X.std(), X.var() # poission distribution Out[64]: <pre>(3.5, 1.8708286933869707, 3.5)</pre> In\u00a0[65]: Copied! <pre>Y.mean(), Y.std(), Y.var() # normal distribution\n</pre> Y.mean(), Y.std(), Y.var() # normal distribution Out[65]: <pre>(0.0, 1.0, 1.0)</pre> <p>Pruebe si dos conjuntos de datos aleatorios (independientes) provienen de la misma distribuci\u00f3n:</p> In\u00a0[66]: Copied! <pre>t_statistic, p_value = stats.ttest_ind(X.rvs(size=1000), X.rvs(size=1000))\n\nprint (f\"t-statistic = {t_statistic}\")\nprint(f\"p-value = {p_value}\")\n</pre> t_statistic, p_value = stats.ttest_ind(X.rvs(size=1000), X.rvs(size=1000))  print (f\"t-statistic = {t_statistic}\") print(f\"p-value = {p_value}\") <pre>t-statistic = -0.15792211673959744\np-value = 0.8745341282601782\n</pre> <p>Dado que el valor $p$ es muy grande, no podemos rechazar la hip\u00f3tesis de que los dos conjuntos de datos aleatorios tienen medias * diferentes *.</p> <p>Para probar si la media de una sola muestra de datos tiene una media de 0,1 (la media verdadera es 0,0):</p> In\u00a0[67]: Copied! <pre>stats.ttest_1samp(Y.rvs(size=1000), 0.1)\n</pre> stats.ttest_1samp(Y.rvs(size=1000), 0.1) Out[67]: <pre>TtestResult(statistic=-1.6163026160897414, pvalue=0.1063446216377087, df=999)</pre> <p>Un valor $p$ bajo significa que podemos rechazar la hip\u00f3tesis de que la media de $Y$ es 0,1.</p> In\u00a0[68]: Copied! <pre>Y.mean()\n</pre> Y.mean() Out[68]: <pre>0.0</pre> In\u00a0[69]: Copied! <pre>stats.ttest_1samp(Y.rvs(size=1000), Y.mean())\n</pre> stats.ttest_1samp(Y.rvs(size=1000), Y.mean()) Out[69]: <pre>TtestResult(statistic=-1.2118368011206664, pvalue=0.22586140260000256, df=999)</pre> <ul> <li>A tutorial on how to get started using SciPy</li> <li>The SciPy source code</li> </ul>"},{"location":"lectures/data_manipulation/sc_02/#scipy","title":"SciPy\u00b6","text":""},{"location":"lectures/data_manipulation/sc_02/#introduccion","title":"Introducci\u00f3n\u00b6","text":""},{"location":"lectures/data_manipulation/sc_02/#special-functions","title":"Special functions\u00b6","text":""},{"location":"lectures/data_manipulation/sc_02/#integration","title":"Integration\u00b6","text":""},{"location":"lectures/data_manipulation/sc_02/#numerical-integration-quadrature","title":"Numerical integration: quadrature\u00b6","text":""},{"location":"lectures/data_manipulation/sc_02/#ordinary-differential-equations-odes","title":"Ordinary differential equations (ODEs)\u00b6","text":"<p>SciPy proporciona dos formas diferentes de resolver EDO: una API basada en la funci\u00f3n <code>odeint</code> y una API orientada a objetos basada en la clase <code>ode</code>. Por lo general, <code>odeint</code> es m\u00e1s f\u00e1cil de empezar, pero la clase <code>ode</code> ofrece un nivel de control m\u00e1s fino.</p> <p>Aqu\u00ed usaremos las funciones <code>odeint</code>. Para obtener m\u00e1s informaci\u00f3n sobre la clase <code>ode</code>, pruebe con <code>help (ode)</code>. Hace pr\u00e1cticamente lo mismo que <code>odeint</code>, pero de una manera orientada a objetos.</p> <p>Para usar <code>odeint</code>, primero imp\u00f3rtelo desde el m\u00f3dulo <code>scipy.integrate</code></p>"},{"location":"lectures/data_manipulation/sc_02/#ejemplo-pendulo-doble","title":"Ejemplo: p\u00e9ndulo doble\u00b6","text":"<p>Consideremos un ejemplo f\u00edsico: el p\u00e9ndulo compuesto doble, descrito con cierto detalle aqu\u00ed: http://en.wikipedia.org/wiki/Double_pendulum</p>"},{"location":"lectures/data_manipulation/sc_02/#ejemplo-oscilador-armonico-amortiguado","title":"Ejemplo: oscilador arm\u00f3nico amortiguado\u00b6","text":"<p>Los problemas de ODE son importantes en f\u00edsica computacional, por lo que veremos un ejemplo m\u00e1s: la oscilaci\u00f3n arm\u00f3nica amortiguada. Este problema est\u00e1 bien descrito en la p\u00e1gina wiki: http://en.wikipedia.org/wiki/Damping</p> <p>La ecuaci\u00f3n de movimiento del oscilador amortiguado es:</p> <p>$\\displaystyle \\frac{\\mathrm{d}^2x}{\\mathrm{d}t^2} + 2\\zeta\\omega_0\\frac{\\mathrm{d}x}{\\mathrm{d}t} + \\omega^2_0 x = 0$</p> <p>donde $ x $ es la posici\u00f3n del oscilador, $ \\ omega_0 $ es la frecuencia y $ \\ zeta $ es la relaci\u00f3n de amortiguaci\u00f3n. Para escribir esta EDO de segundo orden en forma est\u00e1ndar, introducimos $ p = \\ frac {\\ mathrm {d} x} {\\ mathrm {d} t} $:</p> <p>$\\displaystyle \\frac{\\mathrm{d}p}{\\mathrm{d}t} = - 2\\zeta\\omega_0 p - \\omega^2_0 x$</p> <p>$\\displaystyle \\frac{\\mathrm{d}x}{\\mathrm{d}t} = p$</p> <p>En la implementaci\u00f3n de este ejemplo, agregaremos argumentos adicionales a la funci\u00f3n RHS para la ODE, en lugar de usar variables globales como hicimos en el ejemplo anterior. Como consecuencia de los argumentos adicionales al RHS, necesitamos pasar un argumento de palabra clave <code>args</code> a la funci\u00f3n <code>odeint</code>:</p>"},{"location":"lectures/data_manipulation/sc_02/#fourier-transform","title":"Fourier transform\u00b6","text":"<p>Las transformadas de Fourier son una de las herramientas universales de la f\u00edsica computacional, que aparecen una y otra vez en diferentes contextos. SciPy proporciona funciones para acceder a la biblioteca cl\u00e1sica [FFTPACK] (http://www.netlib.org/fftpack/) de NetLib, que es una biblioteca FFT eficiente y bien probada escrita en FORTRAN. La API SciPy tiene algunas funciones de conveniencia adicionales, pero en general, la API est\u00e1 estrechamente relacionada con la biblioteca FORTRAN original.</p> <p>Para usar el m\u00f3dulo <code>fftpack</code> en un programa Python, incl\u00fayalo usando:</p>"},{"location":"lectures/data_manipulation/sc_02/#sparse-matrices","title":"Sparse matrices\u00b6","text":""},{"location":"lectures/data_manipulation/sc_02/#optimization","title":"Optimization\u00b6","text":""},{"location":"lectures/data_manipulation/sc_02/#encontrar-un-minimo","title":"Encontrar un m\u00ednimo\u00b6","text":"<p>Primero veamos c\u00f3mo encontrar los m\u00ednimos de una funci\u00f3n simple de una sola variable:</p>"},{"location":"lectures/data_manipulation/sc_02/#encontrar-una-solucion-a-una-funcion","title":"Encontrar una soluci\u00f3n a una funci\u00f3n\u00b6","text":"<p>Para encontrar la ra\u00edz de una funci\u00f3n de la forma $ f (x) = 0 $ podemos usar la funci\u00f3n <code>fsolve</code>. Requiere una suposici\u00f3n inicial:</p>"},{"location":"lectures/data_manipulation/sc_02/#interpolation","title":"Interpolation\u00b6","text":""},{"location":"lectures/data_manipulation/sc_02/#statistics","title":"Statistics\u00b6","text":""},{"location":"lectures/data_manipulation/sc_02/#statistical-tests","title":"Statistical tests\u00b6","text":""},{"location":"lectures/data_manipulation/sc_02/#referencias","title":"Referencias\u00b6","text":""},{"location":"lectures/data_manipulation/sc_03/","title":"Sympy","text":"<p>Hay dos sistemas de \u00e1lgebra computarizada (CAS) notables para Python:</p> <ul> <li>SymPy: un m\u00f3dulo de Python que se puede utilizar en cualquier programa de Python, o en una sesi\u00f3n de IPython, que proporciona potentes funciones de CAS.</li> <li>Sage - Sage es un entorno CAS muy potente y con todas las funciones que tiene como objetivo proporcionar un sistema de c\u00f3digo abierto que compita con Mathematica y Maple. Sage no es un m\u00f3dulo Python normal, sino un entorno CAS que utiliza Python como lenguaje de programaci\u00f3n.</li> </ul> <p><code>Sage</code> es en algunos aspectos m\u00e1s poderoso que <code>SymPy</code>, pero ambos ofrecen una funcionalidad CAS muy completa. La ventaja de SymPy es que es un m\u00f3dulo Python normal y se integra bien con el port\u00e1til IPython.</p> <p>Para comenzar a usar SymPy en un programa o cuaderno de Python, importe el m\u00f3dulo <code>sympy</code>:</p> In\u00a0[1]: Copied! <pre>from sympy import *\n</pre> from sympy import * <p>Para obtener una salida con formato $\\LaTeX $ atractiva, ejecute:</p> In\u00a0[2]: Copied! <pre>init_printing()\n\n# or with older versions of sympy/ipython, load the IPython extension\n#%load_ext sympy.interactive.ipythonprinting\n# or\n#%load_ext sympyprinting\n</pre> init_printing()  # or with older versions of sympy/ipython, load the IPython extension #%load_ext sympy.interactive.ipythonprinting # or #%load_ext sympyprinting <p>En <code>SymPy</code> necesitamos crear s\u00edmbolos para las variables con las que queremos trabajar. Podemos crear un nuevo s\u00edmbolo usando la clase <code>Symbol</code>:</p> In\u00a0[3]: Copied! <pre>x = Symbol('x')\n</pre> x = Symbol('x') In\u00a0[4]: Copied! <pre>(pi + x)**2\n</pre> (pi + x)**2 Out[4]:  $\\displaystyle \\left(x + \\pi\\right)^{2}$  In\u00a0[5]: Copied! <pre># alternative way of defining symbols\na, b, c = symbols(\"a, b, c\")\n</pre> # alternative way of defining symbols a, b, c = symbols(\"a, b, c\") In\u00a0[6]: Copied! <pre>type(a)\n</pre> type(a) Out[6]: <pre>sympy.core.symbol.Symbol</pre> <p>Podemos agregar suposiciones a los s\u00edmbolos cuando los creamos:</p> In\u00a0[7]: Copied! <pre>x = Symbol('x', real=True)\n</pre> x = Symbol('x', real=True) In\u00a0[8]: Copied! <pre>x.is_imaginary\n</pre> x.is_imaginary Out[8]: <pre>False</pre> In\u00a0[9]: Copied! <pre>x = Symbol('x', positive=True)\n</pre> x = Symbol('x', positive=True) In\u00a0[10]: Copied! <pre>x &gt; 0\n</pre> x &gt; 0 Out[10]:  $\\displaystyle \\text{True}$  In\u00a0[11]: Copied! <pre>1+1*I\n</pre> 1+1*I Out[11]:  $\\displaystyle 1 + i$  In\u00a0[12]: Copied! <pre>I**2\n</pre> I**2 Out[12]:  $\\displaystyle -1$  In\u00a0[13]: Copied! <pre>(x * I + 1)**2\n</pre> (x * I + 1)**2 Out[13]:  $\\displaystyle \\left(i x + 1\\right)^{2}$  In\u00a0[14]: Copied! <pre>r1 = Rational(4,5)\nr2 = Rational(5,4)\n</pre> r1 = Rational(4,5) r2 = Rational(5,4) In\u00a0[15]: Copied! <pre>r1\n</pre> r1 Out[15]:  $\\displaystyle \\frac{4}{5}$  In\u00a0[16]: Copied! <pre>r1+r2\n</pre> r1+r2 Out[16]:  $\\displaystyle \\frac{41}{20}$  In\u00a0[17]: Copied! <pre>r1/r2\n</pre> r1/r2 Out[17]:  $\\displaystyle \\frac{16}{25}$  In\u00a0[18]: Copied! <pre>pi.evalf(n=50)\n</pre> pi.evalf(n=50) Out[18]:  $\\displaystyle 3.1415926535897932384626433832795028841971693993751$  In\u00a0[19]: Copied! <pre>y = (x + pi)**2\n</pre> y = (x + pi)**2 In\u00a0[20]: Copied! <pre>N(y, 5) # same as evalf\n</pre> N(y, 5) # same as evalf Out[20]:  $\\displaystyle 9.8696 \\left(0.31831 x + 1\\right)^{2}$  <p>Cuando evaluamos num\u00e9ricamente expresiones algebraicas, a menudo queremos sustituir un s\u00edmbolo por un valor num\u00e9rico. En <code>SymPy</code> lo hacemos usando la funci\u00f3n <code>subs</code>:</p> In\u00a0[21]: Copied! <pre>y.subs(x, 1.5)\n</pre> y.subs(x, 1.5) Out[21]:  $\\displaystyle \\left(1.5 + \\pi\\right)^{2}$  In\u00a0[22]: Copied! <pre>N(y.subs(x, 1.5))\n</pre> N(y.subs(x, 1.5)) Out[22]:  $\\displaystyle 21.5443823618587$  <p>Por supuesto, la funci\u00f3n <code>subs</code> tambi\u00e9n se puede utilizar para sustituir s\u00edmbolos y expresiones:</p> In\u00a0[23]: Copied! <pre>y.subs(x, a+pi)\n</pre> y.subs(x, a+pi) Out[23]:  $\\displaystyle \\left(a + 2 \\pi\\right)^{2}$  <p>Tambi\u00e9n podemos combinar la evoluci\u00f3n num\u00e9rica de expresiones con matrices <code>Numpy</code>:</p> In\u00a0[24]: Copied! <pre>import numpy\nimport matplotlib.pyplot as plt\n</pre> import numpy import matplotlib.pyplot as plt  In\u00a0[25]: Copied! <pre>x_vec = numpy.arange(0, 10, 0.1)\n</pre> x_vec = numpy.arange(0, 10, 0.1) In\u00a0[26]: Copied! <pre>y_vec = numpy.array([N(((x + pi)**2).subs(x, xx)) for xx in x_vec])\n</pre> y_vec = numpy.array([N(((x + pi)**2).subs(x, xx)) for xx in x_vec]) In\u00a0[27]: Copied! <pre>fig, ax = plt.subplots()\nax.plot(x_vec, y_vec);\n</pre> fig, ax = plt.subplots() ax.plot(x_vec, y_vec); <p>Sin embargo, este tipo de evoluci\u00f3n num\u00e9rica puede ser muy lenta, y hay una manera mucho m\u00e1s eficiente de hacerlo: use la funci\u00f3n <code>lambdify</code> para\" compilar \"una expresi\u00f3n Sympy en una funci\u00f3n que sea mucho m\u00e1s eficiente para evaluar num\u00e9ricamente:</p> In\u00a0[28]: Copied! <pre>f = lambdify([x], (x + pi)**2, 'numpy')  # the first argument is a list of variables that\n                                         # f will be a function of: in this case only x -&gt; f(x)\n</pre> f = lambdify([x], (x + pi)**2, 'numpy')  # the first argument is a list of variables that                                          # f will be a function of: in this case only x -&gt; f(x) In\u00a0[29]: Copied! <pre>y_vec = f(x_vec)  # now we can directly pass a numpy array and f(x) is efficiently evaluated\n</pre> y_vec = f(x_vec)  # now we can directly pass a numpy array and f(x) is efficiently evaluated <p>La aceleraci\u00f3n cuando se utilizan funciones <code>lambdify</code>  en lugar de una evaluaci\u00f3n num\u00e9rica directa puede ser significativa, a menudo de varios \u00f3rdenes de magnitud. Incluso en este ejemplo simple obtenemos una velocidad significativa:</p> In\u00a0[30]: Copied! <pre>%%timeit\n\ny_vec = numpy.array([N(((x + pi)**2).subs(x, xx)) for xx in x_vec])\n</pre> %%timeit  y_vec = numpy.array([N(((x + pi)**2).subs(x, xx)) for xx in x_vec]) <pre>16.9 ms \u00b1 473 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</pre> In\u00a0[31]: Copied! <pre>%%timeit\n\ny_vec = f(x_vec)\n</pre> %%timeit  y_vec = f(x_vec) <pre>2.89 \u00b5s \u00b1 48.3 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n</pre> In\u00a0[32]: Copied! <pre>(x+1)*(x+2)*(x+3)\n</pre> (x+1)*(x+2)*(x+3) Out[32]:  $\\displaystyle \\left(x + 1\\right) \\left(x + 2\\right) \\left(x + 3\\right)$  In\u00a0[33]: Copied! <pre>expand((x+1)*(x+2)*(x+3))\n</pre> expand((x+1)*(x+2)*(x+3)) Out[33]:  $\\displaystyle x^{3} + 6 x^{2} + 11 x + 6$  <p>La funci\u00f3n <code>expand</code> toma un n\u00famero de argumentos de palabras clave que podemos decirle a las funciones qu\u00e9 tipo de expansiones queremos que se realicen. Por ejemplo, para expandir expresiones trigonom\u00e9tricas, use el argumento de palabra clave <code>trig = True</code>:</p> In\u00a0[34]: Copied! <pre>sin(a+b)\n</pre> sin(a+b) Out[34]:  $\\displaystyle \\sin{\\left(a + b \\right)}$  In\u00a0[35]: Copied! <pre>expand(sin(a+b), trig=True)\n</pre> expand(sin(a+b), trig=True) Out[35]:  $\\displaystyle \\sin{\\left(a \\right)} \\cos{\\left(b \\right)} + \\sin{\\left(b \\right)} \\cos{\\left(a \\right)}$  <p>Consulte <code>help (expand)</code> para obtener una explicaci\u00f3n detallada de los distintos tipos de expansiones que pueden realizar las funciones de \u02bbexpand`.</p> <p>Lo contrario, una expansi\u00f3n de producto es, por supuesto, factorizaci\u00f3n. El factor de una expresi\u00f3n en SymPy usa la funci\u00f3n <code>factor</code>:</p> In\u00a0[36]: Copied! <pre>factor(x**3 + 6 * x**2 + 11*x + 6)\n</pre> factor(x**3 + 6 * x**2 + 11*x + 6) Out[36]:  $\\displaystyle \\left(x + 1\\right) \\left(x + 2\\right) \\left(x + 3\\right)$  In\u00a0[37]: Copied! <pre># simplify expands a product\nsimplify((x+1)*(x+2)*(x+3))\n</pre> # simplify expands a product simplify((x+1)*(x+2)*(x+3)) Out[37]:  $\\displaystyle \\left(x + 1\\right) \\left(x + 2\\right) \\left(x + 3\\right)$  In\u00a0[38]: Copied! <pre># simplify uses trigonometric identities\nsimplify(sin(a)**2 + cos(a)**2)\n</pre> # simplify uses trigonometric identities simplify(sin(a)**2 + cos(a)**2) Out[38]:  $\\displaystyle 1$  In\u00a0[39]: Copied! <pre>simplify(cos(x)/sin(x))\n</pre> simplify(cos(x)/sin(x)) Out[39]:  $\\displaystyle \\frac{1}{\\tan{\\left(x \\right)}}$  <p>apart</p> In\u00a0[40]: Copied! <pre>f1 = 1/((a+1)*(a+2))\n</pre> f1 = 1/((a+1)*(a+2)) In\u00a0[41]: Copied! <pre>f1\n</pre> f1 Out[41]:  $\\displaystyle \\frac{1}{\\left(a + 1\\right) \\left(a + 2\\right)}$  In\u00a0[42]: Copied! <pre>apart(f1)\n</pre> apart(f1) Out[42]:  $\\displaystyle - \\frac{1}{a + 2} + \\frac{1}{a + 1}$  <p>together</p> In\u00a0[43]: Copied! <pre>f2 = 1/(a+2) + 1/(a+3)\n</pre> f2 = 1/(a+2) + 1/(a+3) In\u00a0[44]: Copied! <pre>f2\n</pre> f2 Out[44]:  $\\displaystyle \\frac{1}{a + 3} + \\frac{1}{a + 2}$  In\u00a0[45]: Copied! <pre>together(f2)\n</pre> together(f2) Out[45]:  $\\displaystyle \\frac{2 a + 5}{\\left(a + 2\\right) \\left(a + 3\\right)}$  <p>Simplificar generalmente combina fracciones pero no factoriza:</p> In\u00a0[46]: Copied! <pre>simplify(f2)\n</pre> simplify(f2) Out[46]:  $\\displaystyle \\frac{2 a + 5}{\\left(a + 2\\right) \\left(a + 3\\right)}$  In\u00a0[47]: Copied! <pre>y\n</pre> y Out[47]:  $\\displaystyle \\left(x + \\pi\\right)^{2}$  In\u00a0[48]: Copied! <pre>diff(y**2, x)\n</pre> diff(y**2, x) Out[48]:  $\\displaystyle 4 \\left(x + \\pi\\right)^{3}$  <p>Para derivados de orden superior podemos hacer:</p> In\u00a0[49]: Copied! <pre>diff(y**2, x, x)\n</pre> diff(y**2, x, x) Out[49]:  $\\displaystyle 12 \\left(x + \\pi\\right)^{2}$  In\u00a0[50]: Copied! <pre>diff(y**2, x, 2) # same as above\n</pre> diff(y**2, x, 2) # same as above Out[50]:  $\\displaystyle 12 \\left(x + \\pi\\right)^{2}$  <p>Para calcular la derivada de una expresi\u00f3n multivariante, podemos hacer:</p> In\u00a0[51]: Copied! <pre>x, y, z = symbols(\"x,y,z\")\n</pre> x, y, z = symbols(\"x,y,z\") In\u00a0[52]: Copied! <pre>f = sin(x*y) + cos(y*z)\n</pre> f = sin(x*y) + cos(y*z) <p>$\\frac{d^3f}{dxdy^2}$</p> In\u00a0[53]: Copied! <pre>diff(f, x, 1, y, 2)\n</pre> diff(f, x, 1, y, 2) Out[53]:  $\\displaystyle - x \\left(x y \\cos{\\left(x y \\right)} + 2 \\sin{\\left(x y \\right)}\\right)$  In\u00a0[54]: Copied! <pre>f\n</pre> f Out[54]:  $\\displaystyle \\sin{\\left(x y \\right)} + \\cos{\\left(y z \\right)}$  In\u00a0[55]: Copied! <pre>integrate(f, x)\n</pre> integrate(f, x) Out[55]:  $\\displaystyle x \\cos{\\left(y z \\right)} + \\begin{cases} - \\frac{\\cos{\\left(x y \\right)}}{y} &amp; \\text{for}\\: y \\neq 0 \\\\0 &amp; \\text{otherwise} \\end{cases}$  <p>Al proporcionar l\u00edmites para la variable de integraci\u00f3n, podemos evaluar integrales definidas:</p> In\u00a0[56]: Copied! <pre>integrate(f, (x, -1, 1))\n</pre> integrate(f, (x, -1, 1)) Out[56]:  $\\displaystyle 2 \\cos{\\left(y z \\right)}$  <p>y tambi\u00e9n integrales impropias:</p> In\u00a0[57]: Copied! <pre>integrate(exp(-x**2), (x, -oo, oo))\n</pre> integrate(exp(-x**2), (x, -oo, oo)) Out[57]:  $\\displaystyle \\sqrt{\\pi}$  <p>Recuerde, <code>oo</code> es la notaci\u00f3n SymPy para infinito.</p> In\u00a0[58]: Copied! <pre>n = Symbol(\"n\")\n</pre> n = Symbol(\"n\") In\u00a0[59]: Copied! <pre>Sum(1/n**2, (n, 1, 10))\n</pre> Sum(1/n**2, (n, 1, 10)) Out[59]:  $\\displaystyle \\sum_{n=1}^{10} \\frac{1}{n^{2}}$  In\u00a0[60]: Copied! <pre>Sum(1/n**2, (n,1, 10)).evalf()\n</pre> Sum(1/n**2, (n,1, 10)).evalf() Out[60]:  $\\displaystyle 1.54976773116654$  In\u00a0[61]: Copied! <pre>Sum(1/n**2, (n, 1, oo)).evalf()\n</pre> Sum(1/n**2, (n, 1, oo)).evalf() Out[61]:  $\\displaystyle 1.64493406684823$  <p>Los productos funcionan de la misma manera:</p> In\u00a0[62]: Copied! <pre>Product(n, (n, 1, 10)) # 10!\n</pre> Product(n, (n, 1, 10)) # 10! Out[62]:  $\\displaystyle \\prod_{n=1}^{10} n$  In\u00a0[63]: Copied! <pre>limit(sin(x)/x, x, 0)\n</pre> limit(sin(x)/x, x, 0) Out[63]:  $\\displaystyle 1$  <p>Podemos usar <code>limit</code> para verificar el resultado de la derivaci\u00f3n usando la funci\u00f3n <code>diff</code>:</p> In\u00a0[64]: Copied! <pre>f\n</pre> f Out[64]:  $\\displaystyle \\sin{\\left(x y \\right)} + \\cos{\\left(y z \\right)}$  In\u00a0[65]: Copied! <pre>diff(f, x)\n</pre> diff(f, x) Out[65]:  $\\displaystyle y \\cos{\\left(x y \\right)}$  <p>$\\displaystyle \\frac{\\mathrm{d}f(x,y)}{\\mathrm{d}x} = \\frac{f(x+h,y)-f(x,y)}{h}$</p> In\u00a0[66]: Copied! <pre>h = Symbol(\"h\")\n</pre> h = Symbol(\"h\") In\u00a0[67]: Copied! <pre>limit((f.subs(x, x+h) - f)/h, h, 0)\n</pre> limit((f.subs(x, x+h) - f)/h, h, 0) Out[67]:  $\\displaystyle y \\cos{\\left(x y \\right)}$  <p>Podemos cambiar la direcci\u00f3n desde la que nos acercamos al punto l\u00edmite usando el argumento <code>dir</code>:</p> In\u00a0[68]: Copied! <pre>limit(1/x, x, 0, dir=\"+\")\n</pre> limit(1/x, x, 0, dir=\"+\") Out[68]:  $\\displaystyle \\infty$  In\u00a0[69]: Copied! <pre>limit(1/x, x, 0, dir=\"-\")\n</pre> limit(1/x, x, 0, dir=\"-\") Out[69]:  $\\displaystyle -\\infty$  In\u00a0[70]: Copied! <pre>series(exp(x), x)\n</pre> series(exp(x), x) Out[70]:  $\\displaystyle 1 + x + \\frac{x^{2}}{2} + \\frac{x^{3}}{6} + \\frac{x^{4}}{24} + \\frac{x^{5}}{120} + O\\left(x^{6}\\right)$  <p>De forma predeterminada, expande la expresi\u00f3n alrededor de $x = 0$, pero podemos expandir alrededor de cualquier valor de $x$ al incluir expl\u00edcitamente un valor en la llamada a la funci\u00f3n:</p> In\u00a0[71]: Copied! <pre>series(exp(x), x, 1)\n</pre> series(exp(x), x, 1) Out[71]:  $\\displaystyle e + e \\left(x - 1\\right) + \\frac{e \\left(x - 1\\right)^{2}}{2} + \\frac{e \\left(x - 1\\right)^{3}}{6} + \\frac{e \\left(x - 1\\right)^{4}}{24} + \\frac{e \\left(x - 1\\right)^{5}}{120} + O\\left(\\left(x - 1\\right)^{6}; x\\rightarrow 1\\right)$  <p>Y podemos definir expl\u00edcitamente en qu\u00e9 orden se debe realizar la expansi\u00f3n de la serie:</p> In\u00a0[72]: Copied! <pre>series(exp(x), x, 1, 10)\n</pre> series(exp(x), x, 1, 10) Out[72]:  $\\displaystyle e + e \\left(x - 1\\right) + \\frac{e \\left(x - 1\\right)^{2}}{2} + \\frac{e \\left(x - 1\\right)^{3}}{6} + \\frac{e \\left(x - 1\\right)^{4}}{24} + \\frac{e \\left(x - 1\\right)^{5}}{120} + \\frac{e \\left(x - 1\\right)^{6}}{720} + \\frac{e \\left(x - 1\\right)^{7}}{5040} + \\frac{e \\left(x - 1\\right)^{8}}{40320} + \\frac{e \\left(x - 1\\right)^{9}}{362880} + O\\left(\\left(x - 1\\right)^{10}; x\\rightarrow 1\\right)$  <p>La expansi\u00f3n de la serie incluye el orden de la aproximaci\u00f3n, lo cual es muy \u00fatil para realizar un seguimiento del orden de validez cuando hacemos c\u00e1lculos con expansiones de la serie de diferente orden:</p> In\u00a0[73]: Copied! <pre>s1 = cos(x).series(x, 0, 5)\ns1\n</pre> s1 = cos(x).series(x, 0, 5) s1 Out[73]:  $\\displaystyle 1 - \\frac{x^{2}}{2} + \\frac{x^{4}}{24} + O\\left(x^{5}\\right)$  In\u00a0[74]: Copied! <pre>s2 = sin(x).series(x, 0, 2)\ns2\n</pre> s2 = sin(x).series(x, 0, 2) s2 Out[74]:  $\\displaystyle x + O\\left(x^{2}\\right)$  In\u00a0[75]: Copied! <pre>expand(s1 * s2)\n</pre> expand(s1 * s2) Out[75]:  $\\displaystyle x + O\\left(x^{2}\\right)$  <p>Si queremos deshacernos de la informaci\u00f3n del error, podemos usar el m\u00e9todo <code>removeO</code>:</p> In\u00a0[76]: Copied! <pre>expand(s1.removeO() * s2.removeO())\n</pre> expand(s1.removeO() * s2.removeO()) Out[76]:  $\\displaystyle \\frac{x^{5}}{24} - \\frac{x^{3}}{2} + x$  <p>Pero tenga en cuenta que esta no es la expansi\u00f3n correcta de $ \\cos(x) \\sin(x)$ a $ 5 $ \u00e9simo orden:</p> In\u00a0[77]: Copied! <pre>(cos(x)*sin(x)).series(x, 0, 6)\n</pre> (cos(x)*sin(x)).series(x, 0, 6) Out[77]:  $\\displaystyle x - \\frac{2 x^{3}}{3} + \\frac{2 x^{5}}{15} + O\\left(x^{6}\\right)$  In\u00a0[78]: Copied! <pre>m11, m12, m21, m22 = symbols(\"m11, m12, m21, m22\")\nb1, b2 = symbols(\"b1, b2\")\n</pre> m11, m12, m21, m22 = symbols(\"m11, m12, m21, m22\") b1, b2 = symbols(\"b1, b2\") In\u00a0[79]: Copied! <pre>A = Matrix([[m11, m12],[m21, m22]])\nA\n</pre> A = Matrix([[m11, m12],[m21, m22]]) A Out[79]:  $\\displaystyle \\left[\\begin{matrix}m_{11} &amp; m_{12}\\\\m_{21} &amp; m_{22}\\end{matrix}\\right]$  In\u00a0[80]: Copied! <pre>b = Matrix([[b1], [b2]])\nb\n</pre> b = Matrix([[b1], [b2]]) b Out[80]:  $\\displaystyle \\left[\\begin{matrix}b_{1}\\\\b_{2}\\end{matrix}\\right]$  <p>Con las instancias de la clase <code>Matrix</code> podemos hacer las operaciones habituales de \u00e1lgebra matricial:</p> In\u00a0[81]: Copied! <pre>A**2\n</pre> A**2 Out[81]:  $\\displaystyle \\left[\\begin{matrix}m_{11}^{2} + m_{12} m_{21} &amp; m_{11} m_{12} + m_{12} m_{22}\\\\m_{11} m_{21} + m_{21} m_{22} &amp; m_{12} m_{21} + m_{22}^{2}\\end{matrix}\\right]$  In\u00a0[82]: Copied! <pre>A * b\n</pre> A * b Out[82]:  $\\displaystyle \\left[\\begin{matrix}b_{1} m_{11} + b_{2} m_{12}\\\\b_{1} m_{21} + b_{2} m_{22}\\end{matrix}\\right]$  <p>Y calcular determinantes e inversas, y similares:</p> In\u00a0[83]: Copied! <pre>A.det()\n</pre> A.det() Out[83]:  $\\displaystyle m_{11} m_{22} - m_{12} m_{21}$  In\u00a0[84]: Copied! <pre>A.inv()\n</pre> A.inv() Out[84]:  $\\displaystyle \\left[\\begin{matrix}\\frac{m_{22}}{m_{11} m_{22} - m_{12} m_{21}} &amp; - \\frac{m_{12}}{m_{11} m_{22} - m_{12} m_{21}}\\\\- \\frac{m_{21}}{m_{11} m_{22} - m_{12} m_{21}} &amp; \\frac{m_{11}}{m_{11} m_{22} - m_{12} m_{21}}\\end{matrix}\\right]$  In\u00a0[85]: Copied! <pre>solve(x**2 - 1, x)\n</pre> solve(x**2 - 1, x) Out[85]:  $\\displaystyle \\left[ -1, \\  1\\right]$  In\u00a0[86]: Copied! <pre>solve(x**4 - x**2 - 1, x)\n</pre> solve(x**4 - x**2 - 1, x) Out[86]:  $\\displaystyle \\left[ - i \\sqrt{- \\frac{1}{2} + \\frac{\\sqrt{5}}{2}}, \\  i \\sqrt{- \\frac{1}{2} + \\frac{\\sqrt{5}}{2}}, \\  - \\sqrt{\\frac{1}{2} + \\frac{\\sqrt{5}}{2}}, \\  \\sqrt{\\frac{1}{2} + \\frac{\\sqrt{5}}{2}}\\right]$  <p>Sistema de ecuaciones:</p> In\u00a0[87]: Copied! <pre>solve([x + y - 1, x - y - 1], [x,y])\n</pre> solve([x + y - 1, x - y - 1], [x,y]) Out[87]:  $\\displaystyle \\left\\{ x : 1, \\  y : 0\\right\\}$  <p>En cuanto a otras expresiones simb\u00f3licas:</p> In\u00a0[88]: Copied! <pre>solve([x + y - a, x - y - c], [x,y])\n</pre> solve([x + y - a, x - y - c], [x,y]) Out[88]:  $\\displaystyle \\left\\{ x : \\frac{a}{2} + \\frac{c}{2}, \\  y : \\frac{a}{2} - \\frac{c}{2}\\right\\}$"},{"location":"lectures/data_manipulation/sc_03/#sympy","title":"Sympy\u00b6","text":""},{"location":"lectures/data_manipulation/sc_03/#introduccion","title":"Introducci\u00f3n\u00b6","text":""},{"location":"lectures/data_manipulation/sc_03/#variables-simbolicas","title":"Variables simb\u00f3licas\u00b6","text":""},{"location":"lectures/data_manipulation/sc_03/#numeros-complejos","title":"N\u00fameros complejos\u00b6","text":"<p>La unidad imaginaria se denota \"I\" en <code>Sympy</code>.</p>"},{"location":"lectures/data_manipulation/sc_03/#numeros-racionales","title":"Numeros racionales\u00b6","text":"<p>Hay tres tipos num\u00e9ricos diferentes en SymPy: <code>Real</code>,<code> Rational</code>, \u02bbInteger`:</p>"},{"location":"lectures/data_manipulation/sc_03/#evaluacion-numerica","title":"Evaluaci\u00f3n num\u00e9rica\u00b6","text":"<p><code>SymPy</code> usa una biblioteca para precisi\u00f3n art\u00edstica como backend num\u00e9rico, y tiene expresiones <code>SymPy</code> predefinidas para una serie de constantes matem\u00e1ticas, como: <code>pi</code>, \u02bbe<code>, \u02bboo</code> para infinito.</p> <p>Para evaluar una expresi\u00f3n num\u00e9ricamente podemos usar la funci\u00f3n <code>evalf</code> (o <code>N</code>). Toma un argumento \"n\" que especifica el n\u00famero de d\u00edgitos significativos.</p>"},{"location":"lectures/data_manipulation/sc_03/#manipulaciones-algebraicas","title":"Manipulaciones algebraicas\u00b6","text":"<p>Uno de los usos principales de un CAS es realizar manipulaciones algebraicas de expresiones. Por ejemplo, podr\u00edamos querer expandir un producto, factorizar una expresi\u00f3n o simplemente una expresi\u00f3n. Las funciones para realizar estas operaciones b\u00e1sicas en SymPy se muestran en esta secci\u00f3n.</p>"},{"location":"lectures/data_manipulation/sc_03/#expandir-y-factorizar","title":"Expandir y factorizar\u00b6","text":"<p>Los primeros pasos en una manipulaci\u00f3n algebraica</p>"},{"location":"lectures/data_manipulation/sc_03/#simplificar","title":"Simplificar\u00b6","text":"<p>El \"simplificar\" intenta simplificar una expresi\u00f3n en una expresi\u00f3n agradable, utilizando varias t\u00e9cnicas. Tambi\u00e9n existen alternativas m\u00e1s espec\u00edficas a las funciones <code>simplify</code>:<code> trigsimp</code>, <code>powsimp</code>,<code> logcombine</code>, etc.</p> <p>Los usos b\u00e1sicos de estas funciones son los siguientes:</p>"},{"location":"lectures/data_manipulation/sc_03/#separados-y-juntos","title":"Separados y juntos\u00b6","text":"<p>Para manipular expresiones simb\u00f3licas de fracciones, podemos usar las funciones <code>apart</code> y <code>together</code>:</p>"},{"location":"lectures/data_manipulation/sc_03/#calculo","title":"C\u00e1lculo\u00b6","text":"<p>Adem\u00e1s de las manipulaciones algebraicas, el otro uso principal de CAS es hacer c\u00e1lculo, como derivadas e integrales de expresiones algebraicas.</p>"},{"location":"lectures/data_manipulation/sc_03/#diferenciacion","title":"Diferenciaci\u00f3n\u00b6","text":"<p>La diferenciaci\u00f3n suele ser sencilla. Utilice la funci\u00f3n <code>diff</code>. El primer argumento es la expresi\u00f3n para tomar la derivada y el segundo argumento es el s\u00edmbolo por el cual tomar la derivada:</p>"},{"location":"lectures/data_manipulation/sc_03/#integracion","title":"Integraci\u00f3n\u00b6","text":"<p>La integraci\u00f3n se realiza de manera similar:</p>"},{"location":"lectures/data_manipulation/sc_03/#sumas-y-productos","title":"Sumas y productos\u00b6","text":"<p>Podemos evaluar sumas y productos usando las funciones: 'Suma'</p>"},{"location":"lectures/data_manipulation/sc_03/#limites","title":"L\u00edmites\u00b6","text":"<p>Los l\u00edmites se pueden evaluar utilizando la funci\u00f3n <code>limit</code>. Por ejemplo,</p>"},{"location":"lectures/data_manipulation/sc_03/#serie","title":"Serie\u00b6","text":"<p>La expansi\u00f3n de la serie tambi\u00e9n es una de las caracter\u00edsticas m\u00e1s \u00fatiles de un CAS. En SymPy podemos realizar una expansi\u00f3n en serie de una expresi\u00f3n usando la funci\u00f3n <code>series</code>:</p>"},{"location":"lectures/data_manipulation/sc_03/#algebra-lineal","title":"\u00c1lgebra lineal\u00b6","text":""},{"location":"lectures/data_manipulation/sc_03/#matrices","title":"Matrices\u00b6","text":"<p>Las matrices se definen usando la clase <code>Matrix</code>:</p>"},{"location":"lectures/data_manipulation/sc_03/#resolver-ecuaciones","title":"Resolver ecuaciones\u00b6","text":"<p>Para resolver ecuaciones y sistemas de ecuaciones podemos usar la funci\u00f3n <code>resolver</code>:</p>"},{"location":"lectures/data_manipulation/sc_03/#referencias","title":"Referencias\u00b6","text":"<ul> <li>The SymPy projects web page</li> <li>The source code of SymPy</li> <li>Online version of SymPy for testing and demonstrations</li> </ul>"},{"location":"lectures/machine_learning/cla_01/","title":"Clasificaci\u00f3n I","text":"<p>El modelo es entonces obtenido a base de lo que cada ensayo (valor de $i$) y el conjunto de variables explicativas/independientes puedan informar acerca de la probabilidad final. Estas variables explicativas pueden pensarse como un vector $X_i$ k-dimensional y el modelo toma entonces la forma:</p> <p>$$p_i=\\mathbb{E}(\\dfrac{Y_i}{n_i}|X_i)$$</p> <p>Los logits de las probabilidades binomiales desconocidas (i.e., los logaritmos de la raz\u00f3n de momios) son modeladas como una funci\u00f3n lineal de los $X_i$:</p> <p>$$logit(p_i) = ln(\\dfrac{p_i}{1-p_i}) = \\beta_0+\\beta_1x_{1,i} +....+\\beta_kx_{k,i}$$</p> <p>Note que un elemento particular de $X_i$ puede ser ajustado a 1 para todo $i$ obteni\u00e9ndose una constante independiente en el modelo. Los par\u00e1metros desconocidos $\\beta _{j}$ son usualmente estimados a trav\u00e9s de m\u00e1xima verosimilitud.</p> <p>La interpretaci\u00f3n de los estimados del par\u00e1metro $\\beta _{j}$ es como los efectos aditivos en el logaritmo de la raz\u00f3n de momios para una unidad de cambio en la j\u00e9sima variable explicativa. En el caso de una variable explicativa dicot\u00f3mica, por ejemplo g\u00e9nero, $e^{\\beta}$ es la estimaci\u00f3n de la raz\u00f3n de momios (odds ratio) de tener el resultado para, por decir algo, hombres comparados con mujeres. El modelo tiene una formulaci\u00f3n equivalente dada por:</p> <p>$$p_i=\\dfrac{1}{e^{-(\\beta_0+\\beta_1x_{1,i} +....+\\beta_kx_{k,i})}+1}$$</p> Valores categor\u00eda Probabilidad Logit $X_1$ $\\epsilon_1$ $\\pi(X_1)$ $g(X_1)$ $X_2$ $\\epsilon_2$ $\\pi(X_2)$ $g(X_2)$ $X_n$ $\\epsilon_n$ $\\pi(X_n)$ $g(X_n)$ <p>Donde $\u03b5_i$ es \"0\" o \"1\" seg\u00fan el caso y adem\u00e1s:</p> <p>$$0 \\leq \u03c0(X_i) = \\dfrac{1}{i}\\sum_{k=1}^i \u03b5_k\\leq 1 \\, \\ g(X_i) =  ln(\\dfrac{\\pi(X_i)}{1- \\pi(X_i)})=\\beta_0+\\beta_1 X_i $$</p> <p></p> <p>Veamos un peque\u00f1o ejemplo de como se implementa en python. En este ejemplo voy a utilizar el dataset Iris que ya viene junto con Scikit-learn y es ideal para practicar con regresiones log\u00edstica ; el mismo contiene los tipos de flores basado en en largo y ancho de su s\u00e9palo y p\u00e9talo.</p> In\u00a0[1]: Copied! <pre># librerias\n \nimport os\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \npd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes\n\n# Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab\n%matplotlib inline\n</pre> # librerias   import os import numpy as np import pandas as pd  from sklearn import datasets from sklearn.model_selection import train_test_split  import matplotlib.pyplot as plt import seaborn as sns  pd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes  # Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab %matplotlib inline In\u00a0[2]: Copied! <pre># cargar datos\niris = datasets.load_iris()\nprint(iris.DESCR)\n</pre> # cargar datos iris = datasets.load_iris() print(iris.DESCR) <pre>.. _iris_dataset:\n\nIris plants dataset\n--------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 150 (50 in each of three classes)\n    :Number of Attributes: 4 numeric, predictive attributes and the class\n    :Attribute Information:\n        - sepal length in cm\n        - sepal width in cm\n        - petal length in cm\n        - petal width in cm\n        - class:\n                - Iris-Setosa\n                - Iris-Versicolour\n                - Iris-Virginica\n                \n    :Summary Statistics:\n\n    ============== ==== ==== ======= ===== ====================\n                    Min  Max   Mean    SD   Class Correlation\n    ============== ==== ==== ======= ===== ====================\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n    ============== ==== ==== ======= ===== ====================\n\n    :Missing Attribute Values: None\n    :Class Distribution: 33.3% for each of 3 classes.\n    :Creator: R.A. Fisher\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n    :Date: July, 1988\n\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\nfrom Fisher's paper. Note that it's the same as in R, but not as in the UCI\nMachine Learning Repository, which has two wrong data points.\n\nThis is perhaps the best known database to be found in the\npattern recognition literature.  Fisher's paper is a classic in the field and\nis referenced frequently to this day.  (See Duda &amp; Hart, for example.)  The\ndata set contains 3 classes of 50 instances each, where each class refers to a\ntype of iris plant.  One class is linearly separable from the other 2; the\nlatter are NOT linearly separable from each other.\n\n.. topic:: References\n\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n     Mathematical Statistics\" (John Wiley, NY, 1950).\n   - Duda, R.O., &amp; Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n     (Q327.D83) John Wiley &amp; Sons.  ISBN 0-471-22361-1.  See page 218.\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n     Structure and Classification Rule for Recognition in Partially Exposed\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n     on Information Theory, May 1972, 431-433.\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n     conceptual clustering system finds 3 classes in the data.\n   - Many, many more ...\n</pre> In\u00a0[3]: Copied! <pre># dejar en formato dataframe\n\niris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\niris_df['TARGET'] = iris.target\niris_df.head() # estructura de nuestro dataset.\n</pre> # dejar en formato dataframe  iris_df = pd.DataFrame(iris.data, columns=iris.feature_names) iris_df['TARGET'] = iris.target iris_df.head() # estructura de nuestro dataset. Out[3]: sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) TARGET 0 5.1 3.5 1.4 0.2 0 1 4.9 3.0 1.4 0.2 0 2 4.7 3.2 1.3 0.2 0 3 4.6 3.1 1.5 0.2 0 4 5.0 3.6 1.4 0.2 0 <p>Para ver gr\u00e1ficamente el modelo de regresi\u00f3n log\u00edstica, ajustemos el modelo solo a dos variables: petal length (cm), petal width (cm).</p> In\u00a0[4]: Copied! <pre># datos \nfrom sklearn.linear_model import LogisticRegression\n\nX = iris_df[['sepal length (cm)', 'sepal width (cm)']]\nY = iris_df['TARGET']\n\n# split dataset\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state = 2)\n</pre> # datos  from sklearn.linear_model import LogisticRegression  X = iris_df[['sepal length (cm)', 'sepal width (cm)']] Y = iris_df['TARGET']  # split dataset X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state = 2)  In\u00a0[5]: Copied! <pre># print rows train and test sets\nprint('Separando informacion:\\n')\nprint('numero de filas data original : ',len(X))\nprint('numero de filas train set     : ',len(X_train))\nprint('numero de filas test set      : ',len(X_test))\n</pre> # print rows train and test sets print('Separando informacion:\\n') print('numero de filas data original : ',len(X)) print('numero de filas train set     : ',len(X_train)) print('numero de filas test set      : ',len(X_test)) <pre>Separando informacion:\n\nnumero de filas data original :  150\nnumero de filas train set     :  120\nnumero de filas test set      :  30\n</pre> In\u00a0[6]: Copied! <pre># Creando el modelo\nrlog = LogisticRegression()\nrlog.fit(X_train, Y_train) # ajustando el modelo\n</pre> # Creando el modelo rlog = LogisticRegression() rlog.fit(X_train, Y_train) # ajustando el modelo Out[6]: <pre>LogisticRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression<pre>LogisticRegression()</pre> In\u00a0[7]: Copied! <pre>rlog.score(X_train,Y_train)\n</pre> rlog.score(X_train,Y_train) Out[7]: <pre>0.825</pre> In\u00a0[8]: Copied! <pre>rlog.predict_log_proba(X_train)\n</pre> rlog.predict_log_proba(X_train) Out[8]: <pre>array([[ -3.99669749,  -0.66190745,  -0.76409046],\n       [ -0.21107893,  -1.90464333,  -3.18413364],\n       [ -1.8065849 ,  -0.52153993,  -1.41807287],\n       [ -6.47991623,  -3.03584962,  -0.05083842],\n       [ -1.05110824,  -0.71678289,  -1.81936211],\n       [ -0.17004605,  -2.74876413,  -2.3819842 ],\n       [ -4.68728813,  -0.80298536,  -0.61101654],\n       [ -2.96770066,  -0.71008075,  -0.78312858],\n       [ -3.63294174,  -0.2154118 ,  -1.78765395],\n       [ -5.51537947,  -1.46091555,  -0.26925051],\n       [ -2.98670271,  -0.60263228,  -0.91086157],\n       [ -3.64615407,  -0.71257092,  -0.72664838],\n       [ -4.79873937,  -1.4202387 ,  -0.28754402],\n       [ -2.24998139,  -0.20623326,  -2.51385499],\n       [ -0.06253829,  -2.98157867,  -4.61419118],\n       [ -4.37794957,  -0.51550211,  -0.94097214],\n       [ -0.19806777,  -2.0180827 ,  -3.06239155],\n       [ -4.75300091,  -1.24804972,  -0.35053651],\n       [ -0.10414361,  -2.60012179,  -3.703401  ],\n       [ -0.1399092 ,  -2.36113102,  -3.31733432],\n       [ -0.33299812,  -1.72995085,  -2.24492641],\n       [ -1.36052475,  -0.39394677,  -2.67243331],\n       [ -2.00164029,  -1.07409158,  -0.64764129],\n       [ -7.3103791 ,  -3.31679977,  -0.03763674],\n       [ -0.34589815,  -1.61640035,  -2.36655737],\n       [ -4.75300091,  -1.24804972,  -0.35053651],\n       [ -2.32561455,  -1.12330948,  -0.54978327],\n       [ -8.62419813,  -2.24523278,  -0.11214189],\n       [ -5.53049122,  -0.33797257,  -1.26294071],\n       [ -0.06998861,  -2.75798325,  -5.47839753],\n       [ -0.18803569,  -1.9034554 ,  -3.80038456],\n       [ -3.06265058,  -0.4256791 ,  -1.20427132],\n       [ -2.4142983 ,  -0.45329007,  -1.29085265],\n       [ -1.43446384,  -0.44603054,  -2.10707824],\n       [ -9.03137722,  -2.37765551,  -0.09748871],\n       [ -4.69174057,  -0.68098728,  -0.72419992],\n       [ -3.66230617,  -0.60227249,  -0.85153142],\n       [ -0.05027585,  -3.34866781,  -4.27573592],\n       [ -2.4142983 ,  -0.45329007,  -1.29085265],\n       [ -0.06340959,  -3.33994623,  -3.6495755 ],\n       [ -2.71090648,  -0.4751419 ,  -1.16562856],\n       [ -3.422256  ,  -0.38407763,  -1.2507754 ],\n       [ -1.82139702,  -0.30530631,  -2.28964102],\n       [ -0.23342127,  -2.28448194,  -2.24098902],\n       [ -3.25980162,  -0.24347857,  -1.72761517],\n       [ -2.33611822,  -0.62801104,  -0.99521069],\n       [ -4.75300091,  -1.24804972,  -0.35053651],\n       [ -9.67524275,  -2.21910725,  -0.11515155],\n       [ -0.34353786,  -1.38299432,  -3.22095311],\n       [ -2.6423909 ,  -1.03887894,  -0.55345829],\n       [ -0.03480928,  -3.73440813,  -4.57337069],\n       [ -3.06265058,  -0.4256791 ,  -1.20427132],\n       [ -5.18402425,  -1.53076722,  -0.25099663],\n       [ -0.24523474,  -1.68589809,  -3.43575133],\n       [ -3.35645967,  -0.10909088,  -2.68102926],\n       [ -0.24523474,  -1.68589809,  -3.43575133],\n       [ -0.07746962,  -2.84820416,  -4.09855929],\n       [ -0.12756408,  -2.60168695,  -3.08752732],\n       [ -5.46678423,  -1.28586978,  -0.32938621],\n       [ -2.6423909 ,  -1.03887894,  -0.55345829],\n       [ -0.11158762,  -2.48111528,  -3.81957595],\n       [ -0.01601167,  -4.64447482,  -5.07204482],\n       [ -5.40710339,  -0.59393632,  -0.81336006],\n       [-10.32214698,  -2.06359771,  -0.13585311],\n       [ -5.53049122,  -0.33797257,  -1.26294071],\n       [ -5.07086967,  -2.1981711 ,  -0.12475057],\n       [ -1.66675571,  -0.29897084,  -2.66556294],\n       [ -3.66230617,  -0.60227249,  -0.85153142],\n       [ -4.35319491,  -0.99654955,  -0.48129372],\n       [ -0.10414361,  -2.60012179,  -3.703401  ],\n       [ -8.46843914,  -1.83657274,  -0.17384478],\n       [ -2.33611822,  -0.62801104,  -0.99521069],\n       [ -5.03844127,  -0.75293161,  -0.64906835],\n       [ -0.13370583,  -2.14573723,  -4.80718114],\n       [ -2.13015961,  -0.44390775,  -1.42854624],\n       [ -0.14884726,  -2.24361855,  -3.43500331],\n       [ -0.15976538,  -2.12808615,  -3.55465238],\n       [ -3.1182411 ,  -0.3548191 ,  -1.36859278],\n       [ -5.51537947,  -1.46091555,  -0.26925051],\n       [ -2.5357413 ,  -0.32183203,  -1.62975754],\n       [ -4.33853357,  -0.72898717,  -0.68409426],\n       [ -5.57382666,  -1.64581327,  -0.21896676],\n       [ -6.14300989,  -1.15968166,  -0.37940919],\n       [ -3.67733948,  -1.1231079 ,  -0.43164097],\n       [ -5.51537947,  -1.46091555,  -0.26925051],\n       [ -3.25980162,  -0.24347857,  -1.72761517],\n       [ -1.50374348,  -0.6199054 ,  -1.42833279],\n       [ -2.64559993,  -0.66273639,  -0.88286013],\n       [ -3.69170941,  -0.5052252 ,  -0.98966559],\n       [ -3.06265058,  -0.4256791 ,  -1.20427132],\n       [ -1.70787592,  -0.80218252,  -0.99317107],\n       [ -0.03480928,  -3.73440813,  -4.57337069],\n       [ -6.17079981,  -0.42876844,  -1.05958475],\n       [ -1.50374348,  -0.6199054 ,  -1.42833279],\n       [ -3.18432235,  -0.29444982,  -1.54340497],\n       [ -3.06265058,  -0.4256791 ,  -1.20427132],\n       [ -3.65400219,  -0.97332008,  -0.51703462],\n       [ -5.05989815,  -1.02728954,  -0.45306334],\n       [ -0.03094358,  -4.23634452,  -4.13458123],\n       [ -0.09199006,  -2.50522834,  -5.0785667 ],\n       [ -3.98888929,  -0.78054977,  -0.64755132],\n       [ -3.31830082,  -1.0388256 ,  -0.49443459],\n       [ -7.74336497,  -1.78746182,  -0.18370422],\n       [ -0.05357357,  -3.225515  ,  -4.38776458],\n       [ -6.62509295,  -1.61990942,  -0.2221981 ],\n       [ -0.03965368,  -3.48635147,  -4.79567696],\n       [ -4.35319491,  -0.99654955,  -0.48129372],\n       [ -4.08715355,  -1.38461613,  -0.31089182],\n       [ -0.34589815,  -1.61640035,  -2.36655737],\n       [ -3.31177254,  -0.65294575,  -0.81409912],\n       [ -0.07511489,  -2.74125422,  -4.84422965],\n       [ -5.0899367 ,  -1.18377862,  -0.37437096],\n       [ -1.76295435,  -0.6043599 ,  -1.26571138],\n       [ -0.11158762,  -2.48111528,  -3.81957595],\n       [ -4.35319491,  -0.99654955,  -0.48129372],\n       [ -4.71854451,  -1.08714279,  -0.42481105],\n       [ -0.07746962,  -2.84820416,  -4.09855929],\n       [ -0.01626132,  -4.2872282 ,  -6.03778143],\n       [ -5.40710339,  -0.59393632,  -0.81336006],\n       [ -0.03160108,  -4.48990308,  -3.91777686]])</pre> <p>Grafiquemos nuestro resultados:</p> In\u00a0[9]: Copied! <pre># dataframe a matriz\nX = X.values\nY = Y.values\n</pre> # dataframe a matriz X = X.values Y = Y.values In\u00a0[10]: Copied! <pre># grafica de la regresion logistica \nplt.figure(figsize=(12,4))\n\nx_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\ny_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n\nh = .02  # step size in the mesh\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = rlog.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure(1, figsize=(4, 3))\nplt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired, shading='auto')\n\n# Plot also the training points\nplt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\nplt.show()\n</pre> # grafica de la regresion logistica  plt.figure(figsize=(12,4))  x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5 y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5  h = .02  # step size in the mesh xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) Z = rlog.predict(np.c_[xx.ravel(), yy.ravel()])  # Put the result into a color plot Z = Z.reshape(xx.shape) plt.figure(1, figsize=(4, 3)) plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired, shading='auto')  # Plot also the training points plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired) plt.xlabel('Sepal length') plt.ylabel('Sepal width') plt.show() <pre>C:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\db-connectors-RsEieBu8-py3.8\\lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n  warnings.warn(\n</pre> <p>Gr\u00e1ficamente podemos decir que el modelo se ajusta bastante bien, puesto que las clasificaciones son adecuadas y el modelo no se confunde entre una clase y otra. Por otro lado, existe valores num\u00e9ricos que tambi\u00e9n nos pueden ayudar a convensernos de estos, que son las m\u00e9tricas que se habian definidos con anterioridad.</p> <p>Para ello, instanciaremos las distintas metricas del archivo metrics_classification.py y calcularemos sus distintos valores.</p> In\u00a0[11]: Copied! <pre>from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n</pre> from sklearn.metrics import confusion_matrix from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score In\u00a0[12]: Copied! <pre># Evaluar las m\u00e9tricas\ndef classification_metrics(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Aplicar las distintas m\u00e9tricas definidas\n    :param df: dataframe con las columnas: ['y', 'yhat']\n    :return: dataframe con las m\u00e9tricas especificadas\n    \"\"\"\n    y_true = df['y']\n    y_pred = df['yhat']\n\n    accuracy = round(accuracy_score(y_true, y_pred), 4)\n    recall = round(recall_score(y_true, y_pred, average='macro'), 4)\n    precision = round(precision_score(y_true, y_pred, average='macro'), 4)\n    fscore = round(f1_score(y_true, y_pred, average='macro'), 4)\n\n    df_result = pd.DataFrame({'accuracy': [accuracy],\n                              'recall': [recall],\n                              'precision': [precision],\n                              'fscore': [fscore]})\n\n    return df_result\n</pre> # Evaluar las m\u00e9tricas def classification_metrics(df: pd.DataFrame) -&gt; pd.DataFrame:     \"\"\"     Aplicar las distintas m\u00e9tricas definidas     :param df: dataframe con las columnas: ['y', 'yhat']     :return: dataframe con las m\u00e9tricas especificadas     \"\"\"     y_true = df['y']     y_pred = df['yhat']      accuracy = round(accuracy_score(y_true, y_pred), 4)     recall = round(recall_score(y_true, y_pred, average='macro'), 4)     precision = round(precision_score(y_true, y_pred, average='macro'), 4)     fscore = round(f1_score(y_true, y_pred, average='macro'), 4)      df_result = pd.DataFrame({'accuracy': [accuracy],                               'recall': [recall],                               'precision': [precision],                               'fscore': [fscore]})      return df_result In\u00a0[13]: Copied! <pre># metrics\n\ny_true =  list(Y_test)\ny_pred = list(rlog.predict(X_test))\n\nprint('Valores:\\n')\nprint('originales: ', y_true)\nprint('predicho:   ', y_pred)\n</pre> # metrics  y_true =  list(Y_test) y_pred = list(rlog.predict(X_test))  print('Valores:\\n') print('originales: ', y_true) print('predicho:   ', y_pred) <pre>Valores:\n\noriginales:  [0, 0, 2, 0, 0, 2, 0, 2, 2, 0, 0, 0, 0, 0, 1, 1, 0, 1, 2, 1, 1, 1, 2, 1, 1, 0, 0, 2, 0, 2]\npredicho:    [0, 0, 1, 0, 0, 2, 0, 2, 2, 0, 0, 0, 0, 0, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 0, 0, 1, 0, 2]\n</pre> In\u00a0[14]: Copied! <pre>print('\\nMatriz de confusion:\\n ')\nprint(confusion_matrix(y_true,y_pred))\n</pre> print('\\nMatriz de confusion:\\n ') print(confusion_matrix(y_true,y_pred)) <pre>\nMatriz de confusion:\n \n[[13  1  0]\n [ 0  4  4]\n [ 0  2  6]]\n</pre> In\u00a0[15]: Copied! <pre># ejemplo \ndf_temp = pd.DataFrame(\n    {\n        'y':y_true,\n        'yhat':y_pred\n        }\n)\n\ndf_metrics = classification_metrics(df_temp)\nprint(\"\\nMetricas para los regresores : 'sepal length (cm)' y  'sepal width (cm)'\")\nprint(\"\")\ndf_metrics\n</pre> # ejemplo  df_temp = pd.DataFrame(     {         'y':y_true,         'yhat':y_pred         } )  df_metrics = classification_metrics(df_temp) print(\"\\nMetricas para los regresores : 'sepal length (cm)' y  'sepal width (cm)'\") print(\"\") df_metrics <pre>\nMetricas para los regresores : 'sepal length (cm)' y  'sepal width (cm)'\n\n</pre> Out[15]: accuracy recall precision fscore 0 0.7667 0.7262 0.7238 0.721 <p>Basado en las m\u00e9tricas y en la gr\u00e1fica, podemos concluir que el ajuste realizado es bastante asertado.</p> <p>Ahora, calculamos la curva AUC-ROC para nuestro ejemplo. Cabe destacar que esta curva es efectiva solo para clasificaci\u00f3n binaria, por lo que para efectos pr\u00e1cticos convertiremos nuestro TARGET en binarios (0 \u00f3 1).</p> <p>Para efectos pr\u00e1cticos tranformaremos la clase objetivo (en este caso, la clase 0) a 1, y el resto de las clases (clase 1 y 2) las dejaremos en la clase 0.</p> In\u00a0[16]: Copied! <pre>from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n</pre> from sklearn.metrics import roc_curve from sklearn.metrics import roc_auc_score In\u00a0[17]: Copied! <pre># graficar curva roc\ndef plot_roc_curve(fpr, tpr):\n    plt.figure(figsize=(8,8))\n    plt.plot(fpr, tpr, color='orange', label='ROC')\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()\n</pre> # graficar curva roc def plot_roc_curve(fpr, tpr):     plt.figure(figsize=(8,8))     plt.plot(fpr, tpr, color='orange', label='ROC')     plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')     plt.xlabel('False Positive Rate')     plt.ylabel('True Positive Rate')     plt.title('Receiver Operating Characteristic (ROC) Curve')     plt.legend()     plt.show() In\u00a0[18]: Copied! <pre># separar clase 0 del resto\nX = iris_df[['sepal length (cm)', 'sepal width (cm)']]\nY = iris_df['TARGET'].apply(lambda x: 1 if x ==2 else 0)\nmodel =  LogisticRegression()\n\n# split dataset\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state = 2)\n\n# ajustar modelo \nmodel.fit(X_train,Y_train)\n</pre> # separar clase 0 del resto X = iris_df[['sepal length (cm)', 'sepal width (cm)']] Y = iris_df['TARGET'].apply(lambda x: 1 if x ==2 else 0) model =  LogisticRegression()  # split dataset X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state = 2)  # ajustar modelo  model.fit(X_train,Y_train) Out[18]: <pre>LogisticRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression<pre>LogisticRegression()</pre> In\u00a0[19]: Copied! <pre># calcular score AUC\nprobs = model.predict_proba(X_test) # predecir probabilidades para X_test\nprobs_tp = probs[:, 1] # mantener solo las probabilidades de la clase positiva \n       \nauc = roc_auc_score(Y_test, probs_tp)  # calcular score AUC \n\nprint('AUC: %.2f' % auc)\n</pre> # calcular score AUC probs = model.predict_proba(X_test) # predecir probabilidades para X_test probs_tp = probs[:, 1] # mantener solo las probabilidades de la clase positiva          auc = roc_auc_score(Y_test, probs_tp)  # calcular score AUC   print('AUC: %.2f' % auc) <pre>AUC: 0.93\n</pre> In\u00a0[20]: Copied! <pre># calcular curva ROC\nfpr, tpr, thresholds = roc_curve(Y_test, probs_tp) # obtener curva ROC\nplot_roc_curve(fpr, tpr)\n</pre> # calcular curva ROC fpr, tpr, thresholds = roc_curve(Y_test, probs_tp) # obtener curva ROC plot_roc_curve(fpr, tpr) In\u00a0[21]: Copied! <pre>from sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.preprocessing import StandardScaler\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom matplotlib.colors import ListedColormap\n\n\nh = .02  # step size in the mesh\nplt.figure(figsize=(12,12))\nnames = [\"Logistic\",\n         \"RBF SVM\", \n         \"Decision Tree\", \n         \"Random Forest\"\n]\n\nclassifiers = [\n    LogisticRegression(),\n    SVC(gamma=2, C=1),\n    DecisionTreeClassifier(max_depth=5),\n    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n]\n\n\n\nX, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n                           random_state=1, n_clusters_per_class=1)\nrng = np.random.RandomState(2)\nX += 2 * rng.uniform(size=X.shape)\nlinearly_separable = (X, y)\n\ndatasets = [make_moons(noise=0.3, random_state=0),\n            make_circles(noise=0.2, factor=0.5, random_state=1),\n            linearly_separable\n            ]\n\n\n\nfigure = plt.figure(figsize=(27, 9))\ni = 1\n\n\n\n# iterate over datasets\nfor ds_cnt, ds in enumerate(datasets):\n    # preprocess dataset, split into training and test part\n    X, y = ds\n    X = StandardScaler().fit_transform(X)\n    X_train, X_test, y_train, y_test = \\\n        train_test_split(X, y, test_size=.4, random_state=42)\n\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    # just plot the dataset first\n    cm = plt.cm.RdBu\n    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n    if ds_cnt == 0:\n        ax.set_title(\"Input data\")\n    # Plot the training points\n    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n               edgecolors='k')\n    # Plot the testing points\n    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n               edgecolors='k')\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    ax.set_xticks(())\n    ax.set_yticks(())\n    i += 1\n\n    # iterate over classifiers\n    for name, clf in zip(names, classifiers):\n        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n        clf.fit(X_train, y_train)\n        score = clf.score(X_test, y_test)\n\n        # Plot the decision boundary. For that, we will assign a color to each\n        # point in the mesh [x_min, x_max]x[y_min, y_max].\n        if hasattr(clf, \"decision_function\"):\n            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n        else:\n            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n\n        # Put the result into a color plot\n        Z = Z.reshape(xx.shape)\n        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n\n        # Plot the training points\n        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n                   edgecolors='k')\n        # Plot the testing points\n        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n                   edgecolors='k', alpha=0.6)\n\n        ax.set_xlim(xx.min(), xx.max())\n        ax.set_ylim(yy.min(), yy.max())\n        ax.set_xticks(())\n        ax.set_yticks(())\n        if ds_cnt == 0:\n            ax.set_title(name)\n        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n                size=15, horizontalalignment='right')\n        i += 1\n\nplt.tight_layout()\nplt.show()\n</pre> from sklearn.datasets import make_moons, make_circles, make_classification from sklearn.preprocessing import StandardScaler   from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier  from matplotlib.colors import ListedColormap   h = .02  # step size in the mesh plt.figure(figsize=(12,12)) names = [\"Logistic\",          \"RBF SVM\",           \"Decision Tree\",           \"Random Forest\" ]  classifiers = [     LogisticRegression(),     SVC(gamma=2, C=1),     DecisionTreeClassifier(max_depth=5),     RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1), ]    X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,                            random_state=1, n_clusters_per_class=1) rng = np.random.RandomState(2) X += 2 * rng.uniform(size=X.shape) linearly_separable = (X, y)  datasets = [make_moons(noise=0.3, random_state=0),             make_circles(noise=0.2, factor=0.5, random_state=1),             linearly_separable             ]    figure = plt.figure(figsize=(27, 9)) i = 1    # iterate over datasets for ds_cnt, ds in enumerate(datasets):     # preprocess dataset, split into training and test part     X, y = ds     X = StandardScaler().fit_transform(X)     X_train, X_test, y_train, y_test = \\         train_test_split(X, y, test_size=.4, random_state=42)      x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5     y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5     xx, yy = np.meshgrid(np.arange(x_min, x_max, h),                          np.arange(y_min, y_max, h))      # just plot the dataset first     cm = plt.cm.RdBu     cm_bright = ListedColormap(['#FF0000', '#0000FF'])     ax = plt.subplot(len(datasets), len(classifiers) + 1, i)     if ds_cnt == 0:         ax.set_title(\"Input data\")     # Plot the training points     ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,                edgecolors='k')     # Plot the testing points     ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,                edgecolors='k')     ax.set_xlim(xx.min(), xx.max())     ax.set_ylim(yy.min(), yy.max())     ax.set_xticks(())     ax.set_yticks(())     i += 1      # iterate over classifiers     for name, clf in zip(names, classifiers):         ax = plt.subplot(len(datasets), len(classifiers) + 1, i)         clf.fit(X_train, y_train)         score = clf.score(X_test, y_test)          # Plot the decision boundary. For that, we will assign a color to each         # point in the mesh [x_min, x_max]x[y_min, y_max].         if hasattr(clf, \"decision_function\"):             Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])         else:             Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]          # Put the result into a color plot         Z = Z.reshape(xx.shape)         ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)          # Plot the training points         ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,                    edgecolors='k')         # Plot the testing points         ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,                    edgecolors='k', alpha=0.6)          ax.set_xlim(xx.min(), xx.max())         ax.set_ylim(yy.min(), yy.max())         ax.set_xticks(())         ax.set_yticks(())         if ds_cnt == 0:             ax.set_title(name)         ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),                 size=15, horizontalalignment='right')         i += 1  plt.tight_layout() plt.show() <pre>&lt;Figure size 1200x1200 with 0 Axes&gt;</pre> In\u00a0[22]: Copied! <pre>class SklearnClassificationModels:\n    def __init__(self,model,name_model):\n\n        self.model = model\n        self.name_model = name_model\n        \n    @staticmethod\n    def test_train_model(X,y,n_size):\n        X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=n_size , random_state=42)\n        return X_train, X_test, y_train, y_test\n    \n    def fit_model(self,X,y,test_size):\n        X_train, X_test, y_train, y_test = self.test_train_model(X,y,test_size )\n        return self.model.fit(X_train, y_train) \n    \n    def df_testig(self,X,y,test_size):\n        X_train, X_test, y_train, y_test = self.test_train_model(X,y,test_size )\n        model_fit = self.model.fit(X_train, y_train)\n        preds = model_fit.predict(X_test)\n        df_temp = pd.DataFrame(\n            {\n                'y':y_test,\n                'yhat': model_fit.predict(X_test)\n            }\n        )\n        \n        return df_temp\n    \n    def metrics(self,X,y,test_size):\n        df_temp = self.df_testig(X,y,test_size)\n        df_metrics = classification_metrics(df_temp)\n        df_metrics['model'] = self.name_model\n        \n        return df_metrics\n</pre> class SklearnClassificationModels:     def __init__(self,model,name_model):          self.model = model         self.name_model = name_model              @staticmethod     def test_train_model(X,y,n_size):         X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=n_size , random_state=42)         return X_train, X_test, y_train, y_test          def fit_model(self,X,y,test_size):         X_train, X_test, y_train, y_test = self.test_train_model(X,y,test_size )         return self.model.fit(X_train, y_train)           def df_testig(self,X,y,test_size):         X_train, X_test, y_train, y_test = self.test_train_model(X,y,test_size )         model_fit = self.model.fit(X_train, y_train)         preds = model_fit.predict(X_test)         df_temp = pd.DataFrame(             {                 'y':y_test,                 'yhat': model_fit.predict(X_test)             }         )                  return df_temp          def metrics(self,X,y,test_size):         df_temp = self.df_testig(X,y,test_size)         df_metrics = classification_metrics(df_temp)         df_metrics['model'] = self.name_model                  return df_metrics  In\u00a0[23]: Copied! <pre># metrics \n\nimport itertools\n\n# nombre modelos\nnames_models = [\"Logistic\",\n         \"RBF SVM\", \n         \"Decision Tree\", \n         \"Random Forest\"\n]\n\n# modelos\nclassifiers = [\n    LogisticRegression(),\n    SVC(gamma=2, C=1),\n    DecisionTreeClassifier(max_depth=5),\n    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n]\n\n# datasets\nnames_dataset = ['make_moons',\n                 'make_circles',\n                 'linearly_separable'\n                ]\n\nX, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n                           random_state=1, n_clusters_per_class=1)\nrng = np.random.RandomState(2)\nX += 2 * rng.uniform(size=X.shape)\nlinearly_separable = (X, y)\n\ndatasets = [make_moons(noise=0.3, random_state=0),\n            make_circles(noise=0.2, factor=0.5, random_state=1),\n            linearly_separable\n            ]\n\n\n# juntar informacion\nlist_models = list(zip(names_models,classifiers))\nlist_dataset = list(zip(names_dataset,datasets))\n\nframes = []\nfor x in itertools.product(list_models, list_dataset):\n    \n    name_model = x[0][0]\n    classifier = x[0][1]\n    \n    name_dataset = x[1][0]\n    dataset = x[1][1]\n    \n    X = dataset[0]\n    Y =  dataset[1]\n    \n    fit_model =  SklearnClassificationModels( classifier,name_model)\n    df = fit_model.metrics(X,Y,0.2)\n    df['dataset'] = name_dataset\n    \n    frames.append(df)\n</pre> # metrics   import itertools  # nombre modelos names_models = [\"Logistic\",          \"RBF SVM\",           \"Decision Tree\",           \"Random Forest\" ]  # modelos classifiers = [     LogisticRegression(),     SVC(gamma=2, C=1),     DecisionTreeClassifier(max_depth=5),     RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1), ]  # datasets names_dataset = ['make_moons',                  'make_circles',                  'linearly_separable'                 ]  X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,                            random_state=1, n_clusters_per_class=1) rng = np.random.RandomState(2) X += 2 * rng.uniform(size=X.shape) linearly_separable = (X, y)  datasets = [make_moons(noise=0.3, random_state=0),             make_circles(noise=0.2, factor=0.5, random_state=1),             linearly_separable             ]   # juntar informacion list_models = list(zip(names_models,classifiers)) list_dataset = list(zip(names_dataset,datasets))  frames = [] for x in itertools.product(list_models, list_dataset):          name_model = x[0][0]     classifier = x[0][1]          name_dataset = x[1][0]     dataset = x[1][1]          X = dataset[0]     Y =  dataset[1]          fit_model =  SklearnClassificationModels( classifier,name_model)     df = fit_model.metrics(X,Y,0.2)     df['dataset'] = name_dataset          frames.append(df) <pre>C:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\db-connectors-RsEieBu8-py3.8\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n</pre> In\u00a0[24]: Copied! <pre># juntar resultados\npd.concat(frames)\n</pre> # juntar resultados pd.concat(frames) Out[24]: accuracy recall precision fscore model dataset 0 0.90 0.9000 0.9000 0.9000 Logistic make_moons 0 0.35 0.5000 0.1750 0.2593 Logistic make_circles 0 0.95 0.9545 0.9500 0.9499 Logistic linearly_separable 0 0.95 0.9500 0.9545 0.9499 RBF SVM make_moons 0 0.80 0.8462 0.8182 0.7980 RBF SVM make_circles 0 0.95 0.9545 0.9500 0.9499 RBF SVM linearly_separable 0 0.95 0.9500 0.9545 0.9499 Decision Tree make_moons 0 0.75 0.8077 0.7917 0.7494 Decision Tree make_circles 0 0.85 0.8535 0.8500 0.8496 Decision Tree linearly_separable 0 0.95 0.9500 0.9545 0.9499 Random Forest make_moons 0 0.75 0.8077 0.7917 0.7494 Random Forest make_circles 0 0.80 0.8081 0.8081 0.8000 Random Forest linearly_separable"},{"location":"lectures/machine_learning/cla_01/#clasificacion-i","title":"Clasificaci\u00f3n I\u00b6","text":""},{"location":"lectures/machine_learning/cla_01/#regresion-logistica-y-otros-modelos","title":"Regresi\u00f3n Log\u00edstica y otros modelos\u00b6","text":"<p>A modo de recuerdo, el modelo de regresio\u0301n lineal general  supone que, $\\boldsymbol{Y} =  \\boldsymbol{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon},$ donde:</p> <ul> <li>$\\boldsymbol{X} = (x_1,...,x_n)^{T}$: variable explicativa</li> <li>$\\boldsymbol{Y} = (y_1,...,y_n)^{T}$: variable respuesta</li> <li>$\\boldsymbol{\\epsilon} = (\\epsilon_1,...,\\epsilon_n)^{T}$: error se asume un ruido blanco, es decir, $\\epsilon \\sim \\mathcal{N}( \\boldsymbol{0},\\sigma^2I)$</li> <li>$\\boldsymbol{\\beta} = (\\beta_1,...,\\beta_n)^{T}$: coeficientes de regresio\u0301n.</li> </ul> <p>Por otro lado, el modelo de regresi\u00f3n log\u00edstica  analiza datos distribuidos binomialmente de la forma: $Y_i \\sim B(p_i,n_i)$, para $i=1,...,m$ donde los n\u00fameros de ensayos Bernoulli $n_{i}$ son conocidos y las probabilidades de \u00e9xito $p_{i}$ son desconocidas. Un ejemplo de esta distribuci\u00f3n es el porcentaje de semillas $p_{i} $ que germinan despu\u00e9s de que $n_{i}$ son plantadas.</p>"},{"location":"lectures/machine_learning/cla_01/#interpretacion-para-el-caso-binario","title":"Interpretaci\u00f3n para el caso binario\u00b6","text":"<p>La idea es que la regresi\u00f3n log\u00edstica aproxime la probabilidad de obtener  0 (no ocurre cierto suceso) o  1 (ocurre el suceso) con el valor de la variable explicativa $x$.</p> <p>En esas condiciones, la probabilidad aproximada del suceso se aproximar\u00e1 mediante una funci\u00f3n log\u00edstica del tipo:</p> <p>$$\\pi(x) =\\dfrac{e^{\\beta_0+\\beta_1x}}{e^{\\beta_0+\\beta_1x}+1}=\\dfrac{1}{e^{-(\\beta_0+\\beta_1x)}+1}$$</p> <p>que puede reducirse al c\u00e1lculo de una regresi\u00f3n lineal para la funci\u00f3n logit de la probabilidad:</p> <p>$$g(x) = ln(\\dfrac{\\pi(x)}{1- \\pi(x)})=\\beta_0+\\beta_1 x$$</p> <p>El gr\u00e1fico de la funci\u00f3n log\u00edstica se muestra en la figura que encabeza esta secci\u00f3n, la variable independiente es la combinaci\u00f3n lineal $=\\beta_0+\\beta_1$ y la variable dependiente es la probabilidad estimada $ \\pi (x)$. Si se realiza la regresi\u00f3n lineal, la forma de la probabilidad estimada puede ser f\u00e1cilmente recuperada a partir de los coeficientes calculados.</p> <p>Para hacer la regresi\u00f3n deben tomarse los valores $X_i$ de las observaciones ordenados de mayor a menor y formar la siguiente tabla:</p>"},{"location":"lectures/machine_learning/cla_01/#error-de-prediccion","title":"Error de Predicci\u00f3n\u00b6","text":""},{"location":"lectures/machine_learning/cla_01/#matriz-de-confusion","title":"Matriz de confusi\u00f3n\u00b6","text":"<p>Los modelos de clasificacion son ocupadas para predecir valores categ\u00f3ricos, por ejemplo, determinar la especie de una flor basado en el largo (y ancho) de su p\u00e9talo (y s\u00e9palo).Para este caso, es necesario introducir el concepto de matriz de confusi\u00f3n.</p> <p>La matriz de confusi\u00f3n es una herramienta que permite la visualizaci\u00f3n del desempe\u00f1o de un algoritmo Para la clasificaci\u00f3n de dos clases (por ejemplo, 0 y 1), se tiene la siguiente matriz de confusi\u00f3n:</p> <p></p> <p>Ac\u00e1 se define:</p> <ul> <li>TP = Verdadero positivo: el modelo predijo la clase positiva correctamente, para ser una clase positiva.</li> <li>FP = Falso positivo: el modelo predijo la clase negativa incorrectamente, para ser una clase positiva.</li> <li>FN = Falso negativo: el modelo predijo incorrectamente que la clase positiva ser\u00eda la clase negativa.</li> <li>TN = Verdadero negativo: el modelo predijo la clase negativa correctamente, para ser la clase negativa.</li> </ul> <p>En este contexto, los valores TP Y TN muestran los valores correctos que tuve al momento de realizar la predicci\u00f3n, mientras que los valores de de FN Y FP denotan los valores que me equivoque de clase.</p> <p>Los conceptos de FN y FP se pueden interpretar con la siguiente imagen:</p> <p></p>"},{"location":"lectures/machine_learning/cla_01/#metricas-de-error","title":"M\u00e9tricas de Error\u00b6","text":"<p>En este contexto, se busca maximizar el n\u00famero al m\u00e1ximo la suma de los elementos TP Y TN, mientras que se busca disminuir la suma de los elementos de FN y FP. Para esto se definen las siguientes m\u00e9tricas:</p> <ol> <li>Accuracy</li> </ol> <p>$$accuracy(y,\\hat{y}) = \\dfrac{TP+TN}{TP+TN+FP+FN}$$</p> <ol> <li>Recall:</li> </ol> <p>$$recall(y,\\hat{y}) = \\dfrac{TP}{TP+FN}$$</p> <ol> <li>Precision:</li> </ol> <p>$$precision(y,\\hat{y}) = \\dfrac{TP}{TP+FP} $$</p> <ol> <li>F-score:</li> </ol> <p>$$fscore(y,\\hat{y}) = 2\\times \\dfrac{precision(y,\\hat{y})\\times recall(y,\\hat{y})}{precision(y,\\hat{y})+recall(y,\\hat{y})} $$</p>"},{"location":"lectures/machine_learning/cla_01/#curva-aucroc","title":"Curva  AUC\u2013ROC\u00b6","text":"<p>La curva AUC\u2013ROC es una representaci\u00f3n gr\u00e1fica de la sensibilidad frente a la especificidad para un sistema clasificador binario seg\u00fan se var\u00eda el umbral de discriminaci\u00f3n. Otra interpretaci\u00f3n de este gr\u00e1fico es la representaci\u00f3n de la raz\u00f3n o proporci\u00f3n de verdaderos positivos (VPR = Raz\u00f3n de Verdaderos Positivos) frente a la raz\u00f3n o proporci\u00f3n de falsos positivos (FPR = Raz\u00f3n de Falsos Positivos) tambi\u00e9n seg\u00fan se var\u00eda el umbral de discriminaci\u00f3n (valor a partir del cual decidimos que un caso es un positivo). ROC tambi\u00e9n puede significar Relative Operating Characteristic (Caracter\u00edstica Operativa Relativa) porque es una comparaci\u00f3n de dos caracter\u00edsticas operativas (VPR y FPR) seg\u00fan cambiamos el umbral para la decisi\u00f3n.</p> <p>En espa\u00f1ol es preferible mantener el acr\u00f3nimo ingl\u00e9s, aunque es posible encontrar el equivalente espa\u00f1ol COR. No se suele utilizar ROC aislado, debemos decir \u201ccurva ROC\u201d o \u201can\u00e1lisis ROC\u201d.</p> <p></p> <p>El \u00e1rea cubierta por la curva es el \u00e1rea entre la l\u00ednea naranja (ROC) y el eje. Esta \u00e1rea cubierta es AUC. Cuanto m\u00e1s grande sea el \u00e1rea cubierta, mejores ser\u00e1n los modelos de aprendizaje autom\u00e1tico para distinguir las clases dadas. El valor ideal para AUC es 1.</p>"},{"location":"lectures/machine_learning/cla_01/#ejemplo-dataset-iris","title":"Ejemplo: Dataset Iris\u00b6","text":""},{"location":"lectures/machine_learning/cla_01/#otros-modelos-de-clasificacion","title":"Otros modelos de clasificaci\u00f3n\u00b6","text":"<p>Existen varios modelos de clasificaci\u00f3n que podemos ir comparando unos con otros, dentro de los cuales estacamos los siguientes:</p> <ul> <li>Regresi\u00f3n Log\u00edstica</li> <li>Arboles de Decision</li> <li>Random Forest</li> <li>SVM</li> </ul> <p>Nos basaremos en un ejemplo de sklearn que muestra los resultados de aplicar estos cuatro modelos sobre tres conjunto de datos distintos ( make_moons, make_circles, make_classification). Adem\u00e1s, se crea un rutina para comparar los resultados de las distintas m\u00e9tricas.</p>"},{"location":"lectures/machine_learning/cla_01/#graficos","title":"Gr\u00e1ficos\u00b6","text":"<p>Similar al gr\u00e1fico aplicado al conjunto de datos Iris, aca se realiza el mismo ejercicio pero para tres conjunto de datos sobre los distintos modelos.</p>"},{"location":"lectures/machine_learning/cla_01/#metricas","title":"M\u00e9tricas\u00b6","text":"<p>Dado que el sistema de calcular m\u00e9tricas sigue el mismo formato, solo cambiando el conjunto de datos y el modelo, se decide realizar una clase que automatice este proceso.</p>"},{"location":"lectures/machine_learning/cla_01/#referencias","title":"Referencias\u00b6","text":"<ol> <li>Supervised learning</li> </ol>"},{"location":"lectures/machine_learning/ml_intro/","title":"Introducci\u00f3n","text":""},{"location":"lectures/machine_learning/ml_intro/#introduccion","title":"Introducci\u00f3n\u00b6","text":""},{"location":"lectures/machine_learning/ml_intro/#que-es-el-machine-learning","title":"\u00bfQu\u00e9 es el Machine learning?\u00b6","text":"<p>Podemos resumir Machine Learning en cuatro puntos:</p> <ul> <li><p>Estudia y construye sistemas que pueden aprender de los datos, m\u00e1s que seguir instrucciones expl\u00edcitamente programadas.</p> </li> <li><p>Conjunto de t\u00e9cnicas y modelos que permiten el modelamiento predictivo de datos, reunidas a partir de la intersecci\u00f3n de elementos de probabilidad, estad\u00edstica e inteligencia artificial.</p> </li> <li><p>T\u00edpicamente, alguien que trabaja en Machine Learning est\u00e1 en la Academia y busca realizar investigaci\u00f3n y publicar art\u00edculos.</p> </li> <li><p>Pregunta fundamental: \u00bfQu\u00e9 conocimiento emerge a partir de los datos? \u00bfQu\u00e9 modelo/t\u00e9cnica otorga la mejor predicci\u00f3n para estos datos?</p> </li> </ul> <p>Para poder avanzar en el estudio de machine learning es de vital importancia definir el concepto de modelo.</p>"},{"location":"lectures/machine_learning/ml_intro/#que-se-entiende-por-modelo","title":"\u00bfQu\u00e9 se entiende por modelo?\u00b6","text":"<p>Un modelo se entinde como:</p> <ul> <li>Una representaci\u00f3n abstracta y conveniente de un sistema.</li> <li>Una simplificaci\u00f3n del mundo real.</li> <li>Un medio de exploraci\u00f3n y de explicaci\u00f3n para nuestro entendimiento de la realidad.</li> </ul> <p>Por otro lado, un modelo no es:</p> <ul> <li>Igual al mundo real.</li> <li>Un sustituto para mediciones o experimentos.</li> </ul> <p>Los modelos nos permiten:</p> <ul> <li><p>reproducir experimentos donde factores no pueden ser f\u00e1cilmente controlados.</p> <ul> <li>Ejemplo: Comportamiento de c\u00e1psula lunar en gravedad cero y al atravesar atm\u00f3sfera a gran velocidad.</li> </ul> </li> <li><p>simplificar el entendimiento de sistemas complejos, al permitir el an\u00e1lisis y exploraci\u00f3n de cada componente del sistema por separado.</p> <ul> <li>Ejemplo: Adelgazamiento de la capa de hielo polar por calentamiento global.</li> </ul> </li> </ul>"},{"location":"lectures/machine_learning/ml_intro/#que-tipos-de-problemas-podemos-abordar-con-machine-learning","title":"\u00bf Qu\u00e9 tipos de problemas podemos abordar con machine learning?\u00b6","text":"<p>Los problemas que se pueden resolver con machine learning se pueden englobar en tres tipos: aprendeizaje supervisado, aprendizaje no supervisado y aprendizaje reforzado.</p> <p></p>"},{"location":"lectures/machine_learning/ml_intro/#aprendizaje-supervisado","title":"Aprendizaje supervisado\u00b6","text":"<ul> <li>El sistema aprende en base a datos estructurados o no estructurados.</li> <li>Clasificados previamente (se conoce la respuesta).</li> <li>El algoritmo produce una funci\u00f3n que establece una correspondencia entre las entradas y las salidas deseadas del sistema.</li> </ul>"},{"location":"lectures/machine_learning/ml_intro/#aprendizaje-no-supervisado","title":"Aprendizaje no supervisado\u00b6","text":"<ul> <li>Modelo se construye usando un conjunto de datos como entrada, los cuales no han sido clasificados previamente.</li> <li>El sistema tiene que ser capaz de reconocer patrones para poder etiquetar las nuevas entradas.</li> </ul>"},{"location":"lectures/machine_learning/ml_intro/#aprendizaje-por-refuerzo","title":"Aprendizaje por refuerzo\u00b6","text":"<p>Aprendizaje por refuerzo o Aprendizaje reforzado es un \u00e1rea del aprendizaje autom\u00e1tico inspirada en la psicolog\u00eda conductista, cuya ocupaci\u00f3n es determinar qu\u00e9 acciones debe escoger un agente de software en un entorno dado con el fin de maximizar alguna noci\u00f3n de \"recompensa\" o premio acumulado.</p> <p></p>"},{"location":"lectures/machine_learning/ml_intro/#algoritmos-mas-utilizados","title":"Algoritmos m\u00e1s utilizados\u00b6","text":"<p>Los algoritmos que m\u00e1s se suelen utilizar en los problemas de Machine Learning son los siguientes:</p> <ol> <li>Regresi\u00f3n Lineal</li> <li>Regresi\u00f3n Log\u00edstica</li> <li>Arboles de Decision</li> <li>Random Forest</li> <li>SVM</li> <li>KNN</li> <li>K-means</li> </ol> <p>\u00bf Qu\u00e9 se necesita para aprender machine learning?</p> <p>Se necesita tener conocimientos de los siguientes t\u00f3picos.</p> <ul> <li>Algebra Lineal</li> <li>Probabilidad y estad\u00edstica</li> <li>Optimizaci\u00f3n</li> </ul>"},{"location":"lectures/machine_learning/ml_intro/#librerias-de-machine-learning-en-python","title":"Librer\u00edas de machine learning en python\u00b6","text":"<p>Una de las grandes ventajas que ofrece python  sobre otros lenguajes de programaci\u00f3n; es lo grande y prolifera que es la comunidad de desarrolladores que lo rodean; comunidad que ha contribuido con una gran variedad de librer\u00edas de primer nivel que extienden la funcionalidades del lenguaje. Para el caso de Machine Learning , las principales librer\u00edas que podemos utilizar son:</p>"},{"location":"lectures/machine_learning/ml_intro/#scikit-learn","title":"Scikit-Learn\u00b6","text":"<p>Scikit-learn es la principal librer\u00eda que existe para trabajar con Machine Learning, incluye la implementaci\u00f3n de un gran n\u00famero de algoritmos  de aprendizaje. La podemos utilizar para clasificaciones, extraccion de caracter\u00edsticas, regresiones, agrupaciones, reducci\u00f3n de dimensiones, selecci\u00f3n de modelos, o preprocesamiento.</p>"},{"location":"lectures/machine_learning/ml_intro/#statsmodels","title":"Statsmodels\u00b6","text":"<p>Statsmodels es otra gran librer\u00eda que hace foco en modelos estad\u00edsticos y se utiliza principalmente para an\u00e1lisis predictivos y exploratorios.  Las pruebas estad\u00edsticas que ofrece son bastante amplias y abarcan tareas de validaci\u00f3n para la mayor\u00eda de los casos.</p>"},{"location":"lectures/machine_learning/ml_intro/#conceptos-claves-en-machine-learning","title":"Conceptos claves en machine learning\u00b6","text":""},{"location":"lectures/machine_learning/ml_intro/#esquema-machine-learning","title":"Esquema machine learning\u00b6","text":"<p>EL proceso de machine learning se puede resumir a grandes rasgo por el siguiente esquema.</p> <p></p> <ol> <li><p>Recolectar los datos. Podemos recolectar los datos desde muchas fuentes, podemos por ejemplo extraer los datos de un sitio web o obtener los datos utilizando una API o desde una base de datos.</p> </li> <li><p>Preprocesar los datos. Es pr\u00e1cticamente inevitable tener que realizar varias tareas de preprocesamiento antes de poder utilizar los datos. Igualmente este punto suele ser mucho m\u00e1s sencillo que el paso anterior.</p> </li> <li><p>Explorar los datos. Una vez que ya tenemos los datos y est\u00e1n con el formato correcto, podemos realizar un pre an\u00e1lisis para corregir los casos de valores faltantes o intentar encontrar a simple vista alg\u00fan patr\u00f3n en los mismos que nos facilite la construcci\u00f3n del modelo.</p> </li> <li><p>Entrenar el modelo. En esta etapa se entrenan los modelos con los datos que venimos procesando en las etapas anteriores. La idea es que los modelos puedan extraer informaci\u00f3n \u00fatil de los datos que le pasamos para luego poder hacer predicciones.</p> </li> <li><p>Evaluar el modelo. Evaluamos que tan preciso es el modelo en sus predicciones y si no estamos muy conforme con su rendimiento, podemos volver a la etapa anterior y continuar entrenando el modelo cambiando algunos par\u00e1metros hasta lograr un rendimiento aceptable.</p> </li> <li><p>Utilizar el modelo. En esta ultima etapa, ya ponemos a nuestro modelo a enfrentarse al problema real. Aqu\u00ed tambi\u00e9n podemos medir su rendimiento, lo que tal vez nos obligue a revisar todos los pasos anteriores.</p> </li> </ol> <p>Los pasos 1,2,3 son los pasos que ya se han visto con detalle en este curso. Por otro lado, la etapa de modelamiento (entrenar, evaluar y predecir) ser\u00e1 necesario introducir nuevos conceptos.</p>"},{"location":"lectures/machine_learning/ml_intro/#referencias","title":"Referencias\u00b6","text":"<ol> <li>Basic Concepts in Machine Learning</li> <li>An Introduction to Machine Learning Theory and Its Applications: A Visual Tutorial with Examples</li> </ol>"},{"location":"lectures/machine_learning/ns_01/","title":"No supervisado I","text":"<p>Consid\u00e9rense  $\ud835\udc36_1 ,...,  \ud835\udc36_k$  como los sets formados por los \u00edndices de las observaciones de cada uno de los clusters. Por ejemplo, el set  $\ud835\udc36_1$  contiene los \u00edndices de las observaciones agrupadas en el cluster 1. La nomenclatura empleada para indicar que la observaci\u00f3n  $i$ pertenece al cluster  $k$  es:  $i \\in C_k$ . Todos los sets satisfacen dos propiedades:</p> <ul> <li><p>$C_1 \\cup C_2 \\cup ... \\cup C_k = {1,...,n} $ . Significa que toda observaci\u00f3n pertenece a uno de los $k$ clusters.</p> </li> <li><p>$C_i \\cap C_{j} = \\emptyset $   para todo $i \\neq j$   . Implica que los clusters no solapan, ninguna observaci\u00f3n pertenece a m\u00e1s de un cluster a la vez.</p> </li> </ul> <p>El algoritmo consiste en reducir al m\u00ednimo la suma de las distancias cuadradas desde la media dentro del agrupamiento. Matem\u00e1ticamente: \\begin{align*} (P) \\ \\textrm{Minimizar } f(C_l,\\mu_l) = \\sum_{l=1}^k \\sum_{x_n \\in C_l} ||x_n - \\mu_l ||^2 \\textrm{, respecto a } C_l, \\mu_l, \\end{align*} donde $C_l$ es el cluster l-\u00e9simo y $\\mu_l$ es el centroide l-\u00e9simo.</p> <p></p> In\u00a0[1]: Copied! <pre># librerias\n \nimport os\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nfrom sklearn.datasets import make_blobs\n\npd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes\n\n# Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab\n%matplotlib inline\n</pre> # librerias   import os import numpy as np import pandas as pd  import matplotlib.pyplot as plt import seaborn as sns   from sklearn.datasets import make_blobs  pd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes  # Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab %matplotlib inline In\u00a0[2]: Copied! <pre>def init_blobs(N, k, seed=42):\n    X, y = make_blobs(n_samples=N, centers=k,\n                      random_state=seed, cluster_std=0.60)\n    return X\n\n# generar datos\ndata = init_blobs(10000, 6, seed=43)\ndf = pd.DataFrame(data, columns=[\"x\", \"y\"])\n\n\n\ndf.head()\n</pre> def init_blobs(N, k, seed=42):     X, y = make_blobs(n_samples=N, centers=k,                       random_state=seed, cluster_std=0.60)     return X  # generar datos data = init_blobs(10000, 6, seed=43) df = pd.DataFrame(data, columns=[\"x\", \"y\"])    df.head() Out[2]: x y 0 -6.953617 -4.989933 1 -2.681117 7.583914 2 -1.510161 4.933676 3 -9.748491 5.479457 4 -7.438017 -4.597754 <p>Debido a que trabajamos con el concepto de distancia, muchas veces las columnas del dataframe pueden estar en distintas escalas, lo cual puede complicar a los algoritmos ocupados (al menos con sklearn).</p> <p>En estos casos, se suele normalizar los atributos, es decir, dejar los valores en una escala acotada y/o con estimadores fijos. Por ejemplo, en ***sklearn** podemos encontrar las siguientes formas de normalizar:</p> <ul> <li>StandardScaler: se normaliza  restando la media y escalando por su desviaci\u00f3n estanda. $$x_{prep} = \\dfrac{x-u}{s}$$</li> </ul> <p>La ventaja es que la media del nuevo conjunto de datos cumple con la propiedad que su media $\\mu$ es igual a cero y su desviaci\u00f3n estandar $s$ es igual a 1.</p> <ul> <li>MinMaxScaler:  se normaliza ocupando los valores de los m\u00ednimos y m\u00e1ximo del conjunto de datos. $$x_{prep} = \\dfrac{x-x_{min}}{x_{min}-x_{max}}$$</li> </ul> <p>Esta forma de normalizar resulta \u00fatil cuando la desviaci\u00f3n estandar $s$ es muy peque\u00f1a (cercana) a cero, por lo que lo convierte en un estimador m\u00e1s roubusto que el StandardScaler.</p> In\u00a0[3]: Copied! <pre>from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ncolumns = ['x', 'y']\ndf[columns] = scaler.fit_transform(df[columns])\n\ndf.head()\n</pre> from sklearn.preprocessing import StandardScaler  scaler = StandardScaler() columns = ['x', 'y'] df[columns] = scaler.fit_transform(df[columns])  df.head() Out[3]: x y 0 -0.579033 -1.831435 1 0.408821 1.194578 2 0.679560 0.556774 3 -1.225241 0.688121 4 -0.691032 -1.737053 In\u00a0[4]: Copied! <pre># comprobar resultados del estimador\ndf.describe()\n</pre> # comprobar resultados del estimador df.describe() Out[4]: x y count 1.000000e+04 1.000000e+04 mean 2.060574e-16 -2.285105e-15 std 1.000050e+00 1.000050e+00 min -1.638247e+00 -2.410317e+00 25% -8.015576e-01 -4.418042e-01 50% -2.089351e-01 1.863259e-01 75% 5.480066e-01 8.159808e-01 max 2.243358e+00 1.639547e+00 <p>Con esta parametrizaci\u00f3n procedemos a graficar nuestros resultados:</p> In\u00a0[5]: Copied! <pre># graficar \nsns.set(rc={'figure.figsize':(11.7,8.27)})\nax = sns.scatterplot( data=df,x=\"x\", y=\"y\")\n</pre> # graficar  sns.set(rc={'figure.figsize':(11.7,8.27)}) ax = sns.scatterplot( data=df,x=\"x\", y=\"y\") <p>Ahora ajustamos el algoritmo KMeans de sklearn. Primero, comprendamos los hiperpar\u00e1metros m\u00e1s importantes:</p> <ul> <li>n_clusters: El n\u00famero de clusters a crear, o sea K. Por defecto es 8</li> <li>init: M\u00e9todo de inicializaci\u00f3n. Un problema que tiene el algoritmo K-Medias es que la solucci\u00f3n alcanzada varia seg\u00fan la inicializaci\u00f3n de los centroides. <code>sklearn</code> empieza usando el m\u00e9todo <code>kmeans++</code> que es una versi\u00f3n m\u00e1s moderna y que proporciona mejores resultados que la inicializaci\u00f3n aleatoria (random)</li> <li>n_init: El n\u00famero de inicializaciones a probar. B\u00e1sicamente <code>KMeans</code> aplica el algoritmo <code>n_init</code> veces y elige los clusters que minimizan la inercia.</li> <li>max_iter: M\u00e1ximo n\u00famero de iteraciones para llegar al criterio de parada.</li> <li>random_state: semilla para garantizar la reproducibilidad de los resultados.</li> <li>tol: Tolerancia para declarar criterio de parada (cuanto m\u00e1s grande, antes parar\u00e1 el algoritmo).</li> </ul> In\u00a0[6]: Copied! <pre># ajustar modelo: k-means\n\nfrom sklearn.cluster import KMeans\n\nX = np.array(df)\nkmeans = KMeans(n_clusters=6,n_init=25, random_state=123)\nkmeans.fit(X)\n\n\ncentroids = kmeans.cluster_centers_ # centros \nclusters = kmeans.labels_ # clusters\n</pre> # ajustar modelo: k-means  from sklearn.cluster import KMeans  X = np.array(df) kmeans = KMeans(n_clusters=6,n_init=25, random_state=123) kmeans.fit(X)   centroids = kmeans.cluster_centers_ # centros  clusters = kmeans.labels_ # clusters In\u00a0[7]: Copied! <pre># etiquetar los datos con los clusters encontrados\ndf[\"cluster\"] = clusters\ndf[\"cluster\"] = df[\"cluster\"].astype('category')\ncentroids_df = pd.DataFrame(centroids, columns=[\"x\", \"y\"])\ncentroids_df[\"cluster\"] = [1,2,3,4,5,6]\n</pre> # etiquetar los datos con los clusters encontrados df[\"cluster\"] = clusters df[\"cluster\"] = df[\"cluster\"].astype('category') centroids_df = pd.DataFrame(centroids, columns=[\"x\", \"y\"]) centroids_df[\"cluster\"] = [1,2,3,4,5,6] In\u00a0[8]: Copied! <pre># graficar los datos etiquetados con k-means\nfig, ax = plt.subplots(figsize=(11, 8.5))\n\nsns.scatterplot( data=df,\n                     x=\"x\",\n                     y=\"y\",\n                     hue=\"cluster\",\n                     legend='full',\n                     palette=\"Set2\")\n\nsns.scatterplot(x=\"x\", y=\"y\",\n                     s=100, color=\"black\", marker=\"x\",\n                     data=centroids_df)\nplt.show()\n</pre> # graficar los datos etiquetados con k-means fig, ax = plt.subplots(figsize=(11, 8.5))  sns.scatterplot( data=df,                      x=\"x\",                      y=\"y\",                      hue=\"cluster\",                      legend='full',                      palette=\"Set2\")  sns.scatterplot(x=\"x\", y=\"y\",                      s=100, color=\"black\", marker=\"x\",                      data=centroids_df) plt.show() <p>Ahora la pregunta que surge de manera natural es ... \u00bf c\u00f3mo escoger el mejor n\u00famero de clusters?.</p> <p>No existe un criterio objetivo ni ampliamente v\u00e1lido para la  elecci\u00f3n de un n\u00famero \u00f3ptimo de clusters. Aunque no exista un criterio objetivo para la selecci\u00f3n del n\u00famero de clusters, si que se han implementado diferentes m\u00e9todos que nos ayudan a elegir un n\u00famero apropiado de clusters para agrupar los datos; como son,</p> <ul> <li>m\u00e9todo del codo (elbow method)</li> <li>criterio de Calinsky</li> <li>Affinity Propagation (AP)</li> <li>Gap (tambi\u00e9n con su versi\u00f3n estad\u00edstica)</li> <li>Dendrogramas</li> <li>etc.</li> </ul> In\u00a0[9]: Copied! <pre># implementaci\u00f3n de la regla del codo\nNc = range(1, 15)\nkmeans = [KMeans(n_clusters=i, n_init=10) for i in Nc]  # Suppressing the warning here\nscore = [kmeans[i].fit(df).inertia_ for i in range(len(kmeans))]\n\ndf_Elbow = pd.DataFrame({'Number of Clusters': Nc, 'Score': score})\n\ndf_Elbow.head()\n</pre> # implementaci\u00f3n de la regla del codo Nc = range(1, 15) kmeans = [KMeans(n_clusters=i, n_init=10) for i in Nc]  # Suppressing the warning here score = [kmeans[i].fit(df).inertia_ for i in range(len(kmeans))]  df_Elbow = pd.DataFrame({'Number of Clusters': Nc, 'Score': score})  df_Elbow.head() Out[9]: Number of Clusters Score 0 1 49054.876400 1 2 23034.738275 2 3 11952.113532 3 4 6562.349711 4 5 1665.378832 In\u00a0[10]: Copied! <pre># graficar los datos etiquetados con k-means\nfig, ax = plt.subplots(figsize=(11, 8.5))\nplt.title('Elbow Curve')\nsns.lineplot(x=\"Number of Clusters\",\n             y=\"Score\",\n            data=df_Elbow)\nsns.scatterplot(x=\"Number of Clusters\",\n             y=\"Score\",\n             data=df_Elbow)\nplt.show()\n</pre> # graficar los datos etiquetados con k-means fig, ax = plt.subplots(figsize=(11, 8.5)) plt.title('Elbow Curve') sns.lineplot(x=\"Number of Clusters\",              y=\"Score\",             data=df_Elbow) sns.scatterplot(x=\"Number of Clusters\",              y=\"Score\",              data=df_Elbow) plt.show() <p>A partir de 4 clusters la reducci\u00f3n en la suma total de cuadrados internos parece estabilizarse, indicando que $k$ = 4 es una buena opci\u00f3n.</p> <p>En la base del dendrograma, cada observaci\u00f3n forma una terminaci\u00f3n individual conocida como hoja o leaf del \u00e1rbol. A medida que se asciende por la estructura, pares de hojas se fusionan formando las primeras ramas. Estas uniones se corresponden con los pares de observaciones m\u00e1s similares. Tambi\u00e9n ocurre que las ramas se fusionan con otras ramas o con hojas. Cuanto m\u00e1s temprana (m\u00e1s pr\u00f3xima a la base del dendrograma) ocurre una fusi\u00f3n, mayor es la similitud.</p> <p>Para cualquier par de observaciones, se puede identificar el punto del \u00e1rbol en el que las ramas que contienen dichas observaciones se fusionan. La altura a la que esto ocurre (eje vertical) indica c\u00f3mo de similares/diferentes son las dos observaciones. Los dendrogramas, por lo tanto, se deben interpretar \u00fanicamente en base al eje vertical y no por las posiciones que ocupan las observaciones en el eje horizontal, esto \u00faltimo es simplemente por est\u00e9tica y puede variar de un programa a otro.</p> <p>Por ejemplo, la observaci\u00f3n 8 es la m\u00e1s similar a la 10 ya que es la primera fusi\u00f3n que recibe la observaci\u00f3n 10 (y viceversa). Podr\u00eda resultar tentador decir que la observaci\u00f3n 14, situada inmediatamente a la derecha de la 10, es la siguiente m\u00e1s similar, sin embargo, las observaciones 28 y 44 son m\u00e1s similares a la 10 a pesar de que se encuentran m\u00e1s alejadas en el eje horizontal. Del mismo modo, no es correcto decir que la observaci\u00f3n 14 es m\u00e1s similar a la observaci\u00f3n 10 de lo que lo es la 36 por el hecho de que est\u00e1 m\u00e1s pr\u00f3xima en el eje horizontal. Prestando atenci\u00f3n a la altura en que las respectivas ramas se unen, la \u00fanica conclusi\u00f3n v\u00e1lida es que la similitud entre los pares 10-14 y 10-36 es la misma.</p> <p>Cortar el dendograma para generar los clusters</p> <p>Adem\u00e1s de representar en un dendrograma la similitud entre observaciones, se tiene que identificar el n\u00famero de clusters creados y qu\u00e9 observaciones forman parte de cada uno. Si se realiza un corte horizontal a una determinada altura del dendrograma, el n\u00famero de ramas que sobrepasan (en sentido ascendente) dicho corte se corresponde con el n\u00famero de clusters. La siguiente imagen muestra dos veces el mismo dendrograma. Si se realiza el corte a la altura de 5, se obtienen dos clusters, mientras que si se hace a la de 3.5 se obtienen 4. La altura de corte tiene por lo tanto la misma funci\u00f3n que el valor K en K-means-clustering: controla el n\u00famero de clusters obtenidos.</p> <p></p> <p></p> <p>Dos propiedades adicionales se derivan de la forma en que se generan los clusters en el m\u00e9todo de hierarchical clustering:</p> <ul> <li><p>Dada la longitud variable de las ramas, siempre existe un intervalo de altura para el que cualquier corte da lugar al mismo n\u00famero de clusters. En el ejemplo anterior, todos los cortes entre las alturas 5 y 6 tienen como resultado los mismos 2 clusters.</p> </li> <li><p>Con un solo dendrograma se dispone de la flexibilidad para generar cualquier n\u00famero de clusters desde 1 a n. La selecci\u00f3n del n\u00famero \u00f3ptimo puede valorarse de forma visual, tratando de identificar las ramas principales en base a la altura a la que ocurren las uniones. En el ejemplo expuesto es razonable elegir entre 2 o 4 clusters.</p> </li> </ul> In\u00a0[11]: Copied! <pre># Tratamiento de datos\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_blobs\n\n# Gr\u00e1ficos\n# ==============================================================================\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('ggplot') or plt.style.use('ggplot')\n\n# Preprocesado y modelado\n# ==============================================================================\nfrom sklearn.cluster import AgglomerativeClustering\nfrom scipy.cluster.hierarchy import dendrogram\nfrom sklearn.preprocessing import scale\nfrom sklearn.metrics import silhouette_score\n\n# Configuraci\u00f3n warnings\n# ==============================================================================\nimport warnings\nwarnings.filterwarnings('ignore')\n</pre> # Tratamiento de datos # ============================================================================== import numpy as np import pandas as pd from sklearn.datasets import make_blobs  # Gr\u00e1ficos # ============================================================================== import matplotlib.pyplot as plt from matplotlib import style style.use('ggplot') or plt.style.use('ggplot')  # Preprocesado y modelado # ============================================================================== from sklearn.cluster import AgglomerativeClustering from scipy.cluster.hierarchy import dendrogram from sklearn.preprocessing import scale from sklearn.metrics import silhouette_score  # Configuraci\u00f3n warnings # ============================================================================== import warnings warnings.filterwarnings('ignore') In\u00a0[12]: Copied! <pre># generar datos\nX, y = make_blobs(\n        n_samples    = 200, \n        n_features   = 2, \n        centers      = 4, \n        cluster_std  = 0.60, \n        shuffle      = True, \n        random_state = 0\n       )\n\n\ndf = pd.DataFrame({\n    'x':X[:,0],\n    'y':X[:,1]\n})\n\n\n# Escalado de datos\nscaler = StandardScaler()\ncolumns = ['x', 'y']\ndf[columns] = scaler.fit_transform(df[columns])\n\ndf.head()\n</pre> # generar datos X, y = make_blobs(         n_samples    = 200,          n_features   = 2,          centers      = 4,          cluster_std  = 0.60,          shuffle      = True,          random_state = 0        )   df = pd.DataFrame({     'x':X[:,0],     'y':X[:,1] })   # Escalado de datos scaler = StandardScaler() columns = ['x', 'y'] df[columns] = scaler.fit_transform(df[columns])  df.head() Out[12]: x y 0 1.348818 -0.908114 1 -0.638621 -0.534950 2 0.653079 0.027910 3 -1.573023 1.276049 4 0.970706 -1.418431 In\u00a0[13]: Copied! <pre># graficar \nsns.set(rc={'figure.figsize':(11.7,8.27)})\nax = sns.scatterplot( data=df,x=\"x\", y=\"y\")\n</pre> # graficar  sns.set(rc={'figure.figsize':(11.7,8.27)}) ax = sns.scatterplot( data=df,x=\"x\", y=\"y\") In\u00a0[14]: Copied! <pre># Modelos\n\nX = np.array(df[['x','y']])\n</pre> # Modelos  X = np.array(df[['x','y']]) In\u00a0[15]: Copied! <pre># primer modelo\nmodelo_hclust_complete = AgglomerativeClustering(\n                            affinity = 'euclidean',\n                            linkage  = 'complete',\n                            distance_threshold = 0,\n                            n_clusters         = None\n                        )\nmodelo_hclust_complete.fit(X)\n</pre> # primer modelo modelo_hclust_complete = AgglomerativeClustering(                             affinity = 'euclidean',                             linkage  = 'complete',                             distance_threshold = 0,                             n_clusters         = None                         ) modelo_hclust_complete.fit(X) Out[15]: <pre>AgglomerativeClustering(affinity='euclidean', distance_threshold=0,\n                        linkage='complete', n_clusters=None)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.AgglomerativeClustering<pre>AgglomerativeClustering(affinity='euclidean', distance_threshold=0,\n                        linkage='complete', n_clusters=None)</pre> In\u00a0[16]: Copied! <pre># segundo modelo\nmodelo_hclust_average = AgglomerativeClustering(\n                            affinity = 'euclidean',\n                            linkage  = 'average',\n                            distance_threshold = 0,\n                            n_clusters         = None\n                        )\nmodelo_hclust_average.fit(X)\n</pre> # segundo modelo modelo_hclust_average = AgglomerativeClustering(                             affinity = 'euclidean',                             linkage  = 'average',                             distance_threshold = 0,                             n_clusters         = None                         ) modelo_hclust_average.fit(X) Out[16]: <pre>AgglomerativeClustering(affinity='euclidean', distance_threshold=0,\n                        linkage='average', n_clusters=None)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.AgglomerativeClustering<pre>AgglomerativeClustering(affinity='euclidean', distance_threshold=0,\n                        linkage='average', n_clusters=None)</pre> In\u00a0[17]: Copied! <pre># tercer modelo\nmodelo_hclust_ward = AgglomerativeClustering(\n                            affinity = 'euclidean',\n                            linkage  = 'ward',\n                            distance_threshold = 0,\n                            n_clusters         = None\n                     )\nmodelo_hclust_ward.fit(X)\n</pre> # tercer modelo modelo_hclust_ward = AgglomerativeClustering(                             affinity = 'euclidean',                             linkage  = 'ward',                             distance_threshold = 0,                             n_clusters         = None                      ) modelo_hclust_ward.fit(X) Out[17]: <pre>AgglomerativeClustering(affinity='euclidean', distance_threshold=0,\n                        n_clusters=None)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.AgglomerativeClustering<pre>AgglomerativeClustering(affinity='euclidean', distance_threshold=0,\n                        n_clusters=None)</pre> In\u00a0[18]: Copied! <pre>def plot_dendrogram(model, **kwargs):\n    '''\n    Esta funci\u00f3n extrae la informaci\u00f3n de un modelo AgglomerativeClustering\n    y representa su dendograma con la funci\u00f3n dendogram de scipy.cluster.hierarchy\n    '''\n    \n    counts = np.zeros(model.children_.shape[0])\n    n_samples = len(model.labels_)\n    for i, merge in enumerate(model.children_):\n        current_count = 0\n        for child_idx in merge:\n            if child_idx &lt; n_samples:\n                current_count += 1  # leaf node\n            else:\n                current_count += counts[child_idx - n_samples]\n        counts[i] = current_count\n\n    linkage_matrix = np.column_stack([model.children_, model.distances_,\n                                      counts]).astype(float)\n\n    # Plot\n    dendrogram(linkage_matrix, **kwargs)\n</pre> def plot_dendrogram(model, **kwargs):     '''     Esta funci\u00f3n extrae la informaci\u00f3n de un modelo AgglomerativeClustering     y representa su dendograma con la funci\u00f3n dendogram de scipy.cluster.hierarchy     '''          counts = np.zeros(model.children_.shape[0])     n_samples = len(model.labels_)     for i, merge in enumerate(model.children_):         current_count = 0         for child_idx in merge:             if child_idx &lt; n_samples:                 current_count += 1  # leaf node             else:                 current_count += counts[child_idx - n_samples]         counts[i] = current_count      linkage_matrix = np.column_stack([model.children_, model.distances_,                                       counts]).astype(float)      # Plot     dendrogram(linkage_matrix, **kwargs) In\u00a0[19]: Copied! <pre># Dendrogramas\nplt.figure(figsize=(20,10)) \nplot_dendrogram(modelo_hclust_average, color_threshold=0)\nplt.title(\"Distancia eucl\u00eddea, Linkage average\")\nplt.show()\n</pre> # Dendrogramas plt.figure(figsize=(20,10))  plot_dendrogram(modelo_hclust_average, color_threshold=0) plt.title(\"Distancia eucl\u00eddea, Linkage average\") plt.show() In\u00a0[20]: Copied! <pre># Dendrogramas\nplt.figure(figsize=(20,10)) \nplot_dendrogram(modelo_hclust_complete, color_threshold=0)\nplt.title(\"Distancia eucl\u00eddea, Linkage complete\")\nplt.show()\n</pre> # Dendrogramas plt.figure(figsize=(20,10))  plot_dendrogram(modelo_hclust_complete, color_threshold=0) plt.title(\"Distancia eucl\u00eddea, Linkage complete\") plt.show() In\u00a0[21]: Copied! <pre># Dendrogramas\nplt.figure(figsize=(20,10)) \nplot_dendrogram(modelo_hclust_ward, color_threshold=0)\nplt.title(\"Distancia eucl\u00eddea, Linkage ward\")\nplt.show()\n</pre> # Dendrogramas plt.figure(figsize=(20,10))  plot_dendrogram(modelo_hclust_ward, color_threshold=0) plt.title(\"Distancia eucl\u00eddea, Linkage ward\") plt.show() <p>En este caso, los tres tipos de linkage identifican claramente 4 clusters, si bien esto no significa que en los 3 dendrogramas los clusters est\u00e9n formados por exactamente las mismas observaciones.</p> <p>N\u00famero de clusters</p> <p>Una forma de identificar el n\u00famero de clusters, es inspeccionar visualmente el dendograma y decidir a qu\u00e9 altura se corta para generar los clusters. Por ejemplo, para los resultados generados mediante distancia eucl\u00eddea y linkage ward, parece sensato cortar el dendograma a una altura de entre 5 y 10, de forma que se creen 4 clusters.</p> In\u00a0[22]: Copied! <pre>plt.figure(figsize=(20,10)) \naltura_corte = 6\nplot_dendrogram(modelo_hclust_ward, color_threshold=altura_corte)\nplt.title(\"Distancia eucl\u00eddea, Linkage ward\")\nplt.axhline(y=altura_corte, c = 'black', linestyle='--', label='altura corte')\nplt.legend()\nplt.show()\n</pre> plt.figure(figsize=(20,10))  altura_corte = 6 plot_dendrogram(modelo_hclust_ward, color_threshold=altura_corte) plt.title(\"Distancia eucl\u00eddea, Linkage ward\") plt.axhline(y=altura_corte, c = 'black', linestyle='--', label='altura corte') plt.legend() plt.show() <p>Una vez identificado el n\u00famero \u00f3ptimo de clusters, se reentrena el modelo indicando este valor.</p> <p>El cerebro humano identifica f\u00e1cilmente 5 agrupaciones y algunas observaciones aisladas (ruido). V\u00e9anse ahora los clusters que se obtienen si se aplica, por ejemplo, K-means clustering.</p> <p></p> <p>Los clusters generados distan mucho de representar las verdaderas agrupaciones. Esto es as\u00ed porque los m\u00e9todos de partitioning clustering como k-means, hierarchical, k-medoids, ... son buenos encontrando agrupaciones con forma esf\u00e9rica o convexa que no contengan un exceso de outliers o ruido, pero fallan al tratar de identificar formas arbitrarias. De ah\u00ed que el \u00fanico cluster que se corresponde con un grupo real sea el amarillo.</p> <p>DBSCAN evita este problema siguiendo la idea de que, para que una observaci\u00f3n forme parte de un cluster, tiene que haber un m\u00ednimo de observaciones vecinas dentro de un radio de proximidad y de que los clusters est\u00e1n separados por regiones vac\u00edas o con pocas observaciones.</p> <p>El algoritmo DBSCAN necesita dos par\u00e1metros:</p> <ul> <li>Epsilon  ($\\epsilon$) : radio que define la regi\u00f3n vecina a una observaci\u00f3n, tambi\u00e9n llamada  \ud835\udf16 -neighborhood.</li> <li>Minimum points ($min_samples$): n\u00famero m\u00ednimo de observaciones dentro de la regi\u00f3n epsilon.</li> </ul> <p>Empleando estos dos par\u00e1metros, cada observaci\u00f3n del set de datos se puede clasificar en una de las siguientes tres categor\u00edas:</p> <ul> <li><p>Core point: observaci\u00f3n que tiene en su  \ud835\udf16 -neighborhood un n\u00famero de observaciones vecinas igual o mayor a min_samples.</p> </li> <li><p>Border point: observaci\u00f3n no satisface el m\u00ednimo de observaciones vecinas para ser core point pero que pertenece al  $\\epsilon$-neighborhood de otra observaci\u00f3n que s\u00ed es core point.</p> </li> <li><p>Noise-outlier: observaci\u00f3n que no es core point ni border point.</p> </li> </ul> <p>Por \u00faltimo, empleando las tres categor\u00edas anteriores se pueden definir tres niveles de conectividad entre observaciones:</p> <ul> <li><p>Directamente alcanzable (direct density reachable): una observaci\u00f3n  $A$  es directamente alcanzable desde otra observaci\u00f3n  $B$  si  $A$  forma parte del  $\\epsilon$ -neighborhood de  $B$  y  $B$  es un core point. Por definici\u00f3n, las observaciones solo pueden ser directamente alcanzables desde un core point.</p> </li> <li><p>Alcanzable (density reachable): una observaci\u00f3n  $A$  es alcanzable desde otra observaci\u00f3n  $\ud835\udc35$  si existe una secuencia de core points que van desde  $B$  a  $A$ .</p> </li> <li><p>Densamente conectadas (density conected): dos observaciones $A$  y  $B$  est\u00e1n densamente conectadas si existe una observaci\u00f3n core point  $C$  tal que  $A$  y  $B$  son alcanzables desde  $C$ .</p> </li> </ul> <p>La siguiente imagen muestra las conexiones existentes entre un conjunto de observaciones si se emplea  $min_samples = 4$ . La observaci\u00f3n  $A$  y el resto de observaciones marcadas en rojo son core points, ya que todas ellas contienen al menos 4 observaciones vecinas (incluy\u00e9ndose a ellas mismas) en su  $\\epsilon$-neighborhood. Como todas son alcanzables entre ellas, forman un cluster. Las observaciones  $B$  y  $C$  no son core points pero son alcanzables desde  $A$  a trav\u00e9s de otros core points, por lo tanto, pertenecen al mismo cluster que  $A$ . La observaci\u00f3n  $N$  no es ni un core point ni es directamente alcanzable, por lo que se considera como ruido.</p> <p></p> In\u00a0[23]: Copied! <pre># Tratamiento de datos\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_blobs\n\n# Gr\u00e1ficos\n# ==============================================================================\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('ggplot') or plt.style.use('ggplot')\n\n# Preprocesado y modelado\n# ==============================================================================\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import scale\nfrom sklearn.metrics import silhouette_score\n\n# Configuraci\u00f3n warnings\n# ==============================================================================\nimport warnings\nwarnings.filterwarnings('ignore')\n</pre> # Tratamiento de datos # ============================================================================== import numpy as np import pandas as pd from sklearn.datasets import make_blobs  # Gr\u00e1ficos # ============================================================================== import matplotlib.pyplot as plt from matplotlib import style style.use('ggplot') or plt.style.use('ggplot')  # Preprocesado y modelado # ============================================================================== from sklearn.cluster import DBSCAN from sklearn.preprocessing import scale from sklearn.metrics import silhouette_score  # Configuraci\u00f3n warnings # ============================================================================== import warnings warnings.filterwarnings('ignore') In\u00a0[24]: Copied! <pre># leer datos\nurl='https://drive.google.com/file/d/1sLxYBCZJawCHyxEjHnn-hnBDInXx4hby/view?usp=drive_link'\nurl='https://drive.google.com/uc?id=' + url.split('/')[-2]\n\ndf = pd.read_csv(url, sep=\",\")\ndf.head()\n</pre> # leer datos url='https://drive.google.com/file/d/1sLxYBCZJawCHyxEjHnn-hnBDInXx4hby/view?usp=drive_link' url='https://drive.google.com/uc?id=' + url.split('/')[-2]  df = pd.read_csv(url, sep=\",\") df.head() Out[24]: x y shape 0 -0.803739 -0.853053 1 1 0.852851 0.367618 1 2 0.927180 -0.274902 1 3 -0.752626 -0.511565 1 4 0.706846 0.810679 1 In\u00a0[25]: Copied! <pre># graficar \nsns.set(rc={'figure.figsize':(11.7,8.27)})\nax = sns.scatterplot( data=df,x=\"x\", y=\"y\")\n</pre> # graficar  sns.set(rc={'figure.figsize':(11.7,8.27)}) ax = sns.scatterplot( data=df,x=\"x\", y=\"y\") In\u00a0[26]: Copied! <pre># Escalado de datos\nscaler = StandardScaler()\ncolumns = ['x', 'y']\ndf[columns] = scaler.fit_transform(df[columns])\ndf.head()\n</pre> # Escalado de datos scaler = StandardScaler() columns = ['x', 'y'] df[columns] = scaler.fit_transform(df[columns]) df.head() Out[26]: x y shape 0 -1.120749 -0.193616 1 1 1.448907 0.844692 1 2 1.564203 0.298161 1 3 -1.041463 0.096855 1 4 1.222429 1.221561 1 In\u00a0[27]: Copied! <pre># Modelo\nX = np.array(df[['x','y']])\n\nmodelo_dbscan = DBSCAN(\n                    eps          = 0.2,\n                    min_samples  = 5,\n                    metric       = 'euclidean',\n                )\n\n\nmodelo_dbscan.fit(X)\n</pre> # Modelo X = np.array(df[['x','y']])  modelo_dbscan = DBSCAN(                     eps          = 0.2,                     min_samples  = 5,                     metric       = 'euclidean',                 )   modelo_dbscan.fit(X) Out[27]: <pre>DBSCAN(eps=0.2)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DBSCAN<pre>DBSCAN(eps=0.2)</pre> In\u00a0[28]: Copied! <pre># agregar labels\ndf['labels'] = modelo_dbscan.labels_\n</pre> # agregar labels df['labels'] = modelo_dbscan.labels_ In\u00a0[29]: Copied! <pre># graficar \nsns.set(rc={'figure.figsize':(11.7,8.27)})\nax = sns.scatterplot( data=df,x=\"x\", y=\"y\",hue = \"labels\")\n</pre> # graficar  sns.set(rc={'figure.figsize':(11.7,8.27)}) ax = sns.scatterplot( data=df,x=\"x\", y=\"y\",hue = \"labels\") In\u00a0[30]: Copied! <pre># N\u00famero de clusters y observaciones \"outliers\"\nn_clusters = len(set(df['labels'])) - (1 if -1 in df['labels'] else 0)\nn_noise    = list(df['labels']).count(-1)\n\nprint(f'N\u00famero de clusters encontrados: {n_clusters}')\nprint(f'N\u00famero de outliers encontrados: {n_noise}')\n</pre> # N\u00famero de clusters y observaciones \"outliers\" n_clusters = len(set(df['labels'])) - (1 if -1 in df['labels'] else 0) n_noise    = list(df['labels']).count(-1)  print(f'N\u00famero de clusters encontrados: {n_clusters}') print(f'N\u00famero de outliers encontrados: {n_noise}') <pre>N\u00famero de clusters encontrados: 6\nN\u00famero de outliers encontrados: 25\n</pre> In\u00a0[31]: Copied! <pre># Tratamiento de datos\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_blobs\n\n# Gr\u00e1ficos\n# ==============================================================================\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom matplotlib.patches import Ellipse\nfrom matplotlib import style\nstyle.use('ggplot') or plt.style.use('ggplot')\n\n# Preprocesado y modelado\n# ==============================================================================\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.preprocessing import scale\nfrom sklearn.metrics import silhouette_score\n\n# Configuraci\u00f3n warnings\n# ==============================================================================\nimport warnings\nwarnings.filterwarnings('ignore')\n</pre> # Tratamiento de datos # ============================================================================== import numpy as np import pandas as pd from sklearn.datasets import make_blobs  # Gr\u00e1ficos # ============================================================================== import matplotlib.pyplot as plt import matplotlib as mpl from matplotlib.patches import Ellipse from matplotlib import style style.use('ggplot') or plt.style.use('ggplot')  # Preprocesado y modelado # ============================================================================== from sklearn.mixture import GaussianMixture from sklearn.preprocessing import scale from sklearn.metrics import silhouette_score  # Configuraci\u00f3n warnings # ============================================================================== import warnings warnings.filterwarnings('ignore') In\u00a0[32]: Copied! <pre># generar datos\nX, y = make_blobs(\n        n_samples    = 300, \n        n_features   = 2, \n        centers      = 4, \n        cluster_std  = 0.60, \n        shuffle      = True, \n        random_state = 0\n       )\n\n\ndf = pd.DataFrame({\n    'x':X[:,0],\n    'y':X[:,1]\n})\n\n\n# Escalado de datos\nscaler = StandardScaler()\ncolumns = ['x', 'y']\ndf[columns] = scaler.fit_transform(df[columns])\n\ndf.head()\n</pre> # generar datos X, y = make_blobs(         n_samples    = 300,          n_features   = 2,          centers      = 4,          cluster_std  = 0.60,          shuffle      = True,          random_state = 0        )   df = pd.DataFrame({     'x':X[:,0],     'y':X[:,1] })   # Escalado de datos scaler = StandardScaler() columns = ['x', 'y'] df[columns] = scaler.fit_transform(df[columns])  df.head() Out[32]: x y 0 0.516255 -0.707227 1 -0.861664 1.329068 2 0.711174 0.437049 3 -0.619792 1.485573 4 0.782282 -0.801378 In\u00a0[33]: Copied! <pre># graficar \nsns.set(rc={'figure.figsize':(11.7,8.27)})\nax = sns.scatterplot( data=df,x=\"x\", y=\"y\")\n</pre> # graficar  sns.set(rc={'figure.figsize':(11.7,8.27)}) ax = sns.scatterplot( data=df,x=\"x\", y=\"y\") In\u00a0[34]: Copied! <pre># Modelo\n\nX = np.array(df[['x','y']])\nmodelo_gmm = GaussianMixture(n_components=4, covariance_type='full', random_state=123)\nmodelo_gmm.fit(X=X)\n</pre> # Modelo  X = np.array(df[['x','y']]) modelo_gmm = GaussianMixture(n_components=4, covariance_type='full', random_state=123) modelo_gmm.fit(X=X) Out[34]: <pre>GaussianMixture(n_components=4, random_state=123)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianMixture<pre>GaussianMixture(n_components=4, random_state=123)</pre> In\u00a0[35]: Copied! <pre># Media de cada componente\nmodelo_gmm.means_\n</pre> # Media de cada componente modelo_gmm.means_ Out[35]: <pre>array([[ 0.57844185,  0.17292982],\n       [ 1.2180002 , -1.19725866],\n       [-0.96910551, -0.44143927],\n       [-0.83710796,  1.46219241]])</pre> In\u00a0[36]: Copied! <pre># Matriz de covarianza de cada componente\nmodelo_gmm.covariances_\n</pre> # Matriz de covarianza de cada componente modelo_gmm.covariances_ Out[36]: <pre>array([[[ 0.14277634, -0.00527707],\n        [-0.00527707,  0.05201453]],\n\n       [[ 0.12745218, -0.00619666],\n        [-0.00619666,  0.05157763]],\n\n       [[ 0.12131004,  0.00243031],\n        [ 0.00243031,  0.04602115]],\n\n       [[ 0.15451151,  0.0068188 ],\n        [ 0.0068188 ,  0.05660383]]])</pre> <p>Predicci\u00f3n y clasificaci\u00f3n</p> <p>Una vez entrenado el modelo GMMs, se puede predecir la probabilidad que tiene cada observaci\u00f3n de pertenecer a cada una de las componentes (clusters). Para obtener la clasificaci\u00f3n final, se asigna a la componente con mayor probabilidad</p> In\u00a0[37]: Copied! <pre># Probabilidades\n# ==============================================================================\n# Cada fila es una observaci\u00f3n y cada columna la probabilidad de pertenecer a\n# cada una de las componentes.\nprobabilidades = modelo_gmm.predict_proba(X)\nprobabilidades\n</pre> # Probabilidades # ============================================================================== # Cada fila es una observaci\u00f3n y cada columna la probabilidad de pertenecer a # cada una de las componentes. probabilidades = modelo_gmm.predict_proba(X) probabilidades Out[37]: <pre>array([[2.59319058e-02, 9.71686641e-01, 2.38145325e-03, 8.05199375e-21],\n       [7.16006205e-09, 7.13831446e-33, 2.34989165e-15, 9.99999993e-01],\n       [9.99999970e-01, 8.78380305e-12, 9.13663813e-09, 2.04805600e-08],\n       ...,\n       [9.99965889e-01, 4.92493619e-10, 3.41016281e-05, 8.75675460e-09],\n       [3.01319652e-06, 6.45628361e-30, 1.52897049e-18, 9.99996987e-01],\n       [4.39337172e-07, 1.99785604e-11, 9.99999561e-01, 4.05381245e-15]])</pre> In\u00a0[38]: Copied! <pre># Clasificaci\u00f3n (asignaci\u00f3n a la componente de mayor probabilidad)\n# ==============================================================================\n# Cada fila es una observaci\u00f3n y cada columna la probabilidad de pertenecer a\n# cada una de las componentes.\nclasificacion = modelo_gmm.predict(X)\nclasificacion\n</pre> # Clasificaci\u00f3n (asignaci\u00f3n a la componente de mayor probabilidad) # ============================================================================== # Cada fila es una observaci\u00f3n y cada columna la probabilidad de pertenecer a # cada una de las componentes. clasificacion = modelo_gmm.predict(X) clasificacion Out[38]: <pre>array([1, 3, 0, 3, 1, 1, 2, 0, 3, 3, 2, 3, 0, 3, 1, 0, 0, 1, 2, 2, 1, 1,\n       0, 2, 2, 0, 1, 0, 2, 0, 3, 3, 0, 3, 3, 3, 3, 3, 2, 1, 0, 2, 0, 0,\n       2, 2, 3, 2, 3, 1, 2, 1, 3, 1, 1, 2, 3, 2, 3, 1, 3, 0, 3, 2, 2, 2,\n       3, 1, 3, 2, 0, 2, 3, 2, 2, 3, 2, 0, 1, 3, 1, 0, 1, 1, 3, 0, 1, 0,\n       3, 3, 0, 1, 3, 2, 2, 0, 1, 1, 0, 2, 3, 1, 3, 1, 0, 1, 1, 0, 3, 0,\n       2, 2, 1, 3, 1, 0, 3, 1, 1, 0, 2, 1, 2, 1, 1, 1, 1, 2, 1, 2, 3, 2,\n       2, 1, 3, 2, 2, 3, 0, 3, 3, 2, 0, 2, 0, 2, 3, 0, 3, 3, 3, 0, 3, 0,\n       1, 2, 3, 2, 1, 0, 3, 0, 0, 1, 0, 2, 2, 0, 1, 0, 0, 3, 1, 0, 2, 3,\n       1, 1, 0, 2, 1, 0, 2, 2, 0, 0, 0, 0, 1, 3, 0, 2, 0, 0, 2, 2, 2, 0,\n       2, 3, 0, 2, 1, 2, 0, 3, 2, 3, 0, 3, 0, 2, 0, 0, 3, 2, 2, 1, 1, 0,\n       3, 1, 1, 2, 1, 2, 0, 3, 3, 0, 0, 3, 0, 1, 2, 0, 1, 2, 3, 2, 1, 0,\n       1, 3, 3, 3, 3, 2, 2, 3, 0, 2, 1, 0, 2, 2, 2, 1, 1, 3, 0, 0, 2, 1,\n       3, 2, 0, 3, 0, 1, 1, 2, 2, 0, 1, 1, 1, 0, 3, 3, 1, 1, 0, 1, 1, 1,\n       3, 2, 3, 0, 1, 1, 3, 3, 3, 1, 1, 0, 3, 2], dtype=int64)</pre> In\u00a0[39]: Copied! <pre># Representaci\u00f3n gr\u00e1fica\n# ==============================================================================\n# Codigo obtenido de:\n# https://github.com/amueller/COMS4995-s20/tree/master/slides/aml-14-clustering-mixture-models \ndef make_ellipses(gmm, ax):\n    for n in range(gmm.n_components):\n        if gmm.covariance_type == 'full':\n            covariances = gmm.covariances_[n]\n        elif gmm.covariance_type == 'tied':\n            covariances = gmm.covariances_\n        elif gmm.covariance_type == 'diag':\n            covariances = np.diag(gmm.covariances_[n])\n        elif gmm.covariance_type == 'spherical':\n            covariances = np.eye(gmm.means_.shape[1]) * gmm.covariances_[n]\n        v, w = np.linalg.eigh(covariances)\n        u = w[0] / np.linalg.norm(w[0])\n        angle = np.arctan2(u[1], u[0])\n        angle = 180 * angle / np.pi  # convert to degrees\n        v = 2. * np.sqrt(2.) * np.sqrt(v)\n        \n        for i in range(1,3):\n            ell = mpl.patches.Ellipse(gmm.means_[n], i*v[0], i*v[1],\n                                      180 + angle, color=\"blue\")\n            ell.set_clip_box(ax.bbox)\n            ell.set_alpha(0.1)\n            ax.add_artist(ell)\n</pre> # Representaci\u00f3n gr\u00e1fica # ============================================================================== # Codigo obtenido de: # https://github.com/amueller/COMS4995-s20/tree/master/slides/aml-14-clustering-mixture-models  def make_ellipses(gmm, ax):     for n in range(gmm.n_components):         if gmm.covariance_type == 'full':             covariances = gmm.covariances_[n]         elif gmm.covariance_type == 'tied':             covariances = gmm.covariances_         elif gmm.covariance_type == 'diag':             covariances = np.diag(gmm.covariances_[n])         elif gmm.covariance_type == 'spherical':             covariances = np.eye(gmm.means_.shape[1]) * gmm.covariances_[n]         v, w = np.linalg.eigh(covariances)         u = w[0] / np.linalg.norm(w[0])         angle = np.arctan2(u[1], u[0])         angle = 180 * angle / np.pi  # convert to degrees         v = 2. * np.sqrt(2.) * np.sqrt(v)                  for i in range(1,3):             ell = mpl.patches.Ellipse(gmm.means_[n], i*v[0], i*v[1],                                       180 + angle, color=\"blue\")             ell.set_clip_box(ax.bbox)             ell.set_alpha(0.1)             ax.add_artist(ell)          In\u00a0[40]: Copied! <pre>fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n\n# Distribuci\u00f3n de probabilidad de cada componente\nfor i in np.unique(clasificacion):\n    axs[0].scatter(\n        x = X[clasificacion == i, 0],\n        y = X[clasificacion == i, 1], \n        c = plt.rcParams['axes.prop_cycle'].by_key()['color'][i],\n        marker    = 'o',\n        edgecolor = 'black', \n        label= f\"Componente {i}\"\n    )\n\nmake_ellipses(modelo_gmm, ax = axs[0])\naxs[0].set_title('Distribuci\u00f3n de prob. de cada componente')\naxs[0].legend()\n\n\n# Distribuci\u00f3n de probabilidad del modelo completo\nxs = np.linspace(min(X[:, 0]), max(X[:, 0]), 1000)\nys = np.linspace(min(X[:, 1]), max(X[:, 1]), 1000)\nxx, yy = np.meshgrid(xs, ys)\nscores = modelo_gmm.score_samples(np.c_[xx.ravel(), yy.ravel()], )\naxs[1].scatter(X[:, 0], X[:, 1], s=5, alpha=.6, c=plt.cm.tab10(clasificacion))\nscores = np.exp(scores) # Las probabilidades est\u00e1n en log\naxs[1].contour(\n    xx, yy, scores.reshape(xx.shape),\n    levels=np.percentile(scores, np.linspace(0, 100, 10))[1:-1]\n)\naxs[1].set_title('Distribuci\u00f3n de prob. del modelo completo');\n</pre> fig, axs = plt.subplots(1, 2, figsize=(15, 5))  # Distribuci\u00f3n de probabilidad de cada componente for i in np.unique(clasificacion):     axs[0].scatter(         x = X[clasificacion == i, 0],         y = X[clasificacion == i, 1],          c = plt.rcParams['axes.prop_cycle'].by_key()['color'][i],         marker    = 'o',         edgecolor = 'black',          label= f\"Componente {i}\"     )  make_ellipses(modelo_gmm, ax = axs[0]) axs[0].set_title('Distribuci\u00f3n de prob. de cada componente') axs[0].legend()   # Distribuci\u00f3n de probabilidad del modelo completo xs = np.linspace(min(X[:, 0]), max(X[:, 0]), 1000) ys = np.linspace(min(X[:, 1]), max(X[:, 1]), 1000) xx, yy = np.meshgrid(xs, ys) scores = modelo_gmm.score_samples(np.c_[xx.ravel(), yy.ravel()], ) axs[1].scatter(X[:, 0], X[:, 1], s=5, alpha=.6, c=plt.cm.tab10(clasificacion)) scores = np.exp(scores) # Las probabilidades est\u00e1n en log axs[1].contour(     xx, yy, scores.reshape(xx.shape),     levels=np.percentile(scores, np.linspace(0, 100, 10))[1:-1] ) axs[1].set_title('Distribuci\u00f3n de prob. del modelo completo'); <p>N\u00famero de clusters</p> <p>Dado que los modelos GMM son modelos probabil\u00edsticos, se puede recurrir a m\u00e9tricas como el Akaike information criterion (AIC) o Bayesian information criterion (BIC) para identificar c\u00f3mo de bien se ajustan los datos observados a modelo creado.</p> In\u00a0[41]: Copied! <pre>n_components = range(1, 21)\nvalores_bic = []\nvalores_aic = []\n\nfor i in n_components:\n    modelo = GaussianMixture(n_components=i, covariance_type=\"full\")\n    modelo = modelo.fit(X)\n    valores_bic.append(modelo.bic(X))\n    valores_aic.append(modelo.aic(X))\n\nfig, ax = plt.subplots(1, 1, figsize=(12,6))\nax.plot(n_components, valores_bic, label='BIC')\nax.plot(n_components, valores_aic, label='AIC')\nax.set_title(\"Valores BIC y AIC\")\nax.set_xlabel(\"N\u00famero componentes\")\nax.legend();\n</pre> n_components = range(1, 21) valores_bic = [] valores_aic = []  for i in n_components:     modelo = GaussianMixture(n_components=i, covariance_type=\"full\")     modelo = modelo.fit(X)     valores_bic.append(modelo.bic(X))     valores_aic.append(modelo.aic(X))  fig, ax = plt.subplots(1, 1, figsize=(12,6)) ax.plot(n_components, valores_bic, label='BIC') ax.plot(n_components, valores_aic, label='AIC') ax.set_title(\"Valores BIC y AIC\") ax.set_xlabel(\"N\u00famero componentes\") ax.legend(); In\u00a0[42]: Copied! <pre>print(f\"N\u00famero \u00f3ptimo acorde al BIC: {range(1, 21)[np.argmin(valores_bic)]}\")\nprint(f\"N\u00famero \u00f3ptimo acorde al AIC: {range(1, 21)[np.argmin(valores_aic)]}\")\n</pre> print(f\"N\u00famero \u00f3ptimo acorde al BIC: {range(1, 21)[np.argmin(valores_bic)]}\") print(f\"N\u00famero \u00f3ptimo acorde al AIC: {range(1, 21)[np.argmin(valores_aic)]}\") <pre>N\u00famero \u00f3ptimo acorde al BIC: 4\nN\u00famero \u00f3ptimo acorde al AIC: 4\n</pre> <p>Ambas m\u00e9tricas identifican el 4 como n\u00famero \u00f3ptimo de clusters (componentes).</p>"},{"location":"lectures/machine_learning/ns_01/#no-supervisado-i","title":"No supervisado I\u00b6","text":""},{"location":"lectures/machine_learning/ns_01/#clustering","title":"Clustering\u00b6","text":"<p>El Clustering es la tarea de agrupar objetos por similitud, en grupos o conjuntos de manera que los miembros del mismo grupo tengan caracter\u00edsticas similares. Es la tarea principal de la miner\u00eda de datos exploratoria y es una t\u00e9cnica com\u00fan en el an\u00e1lisis de datos estad\u00edsticos.</p>"},{"location":"lectures/machine_learning/ns_01/#k-means","title":"K-means\u00b6","text":""},{"location":"lectures/machine_learning/ns_01/#teoria","title":"Teor\u00eda\u00b6","text":"<p>El algoritmo K-means  (MacQueen, 1967) agrupa las observaciones en un n\u00famero predefinido de $k$ clusters de forma que, la suma de las varianzas internas de los clusters, sea lo menor posible.</p> <p>Existen varias implementaciones de este algoritmo, la m\u00e1s com\u00fan de ellas se conoce como Lloyd\u2019s. En la bibliograf\u00eda es com\u00fan encontrar los t\u00e9rminos inertia, within-cluster sum-of-squares o varianza intra-cluster para referirse a la varianza interna de los clusters.</p>"},{"location":"lectures/machine_learning/ns_01/#algoritmo","title":"Algoritmo\u00b6","text":"<ol> <li>Especificar el n\u00famero $k$ de clusters que se quieren crear.</li> <li>Seleccionar de forma aleatoria $k$ observaciones del set de datos como centroides iniciales.</li> <li>Asignar cada una de las observaciones al centroide m\u00e1s cercano.</li> <li>Para cada uno de los $k$ clusters generados en el paso 3, recalcular su centroide.</li> </ol> <p>Repetir los pasos 3 y 4 hasta que las asignaciones no cambien o se alcance el n\u00famero m\u00e1ximo de iteraciones establecido.</p> <p>El problema anterior es NP-hard (imposible de resolver en tiempo polinomial, del tipo m\u00e1s dif\u00edcil de los probleams NP).</p>"},{"location":"lectures/machine_learning/ns_01/#ventajas-y-desventajas","title":"Ventajas y desventajas\u00b6","text":"<p>K-means es uno de los m\u00e9todos de clustering m\u00e1s utilizados. Destaca por la sencillez y velocidad de su algoritmo, sin embargo, presenta una serie de limitaciones que se deben tener en cuenta.</p> <ul> <li><p>Requiere que se indique de antemano el n\u00famero de clusters que se van a crear. Esto puede ser complicado si no se dispone de informaci\u00f3n adicional sobre los datos con los que se trabaja. Se han desarrollado varias estrategias para ayudar a identificar potenciales valores \u00f3ptimos de $k$ (elbow, shilouette), pero todas ellas son orientativas.</p> </li> <li><p>Dificultad para detectar clusters alargados o con formas irregulares.</p> </li> <li><p>Las agrupaciones resultantes pueden variar dependiendo de la asignaci\u00f3n aleatoria inicial de los centroides. Para minimizar este problema, se recomienda repetir el proceso de clustering entre 25-50 veces y seleccionar como resultado definitivo el que tenga menor suma total de varianza interna. Aun as\u00ed, solo se puede garantizar la reproducibilidad de los resultados si se emplean semillas.</p> </li> <li><p>Presenta problemas de robustez frente a outliers.</p> </li> </ul>"},{"location":"lectures/machine_learning/ns_01/#aplicacion","title":"Aplicaci\u00f3n\u00b6","text":"<p>Veamos un ejemplo de an\u00e1lisis no supervisado ocupando el algoritmo k-means.</p>"},{"location":"lectures/machine_learning/ns_01/#regla-del-codo","title":"Regla del codo\u00b6","text":"<p>Este m\u00e9todo utiliza los valores de la funci\u00f3n de perdida, $f(C_l,\\mu_l)$, obtenidos tras aplicar el $K$-means a diferente n\u00famero de Clusters (desde 1 a $N$ clusters).</p> <p>Una vez obtenidos los valores de la funci\u00f3n de p\u00e9rdida  tras aplicar el K-means de 1 a $N$ clusters, representamos en una gr\u00e1fica lineal la funci\u00f3n de p\u00e9rdida  respecto del n\u00famero de clusters.</p> <p>En esta gr\u00e1fica se deber\u00eda de apreciar un cambio brusco en la evoluci\u00f3n de la funci\u00f3n de p\u00e9rdida, teniendo la l\u00ednea representada una forma similar a la de un brazo y su codo.</p> <p>El punto en el que se observa ese cambio brusco en la funci\u00f3n de p\u00e9rdida nos dir\u00e1 el n\u00famero \u00f3ptimo de clusters a seleccionar para ese data set; o dicho de otra manera: el punto que representar\u00eda al codo del brazo ser\u00e1 el n\u00famero \u00f3ptimo de clusters para ese data set .</p>"},{"location":"lectures/machine_learning/ns_01/#hierarchical-clustering","title":"Hierarchical clustering\u00b6","text":""},{"location":"lectures/machine_learning/ns_01/#teoria","title":"Teor\u00eda\u00b6","text":"<p>Hierarchical clustering es una alternativa a los m\u00e9todos de partitioning clustering que no requiere que se pre-especifique el n\u00famero de clusters. Los m\u00e9todos que engloba el hierarchical clustering se subdividen en dos tipos dependiendo de la estrategia seguida para crear los grupos:</p> <ul> <li><p>Aglomerativo (agglomerative clustering o bottom-up): el agrupamiento se inicia con todas las observaciones separadas, cada una formando un cluster individual. Los clusters se van combinado a medida que la estructura crece hasta converger en uno solo.</p> </li> <li><p>Divisivo (divisive clustering o top-down): es la estrategia opuesta al aglomerativo. Se inicia con todas las observaciones contenidas en un mismo cluster y se suceden divisiones hasta que cada observaci\u00f3n forma un cluster* individual.</p> </li> </ul> <p>En ambos casos, los resultados pueden representarse de forma muy intuitiva en una estructura de \u00e1rbol llamada dendrograma.</p>"},{"location":"lectures/machine_learning/ns_01/#algoritmo","title":"Algoritmo\u00b6","text":""},{"location":"lectures/machine_learning/ns_01/#aglomerativo","title":"Aglomerativo\u00b6","text":"<p>El algoritmo seguido para por el clustering aglomerativo es:</p> <ol> <li><p>Considerar cada una de las n observaciones como un cluster individual, formando as\u00ed la base del dendrograma (hojas).</p> </li> <li><p>Proceso iterativo hasta que todas las observaciones pertenecen a un \u00fanico cluster:</p> <ul> <li><p>Calcular la distancia entre cada posible par de los n clusters. El investigador debe determinar el tipo de medida empleada para cuantificar la similitud entre observaciones o grupos (distancia y linkage).</p> </li> <li><p>Los dos clusters m\u00e1s similares se fusionan, de forma que quedan n-1 clusters.</p> </li> </ul> </li> <li><p>Cortar la estructura de \u00e1rbol generada (dendrograma) a una determinada altura para crear los clusters finales.</p> </li> </ol> <p>Para que el proceso de agrupamiento pueda llevarse a cabo tal como indica el algoritmo anterior, es necesario definir c\u00f3mo se cuantifica la similitud entre dos clusters. Es decir, se tiene que extender el concepto de distancia entre pares de observaciones para que sea aplicable a pares de grupos, cada uno formado por varias observaciones. A este proceso se le conoce como linkage. A continuaci\u00f3n, se describen los 5 tipos de linkage m\u00e1s empleados y sus definiciones.</p> <ul> <li><p>Complete or Maximum: se calcula la distancia entre todos los posibles pares formados por una observaci\u00f3n del cluster A y una del cluster B. La mayor de todas ellas se selecciona como la distancia entre los dos clusters. Se trata de la medida m\u00e1s conservadora (maximal intercluster dissimilarity).</p> </li> <li><p>Single or Minimum: se calcula la distancia entre todos los posibles pares formados por una observaci\u00f3n del cluster A y una del cluster B. La menor de todas ellas se selecciona como la distancia entre los dos clusters. Se trata de la medida menos conservadora (minimal intercluster dissimilarity).</p> </li> <li><p>Average: Se calcula la distancia entre todos los posibles pares formados por una observaci\u00f3n del cluster A y una del cluster B. El valor promedio de todas ellas se selecciona como la distancia entre los dos clusters (mean intercluster dissimilarity).</p> </li> <li><p>Centroid: Se calcula el centroide de cada uno de los clusters y se selecciona la distancia entre ellos como la distancia entre los dos clusters.</p> </li> <li><p>Ward: Se trata de un m\u00e9todo general. La selecci\u00f3n del par de clusters que se combinan en cada paso del agglomerative hierarchical clustering se basa en el valor \u00f3ptimo de una funci\u00f3n objetivo, pudiendo ser esta \u00faltima cualquier funci\u00f3n definida por el analista. El m\u00e9todo Ward's minimum variance es un caso particular en el que el objetivo es minimizar la suma total de varianza intra-cluster. En cada paso, se identifican aquellos 2 clusters cuya fusi\u00f3n conlleva menor incremento de la varianza total intra-cluster. Esta es la misma m\u00e9trica que se minimiza en K-means.</p> </li> </ul> <p>Los m\u00e9todos de complete, average y Ward's minimum variance suelen ser los preferidos por los analistas debido a que generan dendrogramas m\u00e1s compensados. Sin embargo, no se puede determinar que uno sea mejor que otro, ya que depende del caso de estudio en cuesti\u00f3n. Por ejemplo, en gen\u00f3mica, se emplea con frecuencia el m\u00e9todo de centroides. Junto con los resultados de un proceso de hierarchical clustering siempre hay que indicar qu\u00e9 distancia se ha empleado, as\u00ed como el tipo de linkage, ya que, dependiendo de estos, los resultados pueden variar en gran medida.</p>"},{"location":"lectures/machine_learning/ns_01/#divisivo","title":"Divisivo\u00b6","text":"<p>El algoritmo m\u00e1s conocido de divisive hierarchical clustering es DIANA (DIvisive ANAlysis Clustering). Este algoritmo se inicia con un \u00fanico cluster que contiene todas las observaciones. A continuaci\u00f3n, se van sucediendo divisiones hasta que cada observaci\u00f3n forma un cluster independiente. En cada iteraci\u00f3n, se selecciona el cluster con mayor di\u00e1metro, entendiendo por di\u00e1metro de un cluster la mayor de las diferencias entre dos de sus observaciones. Una vez seleccionado el cluster, se identifica la observaci\u00f3n m\u00e1s dispar, que es aquella con mayor distancia promedio respecto al resto de observaciones que forman el cluster. Esta observaci\u00f3n inicia el nuevo cluster. Se reasignan las observaciones en funci\u00f3n de si est\u00e1n m\u00e1s pr\u00f3ximas al nuevo cluster o al resto de la partici\u00f3n, dividiendo as\u00ed el cluster seleccionado en dos nuevos clusters.</p> <ol> <li>Todas las $n$ observaciones forman un \u00fanico cluster.</li> <li>Repetir hasta que haya $n$ clusters:<ul> <li>Calcular para cada cluster la mayor de las distancias entre pares de observaciones (di\u00e1metro del cluster).</li> <li>Seleccionar el cluster con mayor di\u00e1metro.</li> <li>Calcular la distancia media de cada observaci\u00f3n respecto a las dem\u00e1s.</li> <li>La observaci\u00f3n m\u00e1s distante inicia un nuevo cluster.</li> <li>Se reasignan las observaciones restantes al nuevo cluster o al viejo dependiendo de cu\u00e1l est\u00e1 m\u00e1s pr\u00f3ximo.</li> </ul> </li> </ol> <p>A diferencia del clustering aglomerativo, en el que hay que elegir un tipo de distancia y un m\u00e9todo de linkage, en el clustering divisivo solo hay que elegir la distancia, no hay linkage.</p>"},{"location":"lectures/machine_learning/ns_01/#dendograma","title":"Dendograma\u00b6","text":"<p>Los resultados del hierarchical clustering pueden representarse como un \u00e1rbol en el que las ramas representan la jerarqu\u00eda con la que se van sucediendo las uniones de clusters.</p> <p>Sup\u00f3ngase que se dispone de 45 observaciones en un espacio de dos dimensiones, a los que se les aplica hierarchical clustering para intentar identificar grupos. El siguiente dendrograma representa los resultados obtenidos.</p> <p></p>"},{"location":"lectures/machine_learning/ns_01/#aplicacion","title":"Aplicaci\u00f3n\u00b6","text":""},{"location":"lectures/machine_learning/ns_01/#density-based-clustering-dbscan","title":"Density based clustering (DBSCAN)\u00b6","text":""},{"location":"lectures/machine_learning/ns_01/#teoria","title":"Teor\u00eda\u00b6","text":"<p>Density-based spatial clustering of applications with noise (DBSCAN) fue presentado en 1996 por Ester et al. como una forma de identificar clusters siguiendo el modo intuitivo en el que lo hace el cerebro humano, identificando regiones con alta densidad de observaciones separadas por regiones de baja densidad.</p> <p></p>"},{"location":"lectures/machine_learning/ns_01/#algoritmo","title":"Algoritmo\u00b6","text":"<ol> <li><p>Para cada observaci\u00f3n $x_i$ calcular la distancia entre ella y el resto de observaciones. Si en su  $epsilon$ -neighborhood hay un n\u00famero de observaciones $\\geq min_samples$  marcar la observaci\u00f3n como core point, de lo contrario marcarla como visitada.</p> </li> <li><p>Para cada observaci\u00f3n  $x_i$   marcada como core point, si todav\u00eda no ha sido asignada a ning\u00fan cluster, crear uno nuevo y asignarla a \u00e9l. Encontrar recursivamente todas las observaciones densamente conectadas a ella y asignarlas al mismo cluster.</p> </li> <li><p>Iterar el mismo proceso para todas las observaciones que no hayan sido visitadas.</p> </li> <li><p>Aquellas observaciones que tras haber sido visitadas no pertenecen a ning\u00fan cluster se marcan como outliers.</p> </li> </ol> <p>Como resultado, todo cluster cumple dos propiedades: todos los puntos que forman parte de un mismo cluster est\u00e1n densamente conectados entre ellos y, si una observaci\u00f3n $A$  es densamente alcanzable desde cualquier otra observaci\u00f3n de un cluster, entonces $A$  tambi\u00e9n pertenece al cluster.</p>"},{"location":"lectures/machine_learning/ns_01/#hiperparametros","title":"Hiperpar\u00e1metros\u00b6","text":"<p>Como ocurre en muchas otras t\u00e9cnicas estad\u00edsticas, en DBSCAN no existe una forma \u00fanica y exacta de encontrar el valor adecuado de epsilon  ($\\epsilon$))  y  $min_samples$) . A modo orientativo se pueden seguir las siguientes premisas:</p> <ul> <li><p>$min_samples$ : cuanto mayor sea el tama\u00f1o del set de datos, mayor debe ser el valor m\u00ednimo de observaciones vecinas. En el libro Practical Guide to Cluster Analysis in R recomiendan no bajar nunca de 3. Si los datos contienen niveles altos de ruido, aumentar  $min_samples$  favorecer\u00e1 la creaci\u00f3n de clusters significativos menos influenciados por outliers.</p> </li> <li><p>epsilon ($\\epsilon$): una buena forma de escoger el valor de $\\epsilon$  es estudiar las distancias promedio entre las  $k = minsamples\ud835\udc60$  observaciones m\u00e1s pr\u00f3ximas. Al representar estas distancias en funci\u00f3n de  $\\epsilon$ , el punto de inflexi\u00f3n de la curva suele ser un valor \u00f3ptimo. Si el valor de  $\\epsilon$ escogido es muy peque\u00f1o, una proporci\u00f3n alta de las observaciones no se asignar\u00e1n a ning\u00fan cluster, por el contrario, si el valor es demasiado grande, la mayor\u00eda de observaciones se agrupar\u00e1n en un \u00fanico cluster.</p> </li> </ul>"},{"location":"lectures/machine_learning/ns_01/#ventajas-y-desventajas","title":"Ventajas y desventajas\u00b6","text":"<ul> <li><p>Ventajas</p> <ul> <li>No requiere que el usuario especifique el n\u00famero de clusters.</li> <li>Es independiente de la forma que tengan los clusters.</li> <li>Puede identificar outliers, por lo que los clusters generados no se ven influenciados por ellos.</li> </ul> </li> <li><p>Desventajas</p> <ul> <li><p>Es un m\u00e9todo determin\u00edstico siempre y cuando el orden de los datos sea el mismo. Los border points que son alcanzables desde m\u00e1s de un cluster pueden asignarse a uno u otro dependiendo del orden en el que se procesen los datos.</p> </li> <li><p>No genera buenos resultados cuando la densidad de los grupos es muy distinta, ya que no es posible encontrar los par\u00e1metros  \ud835\udf16  y min_samples que sirvan para todos a la vez.</p> </li> </ul> </li> </ul>"},{"location":"lectures/machine_learning/ns_01/#aplicacion","title":"Aplicaci\u00f3n\u00b6","text":"<p>Veamos un ejemplo de an\u00e1lisis no supervisado ocupando el algoritmo DBSCAN.</p>"},{"location":"lectures/machine_learning/ns_01/#gaussian-mixture-models-gmms","title":"Gaussian mixture models (GMMs)\u00b6","text":""},{"location":"lectures/machine_learning/ns_01/#teoria","title":"Teor\u00eda\u00b6","text":"<p>Un Gaussian Mixture model es un modelo probabil\u00edstico en el que se considera que las observaciones siguen una distribuci\u00f3n probabil\u00edstica formada por la combinaci\u00f3n de m\u00faltiples distribuciones normales (componentes). En su aplicaci\u00f3n al clustering, puede entenderse como una generalizaci\u00f3n de K-means con la que, en lugar de asignar cada observaci\u00f3n a un \u00fanico cluster, se obtiene una probabilidad de pertenencia a cada uno.</p> <p>Para estimar los par\u00e1metros que definen la funci\u00f3n de distribuci\u00f3n de cada cluster (media y matriz de covarianza) se recurre al algoritmo de Expectation-Maximization (EM). Una vez aprendidos los par\u00e1metros, se puede calcular la probabilidad que tiene cada observaci\u00f3n de pertenecer a cada cluster y asignarla a aquel con mayor probabilidad.</p> <p></p>"},{"location":"lectures/machine_learning/ns_01/#algoritmo","title":"Algoritmo\u00b6","text":"<p>Junto con el n\u00famero de clusters (componentes), hay que determinar el tipo de matriz de covarianza que pueden tener los clusters. Dependiendo del tipo de matriz, la forma de los clusters puede ser:</p> <ul> <li><p>tied: todos los clusters comparten la misma matriz de covarianza.</p> </li> <li><p>diagonal: las dimensiones de cada cluster a lo largo de cada dimensi\u00f3n puede ser distinto, pero las elipses generadas siempre quedan alineadas con los ejes, es decir, su orientaciones son limitadas.</p> </li> <li><p>spherical: las dimensiones de cada cluster son las mismas en todas las dimensiones. Esto permite generar clusters de distinto tama\u00f1o pero todos esf\u00e9ricos.</p> </li> <li><p>full: cada cluster puede puede ser modelado como una elipse cualquier orientaci\u00f3n y dimensiones.</p> </li> </ul>"},{"location":"lectures/machine_learning/ns_01/#aplicacion","title":"Aplicaci\u00f3n\u00b6","text":""},{"location":"lectures/machine_learning/ns_01/#limitaciones-del-clustering","title":"Limitaciones del clustering\u00b6","text":"<p>El clustering puede ser una herramienta muy \u00fatil para encontrar agrupaciones en los datos, sobre todo a medida que el volumen de los mismos aumenta. Sin embargo, es importante recordar sus limitaciones o problemas que pueden surgir al aplicarlo. Algunas de ellas son:</p> <ul> <li><p>Peque\u00f1as decisiones pueden tener grandes consecuencias:a la hora de utilizar los m\u00e9todos de clustering se tienen que tomar decisiones que influyen en gran medida en los resultados obtenidos. No existe una \u00fanica respuesta correcta, por lo que en la pr\u00e1ctica se prueban diferentes opciones.</p> <ul> <li>Escalado y centrado de las variables</li> <li>Qu\u00e9 medida de distancia/similitud emplear</li> <li>N\u00famero de clusters</li> <li>Tipo de linkage empleado en hierarchical clustering</li> <li>A que altura establecer el corte de un dendrograma</li> </ul> </li> <li><p>Validaci\u00f3n de los clusters obtenidos: no es f\u00e1cil comprobar la validez de los resultados ya que en la mayor\u00eda de escenarios se desconoce la verdadera agrupaci\u00f3n.</p> </li> <li><p>Falta de robustez: los m\u00e9todos de K-means-clustering e hierarchical clustering asignan obligatoriamente cada observaci\u00f3n a un grupo. Si existe en la muestra alg\u00fan outlier, a pesar de que realmente no pertenezca a ning\u00fan grupo, el algoritmo lo asignar\u00e1 a uno de ellos provocando una distorsi\u00f3n significativa del cluster en cuesti\u00f3n. Algunas alternativas son k-medoids y DBSCAN.</p> </li> <li><p>La naturaleza del algoritmo de hierarchical clustering conlleva que, si se realiza una mala divisi\u00f3n en los pasos iniciales, no se pueda corregir en los pasos siguientes.</p> </li> </ul>"},{"location":"lectures/machine_learning/ns_01/#referencias","title":"Referencias\u00b6","text":"<ol> <li>Unsupervised learning</li> <li>Clustering con Python (Joaqu\u00edn Amat Rodrigo)</li> </ol>"},{"location":"lectures/machine_learning/ns_02/","title":"No supervisado II","text":"In\u00a0[1]: Copied! <pre># librerias\n \nimport os\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\n\npd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes\n\n# Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab\n%matplotlib inline\n</pre> # librerias   import os import numpy as np import pandas as pd  import matplotlib.pyplot as plt import seaborn as sns    pd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes  # Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab %matplotlib inline In\u00a0[2]: Copied! <pre>from sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import scale\n</pre> from sklearn.model_selection import train_test_split from sklearn.decomposition import PCA from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.pipeline import Pipeline from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler from sklearn.preprocessing import scale In\u00a0[3]: Copied! <pre># Datos\n\nurl='https://drive.google.com/file/d/1ckxtKR1U_ySdtMy1uWLo_KYGbUYrKfmI/view?usp=drive_link'\nurl='https://drive.google.com/uc?id=' + url.split('/')[-2]\n\n\ndatos = pd.read_csv(url, sep=\",\")\ndatos = datos.rename(columns = {datos.columns[0]:'index'}).set_index('index')\ndatos.head()\n</pre> # Datos  url='https://drive.google.com/file/d/1ckxtKR1U_ySdtMy1uWLo_KYGbUYrKfmI/view?usp=drive_link' url='https://drive.google.com/uc?id=' + url.split('/')[-2]   datos = pd.read_csv(url, sep=\",\") datos = datos.rename(columns = {datos.columns[0]:'index'}).set_index('index') datos.head() Out[3]: Murder Assault UrbanPop Rape index Alabama 13.2 236 58 21.2 Alaska 10.0 263 48 44.5 Arizona 8.1 294 80 31.0 Arkansas 8.8 190 50 19.5 California 9.0 276 91 40.6 <p>Veamos una exploraci\u00f3n inicial de los datos:</p> In\u00a0[4]: Copied! <pre>print('----------------------')\nprint('Media de cada variable')\nprint('----------------------')\ndatos.mean(axis=0)\n</pre> print('----------------------') print('Media de cada variable') print('----------------------') datos.mean(axis=0) <pre>----------------------\nMedia de cada variable\n----------------------\n</pre> Out[4]: <pre>Murder        7.788\nAssault     170.760\nUrbanPop     65.540\nRape         21.232\ndtype: float64</pre> <p>La media de las variables muestra que hay tres veces m\u00e1s secuestros que asesinatos y 8 veces m\u00e1s asaltos que secuestros.</p> In\u00a0[5]: Copied! <pre>print('-------------------------')\nprint('Varianza de cada variable')\nprint('-------------------------')\ndatos.var(axis=0)\n</pre> print('-------------------------') print('Varianza de cada variable') print('-------------------------') datos.var(axis=0) <pre>-------------------------\nVarianza de cada variable\n-------------------------\n</pre> Out[5]: <pre>Murder        18.970465\nAssault     6945.165714\nUrbanPop     209.518776\nRape          87.729159\ndtype: float64</pre> <p>La varianza es muy distinta entre las variables, en el caso de Assault, la varianza es varios \u00f3rdenes de magnitud superior al resto.</p> <p>Si no se estandarizan las variables para que tengan media cero y desviaci\u00f3n est\u00e1ndar de uno antes de realizar el estudio PCA, la variable Assault, que tiene una media y dispersi\u00f3n muy superior al resto, dominar\u00e1 la mayor\u00eda de las componentes principales.</p> <p>Modelo PCA</p> <p>La clase <code>sklearn.decomposition.PCA</code> incorpora las principales funcionalidades que se necesitan a la hora de trabajar con modelos PCA. El argumento <code>n_components</code> determina el n\u00famero de componentes calculados. Si se indica None, se calculan todas las posibles (min(filas, columnas) - 1).</p> <p>Por defecto, <code>PCA()</code> centra los valores pero no los escala. Esto es importante ya que, si las variables tienen distinta dispersi\u00f3n, como en este caso, es necesario escalarlas. Una forma de hacerlo es combinar un <code>StandardScaler()</code> y un <code>PCA()</code> dentro de un <code>pipeline</code>.</p> In\u00a0[6]: Copied! <pre># Entrenamiento modelo PCA con escalado de los datos\n# ==============================================================================\npca_pipe = make_pipeline(StandardScaler(), PCA())\npca_pipe.fit(datos)\n\n# Se extrae el modelo entrenado del pipeline\nmodelo_pca = pca_pipe.named_steps['pca']\n</pre> # Entrenamiento modelo PCA con escalado de los datos # ============================================================================== pca_pipe = make_pipeline(StandardScaler(), PCA()) pca_pipe.fit(datos)  # Se extrae el modelo entrenado del pipeline modelo_pca = pca_pipe.named_steps['pca'] <p>Una vez entrenado el objeto <code>PCA</code>, pude accederse a toda la informaci\u00f3n de las componentes creadas.</p> <p><code>components_</code> contiene el valor de los loadings  \ud835\udf19  que definen cada componente (eigenvector). Las filas se corresponden con las componentes principals (ordenadas de mayor a menor varianza explicada). Las filas se corresponden con las variables de entrada.</p> In\u00a0[7]: Copied! <pre># Se combierte el array a dataframe para a\u00f1adir nombres a los ejes.\npd.DataFrame(\n    data    = modelo_pca.components_,\n    columns = datos.columns,\n    index   = ['PC1', 'PC2', 'PC3', 'PC4']\n)\n</pre> # Se combierte el array a dataframe para a\u00f1adir nombres a los ejes. pd.DataFrame(     data    = modelo_pca.components_,     columns = datos.columns,     index   = ['PC1', 'PC2', 'PC3', 'PC4'] ) Out[7]: Murder Assault UrbanPop Rape PC1 0.535899 0.583184 0.278191 0.543432 PC2 0.418181 0.187986 -0.872806 -0.167319 PC3 -0.341233 -0.268148 -0.378016 0.817778 PC4 0.649228 -0.743407 0.133878 0.089024 <p>Analizar con detalle el vector de loadings que forma cada componente puede ayudar a interpretar qu\u00e9 tipo de informaci\u00f3n recoge cada una de ellas. Por ejemplo, la primera componente es el resultado de la siguiente combinaci\u00f3n lineal de las variables originales:</p> <p>$$PC1=0.535899 Murder+0.583184 Assault+0.278191 UrbanPop+0.543432 Rape$$</p> <p>Los pesos asignados en la primera componente a las variables Assault, Murder y Rape son aproximadamente iguales entre ellos y superiores al asignado a UrbanPoP. Esto significa que la primera componente recoge mayoritariamente la informaci\u00f3n correspondiente a los delitos. En la segunda componente, es la variable UrbanPoP es la que tiene con diferencia mayor peso, por lo que se corresponde principalmente con el nivel de urbanizaci\u00f3n del estado. Si bien en este ejemplo la interpretaci\u00f3n de las componentes es bastante clara, no en todos los casos ocurre lo mismo, sobre todo a medida que aumenta el n\u00famero de variables.</p> <p>La influencia de las variables en cada componente analizarse visualmente con un gr\u00e1fico de tipo heatmap.</p> In\u00a0[8]: Copied! <pre># Heatmap componentes\n# ==============================================================================\nplt.figure(figsize=(12,4))\ncomponentes = modelo_pca.components_\nplt.imshow(componentes.T, cmap='viridis', aspect='auto')\nplt.yticks(range(len(datos.columns)), datos.columns)\nplt.xticks(range(len(datos.columns)), np.arange(modelo_pca.n_components_) + 1)\nplt.grid(False)\nplt.colorbar();\n</pre> # Heatmap componentes # ============================================================================== plt.figure(figsize=(12,4)) componentes = modelo_pca.components_ plt.imshow(componentes.T, cmap='viridis', aspect='auto') plt.yticks(range(len(datos.columns)), datos.columns) plt.xticks(range(len(datos.columns)), np.arange(modelo_pca.n_components_) + 1) plt.grid(False) plt.colorbar(); <p>Una vez calculadas las componentes principales, se puede conocer la varianza explicada por cada una de ellas, la proporci\u00f3n respecto al total y la proporci\u00f3n de varianza acumulada. Esta informaci\u00f3n est\u00e1 almacenada en los atributos <code>explained_variance_</code> y <code>explained_variance_ratio_</code> del modelo.</p> In\u00a0[9]: Copied! <pre># graficar varianza por componente\npercent_variance = np.round(modelo_pca.explained_variance_ratio_* 100, decimals =2)\ncolumns = ['PC1', 'PC2', 'PC3', 'PC4']\n\nplt.figure(figsize=(12,4))\nplt.bar(x= range(1,5), height=percent_variance, tick_label=columns)\nplt.xticks(np.arange(modelo_pca.n_components_) + 1)\n\nplt.ylabel('Componente principal')\nplt.xlabel('Por. varianza explicada')\nplt.title('Porcentaje de varianza explicada por cada componente')\nplt.show()\n</pre> # graficar varianza por componente percent_variance = np.round(modelo_pca.explained_variance_ratio_* 100, decimals =2) columns = ['PC1', 'PC2', 'PC3', 'PC4']  plt.figure(figsize=(12,4)) plt.bar(x= range(1,5), height=percent_variance, tick_label=columns) plt.xticks(np.arange(modelo_pca.n_components_) + 1)  plt.ylabel('Componente principal') plt.xlabel('Por. varianza explicada') plt.title('Porcentaje de varianza explicada por cada componente') plt.show() <p>Ahora realizamos el gr\u00e1fico pero respecto a la suma acumulada.</p> In\u00a0[10]: Copied! <pre># graficar varianza por la suma acumulada de los componente\npercent_variance_cum = np.cumsum(percent_variance)\ncolumns = ['PC1', 'PC1+PC2', 'PC1+PC2+PC3', 'PC1+PC2+PC3+PC4']\n\nplt.figure(figsize=(12,4))\nplt.bar(x= range(1,5), height=percent_variance_cum, tick_label=columns)\nplt.ylabel('Percentate of Variance Explained')\nplt.xlabel('Principal Component Cumsum')\nplt.title('PCA Scree Plot')\nplt.show()\n</pre> # graficar varianza por la suma acumulada de los componente percent_variance_cum = np.cumsum(percent_variance) columns = ['PC1', 'PC1+PC2', 'PC1+PC2+PC3', 'PC1+PC2+PC3+PC4']  plt.figure(figsize=(12,4)) plt.bar(x= range(1,5), height=percent_variance_cum, tick_label=columns) plt.ylabel('Percentate of Variance Explained') plt.xlabel('Principal Component Cumsum') plt.title('PCA Scree Plot') plt.show() <p>Si se empleasen \u00fanicamente las dos primeras componentes se conseguir\u00eda explicar el 87% de la varianza observada.</p> <p>Transformaci\u00f3n</p> <p>Una vez entrenado el modelo, con el m\u00e9todo <code>transform()</code> se puede reducir la dimensionalidad de nuevas observaciones proyect\u00e1ndolas en el espacio definido por las componentes.</p> In\u00a0[11]: Copied! <pre># Proyecci\u00f3n de las observaciones de entrenamiento\n# ==============================================================================\nproyecciones = pca_pipe.transform(X=datos)\nproyecciones = pd.DataFrame(\n    proyecciones,\n    columns = ['PC1', 'PC2', 'PC3', 'PC4'],\n    index   = datos.index\n)\nproyecciones.head()\n</pre> # Proyecci\u00f3n de las observaciones de entrenamiento # ============================================================================== proyecciones = pca_pipe.transform(X=datos) proyecciones = pd.DataFrame(     proyecciones,     columns = ['PC1', 'PC2', 'PC3', 'PC4'],     index   = datos.index ) proyecciones.head() Out[11]: PC1 PC2 PC3 PC4 index Alabama 0.985566 1.133392 -0.444269 0.156267 Alaska 1.950138 1.073213 2.040003 -0.438583 Arizona 1.763164 -0.745957 0.054781 -0.834653 Arkansas -0.141420 1.119797 0.114574 -0.182811 California 2.523980 -1.542934 0.598557 -0.341996 <p>La transformaci\u00f3n es el resultado de multiplicar los vectores que definen cada componente con el valor de las variables. Puede calcularse de forma manual:</p> In\u00a0[12]: Copied! <pre>proyecciones = np.dot(modelo_pca.components_, scale(datos).T)\nproyecciones = pd.DataFrame(proyecciones, index = ['PC1', 'PC2', 'PC3', 'PC4'])\nproyecciones = proyecciones.transpose().set_index(datos.index)\nproyecciones.head()\n</pre> proyecciones = np.dot(modelo_pca.components_, scale(datos).T) proyecciones = pd.DataFrame(proyecciones, index = ['PC1', 'PC2', 'PC3', 'PC4']) proyecciones = proyecciones.transpose().set_index(datos.index) proyecciones.head() Out[12]: PC1 PC2 PC3 PC4 index Alabama 0.985566 1.133392 -0.444269 0.156267 Alaska 1.950138 1.073213 2.040003 -0.438583 Arizona 1.763164 -0.745957 0.054781 -0.834653 Arkansas -0.141420 1.119797 0.114574 -0.182811 California 2.523980 -1.542934 0.598557 -0.341996 <p>Reconstrucci\u00f3n</p> <p>Puede revertirse la transformaci\u00f3n y reconstruir el valor inicial con el m\u00e9todo inverse_transform(). Es importante tener en cuenta que, la reconstrucci\u00f3n, solo ser\u00e1 completa si se han incluido todas las componentes.</p> In\u00a0[13]: Copied! <pre># Recostruccion de las proyecciones\n# ==============================================================================\nrecostruccion = pca_pipe.inverse_transform(proyecciones)\nrecostruccion = pd.DataFrame(\n                    recostruccion,\n                    columns = datos.columns,\n                    index   = datos.index\n)\nprint('------------------')\nprint('Valores originales')\nprint('------------------')\ndisplay(recostruccion.head())\n\nprint('---------------------')\nprint('Valores reconstruidos')\nprint('---------------------')\ndisplay(datos.head())\n</pre> # Recostruccion de las proyecciones # ============================================================================== recostruccion = pca_pipe.inverse_transform(proyecciones) recostruccion = pd.DataFrame(                     recostruccion,                     columns = datos.columns,                     index   = datos.index ) print('------------------') print('Valores originales') print('------------------') display(recostruccion.head())  print('---------------------') print('Valores reconstruidos') print('---------------------') display(datos.head()) <pre>------------------\nValores originales\n------------------\n</pre> Murder Assault UrbanPop Rape index Alabama 13.2 236.0 58.0 21.2 Alaska 10.0 263.0 48.0 44.5 Arizona 8.1 294.0 80.0 31.0 Arkansas 8.8 190.0 50.0 19.5 California 9.0 276.0 91.0 40.6 <pre>---------------------\nValores reconstruidos\n---------------------\n</pre> Murder Assault UrbanPop Rape index Alabama 13.2 236 58 21.2 Alaska 10.0 263 48 44.5 Arizona 8.1 294 80 31.0 Arkansas 8.8 190 50 19.5 California 9.0 276 91 40.6 <p>Figura 2 \u2013 Medici\u00f3n de similitudes por pares en el espacio de alta dimensi\u00f3n 2. El paso 2 es similar al paso 1, pero en lugar de usar una distribuci\u00f3n gaussiana se usa una distribuci\u00f3n t de Student con un grado de libertad, que tambi\u00e9n se conoce como la distribuci\u00f3n de Cauchy (Figura 3). Esto nos da un segundo conjunto de probabilidades ($Q_{ij}$) en el espacio de baja dimensi\u00f3n.</p> <p>Como puede ver, la distribuci\u00f3n t de Student tiene colas m\u00e1s pesadas que la distribuci\u00f3n normal. Las colas pesadas permiten un mejor modelado de distancias muy separadas.</p> <p></p> <p>Figura 3 \u2013 Distribuci\u00f3n noraml vs t-student</p> <ol> <li>El \u00faltimo paso es que queremos que este conjunto de probabilidades del espacio de baja dimensi\u00f3n ($Q_{ij}$) refleje las del espacio de alta dimensi\u00f3n ($P_{ij}$) de la mejor manera posible.</li> </ol> <p>Queremos que las dos estructuras de mapa sean similares. Medimos la diferencia entre las distribuciones de probabilidad de los espacios bidimensionales utilizando la divergencia de Kullback-Liebler (KL).</p> <p>No incluir\u00e9 mucho en KL, excepto que es un enfoque asim\u00e9trico que compara de manera eficiente los grandes valores $P_{ij}$ y $Q_{ij}$. Finalmente, utilizamos el descenso de gradiente para minimizar nuestra funci\u00f3n de costo KL.</p> In\u00a0[14]: Copied! <pre># Load Python Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom time import time\n%matplotlib inline\n</pre> # Load Python Libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from time import time %matplotlib inline In\u00a0[15]: Copied! <pre>from sklearn.datasets import load_digits\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\n</pre> from sklearn.datasets import load_digits from sklearn.manifold import TSNE from sklearn.decomposition import PCA In\u00a0[16]: Copied! <pre>digits = load_digits()\n\ndf = pd.DataFrame(digits['data'])\ndf['label'] = digits['target']\ndf.head()\n</pre> digits = load_digits()  df = pd.DataFrame(digits['data']) df['label'] = digits['target'] df.head() Out[16]: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 label 0 0.0 0.0 5.0 13.0 9.0 1.0 0.0 0.0 0.0 0.0 13.0 15.0 10.0 15.0 5.0 0.0 0.0 3.0 15.0 2.0 0.0 11.0 8.0 0.0 0.0 4.0 12.0 0.0 0.0 8.0 8.0 0.0 0.0 5.0 8.0 0.0 0.0 9.0 8.0 0.0 0.0 4.0 11.0 0.0 1.0 12.0 7.0 0.0 0.0 2.0 14.0 5.0 10.0 12.0 0.0 0.0 0.0 0.0 6.0 13.0 10.0 0.0 0.0 0.0 0 1 0.0 0.0 0.0 12.0 13.0 5.0 0.0 0.0 0.0 0.0 0.0 11.0 16.0 9.0 0.0 0.0 0.0 0.0 3.0 15.0 16.0 6.0 0.0 0.0 0.0 7.0 15.0 16.0 16.0 2.0 0.0 0.0 0.0 0.0 1.0 16.0 16.0 3.0 0.0 0.0 0.0 0.0 1.0 16.0 16.0 6.0 0.0 0.0 0.0 0.0 1.0 16.0 16.0 6.0 0.0 0.0 0.0 0.0 0.0 11.0 16.0 10.0 0.0 0.0 1 2 0.0 0.0 0.0 4.0 15.0 12.0 0.0 0.0 0.0 0.0 3.0 16.0 15.0 14.0 0.0 0.0 0.0 0.0 8.0 13.0 8.0 16.0 0.0 0.0 0.0 0.0 1.0 6.0 15.0 11.0 0.0 0.0 0.0 1.0 8.0 13.0 15.0 1.0 0.0 0.0 0.0 9.0 16.0 16.0 5.0 0.0 0.0 0.0 0.0 3.0 13.0 16.0 16.0 11.0 5.0 0.0 0.0 0.0 0.0 3.0 11.0 16.0 9.0 0.0 2 3 0.0 0.0 7.0 15.0 13.0 1.0 0.0 0.0 0.0 8.0 13.0 6.0 15.0 4.0 0.0 0.0 0.0 2.0 1.0 13.0 13.0 0.0 0.0 0.0 0.0 0.0 2.0 15.0 11.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 12.0 12.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 10.0 8.0 0.0 0.0 0.0 8.0 4.0 5.0 14.0 9.0 0.0 0.0 0.0 7.0 13.0 13.0 9.0 0.0 0.0 3 4 0.0 0.0 0.0 1.0 11.0 0.0 0.0 0.0 0.0 0.0 0.0 7.0 8.0 0.0 0.0 0.0 0.0 0.0 1.0 13.0 6.0 2.0 2.0 0.0 0.0 0.0 7.0 15.0 0.0 9.0 8.0 0.0 0.0 5.0 16.0 10.0 0.0 16.0 6.0 0.0 0.0 4.0 15.0 16.0 13.0 16.0 1.0 0.0 0.0 0.0 0.0 3.0 15.0 10.0 0.0 0.0 0.0 0.0 0.0 2.0 16.0 4.0 0.0 0.0 4 In\u00a0[17]: Copied! <pre># PCA\nscaler = StandardScaler()\n\nX = df.drop(columns='label')\ny = df['label']\n    \nembedding = PCA(n_components=2)\nX_transform = embedding.fit_transform(X)\n    \ndf_pca = pd.DataFrame(X_transform,columns = ['Score1','Score2'])\ndf_pca['label'] = y\n</pre> # PCA scaler = StandardScaler()  X = df.drop(columns='label') y = df['label']      embedding = PCA(n_components=2) X_transform = embedding.fit_transform(X)      df_pca = pd.DataFrame(X_transform,columns = ['Score1','Score2']) df_pca['label'] = y In\u00a0[18]: Copied! <pre># Plot Digits PCA\n\n\n# Set style of scatterplot\nsns.set_context(\"notebook\", font_scale=1.1)\nsns.set_style(\"ticks\")\n\n# Create scatterplot of dataframe\nsns.lmplot(x='Score1',\n           y='Score2',\n           data=df_pca,\n           fit_reg=False,\n           legend=True,\n           height=9,\n           hue='label',\n           scatter_kws={\"s\":200, \"alpha\":0.3})\n\nplt.title('PCA Results: Digits', weight='bold').set_fontsize('14')\nplt.xlabel('Prin Comp 1', weight='bold').set_fontsize('10')\nplt.ylabel('Prin Comp 2', weight='bold').set_fontsize('10')\n</pre> # Plot Digits PCA   # Set style of scatterplot sns.set_context(\"notebook\", font_scale=1.1) sns.set_style(\"ticks\")  # Create scatterplot of dataframe sns.lmplot(x='Score1',            y='Score2',            data=df_pca,            fit_reg=False,            legend=True,            height=9,            hue='label',            scatter_kws={\"s\":200, \"alpha\":0.3})  plt.title('PCA Results: Digits', weight='bold').set_fontsize('14') plt.xlabel('Prin Comp 1', weight='bold').set_fontsize('10') plt.ylabel('Prin Comp 2', weight='bold').set_fontsize('10') <p>Al graficar las dos componentes principales del m\u00e9todo PCA, se observa que no existe una clara distinci\u00f3n entre la distintas clases (solo se ve un gran c\u00famulo de puntos mezclados).</p> In\u00a0[19]: Copied! <pre># tsne\nscaler = StandardScaler()\n\nX = df.drop(columns='label')\ny = df['label']\n    \nembedding = TSNE(n_components=2)\nX_transform = embedding.fit_transform(X)\n    \ndf_tsne = pd.DataFrame(X_transform,columns = ['_DIM_1_','_DIM_2_'])\ndf_tsne['label'] = y\n</pre> # tsne scaler = StandardScaler()  X = df.drop(columns='label') y = df['label']      embedding = TSNE(n_components=2) X_transform = embedding.fit_transform(X)      df_tsne = pd.DataFrame(X_transform,columns = ['_DIM_1_','_DIM_2_']) df_tsne['label'] = y In\u00a0[20]: Copied! <pre># Plot Digits t-SNE\nsns.set_context(\"notebook\", font_scale=1.1)\nsns.set_style(\"ticks\")\n\nsns.lmplot(x='_DIM_1_',\n           y='_DIM_2_',\n           data=df_tsne,\n           fit_reg=False,\n           legend=True,\n           height=9,\n           hue='label',\n           scatter_kws={\"s\":200, \"alpha\":0.3})\n\nplt.title('t-SNE Results: Digits', weight='bold').set_fontsize('14')\nplt.xlabel('Dimension 1', weight='bold').set_fontsize('10')\nplt.ylabel('Dimension 2', weight='bold').set_fontsize('10')\n</pre> # Plot Digits t-SNE sns.set_context(\"notebook\", font_scale=1.1) sns.set_style(\"ticks\")  sns.lmplot(x='_DIM_1_',            y='_DIM_2_',            data=df_tsne,            fit_reg=False,            legend=True,            height=9,            hue='label',            scatter_kws={\"s\":200, \"alpha\":0.3})  plt.title('t-SNE Results: Digits', weight='bold').set_fontsize('14') plt.xlabel('Dimension 1', weight='bold').set_fontsize('10') plt.ylabel('Dimension 2', weight='bold').set_fontsize('10') <p>Para el caso del m\u00e9todo TSNE, se observa una diferenciaci\u00f3n entre los grupos de estudios (aspecto que fue muy distinto al momento de analizar el m\u00e9todo del PCA).</p> <p>Observaci\u00f3n: Si bien se muestra donde el m\u00e9todo TSNE logra ser superior en aspecto de reducci\u00f3n de dimensionalidad que el m\u00e9todo PCA, no significa que para distintos experimientos se tengan los mismo resultados.</p> In\u00a0[21]: Copied! <pre># Librerias\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.decomposition import PCA,TruncatedSVD,NMF\nfrom sklearn.manifold import Isomap,LocallyLinearEmbedding,MDS,SpectralEmbedding,TSNE\n</pre> # Librerias from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split  from sklearn.decomposition import PCA,TruncatedSVD,NMF from sklearn.manifold import Isomap,LocallyLinearEmbedding,MDS,SpectralEmbedding,TSNE In\u00a0[22]: Copied! <pre>from sklearn.metrics import mean_absolute_error, mean_squared_error\n\ndef mean_absolute_percentage_error(y_true, y_pred):\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\ndef regression_metrics(df):\n    \"\"\"\n    Aplicar las distintas m\u00e9tricas definidas\n    :param df: DataFrame con las columnas: ['y', 'yhat']\n    :return: DataFrame con las m\u00e9tricas especificadas\n    \"\"\"\n    df_result = pd.DataFrame()\n\n    y_true = df['y']\n    y_pred = df['yhat']\n\n    df_result['mae'] = [round(mean_absolute_error(y_true, y_pred), 4)]\n    df_result['mse'] = [round(mean_squared_error(y_true, y_pred), 4)]\n    df_result['rmse'] = [round(np.sqrt(mean_squared_error(y_true, y_pred)), 4)]\n    df_result['mape'] = [round(mean_absolute_percentage_error(y_true, y_pred), 4)]\n    df_result['smape'] = [round(2 * mean_absolute_percentage_error(y_true, y_pred) / (mean_absolute_percentage_error(y_true, y_pred) + 100), 4)]\n\n    return df_result\n</pre> from sklearn.metrics import mean_absolute_error, mean_squared_error  def mean_absolute_percentage_error(y_true, y_pred):     return np.mean(np.abs((y_true - y_pred) / y_true)) * 100  def regression_metrics(df):     \"\"\"     Aplicar las distintas m\u00e9tricas definidas     :param df: DataFrame con las columnas: ['y', 'yhat']     :return: DataFrame con las m\u00e9tricas especificadas     \"\"\"     df_result = pd.DataFrame()      y_true = df['y']     y_pred = df['yhat']      df_result['mae'] = [round(mean_absolute_error(y_true, y_pred), 4)]     df_result['mse'] = [round(mean_squared_error(y_true, y_pred), 4)]     df_result['rmse'] = [round(np.sqrt(mean_squared_error(y_true, y_pred)), 4)]     df_result['mape'] = [round(mean_absolute_percentage_error(y_true, y_pred), 4)]     df_result['smape'] = [round(2 * mean_absolute_percentage_error(y_true, y_pred) / (mean_absolute_percentage_error(y_true, y_pred) + 100), 4)]      return df_result In\u00a0[23]: Copied! <pre># Datos\nurl='https://drive.google.com/file/d/1zqoLNbuysK5Idrb9--DFtCoxkapruSDc/view?usp=drive_link'\nurl='https://drive.google.com/uc?id=' + url.split('/')[-2]\n\ndatos = pd.read_csv(url, sep=\",\")\ndatos = datos.drop(columns = datos.columns[0])\ndatos['fat'] = datos['fat'].astype(float)\ndatos.head()\n</pre> # Datos url='https://drive.google.com/file/d/1zqoLNbuysK5Idrb9--DFtCoxkapruSDc/view?usp=drive_link' url='https://drive.google.com/uc?id=' + url.split('/')[-2]  datos = pd.read_csv(url, sep=\",\") datos = datos.drop(columns = datos.columns[0]) datos['fat'] = datos['fat'].astype(float) datos.head() Out[23]: V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20 V21 V22 V23 V24 V25 V26 V27 V28 V29 V30 V31 V32 V33 V34 V35 V36 V37 V38 V39 V40 V41 V42 V43 V44 V45 V46 V47 V48 V49 V50 V51 V52 V53 V54 V55 V56 V57 V58 V59 V60 V61 V62 V63 V64 V65 V66 V67 V68 V69 V70 V71 V72 V73 V74 V75 V76 V77 V78 V79 V80 V81 V82 V83 V84 V85 V86 V87 V88 V89 V90 V91 V92 V93 V94 V95 V96 V97 V98 V99 V100 fat 0 2.61776 2.61814 2.61859 2.61912 2.61981 2.62071 2.62186 2.62334 2.62511 2.62722 2.62964 2.63245 2.63565 2.63933 2.64353 2.64825 2.65350 2.65937 2.66585 2.67281 2.68008 2.68733 2.69427 2.70073 2.70684 2.71281 2.71914 2.72628 2.73462 2.74416 2.75466 2.76568 2.77679 2.78790 2.79949 2.81225 2.82706 2.84356 2.86106 2.87857 2.89497 2.90924 2.92085 2.93015 2.93846 2.94771 2.96019 2.97831 3.00306 3.03506 3.07428 3.11963 3.16868 3.21771 3.26254 3.29988 3.32847 3.34899 3.36342 3.37379 3.38152 3.38741 3.39164 3.39418 3.39490 3.39366 3.39045 3.38541 3.37869 3.37041 3.36073 3.34979 3.33769 3.32443 3.31013 3.29487 3.27891 3.26232 3.24542 3.22828 3.21080 3.19287 3.17433 3.15503 3.13475 3.11339 3.09116 3.06850 3.04596 3.02393 3.00247 2.98145 2.96072 2.94013 2.91978 2.89966 2.87964 2.85960 2.83940 2.81920 22.5 1 2.83454 2.83871 2.84283 2.84705 2.85138 2.85587 2.86060 2.86566 2.87093 2.87661 2.88264 2.88898 2.89577 2.90308 2.91097 2.91953 2.92873 2.93863 2.94929 2.96072 2.97272 2.98493 2.99690 3.00833 3.01920 3.02990 3.04101 3.05345 3.06777 3.08416 3.10221 3.12106 3.13983 3.15810 3.17623 3.19519 3.21584 3.23747 3.25889 3.27835 3.29384 3.30362 3.30681 3.30393 3.29700 3.28925 3.28409 3.28505 3.29326 3.30923 3.33267 3.36251 3.39661 3.43188 3.46492 3.49295 3.51458 3.53004 3.54067 3.54797 3.55306 3.55675 3.55921 3.56045 3.56034 3.55876 3.55571 3.55132 3.54585 3.53950 3.53235 3.52442 3.51583 3.50668 3.49700 3.48683 3.47626 3.46552 3.45501 3.44481 3.43477 3.42465 3.41419 3.40303 3.39082 3.37731 3.36265 3.34745 3.33245 3.31818 3.30473 3.29186 3.27921 3.26655 3.25369 3.24045 3.22659 3.21181 3.19600 3.17942 40.1 2 2.58284 2.58458 2.58629 2.58808 2.58996 2.59192 2.59401 2.59627 2.59873 2.60131 2.60414 2.60714 2.61029 2.61361 2.61714 2.62089 2.62486 2.62909 2.63361 2.63835 2.64330 2.64838 2.65354 2.65870 2.66375 2.66880 2.67383 2.67892 2.68411 2.68937 2.69470 2.70012 2.70563 2.71141 2.71775 2.72490 2.73344 2.74327 2.75433 2.76642 2.77931 2.79272 2.80649 2.82064 2.83541 2.85121 2.86872 2.88905 2.91289 2.94088 2.97325 3.00946 3.04780 3.08554 3.11947 3.14696 3.16677 3.17938 3.18631 3.18924 3.18950 3.18801 3.18498 3.18039 3.17411 3.16611 3.15641 3.14512 3.13241 3.11843 3.10329 3.08714 3.07014 3.05237 3.03393 3.01504 2.99569 2.97612 2.95642 2.93660 2.91667 2.89655 2.87622 2.85563 2.83474 2.81361 2.79235 2.77113 2.75015 2.72956 2.70934 2.68951 2.67009 2.65112 2.63262 2.61461 2.59718 2.58034 2.56404 2.54816 8.4 3 2.82286 2.82460 2.82630 2.82814 2.83001 2.83192 2.83392 2.83606 2.83842 2.84097 2.84374 2.84664 2.84975 2.85307 2.85661 2.86038 2.86437 2.86860 2.87308 2.87789 2.88301 2.88832 2.89374 2.89917 2.90457 2.90991 2.91521 2.92043 2.92565 2.93082 2.93604 2.94128 2.94658 2.95202 2.95777 2.96419 2.97159 2.98045 2.99090 3.00284 3.01611 3.03048 3.04579 3.06194 3.07889 3.09686 3.11629 3.13775 3.16217 3.19068 3.22376 3.26172 3.30379 3.34793 3.39093 3.42920 3.45998 3.48227 3.49687 3.50558 3.51026 3.51221 3.51215 3.51036 3.50682 3.50140 3.49398 3.48457 3.47333 3.46041 3.44595 3.43005 3.41285 3.39450 3.37511 3.35482 3.33376 3.31204 3.28986 3.26730 3.24442 3.22117 3.19757 3.17357 3.14915 3.12429 3.09908 3.07366 3.04825 3.02308 2.99820 2.97367 2.94951 2.92576 2.90251 2.87988 2.85794 2.83672 2.81617 2.79622 5.9 4 2.78813 2.78989 2.79167 2.79350 2.79538 2.79746 2.79984 2.80254 2.80553 2.80890 2.81272 2.81704 2.82184 2.82710 2.83294 2.83945 2.84664 2.85458 2.86331 2.87280 2.88291 2.89335 2.90374 2.91371 2.92305 2.93187 2.94060 2.94986 2.96035 2.97241 2.98606 3.00097 3.01652 3.03220 3.04793 3.06413 3.08153 3.10078 3.12185 3.14371 3.16510 3.18470 3.20140 3.21477 3.22544 3.23505 3.24586 3.26027 3.28063 3.30889 3.34543 3.39019 3.44198 3.49800 3.55407 3.60534 3.64789 3.68011 3.70272 3.71815 3.72863 3.73574 3.74059 3.74357 3.74453 3.74336 3.73991 3.73418 3.72638 3.71676 3.70553 3.69289 3.67900 3.66396 3.64785 3.63085 3.61305 3.59463 3.57582 3.55695 3.53796 3.51880 3.49936 3.47938 3.45869 3.43711 3.41458 3.39129 3.36772 3.34450 3.32201 3.30025 3.27907 3.25831 3.23784 3.21765 3.19766 3.17770 3.15770 3.13753 25.5 <p>El set de datos contiene 101 columnas. Las 100 primeras, nombradas como  $V_1,...,V_{100}$  recogen el valor de absorbancia para cada una de las 100 longitudes de onda analizadas (predictores), y la columna fat el contenido en grasa medido por t\u00e9cnicas qu\u00edmicas (variable respuesta).</p> <p>Muchas de las variables est\u00e1n altamente correlacionadas (correlaci\u00f3n absoluta &gt; 0.8), lo que supone un problema a la hora de emplear modelos de regresi\u00f3n lineal.</p> In\u00a0[24]: Copied! <pre># Correlaci\u00f3n entre columnas num\u00e9ricas\ndef tidy_corr_matrix(corr_mat):\n    '''\n    Funci\u00f3n para convertir una matriz de correlaci\u00f3n de pandas en formato tidy\n    '''\n    corr_mat = corr_mat.stack().reset_index()\n    corr_mat.columns = ['variable_1','variable_2','r']\n    corr_mat = corr_mat.loc[corr_mat['variable_1'] != corr_mat['variable_2'], :]\n    corr_mat['abs_r'] = np.abs(corr_mat['r'])\n    corr_mat = corr_mat.sort_values('abs_r', ascending=False)\n    \n    return(corr_mat)\n\ncorr_matrix = datos.select_dtypes(include=['float64', 'int']) \\\n              .corr(method='pearson')\ndisplay(tidy_corr_matrix(corr_matrix).head(5))\n</pre> # Correlaci\u00f3n entre columnas num\u00e9ricas def tidy_corr_matrix(corr_mat):     '''     Funci\u00f3n para convertir una matriz de correlaci\u00f3n de pandas en formato tidy     '''     corr_mat = corr_mat.stack().reset_index()     corr_mat.columns = ['variable_1','variable_2','r']     corr_mat = corr_mat.loc[corr_mat['variable_1'] != corr_mat['variable_2'], :]     corr_mat['abs_r'] = np.abs(corr_mat['r'])     corr_mat = corr_mat.sort_values('abs_r', ascending=False)          return(corr_mat)  corr_matrix = datos.select_dtypes(include=['float64', 'int']) \\               .corr(method='pearson') display(tidy_corr_matrix(corr_matrix).head(5)) variable_1 variable_2 r abs_r 1019 V11 V10 0.999996 0.999996 919 V10 V11 0.999996 0.999996 1021 V11 V12 0.999996 0.999996 1121 V12 V11 0.999996 0.999996 917 V10 V9 0.999996 0.999996 <p>Se procede aplicar el modelo de regresi\u00f3n lineal .</p> In\u00a0[25]: Copied! <pre># Divisi\u00f3n de los datos en train y test\nX = datos.drop(columns='fat')\ny = datos['fat']\n\nX_train, X_test, y_train, y_test = train_test_split(\n                                        X,\n                                        y.values,\n                                        train_size   = 0.7,\n                                        random_state = 1234,\n                                        shuffle      = True\n                                    )\n</pre> # Divisi\u00f3n de los datos en train y test X = datos.drop(columns='fat') y = datos['fat']  X_train, X_test, y_train, y_test = train_test_split(                                         X,                                         y.values,                                         train_size   = 0.7,                                         random_state = 1234,                                         shuffle      = True                                     ) In\u00a0[26]: Copied! <pre># Creaci\u00f3n y entrenamiento del modelo\nmodelo = LinearRegression()\nmodelo.fit(X = X_train, y = y_train)\n</pre> # Creaci\u00f3n y entrenamiento del modelo modelo = LinearRegression() modelo.fit(X = X_train, y = y_train) Out[26]: <pre>LinearRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression<pre>LinearRegression()</pre> In\u00a0[27]: Copied! <pre># Predicciones test\npredicciones = modelo.predict(X=X_test)\npredicciones = predicciones.flatten()\n\n# Error de test del modelo \ndf_pred = pd.DataFrame({\n    'y':y_test,\n    'yhat':predicciones\n})\n\ndf_summary = regression_metrics(df_pred)\ndf_summary\n</pre> # Predicciones test predicciones = modelo.predict(X=X_test) predicciones = predicciones.flatten()  # Error de test del modelo  df_pred = pd.DataFrame({     'y':y_test,     'yhat':predicciones })  df_summary = regression_metrics(df_pred) df_summary Out[27]: mae mse rmse mape smape 0 2.0904 14.743 3.8397 16.1573 0.2782 <p>Ahora se ocuparan los modelos de reducci\u00f3n de dimensionalidad para entrenar el modelo de regresi\u00f3n lineal. Para ello se ocuparan los distintos algoritmos mencionados. Lo primero es crear una funci\u00f3n que pueda realizar esta tarea de manera autom\u00e1tica.</p> In\u00a0[28]: Copied! <pre># funcion de reduccion de dimensionalidad y aplicacion de la regresion lineal\n\ndef dr_pipeline(df, model_dr):\n    \n    # datos\n    X = df.drop(columns='fat')\n    y = df['fat']\n    \n    # reduccion de la dimensionalidad\n    embedding = model_dr\n    X = embedding.fit_transform(X)\n    \n\n    X_train, X_test, y_train, y_test = train_test_split(\n                                        X,\n                                        y,\n                                        train_size   = 0.7,\n                                        random_state = 1234,\n                                        shuffle      = True\n                                    )\n    \n    # Creaci\u00f3n y entrenamiento del modelo\n    modelo = LinearRegression()\n    modelo.fit(X = X_train, y = y_train)\n\n    \n    # Predicciones test\n    predicciones = modelo.predict(X=X_test)\n    predicciones = predicciones.flatten()\n\n    # Error de test del modelo \n    df_pred = pd.DataFrame({\n        'y':y_test,\n        'yhat':predicciones\n    })\n\n    df_summary = regression_metrics(df_pred)\n    \n    return df_summary\n</pre> # funcion de reduccion de dimensionalidad y aplicacion de la regresion lineal  def dr_pipeline(df, model_dr):          # datos     X = df.drop(columns='fat')     y = df['fat']          # reduccion de la dimensionalidad     embedding = model_dr     X = embedding.fit_transform(X)           X_train, X_test, y_train, y_test = train_test_split(                                         X,                                         y,                                         train_size   = 0.7,                                         random_state = 1234,                                         shuffle      = True                                     )          # Creaci\u00f3n y entrenamiento del modelo     modelo = LinearRegression()     modelo.fit(X = X_train, y = y_train)           # Predicciones test     predicciones = modelo.predict(X=X_test)     predicciones = predicciones.flatten()      # Error de test del modelo      df_pred = pd.DataFrame({         'y':y_test,         'yhat':predicciones     })      df_summary = regression_metrics(df_pred)          return df_summary <p>Enfoque: Algebra lineal</p> In\u00a0[29]: Copied! <pre>modelos_algebra_lineal = [\n    ('PCA',PCA(n_components=5)),\n    ('SVD',TruncatedSVD(n_components=5)),\n    ('NMF',NMF(n_components=5))\n]\n\nnames = [x[0] for x in modelos_algebra_lineal]\nresults = [dr_pipeline(datos,x[1]) for x in modelos_algebra_lineal]\n</pre> modelos_algebra_lineal = [     ('PCA',PCA(n_components=5)),     ('SVD',TruncatedSVD(n_components=5)),     ('NMF',NMF(n_components=5)) ]  names = [x[0] for x in modelos_algebra_lineal] results = [dr_pipeline(datos,x[1]) for x in modelos_algebra_lineal] In\u00a0[30]: Copied! <pre>df_algebra_lineal = pd.concat(results).reset_index(drop=True)\ndf_algebra_lineal['metodo'] =names\ndf_algebra_lineal\n</pre> df_algebra_lineal = pd.concat(results).reset_index(drop=True) df_algebra_lineal['metodo'] =names df_algebra_lineal Out[30]: mae mse rmse mape smape metodo 0 2.8050 12.2999 3.5071 28.6888 0.4459 PCA 1 2.6344 11.3195 3.3645 26.0475 0.4133 SVD 2 6.3295 75.1477 8.6688 68.0810 0.8101 NMF <p>Enfoque: Manifold Learning</p> In\u00a0[31]: Copied! <pre>modelos_manifold= [\n    ('Isomap',Isomap(n_components=5)),\n    ('LocallyLinearEmbedding', LocallyLinearEmbedding(n_components=5)),\n    ('MDS',  MDS(n_components=5)),\n    ('SpectralEmbedding', SpectralEmbedding(n_components=5)),\n    ('TSNE', TSNE(n_components=2)),\n]\n\nnames = [x[0] for x in modelos_manifold]\nresults = [dr_pipeline(datos,x[1]) for x in modelos_manifold]\n\n\ndf_manifold = pd.concat(results).reset_index(drop=True)\ndf_manifold['metodo'] =names\ndf_manifold\n</pre> modelos_manifold= [     ('Isomap',Isomap(n_components=5)),     ('LocallyLinearEmbedding', LocallyLinearEmbedding(n_components=5)),     ('MDS',  MDS(n_components=5)),     ('SpectralEmbedding', SpectralEmbedding(n_components=5)),     ('TSNE', TSNE(n_components=2)), ]  names = [x[0] for x in modelos_manifold] results = [dr_pipeline(datos,x[1]) for x in modelos_manifold]   df_manifold = pd.concat(results).reset_index(drop=True) df_manifold['metodo'] =names df_manifold <pre>C:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\db-connectors-RsEieBu8-py3.8\\lib\\site-packages\\sklearn\\manifold\\_mds.py:299: FutureWarning: The default value of `normalized_stress` will change to `'auto'` in version 1.4. To suppress this warning, manually set the value of `normalized_stress`.\n  warnings.warn(\n</pre> Out[31]: mae mse rmse mape smape metodo 0 9.4021 133.7433 11.5647 99.1430 0.9957 Isomap 1 9.1331 130.6859 11.4318 88.6049 0.9396 LocallyLinearEmbedding 2 7.1867 76.1355 8.7256 83.9630 0.9128 MDS 3 8.9529 130.9545 11.4435 83.3004 0.9089 SpectralEmbedding 4 8.8753 128.9746 11.3567 88.1109 0.9368 TSNE In\u00a0[32]: Copied! <pre>df_manifold.sort_values([\"mae\",\"mape\"])\n</pre> df_manifold.sort_values([\"mae\",\"mape\"]) Out[32]: mae mse rmse mape smape metodo 2 7.1867 76.1355 8.7256 83.9630 0.9128 MDS 4 8.8753 128.9746 11.3567 88.1109 0.9368 TSNE 3 8.9529 130.9545 11.4435 83.3004 0.9089 SpectralEmbedding 1 9.1331 130.6859 11.4318 88.6049 0.9396 LocallyLinearEmbedding 0 9.4021 133.7433 11.5647 99.1430 0.9957 Isomap <p>En este caso en particular, funciona de mejor forma aplicar los m\u00e9todos de descomposici\u00f3n del Algebra Lineal en relaci\u00f3n de los m\u00e9todos de Manifold Learning. La ense\u00f1anza que se lleva de esto que, dependiendo del volumen de datos que se trabaje, la capidad de c\u00f3mputo y las habilidades de programaci\u00f3n suficiente, se pueden probar y automatizar varios de estos m\u00e9todos. Por supuesto, quedar\u00e1 como responsabilidad del programador buscar el criterio para poder seleccionar el mejor m\u00e9todo (dependiendo del caso en estudio).</p>"},{"location":"lectures/machine_learning/ns_02/#no-supervisado-ii","title":"No supervisado II\u00b6","text":""},{"location":"lectures/machine_learning/ns_02/#reduccion-de-dimensionalidad","title":"Reducci\u00f3n de dimensionalidad\u00b6","text":"<p>En aprendizaje autom\u00e1tico y estad\u00edsticas reducci\u00f3n de dimensionalidad  es el proceso de reducci\u00f3n del n\u00famero de variables aleatorias que se trate, y se puede dividir en selecci\u00f3n de funci\u00f3n y extracci\u00f3n de funci\u00f3n</p> <p>Sin embargo, se puede utilizar como un paso de preprocesamiento de transformaci\u00f3n de datos para algoritmos de aprendizaje autom\u00e1tico en conjuntos de datos de modelado predictivo de clasificaci\u00f3n y regresi\u00f3n con algoritmos de aprendizaje supervisado.</p> <p>Hay muchos algoritmos de reducci\u00f3n de dimensionalidad entre los que elegir y no existe el mejor algoritmo para todos los casos. En cambio, es una buena idea explorar una variedad de algoritmos de reducci\u00f3n de dimensionalidad y diferentes configuraciones para cada algoritmo.</p>"},{"location":"lectures/machine_learning/ns_02/#algoritmos-de-reduccion-de-la-dimensionalidad","title":"Algoritmos de reducci\u00f3n de la dimensionalidad\u00b6","text":"<p>Hay muchos algoritmos que pueden ser usados para la reducci\u00f3n de la dimensionalidad.</p> <p>Dos clases principales de m\u00e9todos son los que se extraen del \u00e1lgebra lineal y los que se extraen del aprendizaje m\u00faltiple.</p>"},{"location":"lectures/machine_learning/ns_02/#metodos-de-algebra-lineal","title":"M\u00e9todos de \u00e1lgebra lineal\u00b6","text":"<p>Los m\u00e9todos de factorizaci\u00f3n matricial extra\u00eddos del campo del \u00e1lgebra lineal pueden utilizarse para la dimensionalidad. Algunos de los m\u00e9todos m\u00e1s populares incluyen:</p> <ul> <li>An\u00e1lisis de los componentes principales</li> <li>Descomposici\u00f3n del valor singular</li> <li>Factorizaci\u00f3n de matriz no negativa</li> </ul>"},{"location":"lectures/machine_learning/ns_02/#multiples-metodos-de-aprendizaje","title":"M\u00faltiples m\u00e9todos de aprendizaje\u00b6","text":"<p>Los m\u00faltiples m\u00e9todos de aprendizaje buscan una proyecci\u00f3n de dimensiones inferiores de alta entrada dimensional que capte las propiedades salientes de los datos de entrada.</p> <p>Algunos de los m\u00e9todos m\u00e1s populares incluyen:</p> <ul> <li>Isomap Embedding</li> <li>Locally Linear Embedding</li> <li>Multidimensional Scaling</li> <li>Spectral Embedding</li> <li>t-distributed Stochastic Neighbor Embedding (t-sne)</li> </ul> <p>Cada algoritmo ofrece un enfoque diferente para el desaf\u00edo de descubrir las relaciones naturales en los datos de dimensiones inferiores.</p> <p>No hay un mejor algoritmo de reducci\u00f3n de la dimensionalidad, y no hay una manera f\u00e1cil de encontrar el mejor algoritmo para sus datos sin usar experimentos controlados.</p> <p>Debido a la importancia que se tiene en el mundo del machine lerning, se dar\u00e1 un explicaci\u00f3n formal del m\u00e9todo de PCA y luego se dar\u00e1 una breve rese\u00f1a de los dem\u00e1s m\u00e9todos.</p>"},{"location":"lectures/machine_learning/ns_02/#pca","title":"PCA\u00b6","text":"<p>El an\u00e1lisis de componentes principales (Principal Component Analysis PCA) es un m\u00e9todo de reducci\u00f3n de dimensionalidad que permite simplificar la complejidad de espacios con m\u00faltiples dimensiones a la vez que conserva su informaci\u00f3n.</p> <p>Sup\u00f3ngase que existe una muestra con  $n$  individuos cada uno con  $p$  variables ( $X_1$,...,$X_p$), es decir, el espacio muestral tiene  $p$  dimensiones. PCA permite encontrar un n\u00famero de factores subyacentes  ($z&lt;p$)  que explican aproximadamente lo mismo que las  $p$  variables originales. Donde antes se necesitaban  $p$  valores para caracterizar a cada individuo, ahora bastan  $z$  valores. Cada una de estas  $z$  nuevas variables recibe el nombre de componente principal.</p> <p></p> <p>El m\u00e9todo de PCA permite por lo tanto \"condensar\" la informaci\u00f3n aportada por m\u00faltiples variables en solo unas pocas componentes. Aun as\u00ed, no hay que olvidar que sigue siendo necesario disponer del valor de las variables originales para calcular las componentes. Dos de las principales aplicaciones del PCA son la visualizaci\u00f3n y el preprocesado de predictores previo ajuste de modelos supervisados.</p>"},{"location":"lectures/machine_learning/ns_02/#interpretacion-geometrica-de-las-componentes-principales","title":"Interpretaci\u00f3n geom\u00e9trica de las componentes principales\u00b6","text":"<p>Una forma intuitiva de entender el proceso de PCA es interpretar las componentes principales desde un punto de vista geom\u00e9trico. Sup\u00f3ngase un conjunto de observaciones para las que se dispone de dos variables ( $X_1$, $X_2$ ). El vector que define la primera componente principal ($Z_1$ ) sigue la direcci\u00f3n en la que las observaciones tienen m\u00e1s varianza (l\u00ednea roja). La proyecci\u00f3n de cada observaci\u00f3n sobre esa direcci\u00f3n equivale al valor de la primera componente para dicha observaci\u00f3n (principal component score,  $z_{i1}$ ).</p> <p></p> <p>La segunda componente ( $Z_2$ ) sigue la segunda direcci\u00f3n en la que los datos muestran mayor varianza y que no est\u00e1 correlacionada con la primera componente. La condici\u00f3n de no correlaci\u00f3n entre componentes principales equivale a decir que sus direcciones son perpendiculares/ortogonales.</p> <p></p>"},{"location":"lectures/machine_learning/ns_02/#calculo-de-las-componentes-principales","title":"C\u00e1lculo de las componentes principales\u00b6","text":"<p>Cada componente principal ( $Z_i$ ) se obtiene por combinaci\u00f3n lineal de las variables originales. Se pueden entender como nuevas variables obtenidas al combinar de una determinada forma las variables originales. La primera componente principal de un grupo de variables ( $X_1,...,X_p$ ) es la combinaci\u00f3n lineal normalizada de dichas variables que tiene mayor varianza:</p> <p>$$ Z_1 = \\phi_{11}X_1 + ... + \\phi_{p1}X_p$$</p> <p>Que la combinaci\u00f3n lineal sea normalizada implica que:</p> <p>$$\\sum_{j=1}^p \\phi^2_{j1} = 1$$</p> <p>Los t\u00e9rminos  $\\phi_{11},...,\\phi_{p1}$  reciben en el nombre de loadings y son los que definen las componentes. Por ejemplo,  $\\phi_{11}$  es el loading de la variable  $X_1$  de la primera componente principal. Los loadings pueden interpretarse como el peso/importancia que tiene cada variable en cada componente y, por lo tanto, ayudan a conocer que tipo de informaci\u00f3n recoge cada una de las componentes.</p> <p>Dado un set de datos  $X$  con $n$ observaciones y $p$ variables, el proceso a seguir para calcular la primera componente principal es:</p> <ul> <li><p>Centrar las variables: se resta a cada valor la media de la variable a la que pertenece. Con esto se consigue que todas las variables tengan media cero.</p> </li> <li><p>Se resuelve un problema de optimizaci\u00f3n para encontrar el valor de los loadings con los que se maximiza la varianza. Una forma de resolver esta optimizaci\u00f3n es mediante el c\u00e1lculo de eigenvector-eigenvalue de la matriz de covarianzas.</p> </li> </ul> <p>Una vez calculada la primera componente ( $Z_1$ ), se calcula la segunda ( $Z_2$ ) repitiendo el mismo proceso pero a\u00f1adiendo la condici\u00f3n de que la combinaci\u00f3n lineal no pude estar correlacionada con la primera componente. Esto equivale a decir que  $Z_1$  y  $Z_2$  tienen que ser perpendiculares. EL proceso se repite de forma iterativa hasta calcular todas las posibles componentes (min($n-1, p$)) o hasta que se decida detener el proceso. El orden de importancia de las componentes viene dado por la magnitud del eigenvalue asociado a cada eigenvector.</p>"},{"location":"lectures/machine_learning/ns_02/#caracteristicas-del-pca","title":"Caracter\u00edsticas del PCA\u00b6","text":"<ul> <li>Escalado de las variables: El proceso de PCA identifica las direcciones con mayor varianza.</li> <li>Reproducibilidad de las componentes: El proceso de PCA est\u00e1ndar es determinista, genera siempre las mismas componentes principales, es decir, el valor de los loadings resultantes es el mismo.</li> <li>Influencia de outliers: Al trabajar con varianzas, el m\u00e9todo PCA es muy sensible a outliers, por lo que es recomendable estudiar si los hay. La detecci\u00f3n de valores at\u00edpicos con respecto a una determinada dimensi\u00f3n es algo relativamente sencillo de hacer mediante comprobaciones gr\u00e1ficas.</li> </ul>"},{"location":"lectures/machine_learning/ns_02/#proporcion-de-varianza-explicada","title":"Proporci\u00f3n de varianza explicada\u00b6","text":"<p>Una de las preguntas m\u00e1s frecuentes que surge tras realizar un PCA es: \u00bfCu\u00e1nta informaci\u00f3n presente en el set de datos original se pierde al proyectar las observaciones en un espacio de menor dimensi\u00f3n? o lo que es lo mismo \u00bfCuanta informaci\u00f3n es capaz de capturar cada una de las componentes principales obtenidas? Para contestar a estas preguntas se recurre a la proporci\u00f3n de varianza explicada por cada componente principal.</p> <p>Asumiendo que las variables se han normalizado para tener media cero, la varianza total presente en el set de datos se define como</p> <p>$$\\sum_{j=1}^p Var(X_j) = \\dfrac{1}{n}\\sum_{j=1}^p\\sum_{i=1}^nx_{ij}^2$$</p> <p>y la varianza explicada por la componente m es</p> <p>$$\\dfrac{1}{n}\\sum_{i=1}^n z_{im}^2 = \\dfrac{1}{n}\\sum_{i=1}^n (\\sum_{j=1}^p \\phi_{jm}x_{ij})^2$$</p> <p>Por lo tanto, la proporci\u00f3n de varianza explicada por la componente m viene dada por el ratio</p> <p>$$ \\dfrac{\\sum_{i=1}^n (\\sum_{j=1}^p \\phi_{jm}x_{ij})^2}{\\sum_{j=1}^p\\sum_{i=1}^nx_{ij}^2}$$</p> <p>Tanto la proporci\u00f3n de varianza explicada, como la proporci\u00f3n de varianza explicada acumulada, son dos valores de gran utilidad a la hora de decidir el n\u00famero de componentes principales a utilizar en los an\u00e1lisis posteriores. Si se calculan todas las componentes principales de un set de datos, entonces, aunque transformada, se est\u00e1 almacenando toda la informaci\u00f3n presente en los datos originales. El sumatorio de la proporci\u00f3n de varianza explicada acumulada de todas las componentes es siempre 1.</p>"},{"location":"lectures/machine_learning/ns_02/#numero-optimo-de-componentes-principales","title":"N\u00famero \u00f3ptimo de componentes principales\u00b6","text":"<p>Por lo general, dada una matriz de datos de dimensiones $n \\times p$, el n\u00famero de componentes principales que se pueden calcular es como m\u00e1ximo de $n-1$ o $p$ (el menor de los dos valores es el limitante). Sin embargo, siendo el objetivo del PCA reducir la dimensionalidad, suelen ser de inter\u00e9s utilizar el n\u00famero m\u00ednimo de componentes que resultan suficientes para explicar los datos. No existe una respuesta o m\u00e9todo \u00fanico que permita identificar cual es el n\u00famero \u00f3ptimo de componentes principales a utilizar. Una forma de proceder muy extendida consiste en evaluar la proporci\u00f3n de varianza explicada acumulada y seleccionar el n\u00famero de componentes m\u00ednimo a partir del cual el incremento deja de ser sustancial.</p> <p></p>"},{"location":"lectures/machine_learning/ns_02/#aplicacion","title":"Aplicaci\u00f3n\u00b6","text":"<p>El m\u00e9todo Principal Components Regression PCR consiste en ajustar un modelo de regresi\u00f3n lineal por m\u00ednimos cuadrados empleando como predictores las componentes generadas a partir de un Principal Component Analysis (PCA). De esta forma, con un n\u00famero reducido de componentes se puede explicar la mayor parte de la varianza de los datos.</p> <p>En los estudios observacionales, es frecuente disponer de un n\u00famero elevado de variables que se pueden emplear como predictores, sin embargo, esto no implica necesariamente que se disponga de mucha informaci\u00f3n. Si las variables est\u00e1n correlacionadas entre ellas, la informaci\u00f3n que aportan es redundante y adem\u00e1s, se incumple la condici\u00f3n de no colinealidad necesaria en la regresi\u00f3n por m\u00ednimos cuadrados. Dado que el PCA es \u00fatil eliminando informaci\u00f3n redundante, si se emplean como predictores las componentes principales, se puede mejorar el modelo de regresi\u00f3n. Es importante tener en cuenta que, si bien el Principal Components Regression reduce el n\u00famero de predictores del modelo, no se puede considerar como un m\u00e9todo de selecci\u00f3n de variables ya que todas ellas se necesitan para el c\u00e1lculo de las componentes. La identificaci\u00f3n del n\u00famero \u00f3ptimo de componentes principales que se emplean como predictores en PCR puede identificarse por validaci\u00f3n cruzada.</p> <p>Datos: El set de datos <code>USArrests</code> contiene el porcentaje de asaltos (Assault), asesinatos (Murder) y secuestros (Rape) por cada 100,000 habitantes para cada uno de los 50 estados de USA (1973). Adem\u00e1s, tambi\u00e9n incluye el porcentaje de la poblaci\u00f3n de cada estado que vive en zonas rurales (UrbanPoP).</p>"},{"location":"lectures/machine_learning/ns_02/#t-distributed-stochastic-neighbor-embedding-t-sne","title":"t-Distributed Stochastic Neighbor Embedding (t-SNE)\u00b6","text":"<p>t-Distributed Stochastic Neighbor Embedding (t-SNE)  es una t\u00e9cnica no lineal no supervisada utilizada principalmente para la exploraci\u00f3n de datos y la visualizaci\u00f3n de datos de alta dimensi\u00f3n.</p> <p>En t\u00e9rminos m\u00e1s simples, tSNE le da una sensaci\u00f3n o intuici\u00f3n de c\u00f3mo se organizan los datos en un espacio de alta dimensi\u00f3n. Fue desarrollado por Laurens van der Maatens y Geoffrey Hinton en 2008.</p>"},{"location":"lectures/machine_learning/ns_02/#comparando-con-pca","title":"Comparando con PCA\u00b6","text":"<p>Si est\u00e1 familiarizado con An\u00e1lisis de componentes principales (PCA), entonces como yo , probablemente se est\u00e9 preguntando la diferencia entre PCA y tSNE.</p> <p>Lo primero a tener en cuenta es que PCA se desarroll\u00f3 en 1933, mientras que tSNE se desarroll\u00f3 en 2008. Mucho ha cambiado en el mundo de la ciencia de datos desde 1933, principalmente en el \u00e1mbito del c\u00e1lculo y el tama\u00f1o de los datos.</p> <p>En segundo lugar, PCA es una t\u00e9cnica de reducci\u00f3n de dimensi\u00f3n lineal que busca maximizar la varianza y preserva las distancias pares grandes. En otras palabras, las cosas que son diferentes terminan muy separadas. Esto puede conducir a una visualizaci\u00f3n deficiente, especialmente cuando se trata de estructuras distribuidoras no lineales. Piense en una estructura m\u00faltiple como cualquier forma geom\u00e9trica como: cilindro, bola, curva, etc.</p> <p>tSNE difiere de PCA al preservar solo peque\u00f1as distancias por pares o similitudes locales, mientras que PCA se preocupa por preservar distancias pares grandes para maximizar la varianza.</p> <p>Laurens ilustra bastante bien el enfoque PCA y tSNE utilizando el conjunto de datos Swiss Roll en la Figura 1 [1].</p> <p>Puede ver que debido a la no linealidad de este conjunto de datos de juguete (m\u00faltiple) y la preservaci\u00f3n de grandes distancias, PCA conservar\u00eda incorrectamente la estructura de los datos.</p> <p></p> <p>Figura 1 \u2013 Dataset de rollo suizo. Conservar la distancia peque\u00f1a con tSNE (l\u00ednea continua) frente a la maximizaci\u00f3n de la variaci\u00f3n PCA [1]</p>"},{"location":"lectures/machine_learning/ns_02/#explicacion","title":"Explicaci\u00f3n\u00b6","text":"<p>Ahora que sabemos por qu\u00e9 podr\u00edamos usar tSNE sobre PCA, analicemos c\u00f3mo funciona tSNE. El algoritmo tSNE calcula una medida de similitud entre pares de instancias en el espacio de alta dimensi\u00f3n y en el espacio de baja dimensi\u00f3n. Luego trata de optimizar estas dos medidas de similitud usando una funci\u00f3n de costo. Vamos a dividirlo en 3 pasos b\u00e1sicos.</p> <ol> <li>Paso 1, mide similitudes entre puntos en el espacio de alta dimensi\u00f3n. Piense en un conjunto de puntos de datos dispersos en un espacio 2D (Figura 2).</li> </ol> <p>Para cada punto de datos (xi) centraremos una distribuci\u00f3n Gaussiana sobre ese punto. Luego medimos la densidad de todos los puntos (xj) bajo esa distribuci\u00f3n Gaussiana. Luego renormalize para todos los puntos.</p> <p>Esto nos da un conjunto de probabilidades (Pij) para todos los puntos. Esas probabilidades son proporcionales a las similitudes.</p> <p>Todo lo que eso significa es que si los puntos de datos x1 y x2 tienen valores iguales bajo este c\u00edrculo gaussiano, entonces sus proporciones y similitudes son iguales y, por lo tanto, tienes similitudes locales en la estructura de este espacio de alta dimensi\u00f3n.</p> <p>La distribuci\u00f3n gaussiana o el c\u00edrculo se pueden manipular usando lo que se llama perplejidad, que influye en la varianza de la distribuci\u00f3n (tama\u00f1o del c\u00edrculo) y esencialmente en el n\u00famero de vecinos m\u00e1s cercanos. El rango normal para la perplejidad est\u00e1 entre 5 y 50 [2].</p> <p></p> <p>Figura 2 \u2013 Medici\u00f3n de similitudes por pares en el espacio de alta dimensi\u00f3n</p>"},{"location":"lectures/machine_learning/ns_02/#aplicacion","title":"Aplicaci\u00f3n\u00b6","text":"<p>Laurens van der Maaten menciona el uso de tSNE en \u00e1reas como investigaci\u00f3n del clima, seguridad inform\u00e1tica, bioinform\u00e1tica, investigaci\u00f3n del c\u00e1ncer, etc. tSNE podr\u00eda usarse en datos de alta dimensi\u00f3n y luego el resultado de esas dimensiones se convierte en insumos para alg\u00fan otro modelo de clasificaci\u00f3n .</p> <p>Adem\u00e1s, tSNE podr\u00eda usarse para investigar, aprender o evaluar la segmentaci\u00f3n. Muchas veces seleccionamos la cantidad de segmentos antes del modelado o iteramos despu\u00e9s de los resultados. tSNE a menudo puede mostrar una separaci\u00f3n clara en los datos.</p> <p>Esto se puede usar antes de usar su modelo de segmentaci\u00f3n para seleccionar un n\u00famero de cl\u00faster o despu\u00e9s para evaluar si sus segmentos realmente se mantienen. tSNE, sin embargo, no es un enfoque de agrupamiento, ya que no conserva las entradas como PCA y los valores a menudo pueden cambiar entre ejecuciones, por lo que es pura exploraci\u00f3n.</p> <p>A continuaci\u00f3n se procede a comparar de manera  visual los algoritmos de PCA y tSNE en el conjunto de datos <code>Digits</code> .</p> <p>Datos: El conjunto de datos contiene im\u00e1genes de d\u00edgitos escritos a mano: 10 clases donde cada clase se refiere a un d\u00edgito. Los programas de preprocesamiento puestos a disposici\u00f3n por NIST se utilizaron para extraer mapas de bits normalizados de d\u00edgitos escritos a mano de un formulario preimpreso. De un total de 43 personas, 30 contribuyeron al conjunto de entrenamiento y diferentes 13 al conjunto de prueba. Los mapas de bits de 32x32 se dividen en bloques no superpuestos de 4x4 y se cuenta el n\u00famero de p\u00edxeles en cada bloque. Esto genera una matriz de entrada de 8x8 donde cada elemento es un n\u00famero entero en el rango 0..16. Esto reduce la dimensionalidad y da invariancia a peque\u00f1as distorsiones.</p>"},{"location":"lectures/machine_learning/ns_02/#otros-metodos-de-reduccion-de-dimensionalidad","title":"Otros m\u00e9todos de reducci\u00f3n de dimensionalidad\u00b6","text":"<p>Existen otro m\u00e9todos de reducci\u00f3n de dimencionalidad, a continuaci\u00f3n se deja una referencia con la descripci\u00f3n de cada uno de estos algoritmos.</p> <ul> <li>Descomposici\u00f3n del valor singular </li> <li>Non-Negative Matrix Factorization </li> <li>Isomap Embedding </li> <li>Locally Linear Embedding </li> <li>Multidimensional Scaling </li> <li>Spectral Embedding </li> </ul>"},{"location":"lectures/machine_learning/ns_02/#aplicacion","title":"Aplicaci\u00f3n\u00b6","text":"<p>En este ejemplo se quiere aprovechar las bondades de aplicar la reducci\u00f3n de dimensionalidad para ocupar un modelo de clasificaci\u00f3n (en este caso, el modelo de regresi\u00f3n log\u00edstica). Para ello se ocupar\u00e1 el conjunto de datos <code>meatspec.csv</code></p> <p>Datos: El departamento de calidad de una empresa de alimentaci\u00f3n se encarga de medir el contenido en grasa de la carne que comercializa. Este estudio se realiza mediante t\u00e9cnicas de anal\u00edtica qu\u00edmica, un proceso relativamente costoso en tiempo y recursos. Una alternativa que permitir\u00eda reducir costes y optimizar tiempo es emplear un espectrofot\u00f3metro (instrumento capaz de detectar la absorbancia que tiene un material a diferentes tipos de luz en funci\u00f3n de sus caracter\u00edsticas) e inferir el contenido en grasa a partir de sus medidas.</p> <p>Antes de dar por v\u00e1lida esta nueva t\u00e9cnica, la empresa necesita comprobar qu\u00e9 margen de error tiene respecto al an\u00e1lisis qu\u00edmico. Para ello, se mide el espectro de absorbancia a 100 longitudes de onda en 215 muestras de carne, cuyo contenido en grasa se obtiene tambi\u00e9n por an\u00e1lisis qu\u00edmico, y se entrena un modelo con el objetivo de predecir el contenido en grasa a partir de los valores dados por el espectrofot\u00f3metro.</p>"},{"location":"lectures/machine_learning/ns_02/#referencias","title":"Referencias\u00b6","text":"<ol> <li>In Depth: Principal Component Analysis</li> <li>Unsupervised dimensionality reduction</li> </ol>"},{"location":"lectures/machine_learning/over_01/","title":"Overfitting I","text":"<p>El overfitting ocurre cuando el algoritmo de machine learning captura el ruido de los datos. Intuitivamente, el overfitting ocurre cuando el modelo o el algoritmo se ajusta demasiado bien a los datos. Espec\u00edficamente, el sobreajuste ocurre si el modelo o algoritmo muestra un sesgo bajo pero una varianza alta.</p> <p>El overfitting a menudo es el resultado de un modelo excesivamente complicado, y puede evitarse ajustando m\u00faltiples modelos y utilizando validaci\u00f3n o validaci\u00f3n cruzada para comparar sus precisiones predictivas en los datos de prueba.</p> <p></p> <p>El underfitting ocurre cuando un modelo estad\u00edstico o un algoritmo de machine learning no pueden capturar la tendencia subyacente de los datos. Intuitivamente, el underfitting ocurre cuando el modelo o el algoritmo no se ajustan suficientemente a los datos. Espec\u00edficamente, el underfitting ocurre si el modelo o algoritmo muestra una varianza baja pero un sesgo alto.</p> <p>El underfitting suele ser el resultado de un modelo excesivamente simple.</p> <p></p> <p>\u00bfC\u00f3mo escoger el mejor modelo?</p> <p></p> <ul> <li><p>El sobreajuste va a estar relacionado con la complejidad del modelo, mientras m\u00e1s complejidad le agreguemos, mayor va a ser la tendencia a sobreajuste a los datos.</p> </li> <li><p>No existe una regla general para establecer cual es el nivel ideal de complejidad que le podemos otorgar a nuestro modelo sin caer en el sobreajuste; pero podemos valernos de algunas herramientas anal\u00edticas para intentar entender como el modelo se ajusta a los datos y reconocer el sobreajuste.</p> </li> </ul> <p>Para entender esto, veamos un ejemplo con el m\u00e9todo de \u00e1rboles de decisiones. Los \u00e1rboles de decisi\u00f3n (DT) son un m\u00e9todo de aprendizaje supervisado no param\u00e9trico utilizado para la clasificaci\u00f3n y la regresi\u00f3n.</p> In\u00a0[1]: Copied! <pre># librerias \n\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_classification\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\nimport random\n\nrandom.seed(1982) # semilla\n\n# graficos incrustados\n%matplotlib inline\n\n# parametros esteticos de seaborn\nsns.set_palette(\"deep\", desat=.6)\nsns.set_context(rc={\"figure.figsize\": (12, 4)})\n</pre> # librerias   import pandas as pd import numpy as np  import matplotlib.pyplot as plt  import seaborn as sns  from sklearn.model_selection import train_test_split from sklearn.datasets import make_classification from sklearn.svm import SVC from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor import random  random.seed(1982) # semilla  # graficos incrustados %matplotlib inline  # parametros esteticos de seaborn sns.set_palette(\"deep\", desat=.6) sns.set_context(rc={\"figure.figsize\": (12, 4)}) In\u00a0[2]: Copied! <pre># Create a random dataset\nrng = np.random.RandomState(1)\nX = np.sort(5 * rng.rand(80, 1), axis=0)\ny = np.sin(X).ravel()\ny[::5] += 3 * (0.5 - rng.rand(16))\n\n# separ los datos en train y eval\nx_train, x_eval, y_train, y_eval = train_test_split(X, y, test_size=0.35, \n                                                    train_size=0.65,\n                                                    random_state=1982)\n</pre> # Create a random dataset rng = np.random.RandomState(1) X = np.sort(5 * rng.rand(80, 1), axis=0) y = np.sin(X).ravel() y[::5] += 3 * (0.5 - rng.rand(16))  # separ los datos en train y eval x_train, x_eval, y_train, y_eval = train_test_split(X, y, test_size=0.35,                                                      train_size=0.65,                                                     random_state=1982) In\u00a0[3]: Copied! <pre># Fit regression model\nregr_1 = DecisionTreeRegressor(max_depth=2)\nregr_2 = DecisionTreeRegressor(max_depth=10)\n\nregr_1.fit(x_train,y_train)\nregr_2.fit(x_train,y_train)\n\n# Predict\nX_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\ny_1 = regr_1.predict(X_test)\ny_2 = regr_2.predict(X_test)\ny_3 = regr_1.predict(X_test)\n\n# Plot the results\nfig, ax = plt.subplots(figsize=(11, 8.5))\nplt.scatter(X, y, s=20, edgecolor=\"black\",\n            c=\"darkorange\", label=\"data\")\nplt.plot(X_test, y_1, color=\"cornflowerblue\",\n         label=\"max_depth=2\", linewidth=2)\nplt.plot(X_test, y_2, color=\"yellowgreen\", label=\"max_depth=10\", linewidth=2)\nplt.xlabel(\"data\")\nplt.ylabel(\"target\")\nplt.title(\"Decision Tree Regression\")\nplt.legend()\nplt.show()\n</pre> # Fit regression model regr_1 = DecisionTreeRegressor(max_depth=2) regr_2 = DecisionTreeRegressor(max_depth=10)  regr_1.fit(x_train,y_train) regr_2.fit(x_train,y_train)  # Predict X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis] y_1 = regr_1.predict(X_test) y_2 = regr_2.predict(X_test) y_3 = regr_1.predict(X_test)  # Plot the results fig, ax = plt.subplots(figsize=(11, 8.5)) plt.scatter(X, y, s=20, edgecolor=\"black\",             c=\"darkorange\", label=\"data\") plt.plot(X_test, y_1, color=\"cornflowerblue\",          label=\"max_depth=2\", linewidth=2) plt.plot(X_test, y_2, color=\"yellowgreen\", label=\"max_depth=10\", linewidth=2) plt.xlabel(\"data\") plt.ylabel(\"target\") plt.title(\"Decision Tree Regression\") plt.legend() plt.show() <p>Basado en los gr\u00e1ficos, el modelo de DT con profundidad 2, no se ajuste muy bien a los datos, mientras que el modelo DT con profundidad 10 se ajuste excesivamente demasiado a ellos.</p> <p>Para ver el ajuste de cada modelo, estudiaremos su precisi\u00f3n (score) sobre los conjunto de entrenamiento y de testeo.</p> In\u00a0[4]: Copied! <pre>result  = pd.DataFrame({\n    \n    'model': ['dt_depth_2','dt_depth_10'],\n    'train_score': [ regr_1.score(x_train, y_train), regr_2.score(x_train, y_train)],\n    'test_score': [ regr_1.score(x_eval, y_eval), regr_2.score(x_eval, y_eval)]\n})\nresult\n</pre> result  = pd.DataFrame({          'model': ['dt_depth_2','dt_depth_10'],     'train_score': [ regr_1.score(x_train, y_train), regr_2.score(x_train, y_train)],     'test_score': [ regr_1.score(x_eval, y_eval), regr_2.score(x_eval, y_eval)] }) result Out[4]: model train_score test_score 0 dt_depth_2 0.766363 0.719137 1 dt_depth_10 1.000000 0.661186 <p>Como es de esperar, para el modelo DT con profundidad 10, la precisi\u00f3n sobre el conjunto de entrenamiento es perfecta (igual a 1), no obstante, esta disminuye considerablemente al obtener la presici\u00f3n sobre los datos de testeo (igual a 0.66), por lo que esto es una evidencia para decir que el modelo tiene overfitting.</p> <p>Caso contrario es el modelo  DT con profundidad 2, puesto que es un caso t\u00edpico de underfitting. Cabe destacar que el modelo de underfitting tiene una presici\u00f3n similar tanto para el conjunto de entrenamiento como para el conjunto de testo.</p> <p>Conclusiones del caso</p> <p>Ambos modelos no ajuste de la mejor manera, pero lo hacen de distintas perspectivas. Se debe poner mucho \u00e9nfasis al momento de separar el conjunto de entrenamiento y de testeo, puesto que los resultados se pueden ver altamente sesgado (caso del overfitting). Particularmente para este caso, el ajuste era complejo de realizar puesto que eliminabamos un monto de datos \"significativos\", que hacian que los modelos no captar\u00e1n la continuidad de la funci\u00f3n sinusoidal.</p> In\u00a0[5]: Copied! <pre># Ejemplo en python - \u00e1rboles de decisi\u00f3n\n# dummy data con 100 atributos y 2 clases\nX, y = make_classification(10000, 100, n_informative=3, n_classes=2,\n                          random_state=1982)\n\n# separ los datos en train y eval\nx_train, x_eval, y_train, y_eval = train_test_split(X, y, test_size=0.35, \n                                                    train_size=0.65,\n                                                    random_state=1982)\n\n# Grafico de ajuste del \u00e1rbol de decisi\u00f3n\ntrain_prec =  []\neval_prec = []\nmax_deep_list = list(range(2, 20))\n\n# Entrenar con arboles de distinta profundidad\nfor deep in max_deep_list:\n    model = DecisionTreeClassifier( max_depth=deep)\n    model.fit(x_train, y_train)\n    train_prec.append(model.score(x_train, y_train))\n    eval_prec.append(model.score(x_eval, y_eval))\n</pre> # Ejemplo en python - \u00e1rboles de decisi\u00f3n # dummy data con 100 atributos y 2 clases X, y = make_classification(10000, 100, n_informative=3, n_classes=2,                           random_state=1982)  # separ los datos en train y eval x_train, x_eval, y_train, y_eval = train_test_split(X, y, test_size=0.35,                                                      train_size=0.65,                                                     random_state=1982)  # Grafico de ajuste del \u00e1rbol de decisi\u00f3n train_prec =  [] eval_prec = [] max_deep_list = list(range(2, 20))  # Entrenar con arboles de distinta profundidad for deep in max_deep_list:     model = DecisionTreeClassifier( max_depth=deep)     model.fit(x_train, y_train)     train_prec.append(model.score(x_train, y_train))     eval_prec.append(model.score(x_eval, y_eval)) In\u00a0[6]: Copied! <pre># graficar los resultados.\n\nsns.set(rc={'figure.figsize':(12,9)})\n\ndf1 = pd.DataFrame({'numero_nodos':max_deep_list,\n                   'precision':train_prec,\n                   'datos':'entrenamiento'})\n\ndf2 = pd.DataFrame({'numero_nodos':max_deep_list,\n                   'precision':eval_prec,\n                   'datos':'evaluacion'})\n\ndf_graph = pd.concat([df1,df2])\n\nsns.lineplot(data=df_graph,\n             x='numero_nodos',\n             y='precision',\n             hue='datos',\n             palette=\"Set1\")\n</pre> # graficar los resultados.  sns.set(rc={'figure.figsize':(12,9)})  df1 = pd.DataFrame({'numero_nodos':max_deep_list,                    'precision':train_prec,                    'datos':'entrenamiento'})  df2 = pd.DataFrame({'numero_nodos':max_deep_list,                    'precision':eval_prec,                    'datos':'evaluacion'})  df_graph = pd.concat([df1,df2])  sns.lineplot(data=df_graph,              x='numero_nodos',              y='precision',              hue='datos',              palette=\"Set1\") Out[6]: <pre>&lt;Axes: xlabel='numero_nodos', ylabel='precision'&gt;</pre> <p>El gr\u00e1fico que acabamos de construir se llama gr\u00e1fico de ajuste y muestra la precisi\u00f3n del modelo en funci\u00f3n de su complejidad.</p> <p>El punto con mayor precisi\u00f3n, en los datos de evaluaci\u00f3n, lo obtenemos con un nivel de profundidad de aproximadamente 6 nodos; a partir de all\u00ed el modelo pierde en generalizaci\u00f3n y comienza a estar sobreajustado.</p> <p>Tambi\u00e9n podemos crear un gr\u00e1fico similar con la ayuda de Scikit-learn, utilizando <code>validation_curve</code>.</p> In\u00a0[7]: Copied! <pre># utilizando validation curve de sklearn\nfrom sklearn.model_selection import validation_curve\n\ntrain_prec, eval_prec = validation_curve(estimator=model, X=x_train,\n                                        y=y_train, param_name='max_depth',\n                                        param_range=max_deep_list, cv=5)\n\ntrain_mean = np.mean(train_prec, axis=1)\ntrain_std = np.std(train_prec, axis=1)\ntest_mean = np.mean(eval_prec, axis=1)\ntest_std = np.std(eval_prec, axis=1)\n</pre> # utilizando validation curve de sklearn from sklearn.model_selection import validation_curve  train_prec, eval_prec = validation_curve(estimator=model, X=x_train,                                         y=y_train, param_name='max_depth',                                         param_range=max_deep_list, cv=5)  train_mean = np.mean(train_prec, axis=1) train_std = np.std(train_prec, axis=1) test_mean = np.mean(eval_prec, axis=1) test_std = np.std(eval_prec, axis=1) In\u00a0[8]: Copied! <pre># graficando las curvas\nplt.plot(max_deep_list, train_mean, color='r', marker='o', markersize=5,\n         label='entrenamiento')\nplt.fill_between(max_deep_list, train_mean + train_std, \n                 train_mean - train_std, alpha=0.15, color='r')\nplt.plot(max_deep_list, test_mean, color='b', linestyle='--', \n         marker='s', markersize=5, label='evaluacion')\nplt.fill_between(max_deep_list, test_mean + test_std, \n                 test_mean - test_std, alpha=0.15, color='b')\nplt.legend(loc='center right')\nplt.xlabel('numero_nodos')\nplt.ylabel('precision')\nplt.show()\n</pre> # graficando las curvas plt.plot(max_deep_list, train_mean, color='r', marker='o', markersize=5,          label='entrenamiento') plt.fill_between(max_deep_list, train_mean + train_std,                   train_mean - train_std, alpha=0.15, color='r') plt.plot(max_deep_list, test_mean, color='b', linestyle='--',           marker='s', markersize=5, label='evaluacion') plt.fill_between(max_deep_list, test_mean + test_std,                   test_mean - test_std, alpha=0.15, color='b') plt.legend(loc='center right') plt.xlabel('numero_nodos') plt.ylabel('precision') plt.show()"},{"location":"lectures/machine_learning/over_01/#overfitting-i","title":"Overfitting I\u00b6","text":""},{"location":"lectures/machine_learning/over_01/#ejemplo-con-arboles-de-decision","title":"Ejemplo con  \u00c1rboles de Decisi\u00f3n\u00b6","text":"<p>Los \u00c1rboles de Decisi\u00f3n pueden ser muchas veces una herramienta muy precisa, pero tambi\u00e9n con mucha tendencia al sobreajuste. Para construir estos modelos aplicamos un procedimiento recursivo para encontrar los atributos que nos proporcionan m\u00e1s informaci\u00f3n sobre distintos subconjuntos de datos, cada vez m\u00e1s peque\u00f1os.</p> <p>Si aplicamos este procedimiento en forma reiterada, eventualmente podemos llegar a un \u00e1rbol en el que cada hoja tenga una sola instancia de nuestra variable objetivo a clasificar.</p> <p>En este caso extremo, el \u00c1rbol de Decisi\u00f3n va a tener una pobre generalizaci\u00f3n y estar bastante sobreajustado; ya que cada instancia de los datos de entrenamiento va a encontrar el camino que lo lleve eventualmente a la hoja que lo contiene, alcanzando as\u00ed una precisi\u00f3n del 100% con los datos de entrenamiento.</p>"},{"location":"lectures/machine_learning/over_01/#ejemplo-funcion-sinusoidal","title":"Ejemplo funci\u00f3n sinusoidal\u00b6","text":"<p>Veamos un ejemplo sencillo con la ayuda de python, tratemos de ajustar un modelo de DT sobre una funci\u00f3n senusoidal.</p>"},{"location":"lectures/machine_learning/over_01/#equilibrio-en-el-ajuste-de-modelos","title":"Equilibrio en el ajuste de modelos\u00b6","text":"<p>A continuaci\u00f3n ocuparemos otro conjunto de entrenamientos (make_classification) para mostrar una forma de encoentrar un un equilibrio en la complejidad del modelo y su ajuste a los datos.</p> <p>Siguiendo con el ejemplo de los modelos de \u00e1rbol de decisi\u00f3n, analizaremos la presici\u00f3n (score) para distintas profundidades sobre los distintos conjuntos (entrenamiento y testeo).</p>"},{"location":"lectures/machine_learning/over_01/#referencias","title":"Referencias\u00b6","text":"<ol> <li>Underfitting vs Underfitting</li> <li>Overfitting and Underfitting With Machine Learning Algorithms</li> </ol>"},{"location":"lectures/machine_learning/over_02/","title":"Overfitting II","text":"<p>Veamos un ejemplo en python, ocupando el conjunto de datos make_classification.</p> In\u00a0[1]: Copied! <pre># librerias \n\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_classification\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\nimport random\n\nrandom.seed(1982) # semilla\n\n# graficos incrustados\n%matplotlib inline\n\n# parametros esteticos de seaborn\nsns.set_palette(\"deep\", desat=.6)\nsns.set_context(rc={\"figure.figsize\": (12, 4)})\n</pre> # librerias   import pandas as pd import numpy as np  import matplotlib.pyplot as plt  import seaborn as sns  from sklearn.model_selection import train_test_split from sklearn.datasets import make_classification from sklearn.svm import SVC from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor import random  random.seed(1982) # semilla  # graficos incrustados %matplotlib inline  # parametros esteticos de seaborn sns.set_palette(\"deep\", desat=.6) sns.set_context(rc={\"figure.figsize\": (12, 4)}) In\u00a0[2]: Copied! <pre># Ejemplo en python - \u00e1rboles de decisi\u00f3n\n# dummy data con 100 atributos y 2 clases\nX, y = make_classification(10000, 100, n_informative=3, n_classes=2,\n                          random_state=1982)\n\n# separ los datos en train y eval\nx_train, x_eval, y_train, y_eval = train_test_split(X, y, test_size=0.35, \n                                                    train_size=0.65,\n                                                    random_state=1982)\n\n# Grafico de ajuste del \u00e1rbol de decisi\u00f3n\ntrain_prec =  []\neval_prec = []\nmax_deep_list = list(range(2, 20))\n</pre> # Ejemplo en python - \u00e1rboles de decisi\u00f3n # dummy data con 100 atributos y 2 clases X, y = make_classification(10000, 100, n_informative=3, n_classes=2,                           random_state=1982)  # separ los datos en train y eval x_train, x_eval, y_train, y_eval = train_test_split(X, y, test_size=0.35,                                                      train_size=0.65,                                                     random_state=1982)  # Grafico de ajuste del \u00e1rbol de decisi\u00f3n train_prec =  [] eval_prec = [] max_deep_list = list(range(2, 20)) In\u00a0[3]: Copied! <pre># Ejemplo cross-validation\nfrom sklearn.model_selection import cross_validate,StratifiedKFold\n\n# creando pliegues\n\nskf = StratifiedKFold(n_splits=20)\nprecision = []\nmodel =  DecisionTreeClassifier(criterion='entropy', max_depth=5)\n\nskf.get_n_splits(x_train, y_train)\nfor k, (train_index, test_index) in enumerate(skf.split(X, y)):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    model.fit(X_train,y_train) \n    score = model.score(X_test,y_test)\n    precision.append(score)\n    print('Pliegue: {0:}, Dist Clase: {1:}, Prec: {2:.3f}'.format(k+1,\n                        np.bincount(y_train), score))\n</pre> # Ejemplo cross-validation from sklearn.model_selection import cross_validate,StratifiedKFold  # creando pliegues  skf = StratifiedKFold(n_splits=20) precision = [] model =  DecisionTreeClassifier(criterion='entropy', max_depth=5)  skf.get_n_splits(x_train, y_train) for k, (train_index, test_index) in enumerate(skf.split(X, y)):     X_train, X_test = X[train_index], X[test_index]     y_train, y_test = y[train_index], y[test_index]     model.fit(X_train,y_train)      score = model.score(X_test,y_test)     precision.append(score)     print('Pliegue: {0:}, Dist Clase: {1:}, Prec: {2:.3f}'.format(k+1,                         np.bincount(y_train), score))      <pre>Pliegue: 1, Dist Clase: [4763 4737], Prec: 0.928\nPliegue: 2, Dist Clase: [4763 4737], Prec: 0.914\nPliegue: 3, Dist Clase: [4763 4737], Prec: 0.916\nPliegue: 4, Dist Clase: [4763 4737], Prec: 0.938\nPliegue: 5, Dist Clase: [4763 4737], Prec: 0.924\nPliegue: 6, Dist Clase: [4763 4737], Prec: 0.938\nPliegue: 7, Dist Clase: [4763 4737], Prec: 0.924\nPliegue: 8, Dist Clase: [4762 4738], Prec: 0.938\nPliegue: 9, Dist Clase: [4762 4738], Prec: 0.936\nPliegue: 10, Dist Clase: [4762 4738], Prec: 0.908\nPliegue: 11, Dist Clase: [4762 4738], Prec: 0.936\nPliegue: 12, Dist Clase: [4762 4738], Prec: 0.938\nPliegue: 13, Dist Clase: [4762 4738], Prec: 0.934\nPliegue: 14, Dist Clase: [4762 4738], Prec: 0.922\nPliegue: 15, Dist Clase: [4762 4738], Prec: 0.930\nPliegue: 16, Dist Clase: [4762 4738], Prec: 0.928\nPliegue: 17, Dist Clase: [4762 4738], Prec: 0.924\nPliegue: 18, Dist Clase: [4762 4738], Prec: 0.926\nPliegue: 19, Dist Clase: [4762 4738], Prec: 0.936\nPliegue: 20, Dist Clase: [4762 4738], Prec: 0.920\n</pre> <p>En este ejemplo, utilizamos el iterador <code>StratifiedKFold</code> que nos proporciona Scikit-learn. Este iterador es una versi\u00f3n mejorada de la validaci\u00f3n cruzada, ya que cada pliegue va a estar estratificado para mantener las proporciones entre las clases del conjunto de datos original, lo que suele dar mejores estimaciones del sesgo y la varianza del modelo.</p> <p>Tambi\u00e9n podr\u00edamos utilizar <code>cross_val_score</code> que ya nos proporciona los resultados de la precisi\u00f3n que tuvo el modelo en cada pliegue.</p> In\u00a0[4]: Copied! <pre># Ejemplo con cross_val_score\nfrom sklearn.model_selection import cross_val_score\n\n# separ los datos en train y eval\nx_train, x_eval, y_train, y_eval = train_test_split(X, y, test_size=0.35, \n                                                    train_size=0.65,\n                                                    random_state=1982)\n\n\nmodel = DecisionTreeClassifier(criterion='entropy',\n                               max_depth=5)\n\n\nprecision = cross_val_score(estimator=model,\n                            X=x_train,\n                            y=y_train,\n                            cv=20)\n</pre> # Ejemplo con cross_val_score from sklearn.model_selection import cross_val_score  # separ los datos en train y eval x_train, x_eval, y_train, y_eval = train_test_split(X, y, test_size=0.35,                                                      train_size=0.65,                                                     random_state=1982)   model = DecisionTreeClassifier(criterion='entropy',                                max_depth=5)   precision = cross_val_score(estimator=model,                             X=x_train,                             y=y_train,                             cv=20) In\u00a0[5]: Copied! <pre>precision = [round(x,2) for x in precision]\nprint('Precisiones: {} '.format(precision))\nprint('Precision promedio: {0: .3f} +/- {1: .3f}'.format(np.mean(precision),\n                                          np.std(precision)))\n</pre> precision = [round(x,2) for x in precision] print('Precisiones: {} '.format(precision)) print('Precision promedio: {0: .3f} +/- {1: .3f}'.format(np.mean(precision),                                           np.std(precision))) <pre>Precisiones: [0.93, 0.94, 0.92, 0.94, 0.93, 0.9, 0.92, 0.94, 0.94, 0.93, 0.94, 0.92, 0.91, 0.91, 0.93, 0.94, 0.93, 0.93, 0.92, 0.93] \nPrecision promedio:  0.927 +/-  0.011\n</pre> <p>Para graficar las curvas de aprendizaje es necesario ocupar el comando de sklearn llamado <code>learning_curve</code>.</p> In\u00a0[6]: Copied! <pre># Ejemplo Curvas de aprendizaje\nfrom sklearn.model_selection import  learning_curve\n\ntrain_sizes, train_scores, test_scores = learning_curve(\n                        estimator=model,\n                        X=x_train,\n                        y=y_train, \n                        train_sizes=np.linspace(0.1, 1.0, 20),\n                        cv=10,\n                        n_jobs=-1\n                        )\n\n# calculo de metricas\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n</pre> # Ejemplo Curvas de aprendizaje from sklearn.model_selection import  learning_curve  train_sizes, train_scores, test_scores = learning_curve(                         estimator=model,                         X=x_train,                         y=y_train,                          train_sizes=np.linspace(0.1, 1.0, 20),                         cv=10,                         n_jobs=-1                         )  # calculo de metricas train_mean = np.mean(train_scores, axis=1) train_std = np.std(train_scores, axis=1) test_mean = np.mean(test_scores, axis=1) test_std = np.std(test_scores, axis=1) <p>Veamos que el comando <code>learning_curve</code> va creando conjunto de datos, pero de distintos tama\u00f1os.</p> In\u00a0[7]: Copied! <pre># tamano conjunto de entrenamiento\nfor k in range(len(train_sizes)):\n    print('Tama\u00f1o Conjunto {}: {}'.format(k+1,train_sizes[k]))\n</pre> # tamano conjunto de entrenamiento for k in range(len(train_sizes)):     print('Tama\u00f1o Conjunto {}: {}'.format(k+1,train_sizes[k])) <pre>Tama\u00f1o Conjunto 1: 585\nTama\u00f1o Conjunto 2: 862\nTama\u00f1o Conjunto 3: 1139\nTama\u00f1o Conjunto 4: 1416\nTama\u00f1o Conjunto 5: 1693\nTama\u00f1o Conjunto 6: 1970\nTama\u00f1o Conjunto 7: 2247\nTama\u00f1o Conjunto 8: 2524\nTama\u00f1o Conjunto 9: 2801\nTama\u00f1o Conjunto 10: 3078\nTama\u00f1o Conjunto 11: 3356\nTama\u00f1o Conjunto 12: 3633\nTama\u00f1o Conjunto 13: 3910\nTama\u00f1o Conjunto 14: 4187\nTama\u00f1o Conjunto 15: 4464\nTama\u00f1o Conjunto 16: 4741\nTama\u00f1o Conjunto 17: 5018\nTama\u00f1o Conjunto 18: 5295\nTama\u00f1o Conjunto 19: 5572\nTama\u00f1o Conjunto 20: 5850\n</pre> <p>Finalmente, graficamos las precisiones tanto para el conjunto de entranamiento como de evaluaci\u00f3n para los distintos conjuntos de datos generados.</p> In\u00a0[8]: Copied! <pre># graficando las curvas\nplt.figure(figsize=(12,8))\n\nplt.plot(train_sizes, train_mean, color='r', marker='o', markersize=5,\n         label='entrenamiento')\nplt.fill_between(train_sizes, train_mean + train_std, \n                 train_mean - train_std, alpha=0.15, color='r')\nplt.plot(train_sizes, test_mean, color='b', linestyle='--', \n         marker='s', markersize=5, label='evaluacion')\nplt.fill_between(train_sizes, test_mean + test_std, \n                 test_mean - test_std, alpha=0.15, color='b')\nplt.grid()\nplt.title('Curva de aprendizaje')\nplt.legend(loc='upper right')\nplt.xlabel('Cant de ejemplos de entrenamiento')\nplt.ylabel('Precision')\nplt.show()\n</pre> # graficando las curvas plt.figure(figsize=(12,8))  plt.plot(train_sizes, train_mean, color='r', marker='o', markersize=5,          label='entrenamiento') plt.fill_between(train_sizes, train_mean + train_std,                   train_mean - train_std, alpha=0.15, color='r') plt.plot(train_sizes, test_mean, color='b', linestyle='--',           marker='s', markersize=5, label='evaluacion') plt.fill_between(train_sizes, test_mean + test_std,                   test_mean - test_std, alpha=0.15, color='b') plt.grid() plt.title('Curva de aprendizaje') plt.legend(loc='upper right') plt.xlabel('Cant de ejemplos de entrenamiento') plt.ylabel('Precision') plt.show() <p>En este gr\u00e1fico podemos concluir que:</p> <ul> <li><p>Con pocos datos la precisi\u00f3n entre los datos de entrenamiento y los de evaluaci\u00f3n son muy distintas y luego a medida que la cantidad de datos va aumentando, el modelo puede generalizar mucho mejor y las precisiones se comienzan a emparejar.</p> </li> <li><p>Este gr\u00e1fico tambi\u00e9n puede ser importante a la hora de decidir invertir en la obtenci\u00f3n de m\u00e1s datos, ya que por ejemplo nos indica que a partir las 2500 muestras, el modelo ya no gana mucha m\u00e1s precisi\u00f3n a pesar de obtener m\u00e1s datos.</p> </li> </ul> In\u00a0[9]: Copied! <pre># Ejemplo de grid search con SVM.\nfrom sklearn.model_selection import GridSearchCV\n\n# creaci\u00f3n del modelo\nmodel = DecisionTreeClassifier()\n\n# rango de parametros\nrango_criterion = ['gini','entropy']\nrango_max_depth =np.array( [4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150])\nparam_grid = dict(criterion=rango_criterion, max_depth=rango_max_depth)\nparam_grid\n</pre> # Ejemplo de grid search con SVM. from sklearn.model_selection import GridSearchCV  # creaci\u00f3n del modelo model = DecisionTreeClassifier()  # rango de parametros rango_criterion = ['gini','entropy'] rango_max_depth =np.array( [4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]) param_grid = dict(criterion=rango_criterion, max_depth=rango_max_depth) param_grid Out[9]: <pre>{'criterion': ['gini', 'entropy'],\n 'max_depth': array([  4,   5,   6,   7,   8,   9,  10,  11,  12,  15,  20,  30,  40,\n         50,  70,  90, 120, 150])}</pre> In\u00a0[10]: Copied! <pre># aplicar greed search\n\ngs = GridSearchCV(estimator=model, \n                  param_grid=param_grid, \n                  scoring='accuracy',\n                  cv=5,\n                  n_jobs=-1)\n\ngs = gs.fit(x_train, y_train)\n</pre> # aplicar greed search  gs = GridSearchCV(estimator=model,                    param_grid=param_grid,                    scoring='accuracy',                   cv=5,                   n_jobs=-1)  gs = gs.fit(x_train, y_train) In\u00a0[11]: Copied! <pre># imprimir resultados\nprint(gs.best_score_)\nprint(gs.best_params_)\n</pre> # imprimir resultados print(gs.best_score_) print(gs.best_params_) <pre>0.9332307692307692\n{'criterion': 'entropy', 'max_depth': 6}\n</pre> In\u00a0[12]: Copied! <pre># utilizando el mejor modelo\nmejor_modelo = gs.best_estimator_\nmejor_modelo.fit(x_train, y_train)\nprint('Precisi\u00f3n: {0:.3f}'.format(mejor_modelo.score(x_eval, y_eval)))\n</pre> # utilizando el mejor modelo mejor_modelo = gs.best_estimator_ mejor_modelo.fit(x_train, y_train) print('Precisi\u00f3n: {0:.3f}'.format(mejor_modelo.score(x_eval, y_eval))) <pre>Precisi\u00f3n: 0.939\n</pre> <p>En este ejemplo, primero utilizamos el objeto <code>GridSearchCV</code> que nos permite realizar grid search junto con validaci\u00f3n cruzada, luego comenzamos a ajustar el modelo con las diferentes combinaciones de los valores de los par\u00e1metros <code>criterion</code> y <code>max_depth</code>. Finalmente imprimimos el mejor resultado de precisi\u00f3n y los valores de los par\u00e1metros que utilizamos para obtenerlos; por \u00faltimo utilizamos este mejor modelo para realizar las predicciones con los datos de evaluaci\u00f3n.</p> <p>Podemos ver que la precisi\u00f3n que obtuvimos con los datos de evaluaci\u00f3n es casi id\u00e9ntica a la que nos indic\u00f3 grid search, lo que indica que el modelo generaliza muy bien.</p> <p>Algoritmos para selecci\u00f3n de atributos</p> <p>Podemos encontrar dos clases generales de algoritmos de selecci\u00f3n de atributos: los m\u00e9todos de filtrado, y los m\u00e9todos empaquetados.</p> <ul> <li><p>M\u00e9todos de filtrado:  Estos m\u00e9todos aplican una medida estad\u00edstica para asignar una puntuaci\u00f3n a cada atributo. Los atributos luego son clasificados de acuerdo a su puntuaci\u00f3n y son, o bien seleccionados para su conservaci\u00f3n o eliminados del conjunto de datos. Los m\u00e9todos de filtrado son a menudo univariantes y consideran a cada atributo en forma independiente, o con respecto a la variable dependiente.</p> <ul> <li>Ejemplos : prueba de Chi cuadrado, prueba F de Fisher, ratio de ganancia de informaci\u00f3n y los coeficientes de correlaci\u00f3n.</li> </ul> </li> <li><p>M\u00e9todos empaquetados: Estos m\u00e9todos consideran la selecci\u00f3n de un conjunto de atributos como un problema de b\u00fasqueda, en donde las diferentes combinaciones son evaluadas y comparadas. Para hacer estas evaluaciones se utiliza un modelo predictivo y luego se asigna una puntuaci\u00f3n a cada combinaci\u00f3n basada en la precisi\u00f3n del modelo.</p> <ul> <li>Un ejemplo de este m\u00e9todo es el algoritmo de eliminaci\u00f3n recursiva de atributos.</li> </ul> </li> </ul> <p>Un m\u00e9todo popular en sklearn es el m\u00e9todo SelectKBest, el cual selecciona las  caracter\u00edsticas de acuerdo con las $k$ puntuaciones m\u00e1s altas (de acuerdo al criterio escogido).</p> <p>Para entender este conceptos, transformemos el conjunto de datos anterior a formato pandas DataFrame.</p> In\u00a0[13]: Copied! <pre># Datos\nX, y = make_classification(10000, 100, n_informative=3, n_classes=2,\n                          random_state=1982)\n\ndf = pd.DataFrame(X)\ndf.columns = [f'V{k}' for k in range(1,X.shape[1]+1)]\ndf['y']=y\ndf.head()\n</pre> # Datos X, y = make_classification(10000, 100, n_informative=3, n_classes=2,                           random_state=1982)  df = pd.DataFrame(X) df.columns = [f'V{k}' for k in range(1,X.shape[1]+1)] df['y']=y df.head() Out[13]: V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 ... V92 V93 V94 V95 V96 V97 V98 V99 V100 y 0 0.949283 -1.075706 -0.105733 -0.000047 -0.278974 0.510083 -0.778030 -1.976158 -1.201534 -1.047384 ... -0.630209 -0.331225 -0.202422 -1.786323 1.540031 1.119424 0.507775 -0.848286 -0.027485 1 1 0.183904 0.524554 -1.561357 -1.950628 1.077846 -0.598287 0.153160 -1.206113 0.673170 -0.843770 ... -1.015067 0.319214 0.240570 -2.205400 -0.430933 -0.313175 0.752012 -0.070265 1.390394 0 2 0.499151 -0.625950 2.977037 0.612030 -0.102034 2.076814 1.661343 1.310895 -1.115465 -0.544276 ... 0.311830 -1.130865 0.247865 -0.499241 -1.595737 -0.496805 -0.917257 0.976909 -1.518979 0 3 -0.172063 -0.599516 0.154253 -0.593797 0.931374 0.939714 1.107241 0.146723 -0.446275 0.095896 ... -1.641808 -1.170021 0.815094 -0.722564 -0.263476 -0.715898 1.962313 1.076288 -2.259682 0 4 -0.396408 0.876210 -0.791795 0.999677 0.046859 -0.166211 -0.549437 0.344644 0.349981 -0.207106 ... 1.307020 0.876912 0.882497 -0.704791 -0.743942 -0.075060 0.622693 0.751576 0.907325 0 <p>5 rows \u00d7 101 columns</p> <p>Comencemos con un simple algoritmo univariante que aplica el m\u00e9todo de filtrado. Para esto vamos a utilizar los objetos <code>SelectKBest</code> y <code>f_classif</code> del paquete <code>sklearn.feature_selection</code>.</p> <p>Este algoritmo selecciona a los mejores atributos bas\u00e1ndose en una prueba estad\u00edstica univariante. Al objeto <code>SelectKBest</code> le pasamos la prueba estad\u00edstica que vamos a a aplicar, en este caso una prueba F definida por el objeto <code>f_classif</code>, junto con el n\u00famero de atributos a seleccionar. El algoritmo va a aplicar la prueba a todos los atributos y va a seleccionar los que mejor resultado obtuvieron.</p> In\u00a0[14]: Copied! <pre>from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n</pre> from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import f_classif In\u00a0[15]: Copied! <pre># Separamos las columnas objetivo\nx_training = df.drop(['y',], axis=1)\ny_training = df['y']\n\n# Aplicando el algoritmo univariante de prueba F.\nk = 15  # n\u00famero de atributos a seleccionar\ncolumnas = list(x_training.columns.values)\nseleccionadas = SelectKBest(f_classif, k=k).fit(x_training, y_training)\n</pre> # Separamos las columnas objetivo x_training = df.drop(['y',], axis=1) y_training = df['y']  # Aplicando el algoritmo univariante de prueba F. k = 15  # n\u00famero de atributos a seleccionar columnas = list(x_training.columns.values) seleccionadas = SelectKBest(f_classif, k=k).fit(x_training, y_training) In\u00a0[16]: Copied! <pre>catrib = seleccionadas.get_support()\natributos = [columnas[i] for i in list(catrib.nonzero()[0])]\natributos\n</pre> catrib = seleccionadas.get_support() atributos = [columnas[i] for i in list(catrib.nonzero()[0])] atributos Out[16]: <pre>['V1',\n 'V42',\n 'V46',\n 'V49',\n 'V62',\n 'V64',\n 'V66',\n 'V68',\n 'V69',\n 'V75',\n 'V82',\n 'V86',\n 'V89',\n 'V98',\n 'V100']</pre> <p>Como podemos ver, el algoritmo nos seleccion\u00f3 la cantidad de atributos que le indicamos; en este ejemplo decidimos seleccionar solo 15; obviamente, cuando armemos nuestro modelo final vamos a tomar un n\u00famero mayor de atributos. Ahora se proceder\u00e1 a comparar los resultados de entrenar un modelo en particular con todas las variables y el subconjunto de variables seleccionadas.</p> In\u00a0[23]: Copied! <pre>import time\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n</pre> import time from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression In\u00a0[18]: Copied! <pre>from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n\n# Evaluar las m\u00e9tricas\ndef classification_metrics(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Aplicar las distintas m\u00e9tricas definidas\n    :param df: dataframe con las columnas: ['y', 'yhat']\n    :return: dataframe con las m\u00e9tricas especificadas\n    \"\"\"\n    y_true = df['y']\n    y_pred = df['yhat']\n\n    accuracy = round(accuracy_score(y_true, y_pred), 4)\n    recall = round(recall_score(y_true, y_pred, average='macro'), 4)\n    precision = round(precision_score(y_true, y_pred, average='macro'), 4)\n    fscore = round(f1_score(y_true, y_pred, average='macro'), 4)\n\n    df_result = pd.DataFrame({'accuracy': [accuracy],\n                              'recall': [recall],\n                              'precision': [precision],\n                              'fscore': [fscore]})\n\n    return df_result\n</pre> from sklearn.metrics import confusion_matrix from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score  # Evaluar las m\u00e9tricas def classification_metrics(df: pd.DataFrame) -&gt; pd.DataFrame:     \"\"\"     Aplicar las distintas m\u00e9tricas definidas     :param df: dataframe con las columnas: ['y', 'yhat']     :return: dataframe con las m\u00e9tricas especificadas     \"\"\"     y_true = df['y']     y_pred = df['yhat']      accuracy = round(accuracy_score(y_true, y_pred), 4)     recall = round(recall_score(y_true, y_pred, average='macro'), 4)     precision = round(precision_score(y_true, y_pred, average='macro'), 4)     fscore = round(f1_score(y_true, y_pred, average='macro'), 4)      df_result = pd.DataFrame({'accuracy': [accuracy],                               'recall': [recall],                               'precision': [precision],                               'fscore': [fscore]})      return df_result In\u00a0[24]: Copied! <pre># Record start time\nstart_time = time.time()\n\n# Entrenamiento con todas las variables \nX = df.drop('y', axis=1)\nY = df['y']\n\n# split dataset\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2)\n\n# Creando el modelo\nrlog = LogisticRegression()\nrlog.fit(X_train, Y_train) # ajustando el modelo\n\npredicciones = rlog.predict(X_test)\n\ndf_pred = pd.DataFrame({\n    'y': Y_test,\n    'yhat': predicciones\n})\n\ndf_s1 = classification_metrics(df_pred).assign(name='Todas las variables')\n\n# Calculate the elapsed time\nelapsed_time = time.time() - start_time\nprint(\"Elapsed time:\", elapsed_time, \"seconds\")\n</pre> # Record start time start_time = time.time()  # Entrenamiento con todas las variables  X = df.drop('y', axis=1) Y = df['y']  # split dataset X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2)  # Creando el modelo rlog = LogisticRegression() rlog.fit(X_train, Y_train) # ajustando el modelo  predicciones = rlog.predict(X_test)  df_pred = pd.DataFrame({     'y': Y_test,     'yhat': predicciones })  df_s1 = classification_metrics(df_pred).assign(name='Todas las variables')  # Calculate the elapsed time elapsed_time = time.time() - start_time print(\"Elapsed time:\", elapsed_time, \"seconds\") <pre>Elapsed time: 0.027977705001831055 seconds\n</pre> In\u00a0[25]: Copied! <pre># Record start time\nstart_time = time.time()\n\n# Entrenamiento con las variables seleccionadas\nX = df[atributos]\nY = df['y']\n\n# split dataset\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2)\n\n# Creando el modelo\nrlog = LogisticRegression()\nrlog.fit(X_train, Y_train) # ajustando el modelo\n\npredicciones = rlog.predict(X_test)\n\ndf_pred = pd.DataFrame({\n    'y': Y_test,\n    'yhat': predicciones\n})\n\ndf_s2 = classification_metrics(df_pred).assign(name='Variables Seleccionadas')\n\n# Calculate the elapsed time\nelapsed_time = time.time() - start_time\nprint(\"Elapsed time:\", elapsed_time, \"seconds\")\n</pre> # Record start time start_time = time.time()  # Entrenamiento con las variables seleccionadas X = df[atributos] Y = df['y']  # split dataset X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2)  # Creando el modelo rlog = LogisticRegression() rlog.fit(X_train, Y_train) # ajustando el modelo  predicciones = rlog.predict(X_test)  df_pred = pd.DataFrame({     'y': Y_test,     'yhat': predicciones })  df_s2 = classification_metrics(df_pred).assign(name='Variables Seleccionadas')  # Calculate the elapsed time elapsed_time = time.time() - start_time print(\"Elapsed time:\", elapsed_time, \"seconds\")  <pre>Elapsed time: 0.025048255920410156 seconds\n</pre> <p>Juntando ambos resultados:</p> In\u00a0[26]: Copied! <pre># juntar resultados en formato dataframe\npd.concat([df_s1,df_s2])\n</pre> # juntar resultados en formato dataframe pd.concat([df_s1,df_s2]) Out[26]: accuracy recall precision fscore name 0 0.8905 0.8906 0.8907 0.8905 Todas las variables 0 0.8985 0.8986 0.8986 0.8985 Variables Seleccionadas <p>Las m\u00e9tricas para ambos casos son parecidas y el tiempo de ejecuci\u00f3n del modelo con menos variable resulta ser menor (lo cual era algo esperable). Lo cual nos muestra que trabajando con menos variables, se puede captar las caracter\u00edsticas m\u00e1s relevante del problema, y en la medida que se trabaje con m\u00e1s datos, las mejoras a nivel de capacidad de c\u00f3mputo tendr\u00e1n un mejor desempe\u00f1o.</p>"},{"location":"lectures/machine_learning/over_02/#overfitting-ii","title":"Overfitting II\u00b6","text":"<p>Algunas de las t\u00e9cnicas que podemos utilizar para reducir el overfitting, son:</p> <ul> <li>Recolectar m\u00e1s datos.</li> <li>Introducir una penalizaci\u00f3n a la complejidad con alguna t\u00e9cnica de regularizaci\u00f3n.</li> <li>Utilizar modelos ensamblados.</li> <li>Utilizar validaci\u00f3n cruzada.</li> <li>Optimizar los par\u00e1metros del modelo con grid search.</li> <li>Reducir la dimensi\u00f3n de los datos.</li> <li>Aplicar t\u00e9cnicas de selecci\u00f3n de atributos.</li> </ul> <p>Veremos ejemplos de algunos m\u00e9todos para reducir el sobreajuste (overfitting).</p>"},{"location":"lectures/machine_learning/over_02/#validacion-cruzada","title":"Validaci\u00f3n cruzada\u00b6","text":"<p>La validaci\u00f3n cruzada se inicia mediante el fraccionamiento de un conjunto de datos en un n\u00famero $k$ de particiones (generalmente entre 5 y 10) llamadas pliegues.</p> <p>La validaci\u00f3n cruzada luego itera entre los datos de evaluaci\u00f3n y entrenamiento $k$ veces, de un modo particular. En cada iteraci\u00f3n de la validaci\u00f3n cruzada, un pliegue diferente se elige como los datos de evaluaci\u00f3n. En esta iteraci\u00f3n, los otros pliegues $k-1$ se combinan para formar los datos de entrenamiento. Por lo tanto, en cada iteraci\u00f3n tenemos $(k-1) / k$ de los datos utilizados para el entrenamiento y $1 / k$ utilizado para la evaluaci\u00f3n.</p> <p>Cada iteraci\u00f3n produce un modelo, y por lo tanto una estimaci\u00f3n del rendimiento de la generalizaci\u00f3n, por ejemplo, una estimaci\u00f3n de la precisi\u00f3n. Una vez finalizada la validaci\u00f3n cruzada, todos los ejemplos se han utilizado s\u00f3lo una vez para evaluar pero $k -1$ veces para entrenar. En este punto tenemos estimaciones de rendimiento de todos los pliegues y podemos calcular la media y la desviaci\u00f3n est\u00e1ndar de la precisi\u00f3n del modelo.</p> <p></p>"},{"location":"lectures/machine_learning/over_02/#mas-datos-y-curvas-de-aprendizaje","title":"M\u00e1s datos y curvas de aprendizaje\u00b6","text":"<ul> <li>Muchas veces, reducir el Sobreajuste es tan f\u00e1cil como conseguir m\u00e1s datos, dame m\u00e1s datos y te predecir\u00e9 el futuro!.</li> <li>En la vida real nunca es una tarea tan sencilla conseguir m\u00e1s datos.</li> <li>Una t\u00e9cnica para reducir el sobreajuste son las curvas de aprendizaje, las cuales grafican la precisi\u00f3n en funci\u00f3n del tama\u00f1o de los datos de entrenamiento.</li> </ul>"},{"location":"lectures/machine_learning/over_02/#optimizacion-de-parametros-con-grid-search","title":"Optimizaci\u00f3n de par\u00e1metros con Grid Search\u00b6","text":"<p>La mayor\u00eda de los modelos de Machine Learning cuentan con varios par\u00e1metros para ajustar su comportamiento, por lo tanto, otra alternativa que tenemos para reducir el Sobreajuste es optimizar estos par\u00e1metros por medio de un proceso conocido como grid search e intentar encontrar la combinaci\u00f3n ideal que nos proporcione mayor precisi\u00f3n.</p> <p>El enfoque que utiliza grid search es bastante simple, se trata de una b\u00fasqueda exhaustiva por el paradigma de fuerza bruta en el que se especifica una lista de valores para diferentes par\u00e1metros, y la computadora eval\u00faa el rendimiento del modelo para cada combinaci\u00f3n de \u00e9stos par\u00e1metros para obtener el conjunto \u00f3ptimo que nos brinda el mayor rendimiento.</p> <p></p>"},{"location":"lectures/machine_learning/over_02/#reduccion-de-dimensionalidad","title":"Reducci\u00f3n de dimensionalidad\u00b6","text":"<p>La reducci\u00f3n de dimensiones es frecuentemente usada como una etapa de preproceso en el entrenamiento de sistemas, y consiste en escoger un subconjunto de variables, de tal manera, que el espacio de caracter\u00edsticas quede \u00f3ptimamente reducido de acuerdo a un criterio de evaluaci\u00f3n, cuyo fin es distinguir el subconjunto que representa mejor el espacio inicial de entrenamiento.</p> <p>Como cada caracter\u00edstica que se incluye en el an\u00e1lisis, puede incrementar el costo y el tiempo de proceso de los sistemas, hay una fuerte motivaci\u00f3n para dise\u00f1ar e implementar sistemas con peque\u00f1os conjuntos de caracter\u00edsticas. Sin dejar de lado, que al mismo tiempo, hay una opuesta necesidad de incluir un conjunto suficiente de caracter\u00edsticas para lograr un alto rendimiento.</p> <p>La reducci\u00f3n de dimensionalidad se puede separar en dos tipos: Extracci\u00f3n de atributos y  Selecci\u00f3n de aributos.</p>"},{"location":"lectures/machine_learning/over_02/#extraccion-de-atributos","title":"Extracci\u00f3n de atributos\u00b6","text":"<p>La extracci\u00f3n de atributos comienza a partir de un conjunto inicial de datos medidos y crea valores derivados (caracter\u00edsticas) destinados a ser informativos y no redundantes, lo que facilita los pasos de aprendizaje y generalizaci\u00f3n posteriores, y en algunos casos conduce a a mejores interpretaciones humanas.</p> <p>Cuando los datos de entrada a un algoritmo son demasiado grandes para ser procesados y se sospecha que son redundantes (por ejemplo, la misma medici\u00f3n en pies y metros, o la repetitividad de las im\u00e1genes presentadas como p\u00edxeles), entonces se puede transformar en un conjunto reducido de caracter\u00edsticas (tambi\u00e9n denominado un vector de caracter\u00edsticas).</p> <p>Estos algoritmos fueron analizados con profundidad en la secci\u00f3n de An\u00e1lisis no supervisados - Reducci\u00f3n de la dimensionalidad.</p>"},{"location":"lectures/machine_learning/over_02/#seleccion-de-atributos","title":"Selecci\u00f3n de atributos\u00b6","text":"<p>Proceso por el cual seleccionamos un subconjunto de atributos (representados por cada una de las columnas en un datasetde forma tabular) que son m\u00e1s relevantes para la construcci\u00f3n del modelo predictivo sobre el que estamos trabajando.</p> <p>El objetivo de la selecci\u00f3n de atributos es :</p> <ul> <li>mejorar la capacidad predictiva de nuestro modelo,</li> <li>proporcionando modelos predictivos m\u00e1s r\u00e1pidos y eficientes,</li> <li>proporcionar una mejor comprensi\u00f3n del proceso subyacente que gener\u00f3 los datos.</li> </ul> <p>Los m\u00e9todos de selecci\u00f3n de atributos se pueden utilizar para identificar y eliminar los atributos innecesarios, irrelevantes y redundantes que no contribuyen a la exactitud del modelo predictivo o incluso puedan disminuir su precisi\u00f3n.</p>"},{"location":"lectures/machine_learning/over_02/#referencias","title":"Referencias\u00b6","text":"<ol> <li>K-Fold Cross Validation</li> <li>Cross Validation and Grid Search for Model Selection in Python</li> <li>Feature selection for supervised models using SelectKBest</li> </ol>"},{"location":"lectures/machine_learning/reg_01/","title":"Regresi\u00f3n I","text":"<p>Existen algunas situaciones donde los modelos lineales no son apropiados:</p> <ul> <li>El rango de valores de $Y$ est\u00e1 restringido (ejemplo: datos binarios o de conteos).</li> <li>La varianza de $Y$ depende de la media.</li> </ul> <p>La metodolog\u00eda para encontrar los par\u00e1metros $\\beta$ para el caso de la regresi\u00f3n lineal multiple se extienden de manera natural del modelo de regresi\u00f3n lineal multiple, cuya soluci\u00f3n viene dada por:</p> <p>$$\\beta = (XX^{\\top})^{-1}X^{\\top}y$$</p> <ol> <li><p>M\u00e9tricas absolutas: Las m\u00e9tricas absolutas o no escalada miden el error sin escalar los valores. Las m\u00e9trica absolutas m\u00e1s ocupadas son:</p> <ul> <li>Mean Absolute Error (MAE)</li> </ul> <p>$$\\textrm{MAE}(y,\\hat{y}) = \\dfrac{1}{n}\\sum_{t=1}^{n}\\left | y_{t}-\\hat{y}_{t}\\right |$$</p> <ul> <li>Mean squared error (MSE):</li> </ul> <p>$$\\textrm{MSE}(y,\\hat{y}) =\\dfrac{1}{n}\\sum_{t=1}^{n}\\left ( y_{t}-\\hat{y}_{t}\\right )^2$$</p> </li> </ol> <ol> <li><p>M\u00e9tricas Porcentuales: Las m\u00e9tricas porcentuales o escaladas miden el error de manera escalada, es decir, se busca acotar el error entre valores de 0 a 1, donde 0 significa que el ajuste es perfecto, mientras que 1 ser\u00eda un mal ajuste. Cabe destacar que muchas veces las m\u00e9tricas porcentuales puden tener valores mayores a 1.Las m\u00e9trica Porcentuales m\u00e1s ocupadas son:</p> <ul> <li>Mean absolute percentage error (MAPE):</li> </ul> <p>$$\\textrm{MAPE}(y,\\hat{y}) = \\dfrac{1}{n}\\sum_{t=1}^{n}\\left | \\frac{y_{t}-\\hat{y}_{t}}{y_{t}} \\right |$$</p> <ul> <li>Symmetric mean absolute percentage error (sMAPE):</li> </ul> <p>$$\\textrm{sMAPE}(y,\\hat{y}) = \\dfrac{1}{n}\\sum_{t=1}^{n} \\frac{\\left |y_{t}-\\hat{y}_{t}\\right |}{(\\left | y_{t} \\right |^2+\\left | \\hat{y}_{t} \\right |^2)/2}$$</p> </li> </ol> <p>IMPORTANTE:</p> <ul> <li><p>Cabe destacar que el coeficiente $r^2$ funciona bien en el contexto del mundo de las regresiones lineales. Para el an\u00e1lisis de modelos no lineales, esto coeficiente pierde su interpretaci\u00f3n.</p> </li> <li><p>Se deja la siguiente refrerencia para comprender conceptos claves de test de hip\u00f3tesis, intervalos de confianza, p-valor. Estos t\u00e9rminos son escenciales para comprender la significancia del ajuste realizado.</p> </li> <li><p>Existen muchas m\u00e1s m\u00e9tricas, pero estas son las m\u00e1s usulaes de encontrar. En el archivo metrics.py se definen las distintas m\u00e9tricas presentadas, las cuales serpan de utilidad m\u00e1s adelante.</p> </li> </ul> In\u00a0[1]: Copied! <pre># librerias\n \nimport os\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \npd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes\n\n# Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab\n%matplotlib inline\n</pre> # librerias   import os import numpy as np import pandas as pd  import matplotlib.pyplot as plt import seaborn as sns  pd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes  # Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab %matplotlib inline In\u00a0[2]: Copied! <pre># ejemplo sencillo\n\nn = 100 \nnp.random.seed(n)\n\nbeta = np.array([1,1]) # coeficientes\nx =  np.random.rand(n) # variable independiente\n\nmu, sigma = 0, 0.1 # media y desviacion estandar\nepsilon = np.random.normal(mu, sigma, n) # ruido blanco\n\ny = np.dot(np.c_[ np.ones(n),x]  , beta) + epsilon # variables dependientes\n\n# generar dataframe\ndf = pd.DataFrame({\n    'x':x,\n    'y':y\n})\ndf.head()\n</pre> # ejemplo sencillo  n = 100  np.random.seed(n)  beta = np.array([1,1]) # coeficientes x =  np.random.rand(n) # variable independiente  mu, sigma = 0, 0.1 # media y desviacion estandar epsilon = np.random.normal(mu, sigma, n) # ruido blanco  y = np.dot(np.c_[ np.ones(n),x]  , beta) + epsilon # variables dependientes  # generar dataframe df = pd.DataFrame({     'x':x,     'y':y }) df.head() Out[2]: x y 0 0.543405 1.612417 1 0.278369 1.347058 2 0.424518 1.267849 3 0.844776 1.935274 4 0.004719 1.082601 <p>Grafiquemos los puntos en el plano cartesiano.</p> In\u00a0[3]: Copied! <pre># grafico de puntos\nsns.set(rc={'figure.figsize':(10,8)})\nsns.scatterplot(\n    x='x',\n    y='y',\n    data=df,\n)  \nplt.show()\n</pre> # grafico de puntos sns.set(rc={'figure.figsize':(10,8)}) sns.scatterplot(     x='x',     y='y',     data=df, )   plt.show() <p>Lo primero que debemos hacer es separar nuestro datos en los conjuntos de training set y test set. Concepto de  Train set y Test set</p> <p>Al momento de entrenar los modelos de machine leraning, se debe tener un conjunto para poder entrenar el modelo y otro conjunto para poder evaluar el modelo. Es por esto que el conjunto de datos se separ\u00e1 en dos conjuntos:</p> <ul> <li><p>Train set: Conjunto de entrenamiento con el cual se entrenar\u00e1n los algoritmos de machine learning.</p> </li> <li><p>Test set: Conjunto de testeo para averiguar la confiabilidad del modelo, es decir, cuan bueno es el ajuste del modelo.</p> </li> </ul> <p></p> <p>Tama\u00f1o ideal de cada conjunto</p> <p>La respuesta depende fuertemente del tama\u00f1o del conjunto de datos. A modo de regla emp\u00edrica, se considerar\u00e1 el tama\u00f1o \u00f3ptimo basado en la siguiente tabla:</p> n\u00famero de filas train set test set entre 100-1000 67% 33% entre 1.000- 100.000 80% 20% mayor a 100.000 99% 1% In\u00a0[4]: Copied! <pre>from sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\n# import some data to play with\n\nX = df[['x']] # we only take the first two features.\ny = df['y']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# print rows train and test sets\nprint('Separando informacion:\\n')\nprint('numero de filas data original : ',len(X))\nprint('numero de filas train set     : ',len(X_train))\nprint('numero de filas test set      : ',len(X_test))\n</pre> from sklearn import datasets from sklearn.model_selection import train_test_split  # import some data to play with  X = df[['x']] # we only take the first two features. y = df['y']  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # print rows train and test sets print('Separando informacion:\\n') print('numero de filas data original : ',len(X)) print('numero de filas train set     : ',len(X_train)) print('numero de filas test set      : ',len(X_test)) <pre>Separando informacion:\n\nnumero de filas data original :  100\nnumero de filas train set     :  80\nnumero de filas test set      :  20\n</pre> <p>Existen varias librer\u00edas para poder aplicar modelos de regresi\u00f3n, de los cuales la atenci\u00f3n estar\u00e1 enfocada en las librer\u00edas de <code>statsmodels</code> y <code>sklearn</code>.</p> In\u00a0[5]: Copied! <pre>import statsmodels.api as sm\n\nmodel = sm.OLS(y_train, sm.add_constant(X_train))\nresults = model.fit()\n</pre> import statsmodels.api as sm  model = sm.OLS(y_train, sm.add_constant(X_train)) results = model.fit() <p>En <code>statsmodel</code> existe un comando para ver informaci\u00f3n del modelo en estudio mediante el comando <code>summary</code></p> In\u00a0[6]: Copied! <pre># resultados del modelo\nprint(results.summary())\n</pre> # resultados del modelo print(results.summary()) <pre>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.894\nModel:                            OLS   Adj. R-squared:                  0.893\nMethod:                 Least Squares   F-statistic:                     658.4\nDate:                Sat, 22 Jul 2023   Prob (F-statistic):           8.98e-40\nTime:                        18:28:07   Log-Likelihood:                 69.472\nNo. Observations:                  80   AIC:                            -134.9\nDf Residuals:                      78   BIC:                            -130.2\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.9805      0.021     46.338      0.000       0.938       1.023\nx              1.0099      0.039     25.659      0.000       0.932       1.088\n==============================================================================\nOmnibus:                        0.424   Durbin-Watson:                   1.753\nProb(Omnibus):                  0.809   Jarque-Bera (JB):                0.587\nSkew:                           0.102   Prob(JB):                        0.746\nKurtosis:                       2.633   Cond. No.                         4.17\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n</pre> <p>A continuaci\u00f3n se dara una interpretaci\u00f3n de esta tabla:</p> <p>Descripci\u00f3n del Modelo</p> <p>Estos son estad\u00edsticas relacionadas a la ejecuci\u00f3n del modelo.</p> Variable Descripi\u00f3n Dep. Variable Nombre de la variables dependiente Model Nombre del modelo ocupado Method M\u00e9todo para encontrar los par\u00e1metros \u00f3ptimos Date Fecha de ejecuci\u00f3n No. Observations N\u00famero de observaciones Df Residuals Grados de libertas de los residuos Df Model Grados de libertad del modelo Covariance Type Tipo de covarianza <p>Ajustes del Modelo</p> <p>Estos son estad\u00edsticas relacionadas con la verosimilitud y la confiabilidad del modelo.</p> Variable Descripi\u00f3n R-squared Valor del R-cuadrado Adj. R-squared Valor del R-cuadrado ajustado F-statistic Test para ver si todos los par\u00e1metros son iguales a cero Prob (F-statistic) Probabilidad Asociada al test Log-Likelihood Logaritmo de la funci\u00f3n de verosimilitud AIC Valor del estad\u00edstico AIC BIC Valor del estad\u00edstico BIC <p>En este caso, tanto el r-cuadrado como el r-cuadrado ajustado est\u00e1n cerca del 0.9, se tiene un buen ajuste lineal de los datos. Adem\u00e1s, el test F nos da una probabilidad menor al 0.05, se rechaza la hip\u00f3tess nula que los coeficientes son iguales de cero.</p> <p>Par\u00e1metros del modelo</p> <p>La tabla muestra los valores asociados a los par\u00e1metros del modelo</p> coef std err t P&gt;|t| [0.025 0.975] const 0.9805 0.021 46.338 0.000 0.938 1.023 x 1.0099 0.039 25.659 0.000 0.932 1.088 <p>Ac\u00e1 se tiene:</p> <ul> <li>Variables: Las variables en estudio son <code>const</code> (intercepto) y <code>x</code>.</li> <li>coef: Valor estimado del coeficiente.</li> <li>std err: Desviaci\u00f3n estandar del estimador.</li> <li>t: t = estimate/std error.</li> <li>P&gt;|t|:p-valor individual para cada par\u00e1metro para aceptar o rechazar hip\u00f3tesis nula (par\u00e1metros significativamente distinto de cero).</li> <li>[0.025 | 0.975]: Intervalo de confianza de los par\u00e1metros</li> </ul> <p>En este caso, los valores estimados son cercanos a 1 (algo esperable debido a la simulaci\u00f3n realizadas), adem\u00e1s, se observa que cada uno de los par\u00e1metros es significativamente distinto de cero.</p> <p>Estad\u00edsticos interesantes del modelo</p> Variable Descripci\u00f3n Omnibus Prueba de la asimetr\u00eda y curtosis de los residuos Prob(Omnibus) Probabilidad de que los residuos se distribuyan normalmente Skew Medida de simetr\u00eda de los datos Kurtosis Medida de curvatura de los datos Durbin-Watson Pruebas de homocedasticidad Jarque-Bera (JB) Como la prueba Omnibus, prueba tanto el sesgo como la curtosis. Prob(JB) Probabilidad de que los residuos se distribuyan normalmente Cond. No. N\u00famero de condici\u00f3n. Mide la sensibilidad de la salida de una funci\u00f3n en comparaci\u00f3n con su entrada <p>En este caso:</p> <ul> <li><p>Tanto el test de Omnibus como el test  Jarque-Bera nos arroja una probabilidad cercana a uno, lo cual confirma la hip\u00f3tesis que los residuos se distribuyen de manera normal.</p> </li> <li><p>Para el test de Durbin-Watson, basados en la tablas de valores(tama\u00f1o de la muestra 80 y n\u00famero de variables 2), se tiene que los l\u00edmites para asumir que no existe correlaci\u00f3n en los residuos es de: $[d_u,4-d_u]=[1.66,2.34]$, dado que el valor obtenido (1.753) se encuentra dentro de este rango, se concluye que no hay autocorrelaci\u00f3n de los residuos.</p> </li> <li><p>El n\u00famero de condici\u00f3n es peque\u00f1o (podemos asumir que menor a 30 es un buen resultado) por lo que podemos asumir que no hay colinealidad de los datos.</p> </li> </ul> <p>Ahora, para convencernos de manera visual de los resultados, realicemos un gr\u00e1fico con el ajuste lineal:</p> In\u00a0[7]: Copied! <pre># grafico de puntos\nsns.lmplot(\n    x='x',\n    y='y',\n    data=df,\n    height = 8,\n)  \nplt.show()\n</pre> # grafico de puntos sns.lmplot(     x='x',     y='y',     data=df,     height = 8, )   plt.show() In\u00a0[8]: Copied! <pre># predicciones\ny_pred = results.predict(sm.add_constant(X_test))\n</pre> # predicciones y_pred = results.predict(sm.add_constant(X_test)) <p>Ahora, analizaremos las m\u00e9tricas de error asociado a las predicciones del modelo:</p> In\u00a0[9]: Copied! <pre>from sklearn.metrics import mean_absolute_error, mean_squared_error\n\ndef mean_absolute_percentage_error(y_true, y_pred):\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\ndef regression_metrics(df):\n    \"\"\"\n    Aplicar las distintas m\u00e9tricas definidas\n    :param df: DataFrame con las columnas: ['y', 'yhat']\n    :return: DataFrame con las m\u00e9tricas especificadas\n    \"\"\"\n    df_result = pd.DataFrame()\n\n    y_true = df['y']\n    y_pred = df['yhat']\n\n    df_result['mae'] = [round(mean_absolute_error(y_true, y_pred), 4)]\n    df_result['mse'] = [round(mean_squared_error(y_true, y_pred), 4)]\n    df_result['rmse'] = [round(np.sqrt(mean_squared_error(y_true, y_pred)), 4)]\n    df_result['mape'] = [round(mean_absolute_percentage_error(y_true, y_pred), 4)]\n    df_result['smape'] = [round(2 * mean_absolute_percentage_error(y_true, y_pred) / (mean_absolute_percentage_error(y_true, y_pred) + 100), 4)]\n\n    return df_result\n</pre> from sklearn.metrics import mean_absolute_error, mean_squared_error  def mean_absolute_percentage_error(y_true, y_pred):     return np.mean(np.abs((y_true - y_pred) / y_true)) * 100  def regression_metrics(df):     \"\"\"     Aplicar las distintas m\u00e9tricas definidas     :param df: DataFrame con las columnas: ['y', 'yhat']     :return: DataFrame con las m\u00e9tricas especificadas     \"\"\"     df_result = pd.DataFrame()      y_true = df['y']     y_pred = df['yhat']      df_result['mae'] = [round(mean_absolute_error(y_true, y_pred), 4)]     df_result['mse'] = [round(mean_squared_error(y_true, y_pred), 4)]     df_result['rmse'] = [round(np.sqrt(mean_squared_error(y_true, y_pred)), 4)]     df_result['mape'] = [round(mean_absolute_percentage_error(y_true, y_pred), 4)]     df_result['smape'] = [round(2 * mean_absolute_percentage_error(y_true, y_pred) / (mean_absolute_percentage_error(y_true, y_pred) + 100), 4)]      return df_result In\u00a0[10]: Copied! <pre>from sklearn.metrics import r2_score\n\n# ejemplo \ndf_temp = pd.DataFrame(\n    {\n        'y':y_test,\n        'yhat': y_pred\n        }\n)\n\nprint('\\nMetricas para el regresor consumo_litros_milla:\\n')\nregression_metrics(df_temp)\n</pre> from sklearn.metrics import r2_score  # ejemplo  df_temp = pd.DataFrame(     {         'y':y_test,         'yhat': y_pred         } )  print('\\nMetricas para el regresor consumo_litros_milla:\\n') regression_metrics(df_temp) <pre>\nMetricas para el regresor consumo_litros_milla:\n\n</pre> Out[10]: mae mse rmse mape smape 0 0.1028 0.0171 0.1309 6.7733 0.1269 <p>Funci\u00f3n de Autocorrelaci\u00f3n</p> <p>La funci\u00f3n de autocorrelaci\u00f3n muestra que los residuos se encuentra dentro de la banda de valores cr\u00edticos $(-0.2,0.2)$, concluyendo que no existe correlaci\u00f3n entre los residuos.</p> In\u00a0[11]: Copied! <pre>from statsmodels.graphics.tsaplots import plot_acf\nsns.set(rc={'figure.figsize':(12,8)})\n\n# funcion de autocorrelation\nplot_acf(results.resid)\nplt.show()\n</pre> from statsmodels.graphics.tsaplots import plot_acf sns.set(rc={'figure.figsize':(12,8)})  # funcion de autocorrelation plot_acf(results.resid) plt.show() <p>QQ-plot</p> <p>La gr\u00e1fica de qq-plot nos muestra una comparaci\u00f3n en las distribuci\u00f3n de los residuos respecto a una poblaci\u00f3n con una distribuci\u00f3n normal. En este caso, los puntos (que representan la distribuci\u00f3n de los errores) se encuentran cercana a la recta (distribuci\u00f3n normal), concluyendo que la distribuci\u00f3n de los residuos sigue una distribuci\u00f3n normal.</p> In\u00a0[12]: Copied! <pre>import scipy.stats as stats\nfig = sm.qqplot(results.resid, stats.t, fit=True, line=\"45\")\nplt.show()\n</pre> import scipy.stats as stats fig = sm.qqplot(results.resid, stats.t, fit=True, line=\"45\") plt.show() <p>Histograma</p> <p>Esta es una comparaci\u00f3n directa enntre la distribuci\u00f3n de los residuos versus la distribuci\u00f3n de una variable normal mediante un histograma.</p> In\u00a0[13]: Copied! <pre>df_hist = pd.DataFrame({'error':results.resid})\nsns.histplot(\n    x='error',\n    data=df_hist,\n    kde=True,\n     bins=15\n)  \nplt.show()\n</pre> df_hist = pd.DataFrame({'error':results.resid}) sns.histplot(     x='error',     data=df_hist,     kde=True,      bins=15 )   plt.show() <p>A modo de conclusi\u00f3n, es correcto asumir que los errores siguen la distribuci\u00f3n de un ruido blanco, cumpliendo correctamente con los supuestos de la regresi\u00f3n lineal.</p> In\u00a0[14]: Copied! <pre># ejemplo sencillo\n\nn = 100 \nnp.random.seed(n)\n\nbeta = np.array([1,1]) # coeficientes\nx =  np.random.rand(n) # variable independiente\n</pre> # ejemplo sencillo  n = 100  np.random.seed(n)  beta = np.array([1,1]) # coeficientes x =  np.random.rand(n) # variable independiente  In\u00a0[15]: Copied! <pre>mu, sigma = 0, 0.1 # media y desviacion estandar\nepsilon = np.random.normal(mu, sigma, n) # ruido blanco\n\ny = np.dot(np.c_[ np.ones(n),x]  , beta) + epsilon # variables dependientes\n\ny[:10] = 3.1 # contaminacion\nx[10] = x[10]-1\ny[10]= y[10]-1\nx[11] = x[11] +1\ny[11] = y[11]+1\n\n# etiqueta\noutlier = np.zeros(n)\noutlier[:10] = 1\noutlier[10:12] = 2\n\n# generar dataframe\ndf = pd.DataFrame({\n    'x':x,\n    'y':y,\n    'outlier':outlier\n})\n</pre> mu, sigma = 0, 0.1 # media y desviacion estandar epsilon = np.random.normal(mu, sigma, n) # ruido blanco  y = np.dot(np.c_[ np.ones(n),x]  , beta) + epsilon # variables dependientes  y[:10] = 3.1 # contaminacion x[10] = x[10]-1 y[10]= y[10]-1 x[11] = x[11] +1 y[11] = y[11]+1  # etiqueta outlier = np.zeros(n) outlier[:10] = 1 outlier[10:12] = 2  # generar dataframe df = pd.DataFrame({     'x':x,     'y':y,     'outlier':outlier }) In\u00a0[16]: Copied! <pre># grafico de puntos\nsns.set(rc={'figure.figsize':(10,8)})\nsns.scatterplot(\n    x='x',\n    y='y',\n    hue='outlier',\n    data=df,\n    palette = ['blue','red','black']\n)  \nplt.show()\nplt.show()\n</pre>  # grafico de puntos sns.set(rc={'figure.figsize':(10,8)}) sns.scatterplot(     x='x',     y='y',     hue='outlier',     data=df,     palette = ['blue','red','black'] )   plt.show() plt.show() <p>En este caso, se tiene dos tipos de outliers en este caso:</p> <ul> <li>Significativos: Aquellos outliers que afectan la regresi\u00f3n cambiando la tendencia a este grupo de outliers (puntos rojos).</li> <li>No significativo: Si bien son datos at\u00edpicos  puesto que se encuentran fuera de la nube de puntos, el ajuste de la regresi\u00f3n lineal no se ve afectado (puntos negros).</li> </ul> <p>Veamos el ajuste lineal.</p> In\u00a0[17]: Copied! <pre># grafico de puntos\nsns.lmplot(\n    x='x',\n    y='y',\n    data=df,\n    height = 8,\n)  \nplt.show()\n</pre> # grafico de puntos sns.lmplot(     x='x',     y='y',     data=df,     height = 8, )   plt.show() <p>Otro gr\u00e1fico de inter\u00e9s, es el gr\u00e1fico de influencia, que analiza la distancia de Cook de los residuos.</p> In\u00a0[18]: Copied! <pre># modelos de influencia\nX = df[['x']] # we only take the first two features.\ny = df['y']\nmodel = sm.OLS(y, sm.add_constant(X))\nresults = model.fit()\nsm.graphics.influence_plot(results)\nplt.show()\n</pre> # modelos de influencia X = df[['x']] # we only take the first two features. y = df['y'] model = sm.OLS(y, sm.add_constant(X)) results = model.fit() sm.graphics.influence_plot(results) plt.show() <p>Los puntos grandes se interpretan como puntos que tienen una alta influencia sobre la regresi\u00f3n lineal, mientras aquellos puntos peque\u00f1os tienen una influencia menor.</p> <p>En este caso, la recta se ve fuertemente afectadas por estos valores. Para estos casos se pueden hacer varias cosas:</p> <ul> <li><p>Eliminaci\u00f3n de los outliers: Una vez identificado los outliers (algo que no es tan trivial de identificar para datos multivariables), se puden eliminar y seguir con el paso de modelado.</p> <ul> <li>Ventajas: F\u00e1cil de trabajar la data para los modelos que dependen fuertemente de la media de los datos.</li> <li>Desventajas: Para el caso multivariables no es t\u00e1n trivial encontrar outliers.</li> </ul> </li> <li><p>Modelos m\u00e1s robustos a outliers: Se pueden aplicar otros modelos de regresi\u00f3n cuya estimaci\u00f3n de los par\u00e1metros, no se vea afectado por los valores de outliers.</p> <ul> <li>Ventajas: El an\u00e1lisis se vuelve independiente de los datos.</li> <li>Desventajas: Modelos m\u00e1s costoso computacionalmente y/o m\u00e1s complejos de implementar.</li> </ul> </li> </ul>"},{"location":"lectures/machine_learning/reg_01/#regresion-i","title":"Regresi\u00f3n I\u00b6","text":""},{"location":"lectures/machine_learning/reg_01/#regression-lineal","title":"Regressi\u00f3n Lineal\u00b6","text":"<p>El modelo de regresio\u0301n lineal general o modelo de regresi\u00f3n multiple,  supone que, $\\boldsymbol{Y} =  \\boldsymbol{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon},$ donde:</p> <ul> <li>$\\boldsymbol{X} = (x_1,...,x_n)^{T}$: variable explicativa</li> <li>$\\boldsymbol{Y} = (y_1,...,y_n)^{T}$: variable respuesta</li> <li>$\\boldsymbol{\\epsilon} = (\\epsilon_1,...,\\epsilon_n)^{T}$: error se asume un ruido blanco, es decir, $\\epsilon \\sim \\mathcal{N}( \\boldsymbol{0},\\sigma^2I)$</li> <li>$\\boldsymbol{\\beta} = (\\beta_1,...,\\beta_n)^{T}$: coeficientes de regresio\u0301n.</li> </ul> <p>La idea es tratar de establecer la relaci\u00f3n entre las variables independientes y dependientes por medio de ajustar el mejor hyper plano con respecto a los puntos.</p> <p>Por ejemplo, para el caso de la regresi\u00f3n lineal simple, se tiene la siguiente estructura: $y_i=\\beta_0+\\beta_1x_i+\\epsilon_i.$ En este caso, la regresi\u00f3n lineal corresponder\u00e1 a la recta que mejor pasa por los puntos observados.</p>"},{"location":"lectures/machine_learning/reg_01/#mejores-paremetros-metodo-de-minimos-cudrados","title":"Mejores par\u00e9metros: M\u00e9todo de minimos cudrados\u00b6","text":"<p>El m\u00e9todo de m\u00ednimos cudrados es un m\u00e9todo de optimizaci\u00f3n que busca encontrar la mejor aproximaci\u00f3n mediante la minimizaci\u00f3n de los residuos al cuadrado, es decir, se buscar encontrar:</p> <p>$$(P)\\ \\min \\sum_{i=1}^n e_{i}^2 =\\sum_{i=1}^n (y_{i}-f_{i}(x;\\beta))^2   $$</p> <p>Para el caso de la regresi\u00f3n lineal simple, se busca una funci\u00f3n $$f(x;\\beta) = \\beta_{0} + \\beta_{1}x,$$</p> <p>por lo tanto el problema que se debe resolver es el siguiente:</p> <p>$$(P)\\ \\min \\sum_{i=1}^n e_{i}^2 =\\dfrac{1}{n}\\sum_{i=1}^{n}\\left ( y_{i}-(\\beta_{0} + \\beta_{1}x_{i})\\right )^2$$</p> <p>Lo que significa, que para este problema, se debe encontrar $\\beta = (\\beta_{0},\\beta_{1})$ que minimicen el problema de optimizaci\u00f3n. En este caso la soluci\u00f3n viene dada por:</p> <p>$$\\hat{\\beta}_{1} = \\dfrac{\\sum(x-\\bar{x})(y-\\bar{y})}{\\sum(x-\\bar{x})^2} = \\rho (x,y)\\ ; \\  \\hat{\\beta}_{0} = \\bar{y}-\\hat{\\beta}_{1} \\bar{x} $$</p>"},{"location":"lectures/machine_learning/reg_01/#seleccion-de-modelos","title":"Selecci\u00f3n de modelos\u00b6","text":""},{"location":"lectures/machine_learning/reg_01/#criterio-de-informacion-de-akaike-aic","title":"Criterio de informaci\u00f3n de Akaike (AIC)\u00b6","text":"<p>El criterio de informaci\u00f3n de Akaike (AIC) es una medida de la calidad relativa de un modelo estad\u00edstico, para un conjunto dado de datos. Como tal, el AIC proporciona un medio para la selecci\u00f3n del modelo.</p> <p>AIC maneja un trade-off entre la bondad de ajuste del modelo y la complejidad del modelo. Se basa en la entrop\u00eda de informaci\u00f3n: se ofrece una estimaci\u00f3n relativa de la informaci\u00f3n perdida cuando se utiliza un modelo determinado para representar el proceso que genera los datos.</p> <p>AIC no proporciona una prueba de un modelo en el sentido de probar una hip\u00f3tesis nula, es decir AIC no puede decir nada acerca de la calidad del modelo en un sentido absoluto. Si todos los modelos candidatos encajan mal, AIC no dar\u00e1 ning\u00fan aviso de ello.</p> <p>En el caso general, el AIC es</p> <p>$$AIC = 2k-2\\ln(L)$$</p> <p>donde $k$ es el n\u00famero de par\u00e1metros en el modelo estad\u00edstico , y $L$ es el m\u00e1ximo valor de la funci\u00f3n de verosimilitud para el modelo estimado.</p>"},{"location":"lectures/machine_learning/reg_01/#criterio-de-informacion-bayesiano-bic","title":"Criterio de informaci\u00f3n bayesiano (BIC)\u00b6","text":"<p>En estad\u00edstica, el criterio de informaci\u00f3n bayesiano (BIC) o el m\u00e1s general criterio de Schwarz (SBC tambi\u00e9n, SBIC) es un criterio para la selecci\u00f3n de modelos entre un conjunto finito de modelos. Se basa, en parte, de la funci\u00f3n de probabilidad y que est\u00e1 estrechamente relacionado con el Criterio de Informaci\u00f3n de Akaike (AIC).</p> <p>Cuando el ajuste de modelos, es posible aumentar la probabilidad mediante la adici\u00f3n de par\u00e1metros, pero si lo hace puede resultar en sobreajuste. Tanto el BIC y AIC resuelven este problema mediante la introducci\u00f3n de un t\u00e9rmino de penalizaci\u00f3n para el n\u00famero de par\u00e1metros en el modelo, el t\u00e9rmino de penalizaci\u00f3n es mayor en el BIC que en el AIC.</p> <p>El BIC fue desarrollado por Gideon E. Schwarz, quien dio un argumento bayesiano a favor de su adopci\u00f3n.1\u200b Akaike tambi\u00e9n desarroll\u00f3 su propio formalismo Bayesiano, que ahora se conoce como la ABIC por Criterio de Informaci\u00f3n Bayesiano de Akaike \"</p> <p>En el caso general, el BIC es</p> <p>$$BIC =k\\ln(n)-2\\ln(L)$$</p> <p>donde $k$ es el n\u00famero de par\u00e1metros en el modelo estad\u00edstico, $n$ es la cantidad de datos disponibles y $L$ es el m\u00e1ximo valor de la funci\u00f3n de verosimilitud para el modelo estimado.</p>"},{"location":"lectures/machine_learning/reg_01/#r-cuadrado","title":"R-cuadrado\u00b6","text":"<p>El coeficiente de determinaci\u00f3n o R-cuadrado ($r^2$ ) , es un estad\u00edstico usado en el contexto de un modelo estad\u00edstico cuyo principal prop\u00f3sito es predecir futuros resultados o probar una hip\u00f3tesis. El coeficiente determina la calidad del modelo para replicar los resultados, y la proporci\u00f3n de variaci\u00f3n de los resultados que puede explicarse por el modelo.</p> <p>El valor del $r^2$ habitualmente entre 0 y 1, donde 0 significa una mala calidad de ajuste en el modelo y 1 corresponde a un ajuste lineal perfecto. A menudo, este estad\u00edstico es ocupado para modelos lineales.</p> <p>Se define por la f\u00f3rmula:</p> <p>$$r^2 = \\dfrac{SS_{reg}}{SS_{tot}} = 1 - \\dfrac{SS_{res}}{SS_{tot}},$$</p> <p>donde:</p> <ul> <li><p>$SS_{reg}$ ( suma explicada de cuadrados (ESS)): $\\sum_{i}(\\hat{y}-\\bar{y})^2$</p> </li> <li><p>$SS_{res}$: ( suma residual de cuadrados (RSS)): $\\sum_{i}(y_{i}-\\hat{y})^2 = \\sum_{i}e_{i}^2$</p> </li> <li><p>$SS_{tot}$: ( varianza): $\\sum_{i}(y_{i}-\\bar{y})$, donde: $SS_{tot}=SS_{reg}+SS_{res}$</p> </li> </ul> <p>En una forma general, se puede ver que $r^2$ est\u00e1 relacionado con la fracci\u00f3n de varianza inexplicada (FVU), ya que el segundo t\u00e9rmino compara la varianza inexplicada (varianza de los errores del modelo) con la varianza total (de los datos).</p> <p></p> <ul> <li><p>Las \u00e1reas de los cuadrados azules representan los residuos cuadrados con respecto a la regresi\u00f3n lineal ($SS_{tot}$).</p> </li> <li><p>Las \u00e1reas de los cuadrados rojos representan los residuos al cuadrado con respecto al valor promedio ($SS_{res}$).</p> </li> </ul> <p>Por otro lado, a medida que m\u00e1s variables explicativas se agregan al modelo, el $r^2$ aumenta de forma autom\u00e1tica, es decir, entre m\u00e1s variables explicativas se agreguen, mejor ser\u00e1 la calidad ser\u00e1 el ajuste (un falso argumento).</p> <p>Es por ello que se define el R cuadrado ajustado, que viene a ser  una modificaci\u00f3n del $r^2$, ajustando por el n\u00famero de variables explicativas en un modelo ($p$) en relaci\u00f3n con el n\u00famero de puntos de datos ($n$).</p> <p>$$r^2_{ajustado} = 1-(1-r^2)\\dfrac{n-1}{n-p-1} ,$$</p>"},{"location":"lectures/machine_learning/reg_01/#error-de-un-modelo","title":"Error de un modelo\u00b6","text":""},{"location":"lectures/machine_learning/reg_01/#definicion","title":"Definici\u00f3n\u00b6","text":"<p>El error corresponde a la diferencia entre el valor original y el valor predicho,es decir:</p> <p>$$e_{i}=y_{i}-\\hat{y}_{i} $$</p> <p></p>"},{"location":"lectures/machine_learning/reg_01/#formas-de-medir-el-error-de-un-modelo","title":"Formas de medir el error de un modelo\u00b6","text":"<p>Para medir el ajuste de un modelo se ocupan las denominadas funciones de distancias o m\u00e9tricas. Existen varias m\u00e9tricas, dentro de las cuales encontramos:</p>"},{"location":"lectures/machine_learning/reg_01/#otros-estadisticos-interesantes-del-modelo","title":"Otros estad\u00edsticos interesantes del modelo\u00b6","text":""},{"location":"lectures/machine_learning/reg_01/#test-f","title":"Test F\u00b6","text":"<p>EL test F para regresi\u00f3n lineal prueba si alguna de las variables independientes en un modelo de regresi\u00f3n lineal m\u00faltiple es significativa.</p> <p>En t\u00e9rminos de test de hip\u00f3tesis, se quiere contrastar lo siguiente:</p> <ul> <li>$H_0: \\beta_1 = \\beta_2 = ... = \\beta_{p-1} = 0$</li> <li>$H_1: \\beta_j \u2260 0$, para al menos un valor de $j$</li> </ul>"},{"location":"lectures/machine_learning/reg_01/#test-omnibus","title":"Test Omnibus\u00b6","text":"<p>EL test Omnibusesta relacionado con la simetr\u00eda y curtosis del resido. Se espera ver un valor cercano a cero que indicar\u00eda normalidad. El Prob (Omnibus) realiza una prueba estad\u00edstica que indica la probabilidad de que los residuos se distribuyan normalmente.</p>"},{"location":"lectures/machine_learning/reg_01/#test-durbin-watson","title":"Test Durbin-Watson\u00b6","text":"<p>El Test Durbin-Watson es un test de homocedasticidad. Para ver los l\u00edmites relacionados de este test, se puede consultar la siguiente tablas de valores.</p>"},{"location":"lectures/machine_learning/reg_01/#test-jarque-bera","title":"Test Jarque-Bera\u00b6","text":"<p>Como el test Omnibus en que prueba tanto el sesgo como la curtosis. Esperamos ver en esta prueba una confirmaci\u00f3n de la prueba Omnibus.</p>"},{"location":"lectures/machine_learning/reg_01/#aplicacion-con-python","title":"Aplicaci\u00f3n con python\u00b6","text":""},{"location":"lectures/machine_learning/reg_01/#ejemplo-sencillo","title":"Ejemplo sencillo\u00b6","text":"<p>Para comprender los modelos de regresi\u00f3n lineal, mostraremos un caso sencillo de uso. Para ello realizaremos un simulaci\u00f3n de una recta, en el cual le agregaremos un ruido blanco.</p>"},{"location":"lectures/machine_learning/reg_01/#ejemplo-con-statsmodel","title":"Ejemplo con Statsmodel\u00b6","text":"<p>Para trabajar los modelos de <code>statsmodel</code>, basta con instanciar el comando <code>OLS</code>. El modelo no considera intercepto, por lo tanto, para agregar el intercepto, a las variables independientes se le debe agregar un vector de unos (tanto para el conjunto de entranamiento como de testeo).</p>"},{"location":"lectures/machine_learning/reg_01/#analisis-del-error","title":"An\u00e1lisis del error\u00b6","text":""},{"location":"lectures/machine_learning/reg_01/#predicciones","title":"Predicciones\u00b6","text":"<p>Ahora que ya se tiene el modelo entrenado y se ha analizado sus principales caracter\u00edsticas, se pueden realizar predicciones de los valores que se desconocen, de la siguiente manera:</p>"},{"location":"lectures/machine_learning/reg_01/#normalidad-de-los-residuos","title":"Normalidad de los residuos\u00b6","text":"<p>Basados en los distintos test (Durbin-Watson,Omnibus,Jarque-Bera ) se concluye que los residuos del modelo son un ruido blanco. Para convencernos de esto de manera gr\u00e1fica, se realizan los siguientes gr\u00e1ficos de inter\u00e9s.</p>"},{"location":"lectures/machine_learning/reg_01/#outliers","title":"Outliers\u00b6","text":"<p>Un outlier (o valor at\u00edpico) una observaci\u00f3n que es num\u00e9ricamente distante del resto de los datos. Las estad\u00edsticas derivadas de los conjuntos de datos que incluyen valores at\u00edpicos ser\u00e1n frecuentemente enga\u00f1osas. Estos valores pueden afectar fuertemente al modelo de regresi\u00f3n log\u00edstica. Veamos un ejemplo:</p>"},{"location":"lectures/machine_learning/reg_01/#que-hacer-ante-la-presencia-de-outliers","title":"\u00bf Qu\u00e9 hacer ante la presencia de outliers?\u00b6","text":""},{"location":"lectures/machine_learning/reg_01/#conclusion","title":"Conclusi\u00f3n\u00b6","text":"<ul> <li>Los modelos de regresi\u00f3n lineal son una gran herramienta para realizar predicciones.</li> <li>Los outliers afectan considerablemente a la regresi\u00f3n lineal, por lo que se debn buscar estrategias para abordar esta problem\u00e1tica.</li> <li>En esta oportunidad se hizo un detalle t\u00e9cnico de disntintos est\u00e1disticos asociados a la regresi\u00f3n l\u00edneal (apuntando a un an\u00e1lisis inferencial ), no obstante, en los pr\u00f3ximos modelos, se estar\u00e1 interesado en analizar las predicciones del modelo y los errores asociados a ella, por lo cual los aspectos t\u00e9cnico quedar\u00e1n como lecturas complementarias.</li> <li>Existen varios casos donde los modelos de regresi\u00f3n l\u00edneal no realizan un correcto ajuste de los datos, pero es una gran herramienta para comenzar.</li> </ul>"},{"location":"lectures/machine_learning/reg_01/#referencias","title":"Referencias\u00b6","text":"<ol> <li>Linear Regression in Python</li> </ol>"},{"location":"lectures/machine_learning/reg_02/","title":"Regressi\u00f3n II","text":"In\u00a0[1]: Copied! <pre># librerias\n \nimport os\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \npd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes\n\n# Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab\n%matplotlib inline\n</pre> # librerias   import os import numpy as np import pandas as pd  import matplotlib.pyplot as plt import seaborn as sns  pd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes  # Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab %matplotlib inline In\u00a0[2]: Copied! <pre># sklearn models\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import fetch_openml\n</pre> # sklearn models from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.datasets import fetch_openml In\u00a0[3]: Copied! <pre># Load the Boston Housing dataset using fetch_openml\ndata = fetch_openml(data_id=531)\n\n# Create a DataFrame from the data\nboston_df = pd.DataFrame(data.data, columns=data.feature_names)\nboston_df['TARGET'] = data.target\nboston_df = boston_df.astype(float)\nboston_df.head()\n</pre> # Load the Boston Housing dataset using fetch_openml data = fetch_openml(data_id=531)  # Create a DataFrame from the data boston_df = pd.DataFrame(data.data, columns=data.feature_names) boston_df['TARGET'] = data.target boston_df = boston_df.astype(float) boston_df.head() <pre>C:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\db-connectors-RsEieBu8-py3.8\\lib\\site-packages\\sklearn\\datasets\\_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n  warn(\n</pre> Out[3]: CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT TARGET 0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.98 24.0 1 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.14 21.6 2 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.03 34.7 3 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.94 33.4 4 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33 36.2 In\u00a0[4]: Copied! <pre># descripcion del conjunto de datos\nboston_df.describe()\n</pre> # descripcion del conjunto de datos boston_df.describe() Out[4]: CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT TARGET count 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 mean 3.613524 11.363636 11.136779 0.069170 0.554695 6.284634 68.574901 3.795043 9.549407 408.237154 18.455534 356.674032 12.653063 22.532806 std 8.601545 23.322453 6.860353 0.253994 0.115878 0.702617 28.148861 2.105710 8.707259 168.537116 2.164946 91.294864 7.141062 9.197104 min 0.006320 0.000000 0.460000 0.000000 0.385000 3.561000 2.900000 1.129600 1.000000 187.000000 12.600000 0.320000 1.730000 5.000000 25% 0.082045 0.000000 5.190000 0.000000 0.449000 5.885500 45.025000 2.100175 4.000000 279.000000 17.400000 375.377500 6.950000 17.025000 50% 0.256510 0.000000 9.690000 0.000000 0.538000 6.208500 77.500000 3.207450 5.000000 330.000000 19.050000 391.440000 11.360000 21.200000 75% 3.677083 12.500000 18.100000 0.000000 0.624000 6.623500 94.075000 5.188425 24.000000 666.000000 20.200000 396.225000 16.955000 25.000000 max 88.976200 100.000000 27.740000 1.000000 0.871000 8.780000 100.000000 12.126500 24.000000 711.000000 22.000000 396.900000 37.970000 50.000000 In\u00a0[5]: Copied! <pre>boston_df.info()\n</pre> boston_df.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 506 entries, 0 to 505\nData columns (total 14 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   CRIM     506 non-null    float64\n 1   ZN       506 non-null    float64\n 2   INDUS    506 non-null    float64\n 3   CHAS     506 non-null    float64\n 4   NOX      506 non-null    float64\n 5   RM       506 non-null    float64\n 6   AGE      506 non-null    float64\n 7   DIS      506 non-null    float64\n 8   RAD      506 non-null    float64\n 9   TAX      506 non-null    float64\n 10  PTRATIO  506 non-null    float64\n 11  B        506 non-null    float64\n 12  LSTAT    506 non-null    float64\n 13  TARGET   506 non-null    float64\ndtypes: float64(14)\nmemory usage: 55.5 KB\n</pre> In\u00a0[6]: Copied! <pre>#matriz de correlacion\ncorr_mat=boston_df.corr(method='pearson')\nplt.figure(figsize=(20,10))\nsns.heatmap(corr_mat,vmax=1,square=True,annot=True,cmap='cubehelix',fmt='.2f')\nplt.show()\n</pre> #matriz de correlacion corr_mat=boston_df.corr(method='pearson') plt.figure(figsize=(20,10)) sns.heatmap(corr_mat,vmax=1,square=True,annot=True,cmap='cubehelix',fmt='.2f') plt.show() <p>Apliquemos el modelo de regresi\u00f3n lineal multiple con sklearn</p> In\u00a0[7]: Copied! <pre># datos para la regresion lineal simple\nX = boston_df.drop(\"TARGET\",axis=1) \nY = boston_df[\"TARGET\"]\n\n# split dataset\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state = 2) \n\n# ajustar el modelo\nmodel_rl = LinearRegression() # Creando el modelo.\nmodel_rl.fit(X_train, Y_train) # ajustando el modelo\n\n# prediciones\nY_predict = model_rl.predict(X_test)\n</pre> # datos para la regresion lineal simple X = boston_df.drop(\"TARGET\",axis=1)  Y = boston_df[\"TARGET\"]  # split dataset X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state = 2)   # ajustar el modelo model_rl = LinearRegression() # Creando el modelo. model_rl.fit(X_train, Y_train) # ajustando el modelo  # prediciones Y_predict = model_rl.predict(X_test) In\u00a0[8]: Copied! <pre>from sklearn.metrics import mean_absolute_error, mean_squared_error\n\ndef mean_absolute_percentage_error(y_true, y_pred):\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\ndef regression_metrics(df):\n    \"\"\"\n    Aplicar las distintas m\u00e9tricas definidas\n    :param df: DataFrame con las columnas: ['y', 'yhat']\n    :return: DataFrame con las m\u00e9tricas especificadas\n    \"\"\"\n    df_result = pd.DataFrame()\n\n    y_true = df['y']\n    y_pred = df['yhat']\n\n    df_result['mae'] = [round(mean_absolute_error(y_true, y_pred), 4)]\n    df_result['mse'] = [round(mean_squared_error(y_true, y_pred), 4)]\n    df_result['rmse'] = [round(np.sqrt(mean_squared_error(y_true, y_pred)), 4)]\n    df_result['mape'] = [round(mean_absolute_percentage_error(y_true, y_pred), 4)]\n    df_result['smape'] = [round(2 * mean_absolute_percentage_error(y_true, y_pred) / (mean_absolute_percentage_error(y_true, y_pred) + 100), 4)]\n\n    return df_result\n</pre> from sklearn.metrics import mean_absolute_error, mean_squared_error  def mean_absolute_percentage_error(y_true, y_pred):     return np.mean(np.abs((y_true - y_pred) / y_true)) * 100  def regression_metrics(df):     \"\"\"     Aplicar las distintas m\u00e9tricas definidas     :param df: DataFrame con las columnas: ['y', 'yhat']     :return: DataFrame con las m\u00e9tricas especificadas     \"\"\"     df_result = pd.DataFrame()      y_true = df['y']     y_pred = df['yhat']      df_result['mae'] = [round(mean_absolute_error(y_true, y_pred), 4)]     df_result['mse'] = [round(mean_squared_error(y_true, y_pred), 4)]     df_result['rmse'] = [round(np.sqrt(mean_squared_error(y_true, y_pred)), 4)]     df_result['mape'] = [round(mean_absolute_percentage_error(y_true, y_pred), 4)]     df_result['smape'] = [round(2 * mean_absolute_percentage_error(y_true, y_pred) / (mean_absolute_percentage_error(y_true, y_pred) + 100), 4)]      return df_result In\u00a0[9]: Copied! <pre>from sklearn.metrics import r2_score\n\n# ejemplo: boston df\ndf_temp = pd.DataFrame(\n    {\n        'y':Y_test,\n        'yhat': model_rl.predict(X_test)\n        }\n)\n\ndf_metrics = regression_metrics(df_temp)\ndf_metrics['r2'] =  round(r2_score(Y_test, model_rl.predict(X_test)),4)\n\nprint('\\nMetricas para el regresor CRIM:')\ndf_metrics\n</pre> from sklearn.metrics import r2_score  # ejemplo: boston df df_temp = pd.DataFrame(     {         'y':Y_test,         'yhat': model_rl.predict(X_test)         } )  df_metrics = regression_metrics(df_temp) df_metrics['r2'] =  round(r2_score(Y_test, model_rl.predict(X_test)),4)  print('\\nMetricas para el regresor CRIM:') df_metrics <pre>\nMetricas para el regresor CRIM:\n</pre> Out[9]: mae mse rmse mape smape r2 0 3.113 18.4954 4.3006 16.036 0.2764 0.7789 <p>Cuando se aplica el modelo de regresi\u00f3n lineal con todas las variables regresoras, las m\u00e9tricas disminuyen considerablemente, lo implica una mejora en el modelo</p> <p>Un problema que se tiene, a diferencia de la regresi\u00f3n lineal simple,es que no se puede ver gr\u00e1ficamente la calidad del ajuste, por lo que solo se puede confiar en las m\u00e9tricas calculadas. Adem\u00e1s, se dejan las siguientes preguntas:</p> <ul> <li>\u00bf Entre m\u00e1s regresores, mejor ser\u00e1 el modelo de regresi\u00f3n lineal?</li> <li>\u00bf Qu\u00e9 se debe tener en cuenta antes de agregar otro variable regresora al modelo de regresi\u00f3n lineal ?</li> <li>\u00bf Qu\u00e9 sucede si se tienen outliers ?</li> <li>\u00bf Existen otros modelos mejor que la regresi\u00f3n lineal ?</li> </ul> <p>Ya se han discutido algunos de estos puntos, por lo que la atenci\u00f3n estar\u00e1 en abordar otros modelos.</p> In\u00a0[10]: Copied! <pre>from sklearn import linear_model\nfrom sklearn import tree\nfrom sklearn import svm\nfrom sklearn import neighbors\n</pre> from sklearn import linear_model from sklearn import tree from sklearn import svm from sklearn import neighbors In\u00a0[11]: Copied! <pre>class SklearnRegressionModels:\n    def __init__(self,model,name_model):\n\n        self.model = model\n        self.name_model = name_model\n        \n    @staticmethod\n    def test_train_model(X,y,n_size):\n        X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=n_size , random_state=42)\n        return X_train, X_test, y_train, y_test\n    \n    def fit_model(self,X,y,test_size):\n        X_train, X_test, y_train, y_test = self.test_train_model(X,y,test_size )\n        return self.model.fit(X_train, y_train) \n    \n    def df_testig(self,X,y,test_size):\n        X_train, X_test, y_train, y_test = self.test_train_model(X,y,test_size )\n        model_fit = self.model.fit(X_train, y_train)\n        preds = model_fit.predict(X_test)\n        df_temp = pd.DataFrame(\n            {\n                'y':y_test,\n                'yhat': model_fit.predict(X_test)\n            }\n        )\n        \n        return df_temp\n    \n    def metrics(self,X,y,test_size):\n        df_temp = self.df_testig(X,y,test_size)\n        df_metrics = regression_metrics(df_temp)\n        df_metrics['r2'] =  round(r2_score(df_temp['y'],df_temp['yhat']),4)\n\n        df_metrics['model'] = self.name_model\n        \n        return df_metrics\n\n    def parameters(self,X,y,test_size):\n        model_fit = self.fit_model(X,y,test_size)\n        \n        list_betas = [\n             ('beta_0',model_fit.intercept_)\n                ]\n            \n        betas = model_fit.coef_\n        \n        for num, beta in enumerate(betas):\n            name_beta = f'beta_{num+1}'\n            list_betas.append((name_beta,round(beta,2)))\n\n        result = pd.DataFrame(\n            columns = ['coef','value'],\n            data = list_betas\n        )\n        \n        result['model'] = self.name_model\n        return result\n</pre> class SklearnRegressionModels:     def __init__(self,model,name_model):          self.model = model         self.name_model = name_model              @staticmethod     def test_train_model(X,y,n_size):         X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=n_size , random_state=42)         return X_train, X_test, y_train, y_test          def fit_model(self,X,y,test_size):         X_train, X_test, y_train, y_test = self.test_train_model(X,y,test_size )         return self.model.fit(X_train, y_train)           def df_testig(self,X,y,test_size):         X_train, X_test, y_train, y_test = self.test_train_model(X,y,test_size )         model_fit = self.model.fit(X_train, y_train)         preds = model_fit.predict(X_test)         df_temp = pd.DataFrame(             {                 'y':y_test,                 'yhat': model_fit.predict(X_test)             }         )                  return df_temp          def metrics(self,X,y,test_size):         df_temp = self.df_testig(X,y,test_size)         df_metrics = regression_metrics(df_temp)         df_metrics['r2'] =  round(r2_score(df_temp['y'],df_temp['yhat']),4)          df_metrics['model'] = self.name_model                  return df_metrics      def parameters(self,X,y,test_size):         model_fit = self.fit_model(X,y,test_size)                  list_betas = [              ('beta_0',model_fit.intercept_)                 ]                      betas = model_fit.coef_                  for num, beta in enumerate(betas):             name_beta = f'beta_{num+1}'             list_betas.append((name_beta,round(beta,2)))          result = pd.DataFrame(             columns = ['coef','value'],             data = list_betas         )                  result['model'] = self.name_model         return result  In\u00a0[12]: Copied! <pre># boston dataframe\nX = boston_df.drop(\"TARGET\",axis=1) \nY = boston_df[\"TARGET\"]\n</pre> # boston dataframe X = boston_df.drop(\"TARGET\",axis=1)  Y = boston_df[\"TARGET\"] In\u00a0[13]: Copied! <pre># models\nreg_lineal = linear_model.LinearRegression()\nreg_ridge = linear_model.Ridge(alpha=.5)\nreg_lasso = linear_model.Lasso(alpha=0.1)\n\nreg_knn = neighbors.KNeighborsRegressor(5,weights='uniform')\nreg_bayesian = linear_model.BayesianRidge()\nreg_tree = tree.DecisionTreeRegressor(max_depth=5)\nreg_svm = svm.SVR(kernel='linear')\n\n\nlist_models =[\n    [reg_lineal,'lineal'],\n    [reg_ridge,'ridge'],\n    [reg_lasso,'lasso'],\n    [reg_knn,'knn'],\n    [reg_bayesian,'bayesian'],\n    [reg_tree,'decision_tree'],\n    [reg_svm,'svm'],\n]\n</pre> # models reg_lineal = linear_model.LinearRegression() reg_ridge = linear_model.Ridge(alpha=.5) reg_lasso = linear_model.Lasso(alpha=0.1)  reg_knn = neighbors.KNeighborsRegressor(5,weights='uniform') reg_bayesian = linear_model.BayesianRidge() reg_tree = tree.DecisionTreeRegressor(max_depth=5) reg_svm = svm.SVR(kernel='linear')   list_models =[     [reg_lineal,'lineal'],     [reg_ridge,'ridge'],     [reg_lasso,'lasso'],     [reg_knn,'knn'],     [reg_bayesian,'bayesian'],     [reg_tree,'decision_tree'],     [reg_svm,'svm'], ] In\u00a0[14]: Copied! <pre>frames_metrics = []\nframes_coef = []\n\nfor model,name_models in list_models:\n    fit_model =  SklearnRegressionModels( model,name_models)\n    frames_metrics.append(fit_model.metrics(X,Y,0.2))\n    if name_models in ['lineal','ridge','lasso']:\n        frames_coef.append(fit_model.parameters(X,Y,0.2))\n</pre> frames_metrics = [] frames_coef = []  for model,name_models in list_models:     fit_model =  SklearnRegressionModels( model,name_models)     frames_metrics.append(fit_model.metrics(X,Y,0.2))     if name_models in ['lineal','ridge','lasso']:         frames_coef.append(fit_model.parameters(X,Y,0.2)) In\u00a0[15]: Copied! <pre># juntar resultados: metricas\npd.concat(frames_metrics).sort_values('rmse')\n</pre> # juntar resultados: metricas pd.concat(frames_metrics).sort_values('rmse') Out[15]: mae mse rmse mape smape r2 model 0 2.6062 20.3563 4.5118 15.0760 0.2620 0.7224 decision_tree 0 3.1891 24.2911 4.9286 16.8664 0.2886 0.6688 lineal 0 3.1493 24.3776 4.9374 16.6837 0.2860 0.6676 ridge 0 3.1251 24.6471 4.9646 16.5449 0.2839 0.6639 bayesian 0 3.1452 25.1556 5.0155 16.7519 0.2870 0.6570 lasso 0 3.6639 25.8601 5.0853 18.8859 0.3177 0.6474 knn 0 3.1404 29.4359 5.4255 16.7713 0.2873 0.5986 svm <p>Basados en los distintos estad\u00edsticos, el mejor modelo corresponde al modelo de decision_tree. Por otro lado, podemos analizar los coeficientes de los modelos l\u00edneales ordinarios,Ridge y Lasso.</p> In\u00a0[16]: Copied! <pre># juntar resultados: coeficientes\npd.concat(frames_coef)\n</pre> # juntar resultados: coeficientes pd.concat(frames_coef) Out[16]: coef value model 0 beta_0 30.246751 lineal 1 beta_1 -0.110000 lineal 2 beta_2 0.030000 lineal 3 beta_3 0.040000 lineal 4 beta_4 2.780000 lineal 5 beta_5 -17.200000 lineal 6 beta_6 4.440000 lineal 7 beta_7 -0.010000 lineal 8 beta_8 -1.450000 lineal 9 beta_9 0.260000 lineal 10 beta_10 -0.010000 lineal 11 beta_11 -0.920000 lineal 12 beta_12 0.010000 lineal 13 beta_13 -0.510000 lineal 0 beta_0 26.891132 ridge 1 beta_1 -0.110000 ridge 2 beta_2 0.030000 ridge 3 beta_3 0.020000 ridge 4 beta_4 2.640000 ridge 5 beta_5 -12.270000 ridge 6 beta_6 4.460000 ridge 7 beta_7 -0.010000 ridge 8 beta_8 -1.380000 ridge 9 beta_9 0.250000 ridge 10 beta_10 -0.010000 ridge 11 beta_11 -0.860000 ridge 12 beta_12 0.010000 ridge 13 beta_13 -0.520000 ridge 0 beta_0 19.859769 lasso 1 beta_1 -0.100000 lasso 2 beta_2 0.030000 lasso 3 beta_3 -0.020000 lasso 4 beta_4 0.920000 lasso 5 beta_5 -0.000000 lasso 6 beta_6 4.310000 lasso 7 beta_7 -0.020000 lasso 8 beta_8 -1.150000 lasso 9 beta_9 0.240000 lasso 10 beta_10 -0.010000 lasso 11 beta_11 -0.730000 lasso 12 beta_12 0.010000 lasso 13 beta_13 -0.560000 lasso <p>Al comparar los resultados entre ambos modelos, se observa que hay coeficientes en la regresi\u00f3n Lasso que se van a cero directamente, pudiendo eliminar estas variables del modelo. Por otro lado, queda como tarea para el lector, hacer una eliminaci\u00f3n de outliers del modelo y probar estos modelos lineales para ver si existe alg\u00fan tipo de diferencia.</p>"},{"location":"lectures/machine_learning/reg_02/#regression-ii","title":"Regressi\u00f3n II\u00b6","text":""},{"location":"lectures/machine_learning/reg_02/#regression-multiple","title":"Regressi\u00f3n M\u00faltiple\u00b6","text":"<p>Los modelos de regresi\u00f3n multiple son los m\u00e1s utilizados en el mundo de machine learning, puestos que se dispone de varios caracter\u00edstica de la poblaci\u00f3n objetivo. A menudo, se estar\u00e1 abordando estos modelos de la perspectiva de los modelos lineales, por lo cual se debetener en mente algunos supuestos antes de comenzar:</p> <ul> <li>No colinialidad o multicolinialidad: En los modelos lineales m\u00faltiples los predictores deben ser independientes, no debe de haber colinialidad entre ellos</li> <li>Parsimonia: Este t\u00e9rmino hace referencia a que el mejor modelo es aquel capaz de explicar con mayor precisi\u00f3n la variabilidad observada en la variable respuesta empleando el menor n\u00famero de predictores, por lo tanto, con menos asunciones.</li> <li>Homocedasticidad:La varianza de los residuos debe de ser constante en todo el rango de observaciones.</li> <li>Otros Factores:<ul> <li>Distribuci\u00f3n normal de los residuos</li> <li>No autocorrelaci\u00f3n (Independencia)</li> <li>Valores at\u00edpicos, con alto leverage o influyentes</li> <li>Tama\u00f1o de la muestra</li> </ul> </li> </ul> <p>Por otro lado, existen otros tipos de modelos de regresi\u00f3n, en los cuales se  necesitan menos supuestos que los modelos de regresi\u00f3n lineal, a cambio se pierde un poco de interpretabilidad en sus par\u00e1metros y centran su atenci\u00f3n en los resultados obtenidos de las predicciones.</p>"},{"location":"lectures/machine_learning/reg_02/#aplicacion-con-python","title":"Aplicaci\u00f3n con python\u00b6","text":""},{"location":"lectures/machine_learning/reg_02/#dataset-boston-house-prices","title":"Dataset  Boston house prices\u00b6","text":"<p>En este ejemplo se va utilizar el dataset Boston que ya viene junto con sklearn y es ideal para practicar con Regresiones Lineales; el mismo contiene precios de casas de varias \u00e1reas de la ciudad de Boston.</p>"},{"location":"lectures/machine_learning/reg_02/#otros-modelos-de-regresion","title":"Otros modelos de Regresi\u00f3n\u00b6","text":"<p>Existen varios modelos de regresi\u00f3n, sin embargo, la intepretaci\u00f3n de sus par\u00e1metros y el an\u00e1lisis de confiabilidad no es tan directo como los modelos de regresi\u00f3n lineal. Por este motivo, la atenci\u00f3n estar\u00e1 centrada en la predicci\u00f3n m\u00e1s que en la confiabilidad como tal del modelo.</p>"},{"location":"lectures/machine_learning/reg_02/#modelos-lineales","title":"Modelos lineales\u00b6","text":"<p>Existen varios modelos lineales que podemos trabajar en sklearn (ver referencia), los cualeas podemos utilizar e ir comparando unos con otros.</p> <p>De lo modelos lineales, destacamos los siguientes:</p> <ul> <li>regresi\u00f3n lineal cl\u00e1sica: regresi\u00f3n cl\u00e1sica por m\u00ednimos cudrados. $$(P)\\ \\min \\sum_{i=1}^n (y_{i}-f_{i}(x;\\beta))^2   $$</li> <li>lasso): se ocupa cuando tenemos un gran n\u00famero de regresores y queremos que disminuya el problema de colinealidad (es decir, estimar como cero los par\u00e1metros poco relevantes). $$(P)\\ \\min \\sum_{i=1}^n (y_{i}-f_{i}(x;\\beta))^2 + \\lambda \\sum_{i=1}^n |\\beta_{i}| $$</li> <li>ridge: tambi\u00e9n sirve para disminuir el problema de colinealidad, y adem\u00e1s trata de que los coeficientes sean m\u00e1s rocuesto bajo outliers. $$(P)\\ \\min \\sum_{i=1}^n (y_{i}-f_{i}(x;\\beta))^2  + \\lambda \\sum_{i=1}^n \\beta_{i}^2 $$</li> </ul> <p>Dado que en sklearn, la forma de entrenar, estimar y predecir modelos de regresi\u00f3n siguen una misma estructura, para fectos pr\u00e1cticos, definimos una rutina para estimar las distintas m\u00e9tricas de la siguiente manera:</p>"},{"location":"lectures/machine_learning/reg_02/#bayesian-regression","title":"Bayesian Regression\u00b6","text":"<p>En estad\u00edstica, la regresi\u00f3n lineal bayesiana es un enfoque de regresi\u00f3n lineal en el que el an\u00e1lisis estad\u00edstico se realiza dentro del contexto de la inferencia bayesiana. Cuando el modelo de regresi\u00f3n tiene errores que tienen una distribuci\u00f3n normal, y si se asume una forma particular de distribuci\u00f3n previa, los resultados expl\u00edcitos est\u00e1n disponibles para las distribuciones de probabilidad posteriores de los par\u00e1metros del modelo.</p> <p></p>"},{"location":"lectures/machine_learning/reg_02/#k-vecinos-mas-cercanos-knn","title":"k-vecinos m\u00e1s cercanos  Knn\u00b6","text":"<p>El m\u00e9todo de los $k$ vecinos m\u00e1s cercanos (en ingl\u00e9s, k-nearest neighbors, abreviado $knn$) es un m\u00e9todo de clasificaci\u00f3n supervisada (Aprendizaje, estimaci\u00f3n basada en un conjunto de entrenamiento y prototipos) que sirve para estimar la funci\u00f3n de densidad $F(x/C_j)$ de las predictoras $x$ por cada clase  $C_{j}$.</p> <p>Este es un m\u00e9todo de clasificaci\u00f3n no param\u00e9trico, que estima el valor de la funci\u00f3n de densidad de probabilidad o directamente la probabilidad a posteriori de que un elemento $x$ pertenezca a la clase $C_j$ a partir de la informaci\u00f3n proporcionada por el conjunto de prototipos. En el proceso de aprendizaje no se hace ninguna suposici\u00f3n acerca de la distribuci\u00f3n de las variables predictoras.</p> <p>En el reconocimiento de patrones, el algoritmo $knn$ es usado como m\u00e9todo de clasificaci\u00f3n de objetos (elementos) basado en un entrenamiento mediante ejemplos cercanos en el espacio de los elementos. $knn$ es un tipo de aprendizaje vago (lazy learning), donde la funci\u00f3n se aproxima solo localmente y todo el c\u00f3mputo es diferido a la clasificaci\u00f3n. La normalizaci\u00f3n de datos puede mejorar considerablemente la exactitud del algoritmo $knn$.</p> <p></p>"},{"location":"lectures/machine_learning/reg_02/#decision-tree-regressor","title":"Decision Tree Regressor\u00b6","text":"<p>Un \u00e1rbol de decisi\u00f3n es un modelo de predicci\u00f3n utilizado en diversos \u00e1mbitos que van desde la inteligencia artificial hasta la Econom\u00eda. Dado un conjunto de datos se fabrican diagramas de construcciones l\u00f3gicas, muy similares a los sistemas de predicci\u00f3n basados en reglas, que sirven para representar y categorizar una serie de condiciones que ocurren de forma sucesiva, para la resoluci\u00f3n de un problema.</p> <p></p> <p>Vamos a explicar c\u00f3mo se construye un \u00e1rbol de decisi\u00f3n. Para ello, vamos a hacer hincapi\u00e9 en varios aspectos</p>"},{"location":"lectures/machine_learning/reg_02/#elementos","title":"Elementos\u00b6","text":"<p>Los \u00e1rboles de decisi\u00f3n est\u00e1n formados por nodos, vectores de n\u00fameros, flechas y etiquetas.</p> <ul> <li>Cada nodo se puede definir como el momento en el que se ha de tomar una decisi\u00f3n de entre varias posibles, lo que va haciendo que a medida que aumenta el n\u00famero de nodos aumente el n\u00famero de posibles finales a los que puede llegar el individuo. Esto hace que un \u00e1rbol con muchos nodos sea complicado de dibujar a mano y de analizar debido a la existencia de numerosos caminos que se pueden seguir.</li> <li>Los vectores de n\u00fameros ser\u00edan la soluci\u00f3n final a la que se llega en funci\u00f3n de las diversas posibilidades que se tienen, dan las utilidades en esa soluci\u00f3n.</li> <li>Las flechas son las uniones entre un nodo y otro y representan cada acci\u00f3n distinta.</li> <li>Las etiquetas se encuentran en cada nodo y cada flecha y dan nombre a cada acci\u00f3n.</li> </ul>"},{"location":"lectures/machine_learning/reg_02/#conceptos","title":"Conceptos\u00b6","text":"<p>Cuando tratemos en el desarrollo de \u00e1rboles utilizaremos frecuentemente estos conceptos:</p> <ul> <li>Costo. Se refiere a dos conceptos diferentes: el costo de medici\u00f3n para determinar el valor de una determinada propiedad (atributo) exhibida por el objeto y el costo de clasificaci\u00f3n err\u00f3nea al decidir que el objeto pertenece a la clase $X$ cuando su clase real es $Y$.</li> <li>Sobreajuste (Overfitting). Se produce cuando los datos de entrenamiento son pocos o contienen incoherencias. Al tomar un espacio de hip\u00f3tesis $H$, se dice que una hip\u00f3tesis $h \u2208 H$ sobreajusta un conjunto de entrenamiento $C$ si existe alguna hip\u00f3tesis alternativa $h' \u2208 H$ tal que $h$ clasifica mejor que $h'$ los elementos del conjunto de entrenamiento, pero $h'$ clasifica mejor que h el conjunto completo de posibles instancias.</li> <li>Poda (Prunning). La poda consiste en eliminar una rama de un nodo transform\u00e1ndolo en una hoja (terminal), asign\u00e1ndole la clasificaci\u00f3n m\u00e1s com\u00fan de los ejemplos de entrenamiento considerados en ese nodo.</li> <li>La validaci\u00f3n cruzada. Es el proceso de construir un \u00e1rbol con la mayor\u00eda de los datos y luego usar la parte restante de los datos para probar la precisi\u00f3n del \u00e1rbol.</li> </ul>"},{"location":"lectures/machine_learning/reg_02/#reglas","title":"Reglas\u00b6","text":"<p>En los \u00e1rboles de decisi\u00f3n se tiene que cumplir una serie de reglas.</p> <ol> <li>Al comienzo del juego se da un nodo inicial que no es apuntado por ninguna flecha, es el \u00fanico del juego con esta caracter\u00edstica.</li> <li>El resto de los nodos del juego son apuntados por una \u00fanica flecha.</li> <li>De esto se deduce que hay un \u00fanico camino para llegar del nodo inicial a cada uno de los nodos del juego. No hay varias formas de llegar a la misma soluci\u00f3n final, las decisiones son excluyentes.</li> </ol> <p>En los \u00e1rboles de decisiones las decisiones que se eligen son lineales, a medida que vas seleccionando entre varias opciones se van cerrando otras, lo que implica normalmente que no hay marcha atr\u00e1s. En general se podr\u00eda decir que las normas siguen una forma condicional:</p> <p>$$\\textrm{Opci\u00f3n }1-&gt;\\textrm{opci\u00f3n }2-&gt;\\textrm{opci\u00f3n }3-&gt;\\textrm{Resultado Final }X$$</p> <p>Estas reglas suelen ir impl\u00edcitas en el conjunto de datos a ra\u00edz del cual se construye el \u00e1rbol de decisi\u00f3n.</p>"},{"location":"lectures/machine_learning/reg_02/#svm","title":"SVM\u00b6","text":"<p>Las m\u00e1quinas de vectores de soporte  (del ingl\u00e9s Support Vector Machines, SVM) son un conjunto de algoritmos de aprendizaje supervisado desarrollados por Vladimir Vapnik y su equipo en los laboratorios AT&amp;T.</p> <p>Estos m\u00e9todos est\u00e1n propiamente relacionados con problemas de clasificaci\u00f3n y regresi\u00f3n. Dado un conjunto de ejemplos de entrenamiento (de muestras) podemos etiquetar las clases y entrenar una SVM para construir un modelo que prediga la clase de una nueva muestra. Intuitivamente, una SVM es un modelo que representa a los puntos de muestra en el espacio, separando las clases a 2 espacios lo m\u00e1s amplios posibles mediante un hiperplano de separaci\u00f3n definido como el vector entre los 2 puntos, de las 2 clases, m\u00e1s cercanos al que se llama vector soporte. Cuando las nuevas muestras se ponen en correspondencia con dicho modelo, en funci\u00f3n de los espacios a los que pertenezcan, pueden ser clasificadas a una o la otra clase.</p> <p>M\u00e1s formalmente, una SVM construye un hiperplano o conjunto de hiperplanos en un espacio de dimensionalidad muy alta (o incluso infinita) que puede ser utilizado en problemas de clasificaci\u00f3n o regresi\u00f3n. Una buena separaci\u00f3n entre las clases permitir\u00e1 una clasificaci\u00f3n correcta.</p> <p></p>"},{"location":"lectures/machine_learning/reg_02/#idea-basica","title":"Idea B\u00e1sica\u00b6","text":"<p>Dado un conjunto de puntos, subconjunto de un conjunto mayor (espacio), en el que cada uno de ellos pertenece a una de dos posibles categor\u00edas, un algoritmo basado en SVM construye un modelo capaz de predecir si un punto nuevo (cuya categor\u00eda desconocemos) pertenece a una categor\u00eda o a la otra.</p> <p>Como en la mayor\u00eda de los m\u00e9todos de clasificaci\u00f3n supervisada, los datos de entrada (los puntos) son vistos como un vector $p-dimensional$ (una lista ordenada de $p$ n\u00fameros).</p> <p>La SVM busca un hiperplano que separe de forma \u00f3ptima a los puntos de una clase de la de otra, que eventualmente han podido ser previamente proyectados a un espacio de dimensionalidad superior.</p> <p>En ese concepto de \"separaci\u00f3n \u00f3ptima\" es donde reside la caracter\u00edstica fundamental de las SVM: este tipo de algoritmos buscan el hiperplano que tenga la m\u00e1xima distancia (margen) con los puntos que est\u00e9n m\u00e1s cerca de \u00e9l mismo. Por eso tambi\u00e9n a veces se les conoce a las SVM como clasificadores de margen m\u00e1ximo. De esta forma, los puntos del vector que son etiquetados con una categor\u00eda estar\u00e1n a un lado del hiperplano y los casos que se encuentren en la otra categor\u00eda estar\u00e1n al otro lado.</p> <p>Los algoritmos SVM pertenecen a la familia de los clasificadores lineales. Tambi\u00e9n pueden ser considerados un caso especial de la regularizaci\u00f3n de Tikhonov.</p> <p>En la literatura de las SVM, se llama atributo a la variable predictora y caracter\u00edstica a un atributo transformado que es usado para definir el hiperplano. La elecci\u00f3n de la representaci\u00f3n m\u00e1s adecuada del universo estudiado, se realiza mediante un proceso denominado selecci\u00f3n de caracter\u00edsticas.</p> <p>Al vector formado por los puntos m\u00e1s cercanos al hiperplano se le llama vector de soporte.</p> <p>Los modelos basados en SVM est\u00e1n estrechamente relacionados con las redes neuronales. Usando una funci\u00f3n kernel, resultan un m\u00e9todo de entrenamiento alternativo para clasificadores polinomiales, funciones de base radial y perceptr\u00f3n multicapa.</p> <p></p>"},{"location":"lectures/machine_learning/reg_02/#aplicando-varios-modelos-al-mismo-tiempo","title":"Aplicando varios modelos al mismo tiempo\u00b6","text":"<p>Veremos el performance de los distintos modelos estudiados.</p>"},{"location":"lectures/machine_learning/reg_02/#conclusion","title":"Conclusi\u00f3n\u00b6","text":"<ul> <li>Existen distintos modelos de regresi\u00f3n lineal: normal, Ridge y Lasso. Cada uno con sus respectivs ventajas y desventajas.</li> <li>Existen otros tipos de modelos de regresi\u00f3n (bayesiano, knn, arboles de decisi\u00f3n, svm, entre otros). Por ahora, nos interesa saber como funcionan, para poder configurar los hiperpar\u00e1metros de los modelos ocupados en python (principalmente de la librer\u00eda sklearn).</li> <li>En el mundo del machine learning se estar\u00e1 interesado m\u00e1s en predecir con el menor error posible (siempre tomando como referencia alguna de las m\u00e9tricas mencionadas) que hacer un an\u00e1lisis exhaustivo de la confiabilidad del modelo. Siendo este el caso y si la capacidad computacional lo permite, lo ideal es probar varios modelos al mismo tiempo y poder discriminar bajo un determinado criterio (a menudo el error cuadr\u00e1tico medio (rmse) o el mape).</li> </ul>"},{"location":"lectures/machine_learning/reg_02/#referencias","title":"Referencias\u00b6","text":"<ol> <li>Supervised learning</li> </ol>"},{"location":"lectures/machine_learning/ts_01/","title":"Series Temporales I","text":"<ul> <li><p>Una serie temporal o cronol\u00f3gica es una sucesi\u00f3n de datos medidos en determinados momentos y ordenados cronol\u00f3gicamente. Los datos pueden estar espaciados a intervalos iguales (como la temperatura en un observatorio meteorol\u00f3gico en d\u00edas sucesivos al mediod\u00eda) o desiguales (como el peso de una persona en sucesivas mediciones en el consultorio m\u00e9dico, la farmacia, etc.).</p> </li> <li><p>Uno de los usos m\u00e1s habituales de las series de datos temporales es su an\u00e1lisis para predicci\u00f3n y pron\u00f3stico (as\u00ed se hace por ejemplo con los datos clim\u00e1ticos, las acciones de bolsa, o las series de datos demogr\u00e1ficos). Resulta dif\u00edcil imaginar una rama de las ciencias en la que no aparezcan datos que puedan ser considerados como series temporales. Las series temporales se estudian en estad\u00edstica, procesamiento de se\u00f1ales, econometr\u00eda y muchas otras \u00e1reas.</p> </li> </ul> In\u00a0[1]: Copied! <pre># librerias \n\nimport os\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\n# graficos incrustados\nsns.set_style(\"whitegrid\")\n%matplotlib inline\n\n# parametros esteticos de seaborn\nsns.set_palette(\"deep\", desat=.6)\nsns.set_context(rc={\"figure.figsize\": (12, 4)})\n</pre> # librerias   import os import pandas as pd import numpy as np  import matplotlib.pyplot as plt  import seaborn as sns   # graficos incrustados sns.set_style(\"whitegrid\") %matplotlib inline  # parametros esteticos de seaborn sns.set_palette(\"deep\", desat=.6) sns.set_context(rc={\"figure.figsize\": (12, 4)}) In\u00a0[2]: Copied! <pre># cargar datos\nurl='https://drive.google.com/file/d/1a9e0hoBLYof4mJfCeifOm_-H4J12pP-b/view?usp=drive_link'\nurl='https://drive.google.com/uc?id=' + url.split('/')[-2]\n\ndf = pd.read_csv(url, sep=\",\")\ndf.columns = ['date','passengers']\ndf.head()\n</pre> # cargar datos url='https://drive.google.com/file/d/1a9e0hoBLYof4mJfCeifOm_-H4J12pP-b/view?usp=drive_link' url='https://drive.google.com/uc?id=' + url.split('/')[-2]  df = pd.read_csv(url, sep=\",\") df.columns = ['date','passengers'] df.head() Out[2]: date passengers 0 1949-01 112 1 1949-02 118 2 1949-03 132 3 1949-04 129 4 1949-05 121 In\u00a0[3]: Copied! <pre># resumen\ndf.describe()\n</pre> # resumen df.describe() Out[3]: passengers count 144.000000 mean 280.298611 std 119.966317 min 104.000000 25% 180.000000 50% 265.500000 75% 360.500000 max 622.000000 In\u00a0[4]: Copied! <pre># fechas\nprint('Fecha Inicio: {}\\nFecha Fin:    {}'.format(df.date.min(),df.date.max()))\n</pre> # fechas print('Fecha Inicio: {}\\nFecha Fin:    {}'.format(df.date.min(),df.date.max())) <pre>Fecha Inicio: 1949-01\nFecha Fin:    1960-12\n</pre> In\u00a0[5]: Copied! <pre># formato datetime de las fechas\ndf['date'] = pd.to_datetime(df['date'], format='%Y-%m')\n\n# dejar en formato ts\ny = df.set_index('date').resample('M').mean()\n\ny.head()\n</pre> # formato datetime de las fechas df['date'] = pd.to_datetime(df['date'], format='%Y-%m')  # dejar en formato ts y = df.set_index('date').resample('M').mean()  y.head() Out[5]: passengers date 1949-01-31 112.0 1949-02-28 118.0 1949-03-31 132.0 1949-04-30 129.0 1949-05-31 121.0 In\u00a0[6]: Copied! <pre># graficar datos\ny.plot(figsize=(15, 3),color = 'blue')\nplt.show()\n</pre> # graficar datos y.plot(figsize=(15, 3),color = 'blue') plt.show() <p>Basado en el gr\u00e1fico, uno podria decir que tiene un comportamiento estacionario en el tiempo, es decir, que se repita cada cierto instante de tiempo. Adem\u00e1s, la demanda ha tendido a incrementar a\u00f1o a a\u00f1o.</p> <p>Por otro lado, podemos agregar valor a nuestro entendimiento de nuestra serie temporal, realizando un diagrama de box-plot a los distintos a\u00f1os.</p> In\u00a0[7]: Copied! <pre># Create the boxplot\nfig, ax = plt.subplots(figsize=(15, 6))\nsns.boxplot(x=y.index.year, y=y[\"passengers\"], data=y, ax=ax)\n\n# Set labels and title\nplt.xlabel(\"Year\")\nplt.ylabel(\"Passengers\")\nplt.title(\"Boxplot of Passengers by Year\")\n\n# Display the plot\nplt.show()\n</pre>  # Create the boxplot fig, ax = plt.subplots(figsize=(15, 6)) sns.boxplot(x=y.index.year, y=y[\"passengers\"], data=y, ax=ax)  # Set labels and title plt.xlabel(\"Year\") plt.ylabel(\"Passengers\") plt.title(\"Boxplot of Passengers by Year\")  # Display the plot plt.show() In\u00a0[8]: Copied! <pre>from pylab import rcParams\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nrcParams['figure.figsize'] = 18, 8\ndecomposition = sm.tsa.seasonal_decompose(y, model='multiplicative')\nfig = decomposition.plot()\nplt.show()\n</pre> from pylab import rcParams import statsmodels.api as sm import matplotlib.pyplot as plt  rcParams['figure.figsize'] = 18, 8 decomposition = sm.tsa.seasonal_decompose(y, model='multiplicative') fig = decomposition.plot() plt.show() <p>Analicemos cada uno de estos gr\u00e1ficos:</p> <ul> <li><p>gr\u00e1fico 01 (serie original): este gr\u00e1fico simplemente nos muestra la serie original graficada en el tiempo.</p> </li> <li><p>gr\u00e1fico 02 (tendencia): este gr\u00e1fico nos muestra la tendencia de la serie, para este caso, se tiene una tendencial lineal positiva.</p> </li> <li><p>gr\u00e1fico 03 (estacionariedad): este gr\u00e1fico nos muestra la estacionariedad de la serie, para este caso, se muestra una estacionariedad a\u00f1o a a\u00f1o, esta estacionariedad se puede ver como una curva invertida (funci\u00f3n cuadr\u00e1tica negativa), en donde a aumenta hasta hasta a mediados de a\u00f1os (o un poco m\u00e1s) y luego esta cae suavemente a final de a\u00f1o.</p> </li> <li><p>gr\u00e1fico 04 (error): este gr\u00e1fico nos muestra el error de la serie, para este caso, el error oscila entre 0 y 1. En general se busca que el error sea siempre lo m\u00e1s peque\u00f1o posible y que tenga el comportamiento de una distribuci\u00f3n normal. Cuando el error sigue una distribuci\u00f3n normal con media cero y varianza 1, se dice que el error es un ruido blanco.</p> </li> </ul> <p>\u00bf c\u00f3mo es un ruido blanco?, mostremos un ruido blanco en python y veamos como este luce:</p> In\u00a0[9]: Copied! <pre># grafico: lineplot \n\nnp.random.seed(42) # fijar semilla\n\nmean = 0\nstd = 1 \nnum_samples = 300\n\n\nsamples = np.random.normal(mean, std, size=num_samples)\n\nplt.plot(samples)\nplt.title(\"Ruido blanco: N(0,1)\")\nplt.show()\n</pre> # grafico: lineplot   np.random.seed(42) # fijar semilla  mean = 0 std = 1  num_samples = 300   samples = np.random.normal(mean, std, size=num_samples)  plt.plot(samples) plt.title(\"Ruido blanco: N(0,1)\") plt.show() <p>Observemos que el ruido blanco oscila sobre el valor 0 y tiene una varianza constante (igual a 1).</p> <p>Veamos su histograma:</p> In\u00a0[10]: Copied! <pre># grafico: histograma\nplt.hist(samples,bins = 10)\nplt.title(\"Ruido blanco: N(0,1)\")\nplt.show()\n</pre> # grafico: histograma plt.hist(samples,bins = 10) plt.title(\"Ruido blanco: N(0,1)\") plt.show() <p>EL histograma de una variable normal, se caracteriza por esa forma de campana sim\u00e9trica entorno a un valor, en este caso, entorno al valor 0.</p> <p></p> <p>La imagen A: Este es un excelente ejemplo de una serie estacionaria, a simple vista se puede ver que los valores se encuentran oscilando en un rango acotado (tendencia constante) y la variabilidad es constante.</p> <p>Las im\u00e1genes B, C y D no son estacionarias porque:</p> <ul> <li><p>imagen B: existe una una tendencia no contante, para este caso lineal (similar al caso que estamos analizando).</p> </li> <li><p>imagen C: existe varianza no constante, si bien varia dentro de valores acotados, la oscilaciones son err\u00e1ticas.</p> </li> <li><p>imagen D: existe codependencia entre los distintos instantes de tiempo.</p> </li> </ul> <p>\u00bf Por qu\u00e9 es importante este concepto ?</p> <ul> <li>Supuesto base de muchos modelos de series temporales (desde el punto de vista estad\u00edstico)</li> <li>No se requiere muchas complicaciones extras para que las predicciones sean \"buenas\".</li> </ul> <p>Autocorrelaci\u00f3n (ACF) y autocorrelaci\u00f3n parcial PACF</p> <p>Definamos a grandes rasgos estos conceptos:</p> <ul> <li><p>Funci\u00f3n de autocorrelaci\u00f3n (ACF). En el retardo $k$, es la autocorrelaci\u00f3n entre los valores de las series que se encuentran a $k$ intervalos de distancia.</p> </li> <li><p>Funci\u00f3n de autocorrelaci\u00f3n parcial (PACF). En el retardo $k$, es la autocorrelaci\u00f3n entre los valores de las series que se encuentran a $k$ intervalos de distancia, teniendo en cuenta los valores de los intervalos intermedios.</p> </li> </ul> <p>Si la serie temporal es estacionaria, los gr\u00e1ficos ACF / PACF mostrar\u00e1n una r\u00e1pida disminuci\u00f3n de la correlaci\u00f3n despu\u00e9s de un peque\u00f1o retraso entre los puntos.</p> <p>Gr\u00e1fiquemos la acf y pacf de nuestra serie temporal ocupando los comandos plot_acf y plot_pacf, respectivamente.</p> In\u00a0[11]: Copied! <pre>from statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\nfrom matplotlib import pyplot\npyplot.figure(figsize=(12,9))\n\n# acf\npyplot.subplot(211)\nplot_acf(y.passengers, ax=pyplot.gca(), lags = 30)\n\n#pacf\npyplot.subplot(212)\nplot_pacf(y.passengers, ax=pyplot.gca(), lags = 30)\npyplot.show()\n</pre> from statsmodels.graphics.tsaplots import plot_acf from statsmodels.graphics.tsaplots import plot_pacf from matplotlib import pyplot pyplot.figure(figsize=(12,9))  # acf pyplot.subplot(211) plot_acf(y.passengers, ax=pyplot.gca(), lags = 30)  #pacf pyplot.subplot(212) plot_pacf(y.passengers, ax=pyplot.gca(), lags = 30) pyplot.show() <pre>C:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\db-connectors-RsEieBu8-py3.8\\lib\\site-packages\\statsmodels\\graphics\\tsaplots.py:348: FutureWarning: The default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n  warnings.warn(\n</pre> <p>Se observa de ambas imagenes, que estas no decaen r\u00e1pidamente a cero, por lo cual se puede decir que la serie en estudio no es estacionaria.</p> <p>Prueba  Dickey-Fuller</p> <p>En estad\u00edstica, la prueba Dickey-Fuller prueba la hip\u00f3tesis nula de que una ra\u00edz unitaria est\u00e1 presente en un modelo autorregresivo. La hip\u00f3tesis alternativa es diferente seg\u00fan la versi\u00f3n de la prueba utilizada, pero generalmente es estacionariedad o tendencia-estacionaria. Lleva el nombre de los estad\u00edsticos David Dickey y Wayne Fuller, quienes desarrollaron la prueba en 1979.</p> <p>Para efectos pr\u00e1ticos, para el caso de estacionariedad se puede definir el test como:</p> <ul> <li>Hip\u00f3tesis nula: la serie temporal no es estacionaria.</li> <li>Hip\u00f3tesis alternativa: la serie temporal es alternativa.</li> </ul> <p>Rechazar la hip\u00f3tesis nula (es decir, un valor p muy bajo) indicar\u00e1 estacionariedad</p> In\u00a0[12]: Copied! <pre>from statsmodels.tsa.stattools import adfuller\n\n#test Dickey-Fulle:\nprint ('Resultados del test de Dickey-Fuller:')\ndftest = adfuller(y.passengers, autolag='AIC')\ndfoutput = pd.Series(dftest[0:4], \n                     index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n\nprint(dfoutput)\n</pre> from statsmodels.tsa.stattools import adfuller  #test Dickey-Fulle: print ('Resultados del test de Dickey-Fuller:') dftest = adfuller(y.passengers, autolag='AIC') dfoutput = pd.Series(dftest[0:4],                       index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])  print(dfoutput) <pre>Resultados del test de Dickey-Fuller:\nTest Statistic                   0.815369\np-value                          0.991880\n#Lags Used                      13.000000\nNumber of Observations Used    130.000000\ndtype: float64\n</pre> <p>Dado que el p-value es 0.991880, se concluye que la serie temporal no es estacionaria.</p> <p>\u00bf Qu\u00e9 se puede hacer si la serie no es etacionaria ?</p> <p>Nuestro caso en estudio resulto ser una serie no estacionaria, no obstante, se pueden realizar tranformaciones para solucionar este problema.</p> <p>Como es de esperar, estas tranformaciones deben cumplir ciertas porpiedades (inyectividad, diferenciables, etc.). A continuaci\u00f3n, se presentan algunas de las tranformaciones m\u00e1s ocupadas en el \u00e1mbito de series temporales:</p> <p>Sea $X_{t}$ una serie temporal, entonces uno puede definir una nueva serie temporal $Y_{t}$ de la siguiente manera:</p> <ul> <li><p>diferenciaci\u00f3n:  $Y_{t} =\\Delta X_{t}  =X_{t}-X_{t-1}$.</p> </li> <li><p>transformaciones de Box-Cox: $$Y_{t} = \\left\\{\\begin{matrix} \\dfrac{X_{t}^{\\lambda}-1}{\\lambda}, \\ \\  \\textrm{si }  \\lambda &gt; 0\\\\  \\textrm{log}(X_{t}), \\ \\  \\textrm{si }  \\lambda = 0 \\end{matrix}\\right.$$</p> </li> </ul> <p>Ayudemonos con python para ver el resultado de las distintas transformaciones.</p> In\u00a0[13]: Copied! <pre># diferenciacion\n\nY_diff = y.diff()\n\nrcParams['figure.figsize'] = 12, 4\nplt.plot(Y_diff)\nplt.title(\"Serie diferenciada\")\nplt.show()\n</pre> # diferenciacion  Y_diff = y.diff()  rcParams['figure.figsize'] = 12, 4 plt.plot(Y_diff) plt.title(\"Serie diferenciada\") plt.show() In\u00a0[14]: Copied! <pre>def box_transformations(y,param):\n    if param&gt;0:\n        return y.apply(lambda x: (x**(param)-1)/param)\n    elif param==0:\n        return np.log(y)\n    else:\n        print(\"lambda es negativo, se devulve la serie original\")\n        return y\n</pre> def box_transformations(y,param):     if param&gt;0:         return y.apply(lambda x: (x**(param)-1)/param)     elif param==0:         return np.log(y)     else:         print(\"lambda es negativo, se devulve la serie original\")         return y In\u00a0[15]: Copied! <pre># logaritmo\n\nY_log = box_transformations(y,0)\n\nrcParams['figure.figsize'] = 12, 4\nplt.plot(Y_log)\nplt.title(\"Serie logaritmica\")\nplt.show()\n</pre> # logaritmo  Y_log = box_transformations(y,0)  rcParams['figure.figsize'] = 12, 4 plt.plot(Y_log) plt.title(\"Serie logaritmica\") plt.show() In\u00a0[16]: Copied! <pre># cuadratica\n\nY_quad = box_transformations(y,2)\n\nrcParams['figure.figsize'] = 12, 4\nplt.plot(Y_log)\nplt.title(\"Serie cuadratica\")\nplt.show()\n</pre> # cuadratica  Y_quad = box_transformations(y,2)  rcParams['figure.figsize'] = 12, 4 plt.plot(Y_log) plt.title(\"Serie cuadratica\") plt.show() <p>Se deja como tarea al lector profundizar m\u00e1s en estos conceptos.</p> In\u00a0[17]: Copied! <pre>target_date =  '1958-01-01'\n\n# crear conjunto de entrenamiento y de testeo\nmask_ds = y.index &lt; target_date\n\ny_train = y[mask_ds]\ny_test = y[~mask_ds]\n\n#plotting the data\ny_train['passengers'].plot()\ny_test['passengers'].plot()\nplt.show()\n</pre> target_date =  '1958-01-01'  # crear conjunto de entrenamiento y de testeo mask_ds = y.index &lt; target_date  y_train = y[mask_ds] y_test = y[~mask_ds]  #plotting the data y_train['passengers'].plot() y_test['passengers'].plot() plt.show() <p>Una pregunta natural que surgue es: \u00bf por qu\u00e9 no se toman datos de manera aleatoria?.</p> <p>La respuesta es que como se trabaja el la variable tiempo, por lo tanto los datos siguen un orden natural de los sucesos, en cambio, en los problemas de regresi\u00f3n no existe orden en los sucesos, por cada par de punto es de cierta forma independiente uno con otros. Adem\u00e1s, si se sacar\u00e1n puntos de testeo de manera aleatoria, podr\u00eda romper con la tendencia y estacionariedad original de la serie.</p> <p>Veamos un ejemplo sensillo de este caso en python:</p> In\u00a0[18]: Copied! <pre>from statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\ndef mean_absolute_percentage_error(y_true, y_pred):\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\ndef regression_metrics(df):\n    \"\"\"\n    Aplicar las distintas m\u00e9tricas definidas\n    :param df: DataFrame con las columnas: ['y', 'yhat']\n    :return: DataFrame con las m\u00e9tricas especificadas\n    \"\"\"\n    df_result = pd.DataFrame()\n\n    y_true = df['y']\n    y_pred = df['yhat']\n\n    df_result['mae'] = [round(mean_absolute_error(y_true, y_pred), 4)]\n    df_result['mse'] = [round(mean_squared_error(y_true, y_pred), 4)]\n    df_result['rmse'] = [round(np.sqrt(mean_squared_error(y_true, y_pred)), 4)]\n    df_result['mape'] = [round(mean_absolute_percentage_error(y_true, y_pred), 4)]\n    df_result['smape'] = [round(2 * mean_absolute_percentage_error(y_true, y_pred) / (mean_absolute_percentage_error(y_true, y_pred) + 100), 4)]\n\n    return df_result\n</pre> from statsmodels.tsa.statespace.sarimax import SARIMAX from sklearn.metrics import mean_absolute_error, mean_squared_error  def mean_absolute_percentage_error(y_true, y_pred):     return np.mean(np.abs((y_true - y_pred) / y_true)) * 100  def regression_metrics(df):     \"\"\"     Aplicar las distintas m\u00e9tricas definidas     :param df: DataFrame con las columnas: ['y', 'yhat']     :return: DataFrame con las m\u00e9tricas especificadas     \"\"\"     df_result = pd.DataFrame()      y_true = df['y']     y_pred = df['yhat']      df_result['mae'] = [round(mean_absolute_error(y_true, y_pred), 4)]     df_result['mse'] = [round(mean_squared_error(y_true, y_pred), 4)]     df_result['rmse'] = [round(np.sqrt(mean_squared_error(y_true, y_pred)), 4)]     df_result['mape'] = [round(mean_absolute_percentage_error(y_true, y_pred), 4)]     df_result['smape'] = [round(2 * mean_absolute_percentage_error(y_true, y_pred) / (mean_absolute_percentage_error(y_true, y_pred) + 100), 4)]      return df_result In\u00a0[19]: Copied! <pre># parametros\nparam = [(1,0,0),(0,0,0,12)]\n\n# modelo\nmodel = SARIMAX(y_train,\n                        order=param[0],\n                        seasonal_order=param[1],\n                        enforce_stationarity=False,\n                        enforce_invertibility=False)\n# ajustar modelo\nmodel_fit = model.fit(disp=0)\n\n# fecha de las predicciones        \nstart_index = y_test.index.min()\nend_index = y_test.index.max()\n\npreds = model_fit.get_prediction(start=start_index,end=end_index, dynamic=False)\ndf_temp = pd.DataFrame(\n            {\n                'y':y_test['passengers'],\n                'yhat': preds.predicted_mean\n            }\n        )\n\n# resultados del ajuste\ndf_temp.head()\n</pre> # parametros param = [(1,0,0),(0,0,0,12)]  # modelo model = SARIMAX(y_train,                         order=param[0],                         seasonal_order=param[1],                         enforce_stationarity=False,                         enforce_invertibility=False) # ajustar modelo model_fit = model.fit(disp=0)  # fecha de las predicciones         start_index = y_test.index.min() end_index = y_test.index.max()  preds = model_fit.get_prediction(start=start_index,end=end_index, dynamic=False) df_temp = pd.DataFrame(             {                 'y':y_test['passengers'],                 'yhat': preds.predicted_mean             }         )  # resultados del ajuste df_temp.head() Out[19]: y yhat 1958-01-31 340.0 336.756041 1958-02-28 318.0 337.513783 1958-03-31 362.0 338.273230 1958-04-30 348.0 339.034386 1958-05-31 363.0 339.797254 In\u00a0[20]: Copied! <pre># resultados de las m\u00e9tricas\ndf_metrics = regression_metrics(df_temp)\ndf_metrics['model'] = f\"SARIMA_{param[0]}X{param[1]}\".replace(' ','')\ndf_metrics\n</pre> # resultados de las m\u00e9tricas df_metrics = regression_metrics(df_temp) df_metrics['model'] = f\"SARIMA_{param[0]}X{param[1]}\".replace(' ','') df_metrics Out[20]: mae mse rmse mape smape model 0 81.8529 11619.4305 107.7935 17.0068 0.2907 SARIMA_(1,0,0)X(0,0,0,12) In\u00a0[21]: Copied! <pre># graficamos resultados\n\npreds = df_temp['yhat']\nax = y['1949':].plot(label='observed')\npreds.plot(ax=ax, label='Forecast', alpha=.7, figsize=(14, 7))\nax.set_xlabel('Date')\nax.set_ylabel('Passengers')\nplt.legend()\nplt.show()\n</pre> # graficamos resultados  preds = df_temp['yhat'] ax = y['1949':].plot(label='observed') preds.plot(ax=ax, label='Forecast', alpha=.7, figsize=(14, 7)) ax.set_xlabel('Date') ax.set_ylabel('Passengers') plt.legend() plt.show() <p>Observamos a simple vista que el ajuste no es tan bueno que digamos (analizando las m\u00e9tricas y el gr\u00e1fico).</p> <p>Entonces, \u00bf qu\u00e9 se puede hacer?. La respuesta es simple \u00a1 probar varios modelos sarima !.</p> <p>Ahora, \u00bf c\u00f3mo lo hacemos?. Lo primero es definir una clase llamada <code>SarimaModels</code> que automatice el proceso anterior, y nos quedamos con aquel modelo que minimice alguna de las m\u00e9tricas propuestas, por ejemplo, minimizar las m\u00e9tricas de mae y mape</p> In\u00a0[22]: Copied! <pre># creando clase SarimaModels\n\nclass SarimaModels:\n    def __init__(self,params):\n\n        self.params = params\n        \n        \n    @property\n    def name_model(self):\n        return f\"SARIMA_{self.params[0]}X{self.params[1]}\".replace(' ','')\n    \n    @staticmethod\n    def test_train_model(y,date):\n        mask_ds = y.index &lt; date\n\n        y_train = y[mask_ds]\n        y_test = y[~mask_ds]        \n        \n        return y_train, y_test\n    \n    def fit_model(self,y,date):\n        y_train, y_test = self.test_train_model(y,date )\n        model = SARIMAX(y_train,\n                        order=self.params[0],\n                        seasonal_order=self.params[1],\n                        enforce_stationarity=False,\n                        enforce_invertibility=False)\n        \n        model_fit = model.fit(disp=0)\n\n        return model_fit\n    \n    def df_testig(self,y,date):\n        y_train, y_test = self.test_train_model(y,date )\n        model = SARIMAX(y_train,\n                        order=self.params[0],\n                        seasonal_order=self.params[1],\n                        enforce_stationarity=False,\n                        enforce_invertibility=False)\n        \n        model_fit = model.fit(disp=0)\n        \n        start_index = y_test.index.min()\n        end_index = y_test.index.max()\n\n        preds = model_fit.get_prediction(start=start_index,end=end_index, dynamic=False)\n        df_temp = pd.DataFrame(\n            {\n                'y':y_test['passengers'],\n                'yhat': preds.predicted_mean\n            }\n        )\n        \n        return df_temp\n    \n    def metrics(self,y,date):\n        df_temp = self.df_testig(y,date)\n        df_metrics = regression_metrics(df_temp)\n        df_metrics['model'] = self.name_model\n        \n        return df_metrics\n</pre> # creando clase SarimaModels  class SarimaModels:     def __init__(self,params):          self.params = params                       @property     def name_model(self):         return f\"SARIMA_{self.params[0]}X{self.params[1]}\".replace(' ','')          @staticmethod     def test_train_model(y,date):         mask_ds = y.index &lt; date          y_train = y[mask_ds]         y_test = y[~mask_ds]                          return y_train, y_test          def fit_model(self,y,date):         y_train, y_test = self.test_train_model(y,date )         model = SARIMAX(y_train,                         order=self.params[0],                         seasonal_order=self.params[1],                         enforce_stationarity=False,                         enforce_invertibility=False)                  model_fit = model.fit(disp=0)          return model_fit          def df_testig(self,y,date):         y_train, y_test = self.test_train_model(y,date )         model = SARIMAX(y_train,                         order=self.params[0],                         seasonal_order=self.params[1],                         enforce_stationarity=False,                         enforce_invertibility=False)                  model_fit = model.fit(disp=0)                  start_index = y_test.index.min()         end_index = y_test.index.max()          preds = model_fit.get_prediction(start=start_index,end=end_index, dynamic=False)         df_temp = pd.DataFrame(             {                 'y':y_test['passengers'],                 'yhat': preds.predicted_mean             }         )                  return df_temp          def metrics(self,y,date):         df_temp = self.df_testig(y,date)         df_metrics = regression_metrics(df_temp)         df_metrics['model'] = self.name_model                  return df_metrics In\u00a0[23]: Copied! <pre># definir parametros \n\nimport itertools\n\np = d = q = range(0, 2)\npdq = list(itertools.product(p, d, q))\nseasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n\nparams = list(itertools.product(pdq,seasonal_pdq))\ntarget_date = '1958-01-01'\n</pre> # definir parametros   import itertools  p = d = q = range(0, 2) pdq = list(itertools.product(p, d, q)) seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]  params = list(itertools.product(pdq,seasonal_pdq)) target_date = '1958-01-01' In\u00a0[24]: Copied! <pre># iterar para los distintos escenarios\n\nframes = []\nfor param in params:\n    try:\n        sarima_model = SarimaModels(param)\n        df_metrics = sarima_model.metrics(y,target_date)\n        frames.append(df_metrics)\n    except:\n        pass\n</pre> # iterar para los distintos escenarios  frames = [] for param in params:     try:         sarima_model = SarimaModels(param)         df_metrics = sarima_model.metrics(y,target_date)         frames.append(df_metrics)     except:         pass In\u00a0[25]: Copied! <pre># juntar resultados de las m\u00e9tricas y comparar\ndf_metrics_result = pd.concat(frames)\ndf_metrics_result.sort_values(['mae','mape'])\n</pre> # juntar resultados de las m\u00e9tricas y comparar df_metrics_result = pd.concat(frames) df_metrics_result.sort_values(['mae','mape']) Out[25]: mae mse rmse mape smape model 0 16.4272 406.5114 20.1621 4.1002 0.0788 SARIMA_(0,1,0)X(1,0,0,12) 0 17.7173 469.8340 21.6757 4.2001 0.0806 SARIMA_(0,1,1)X(1,1,1,12) 0 17.7204 480.9764 21.9312 4.1431 0.0796 SARIMA_(1,1,0)X(1,1,1,12) 0 17.8053 501.0603 22.3844 4.1164 0.0791 SARIMA_(1,1,1)X(0,1,0,12) 0 17.8056 505.4167 22.4815 4.1061 0.0789 SARIMA_(0,1,0)X(0,1,0,12) ... ... ... ... ... ... ... 0 94.9444 14674.5556 121.1386 19.8867 0.3318 SARIMA_(0,1,0)X(0,0,0,12) 0 360.7115 150709.0664 388.2127 82.0329 0.9013 SARIMA_(0,0,1)X(0,0,1,12) 0 366.5303 153175.7993 391.3768 83.7030 0.9113 SARIMA_(0,0,0)X(0,0,1,12) 0 422.9626 187068.9748 432.5147 98.3713 0.9918 SARIMA_(0,0,1)X(0,0,0,12) 0 428.5000 189730.5556 435.5807 100.0000 1.0000 SARIMA_(0,0,0)X(0,0,0,12) <p>64 rows \u00d7 6 columns</p> <p>En este caso el mejor modelo resulta ser el modelo $SARIMA(0,1,0)X(1,0,0,12)$. Veamos gr\u00e1ficamente que tal el ajuste de este modelo.</p> In\u00a0[26]: Copied! <pre># ajustar mejor modelo\n\nparam = [(0,1,0),(1,0,0,12)]\nsarima_model =  SarimaModels(param)\nmodel_fit = sarima_model.fit_model(y,target_date)\nbest_model = sarima_model.df_testig(y,target_date)\nbest_model.head()\n</pre> # ajustar mejor modelo  param = [(0,1,0),(1,0,0,12)] sarima_model =  SarimaModels(param) model_fit = sarima_model.fit_model(y,target_date) best_model = sarima_model.df_testig(y,target_date) best_model.head() Out[26]: y yhat 1958-01-31 340.0 345.765806 1958-02-28 318.0 330.574552 1958-03-31 362.0 390.254475 1958-04-30 348.0 381.573759 1958-05-31 363.0 389.169386 In\u00a0[27]: Copied! <pre># graficar mejor modelo\n\npreds = best_model['yhat']\nax = y['1949':].plot(label='observed')\npreds.plot(ax=ax, label='Forecast', alpha=.7, figsize=(14, 7))\nax.set_xlabel('Date')\nax.set_ylabel('Passengers')\nplt.legend()\nplt.show()\n</pre> # graficar mejor modelo  preds = best_model['yhat'] ax = y['1949':].plot(label='observed') preds.plot(ax=ax, label='Forecast', alpha=.7, figsize=(14, 7)) ax.set_xlabel('Date') ax.set_ylabel('Passengers') plt.legend() plt.show() <p>Para este caso, el mejor modelo encontrado se ajusta bastante bien a los datos.</p> <p>Finalmente, vemos algunos resultados del error asosciado al modelo. Para esto ocupamos la herramienta plot_diagnostics, el cual nos arroja cuatro gr\u00e1ficos analizando el error de diferentes manera.</p> In\u00a0[28]: Copied! <pre># resultados del error \nmodel_fit.plot_diagnostics(figsize=(16, 8))\nplt.show()\n</pre> # resultados del error  model_fit.plot_diagnostics(figsize=(16, 8)) plt.show() <ul> <li><p>gr\u00e1fico 01 (standarized residual): Este gr\u00e1fico nos muestra el error estandarizado en el tiempo. En este caso se observa que esta nueva serie de tiempo corresponde a una serie estacionaria que oscila entorno al cero, es decir, un ruido blanco.</p> </li> <li><p>gr\u00e1fico 02 (histogram plus estimated density): Este gr\u00e1fico nos muestra el histograma del error. En este caso, el histograma es muy similar al histograma de una variable $\\mathcal{N}(0,1)$ (ruido blanco).</p> </li> <li><p>gr\u00e1fico 03 (normal QQ):  el gr\u00e1fico Q-Q (\"Q\" viene de cuantil) es un m\u00e9todo gr\u00e1fico para el diagn\u00f3stico de diferencias entre la distribuci\u00f3n de probabilidad de una poblaci\u00f3n de la que se ha extra\u00eddo una muestra aleatoria y una distribuci\u00f3n usada para la comparaci\u00f3n. En este caso se comparar la distribuci\u00f3n del error versus una distribuci\u00f3n normal. Cuando mejor es el ajuste lineal sobre los puntos, m\u00e1s parecida es la distribuci\u00f3n entre la muestra obtenida y la distribuci\u00f3n de prueba (distribuci\u00f3n normal).</p> </li> <li><p>gr\u00e1fico 04 (correlogram): Este gr\u00e1fico nos muestra el gr\u00e1fico de autocorrelaci\u00f3n entre las variables del error, se observa que no hay correlaci\u00f3n entre ninguna de las variables, por lo que se puedan dar indicios de independencia entre las variables.</p> </li> </ul> <p>En conclusi\u00f3n, el error asociado al modelo en estudio corresponde a un ruido blanco.</p> In\u00a0[29]: Copied! <pre>from prophet import Prophet\n</pre> from prophet import Prophet <pre>C:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\db-connectors-RsEieBu8-py3.8\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[30]: Copied! <pre># rename \ny_train_prophet = y_train.reset_index()\ny_train_prophet.columns = [\"ds\",\"y\"]\n\ny_test_prophet = y_test.reset_index()\ny_test_prophet.columns = [\"ds\",\"y\"]\n</pre> # rename  y_train_prophet = y_train.reset_index() y_train_prophet.columns = [\"ds\",\"y\"]  y_test_prophet = y_test.reset_index() y_test_prophet.columns = [\"ds\",\"y\"] In\u00a0[31]: Copied! <pre># model\nm = Prophet()\nm.fit(y_train_prophet)\n</pre> # model m = Prophet() m.fit(y_train_prophet) <pre>21:45:53 - cmdstanpy - INFO - Chain [1] start processing\n21:45:53 - cmdstanpy - INFO - Chain [1] done processing\n</pre> Out[31]: <pre>&lt;prophet.forecaster.Prophet at 0x20c23ff6d60&gt;</pre> In\u00a0[32]: Copied! <pre># forecast\nfuture = m.make_future_dataframe(periods=365*4)\nforecast = m.predict(future)[['ds', 'yhat']]\nforecast.tail()\n</pre> # forecast future = m.make_future_dataframe(periods=365*4) forecast = m.predict(future)[['ds', 'yhat']] forecast.tail() Out[32]: ds yhat 1563 1961-12-26 535.962819 1564 1961-12-27 534.927951 1565 1961-12-28 533.244132 1566 1961-12-29 530.943853 1567 1961-12-30 528.076593 In\u00a0[33]: Copied! <pre># metrics\nresult = y_test_prophet.merge(forecast,on = 'ds',how='inner')\nregression_metrics(result)\n</pre> # metrics result = y_test_prophet.merge(forecast,on = 'ds',how='inner') regression_metrics(result) Out[33]: mae mse rmse mape smape 0 39.9557 2013.1356 44.868 9.8525 0.1794 In\u00a0[34]: Copied! <pre>preds = result[['ds','yhat']].set_index(\"ds\")\nax = y['1949':].plot(label='observed')\npreds.plot(ax=ax, label='Forecast', alpha=.7, figsize=(14, 7))\n\nax.set_xlabel('Date')\nax.set_ylabel('Passengers')\nplt.legend()\nplt.show()\n</pre> preds = result[['ds','yhat']].set_index(\"ds\") ax = y['1949':].plot(label='observed') preds.plot(ax=ax, label='Forecast', alpha=.7, figsize=(14, 7))  ax.set_xlabel('Date') ax.set_ylabel('Passengers') plt.legend() plt.show()"},{"location":"lectures/machine_learning/ts_01/#series-temporales-i","title":"Series Temporales I\u00b6","text":""},{"location":"lectures/machine_learning/ts_01/#introduccion","title":"Introducci\u00f3n\u00b6","text":""},{"location":"lectures/machine_learning/ts_01/#analisis-de-series-temporales","title":"An\u00e1lisis de series temporales\u00b6","text":"<p>Para entender mejor el comportamiento de una serie temporal, nos iremos ayudando con python.</p>"},{"location":"lectures/machine_learning/ts_01/#descripcion-del-problema","title":"Descripci\u00f3n del problema\u00b6","text":"<p>El conjunto de datos se llama <code>AirPassengers.csv</code>, el cual contiene la informaci\u00f3n del total de pasajeros (a nivel de mes) entre los a\u00f1o 1949 y 1960.</p> <p>En terminos  estad\u00edsticos, el problema puede ser presentado por la serie temporal $\\left \\{ X_t:  t \\in T \\right \\}$ , donde:</p> <ul> <li>$X_{t}$: corresponde al total de pasajeros en el tiempo t</li> <li>$t$: tiempo medido a nivel de mes.</li> </ul> <p>Comparando la serie temporal con un dataframe, este ser\u00eda un dataframe de una sola columna, en cuyo \u00edndice se encuentra las distintas fechas.</p> <p>El objetivo es poder desarrollar un modelo predictivo que me indique el n\u00famero de pasajeros para los pr\u00f3ximos dos a\u00f1os. Antes de ajustar el modelo, se debe entender el comportamiento de la serie de tiempo en estudio y con esta informaci\u00f3n, encontrar el modelo que mejor se puede ajustar (en caso que exista).</p> <p>Como son muchos los conceptos que se presentar\u00e1n, es necesario ir apoyandose con alguna herramienta de programaci\u00f3n, en nuestro caso python. Dentro de python, la librer\u00eda statsmodels es ideal para hacer este tipo de an\u00e1lisis.</p> <p>Lo primero es cargar, transformar y visualizar el conjunto de datos.</p>"},{"location":"lectures/machine_learning/ts_01/#descomposicion-stl","title":"Descomposici\u00f3n STL\u00b6","text":"<p>Una serie temporal la podemos descomponer en tres componentes:</p> <ul> <li>tendencia ($T$): trayectoria de los datos en el tiempo (direcci\u00f3n positiva o negativa).</li> <li>estacionalidad($S$):  fluctuaciones regulares y predecibles en un periodo determinado (anual, semestral,etc.)</li> <li>ruido($e$): error intr\u00ednsico al tomar una serie temporal (instrumenos, medici\u00f3n humana, etc.)</li> </ul> <p>Cuando un descompone la serie temporla en sus tres componenctes (tendencia, estacionalidad, ruido) se habla de descompocisi\u00f3n STL. En muchas ocasiones no es posible descomponer adecuadamente la serie temporal, puesto que la muestra obtenida no presenta un comportamiento ciclico o repetitivo en el periodo de tiempo analizado.</p> <p>Por otro lado, esta descomposici\u00f3n se puede realizar de dos maneras diferentes:</p> <ul> <li>Aditiva: $$X_{t} = T_{t} + S_{t} + e_{t}$$</li> <li>Multiplicativa: $$X_{t} = T_{t} * S_{t} * e_{t}$$</li> </ul> <p>Por suepuesto esta no es la \u00fanica forma de descomponer una serie, pero sirve como punto de partida para comprender nuestra serie en estudio.</p> <p>Realizaremos un descomposici\u00f3n del tipo multiplicativa, ocupando el comando de statsmodels seasonal_decompose.</p>"},{"location":"lectures/machine_learning/ts_01/#series-estacionarias","title":"Series Estacionarias\u00b6","text":"<p>Un concepto importante para el \u00e1nalisis de series temporales, es el concepto de estacionariedad.</p>"},{"location":"lectures/machine_learning/ts_01/#definicion","title":"Definici\u00f3n\u00b6","text":"<p>Sea $\\left \\{ X_t: t \\in T \\right \\}$ una serie temporal. Se dice que una serie temporal es d\u00e9bilmente estacionaria si:</p> <ul> <li>$\\mathbb{V}(X_t) &lt; \\infty$, para todo $t \\in T$.</li> <li>$\\mu_{X}(t)= \\mu$, para todo $t \\in T$.</li> <li>$\\gamma_{X}(r,s)= \\gamma_{X}(r+h,s+h)=\\gamma_{X}(h)  $, para todo $r,s,h\\in T$.</li> </ul> <p>En palabras simple, una serie temporal es d\u00e9bilmente estacionaria si var\u00eda poco respecto a su propia media.</p> <p>Veamos las siguientes im\u00e1genes:</p>"},{"location":"lectures/machine_learning/ts_01/#formas-de-detectar-la-estacionariedad","title":"Formas de detectar la estacionariedad\u00b6","text":"<p>La manera m\u00e1s simple es gr\u00e1ficarla e inferir el comportamiento de esta. La ventaja que este m\u00e9todo es r\u00e1pido, sin embargo, se encuentra sesgado por el criterio del ojo humano.</p> <p>Por otro lado existen algunas alternativas que aqu\u00ed presentamos:</p>"},{"location":"lectures/machine_learning/ts_01/#modelos-de-forecast-pronostico","title":"Modelos de forecast  (Pron\u00f3stico)\u00b6","text":"<p>Para realizar el pron\u00f3stico de series, existen varios modelos cl\u00e1sicos para analizar series temporales:</p> <p>Modelos con variabilidad (varianza) constante</p> <ul> <li><p>Modelos de media m\u00f3vil (MA(d)): el modelo queda en funci\u00f3n de los ruidos $e_{1},e_{2},...,e_{d}$</p> </li> <li><p>Modelos autorregresivos (AR(q)): el modelo queda en funci\u00f3n de los ruidos $X_{1},X_{2},...,X_{q}$</p> </li> <li><p>Modelos ARMA  (ARMA(p,q)): Mezcla de los modelos $MA(d)$ y $AR(q)$</p> </li> <li><p>Modelos ARIMA (ARIMA(p,d,q)):: Mezcla de los modelos $MA(d)$ y $AR(q)$ sobre la serie diferenciada $d$ veces.</p> </li> <li><p>Modelos SARIMA (SARIMA(p,d,q)x(P,D,Q,S)): Mezcla de los modelos ARIMA(p,d,q) agregando componentes de estacionariedad ($S$).</p> </li> </ul> <p>Dentro de estos modelos, se tiene que uno son un caso particular de otro m\u00e1s general:</p> <p>$$MA(d),AR(q) \\subset ARMA(p,q) \\subset ARIMA(p,d,q)  \\subset SARIMA(p,d,q)x(P,D,Q,S)  $$</p> <p>Modelos de volatibilidad</p> <ul> <li><p>Arch</p> </li> <li><p>Garch</p> </li> <li><p>Modelos de espacio estado</p> </li> </ul>"},{"location":"lectures/machine_learning/ts_01/#realizar-pronostico-con-statsmodels","title":"Realizar pron\u00f3stico con statsmodels\u00b6","text":"<p>El pron\u00f3stico lo realizaremos ocupando los modelos disponible en statsmodels, particularmen los modelos $SARIMA(p,d,q)x(P,D,Q,S)$.</p> <p>Como todo buen proceso de machine learning es necesario separar nuestro conjunto de datos en dos (entrenamiento y testeo). \u00bf C\u00f3mo se realiza esto con series temporales ?.</p> <p>El camino correcto para considerar una fecha objetivo (target_date), el cual separ\u00e9 en dos conjuntos, de la siguiente manera:</p> <ul> <li>y_train: todos los datos hasta la fecha target_date</li> <li>y_test: todos los datos despu\u00e9s la fecha target_date</li> </ul>"},{"location":"lectures/machine_learning/ts_01/#prophet","title":"Prophet\u00b6","text":"<p>Prophet es un procedimiento para pronosticar datos de series temporales basado en un modelo aditivo en el que las tendencias no lineales se ajustan a la estacionalidad anual, semanal y diaria, adem\u00e1s de los efectos de las vacaciones. Funciona mejor con series temporales que tienen fuertes efectos estacionales y varias temporadas de datos hist\u00f3ricos. Prophet es resistente a los datos faltantes y los cambios en la tendencia, y por lo general maneja bien los valores at\u00edpicos.</p> <p>Prophet es un software de c\u00f3digo abierto lanzado por el equipo Core Data Science de Facebook. Est\u00e1 disponible para descargar en CRAN y PyPI.</p> <p>Nota: Para entender mejor este algoritmo, puede leer el siguiente paper.</p>"},{"location":"lectures/machine_learning/ts_01/#conclusion","title":"Conclusi\u00f3n\u00b6","text":"<p>Este fue una introducci\u00f3n amigable a los conceptos claves de series temporales, a medida que m\u00e1s se profundice en la teor\u00eda, mejor ser\u00e1n las t\u00e9cnicas empleadas sobre la serie temporal en fin de obtener el mejor pron\u00f3stico posible.</p> <p>En esta secci\u00f3n nos limitamos a algunos modelos y algunos criterios de verificaci\u00f3n de estacionariedad. En la literatura existen muchas m\u00e1s, pero con los mostrados de momento y un poco de expertice en el tema, se pueden abordar casi todos los problemas de series temporales.</p>"},{"location":"lectures/machine_learning/ts_01/#referencias","title":"Referencias\u00b6","text":"<ol> <li>A comprehensive beginner\u2019s guide to create a Time Series Forecast (with Codes in Python and R)</li> <li>A Gentle Introduction to SARIMA for Time Series Forecasting in Python</li> <li>An Introductory Study on Time Series Modeling and Forecasting </li> </ol>"},{"location":"lectures/toolkit/bt_01/","title":"Python","text":"<p>Nota: Para profundizar en conceptos b\u00e1sicos de Python, se recomienda estudiar del curso Introducci\u00f3n a Python.</p> <p></p> In\u00a0[1]: Copied! <pre># imprimir \"Hola Mundo!\"\nprint(\"Hola Mundo!\");\n</pre> # imprimir \"Hola Mundo!\" print(\"Hola Mundo!\"); <pre>Hola Mundo!\n</pre> In\u00a0[2]: Copied! <pre># crear e imprimir variables\na = 5\nprint(\"a =\", 5)\na = \"cinco\"\nprint(\"a =\", a)\n</pre> # crear e imprimir variables a = 5 print(\"a =\", 5) a = \"cinco\" print(\"a =\", a) <pre>a = 5\na = cinco\n</pre> In\u00a0[3]: Copied! <pre># operaciones basicas\n\nx = 14\ny = 4\n\n# suma\nprint('x + y =', x+y) # Output: x + y = 18\n\n# resta\nprint('x - y =', x-y) # Output: x - y = 10\n\n# multiplicacion\nprint('x * y =', x*y) # Output: x * y = 56\n\n# division\nprint('x / y =', x/y) # Output: x / y = 3.5\n\n# cuociente\nprint('x // y =', x//y) # Output: x // y = 3\n\n# resto\nprint('x % y =', x%y) # Output: x % y = 2\n</pre> # operaciones basicas  x = 14 y = 4  # suma print('x + y =', x+y) # Output: x + y = 18  # resta print('x - y =', x-y) # Output: x - y = 10  # multiplicacion print('x * y =', x*y) # Output: x * y = 56  # division print('x / y =', x/y) # Output: x / y = 3.5  # cuociente print('x // y =', x//y) # Output: x // y = 3  # resto print('x % y =', x%y) # Output: x % y = 2 <pre>x + y = 18\nx - y = 10\nx * y = 56\nx / y = 3.5\nx // y = 3\nx % y = 2\n</pre> In\u00a0[4]: Copied! <pre># operadores de asignacion\n\nx = 5\n\n# x += 5 ----&gt; x = x + 5\nx +=5\nprint(x) # Output: 10\n\n# x /= 5 ----&gt; x = x / 5\nx /= 5\nprint(x) # Output: 2.0\n</pre> # operadores de asignacion  x = 5  # x += 5 ----&gt; x = x + 5 x +=5 print(x) # Output: 10  # x /= 5 ----&gt; x = x / 5 x /= 5 print(x) # Output: 2.0 <pre>10\n2.0\n</pre> <pre># inputs por el usuario\ninputString = input('Escriba una oracion:')\nprint('Su oracion es:', inputString)\n</pre> In\u00a0[5]: Copied! <pre># correcto\nnum_int = 123  # integer type\nnum_flo = 1.23 # float type\n\nnum_new = num_int + num_flo\n\nprint(\"Valor de num_new:\",num_new)\nprint(\"tipo de datos de  num_new:\",type(num_new))\n</pre> # correcto num_int = 123  # integer type num_flo = 1.23 # float type  num_new = num_int + num_flo  print(\"Valor de num_new:\",num_new) print(\"tipo de datos de  num_new:\",type(num_new)) <pre>Valor de num_new: 124.23\ntipo de datos de  num_new: &lt;class 'float'&gt;\n</pre> In\u00a0[6]: Copied! <pre># incorrecto\nnum_int = 123     # int type\nnum_str = \"456\"   # str type\n\nprint(num_int+num_str)\n</pre> # incorrecto num_int = 123     # int type num_str = \"456\"   # str type  print(num_int+num_str) <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nInput In [6], in &lt;cell line: 5&gt;()\n      2 num_int = 123     # int type\n      3 num_str = \"456\"   # str type\n----&gt; 5 print(num_int+num_str)\n\nTypeError: unsupported operand type(s) for +: 'int' and 'str'</pre> In\u00a0[\u00a0]: Copied! <pre>num_int = 123  # int type\nnum_str = \"456\" # str type\n\n# explicitly converted to int type\nnum_str = int(num_str) \n\nprint(num_int+num_str)\n</pre> num_int = 123  # int type num_str = \"456\" # str type  # explicitly converted to int type num_str = int(num_str)   print(num_int+num_str) In\u00a0[7]: Copied! <pre>print(type(5))\n\n# Output: &lt;class&gt;\nprint(type(5.0))\n\n# &lt;class&gt;\nc = 5 + 3j\nprint(type(c))\n</pre> print(type(5))  # Output:  print(type(5.0))  #  c = 5 + 3j print(type(c)) <pre>&lt;class 'int'&gt;\n&lt;class 'float'&gt;\n&lt;class 'complex'&gt;\n</pre> In\u00a0[8]: Copied! <pre># importar modulo sys\nimport sys\n\nlista_random = ['a', 0, 2]\n\nfor elemento in lista_random:\n    try:\n        print(\"La entrada es \", elemento)\n        r = 1/int(elemento)\n        break\n    except:\n        print(\"Oops! ocurrio un\",sys.exc_info()[0])\n        print(\"Siguiente entrada.\")\n        print()\n        \nprint(\"El reciproco de\",elemento,\"es\",r)\n</pre> # importar modulo sys import sys  lista_random = ['a', 0, 2]  for elemento in lista_random:     try:         print(\"La entrada es \", elemento)         r = 1/int(elemento)         break     except:         print(\"Oops! ocurrio un\",sys.exc_info()[0])         print(\"Siguiente entrada.\")         print()          print(\"El reciproco de\",elemento,\"es\",r) <pre>La entrada es  a\nOops! ocurrio un &lt;class 'ValueError'&gt;\nSiguiente entrada.\n\nLa entrada es  0\nOops! ocurrio un &lt;class 'ZeroDivisionError'&gt;\nSiguiente entrada.\n\nLa entrada es  2\nEl reciproco de 2 es 0.5\n</pre> <p>Algunas Excepciones Comunes</p> <ul> <li>NameError: Esta excepci\u00f3n es levantada cuando el programa no puede encontrar un nombre local o global. El nombre que podr\u00eda no ser encontrado est\u00e1 incluido en el mensaje de error.</li> <li>TypeError: Esta excepci\u00f3n es levantada cuando una funci\u00f3n se le pasa un objeto del tipo inapropiado como su argumento. M\u00e1s detalles sobre el tipo incorrecto son proporcionados en el mensaje de error.</li> <li>ValueError: Esta excepci\u00f3n ocurre cuando un argumento de funci\u00f3n tiene el tipo correcto pero un valor inapropiado.</li> <li>NotImplementedError: Esta excepci\u00f3n es levantada cuando se supone que un objeto apoye una operaci\u00f3n pero no ha sido implementado a\u00fan. No deber\u00edas usar este error cuando la funci\u00f3n dada no deba apoyar al tipo de argumento de entrada. En esas situaciones, levantar una excepci\u00f3n TypeError es m\u00e1s apropiado.</li> <li>ZeroDivisionError: Esta excepci\u00f3n es levantada cuando proporcionas el segundo argumento para una operaci\u00f3n de divisi\u00f3n o m\u00f3dulo como cero.</li> <li>FileNotFoundError: Esta excepci\u00f3n es levantada cuando el archivo o diccionario que el programa solicit\u00f3 no existe.</li> </ul> In\u00a0[9]: Copied! <pre># definir funcion\ndef suma(x,y):\n    return x+y\n</pre> # definir funcion def suma(x,y):     return x+y In\u00a0[10]: Copied! <pre># ejemplo correcto\nassert suma(1,1)==2, \"ejemplo invalido\"\n</pre> # ejemplo correcto assert suma(1,1)==2, \"ejemplo invalido\" In\u00a0[11]: Copied! <pre># ejemplo incorrecto\nassert suma(1,1)==3, \"ejemplo invalido\"\n</pre> # ejemplo incorrecto assert suma(1,1)==3, \"ejemplo invalido\" <pre>\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\nInput In [11], in &lt;cell line: 2&gt;()\n      1 # ejemplo incorrecto\n----&gt; 2 assert suma(1,1)==3, \"ejemplo invalido\"\n\nAssertionError: ejemplo invalido</pre> <p></p> In\u00a0[12]: Copied! <pre># lista vacia\nlista = []\n# lista de enteros\nlista = [1, 2, 3]\n# lista mixta\nlista = [1, \"hola\", 3.4]\n</pre> # lista vacia lista = [] # lista de enteros lista = [1, 2, 3] # lista mixta lista = [1, \"hola\", 3.4] <p>Tambi\u00e9n, se pueden acceder a cada uno de sus eleentos</p> In\u00a0[13]: Copied! <pre>numeros = [\"uno\", \"dos\", \"tres\", \"cuatro\"]\n\n# acceder al primer elemento\nprint(numeros[0])\n\n\n# acceder al cuarto elemento\nprint(numeros[1:3])\n</pre> numeros = [\"uno\", \"dos\", \"tres\", \"cuatro\"]  # acceder al primer elemento print(numeros[0])   # acceder al cuarto elemento print(numeros[1:3]) <pre>uno\n['dos', 'tres']\n</pre> In\u00a0[14]: Copied! <pre>numeros = (\"uno\", \"dos\", \"tres\", \"cuatro\")\nprint(numeros)\n</pre> numeros = (\"uno\", \"dos\", \"tres\", \"cuatro\") print(numeros) <pre>('uno', 'dos', 'tres', 'cuatro')\n</pre> In\u00a0[15]: Copied! <pre>numeros = (\"uno\", \"dos\", \"tres\", \"cuatro\")\n\nprint(numeros[1]) #Output: dos\nprint(numeros[3]) #Output: cuatro\nprint(numeros[-1]) # Output: cuatro\n</pre> numeros = (\"uno\", \"dos\", \"tres\", \"cuatro\")  print(numeros[1]) #Output: dos print(numeros[3]) #Output: cuatro print(numeros[-1]) # Output: cuatro <pre>dos\ncuatro\ncuatro\n</pre> In\u00a0[16]: Copied! <pre># conjunto de enteros\nconjunto = {1, 2, 3}\nprint(conjunto)\n\n# conjunto mixto\nconjunto = {1.0, \"hola\", (1, 2, 3)}\nprint(conjunto)\n</pre> # conjunto de enteros conjunto = {1, 2, 3} print(conjunto)  # conjunto mixto conjunto = {1.0, \"hola\", (1, 2, 3)} print(conjunto) <pre>{1, 2, 3}\n{1.0, (1, 2, 3), 'hola'}\n</pre> In\u00a0[17]: Copied! <pre># conjunto de enteros\nconjunto = {1, 2, 3}\n\nconjunto.add(4)\nprint(conjunto) # Output: {1, 2, 3, 4}\n\nconjunto.add(2)\nprint(conjunto) # Output: {1, 2, 3, 4}\n\nconjunto.update([3, 4, 5])\nprint(conjunto) # Output: {1, 2, 3, 4, 5}\n\nconjunto.remove(4)\nprint(conjunto) # Output: {1, 2, 3, 5}\n</pre> # conjunto de enteros conjunto = {1, 2, 3}  conjunto.add(4) print(conjunto) # Output: {1, 2, 3, 4}  conjunto.add(2) print(conjunto) # Output: {1, 2, 3, 4}  conjunto.update([3, 4, 5]) print(conjunto) # Output: {1, 2, 3, 4, 5}  conjunto.remove(4) print(conjunto) # Output: {1, 2, 3, 5} <pre>{1, 2, 3, 4}\n{1, 2, 3, 4}\n{1, 2, 3, 4, 5}\n{1, 2, 3, 5}\n</pre> In\u00a0[18]: Copied! <pre># diccionario vacio\ndct = {}\n\n# diccionario de enteros\ndct = {1: 'apple', 2: 'ball'}\n\n# diccionario dde llaves mixtas\ndct = {'name': 'John', 1: [2, 4, 3]}\n</pre> # diccionario vacio dct = {}  # diccionario de enteros dct = {1: 'apple', 2: 'ball'}  # diccionario dde llaves mixtas dct = {'name': 'John', 1: [2, 4, 3]} In\u00a0[19]: Copied! <pre>dct = {'nombre':'Jack', 'edad': 26, 'salario': 4534.2}\nprint(dct['edad']) # Output: 26\n</pre> dct = {'nombre':'Jack', 'edad': 26, 'salario': 4534.2} print(dct['edad']) # Output: 26 <pre>26\n</pre> In\u00a0[20]: Copied! <pre>dct = {'nombre':'Jack', 'edad': 26}\n\n# cambiar edad\ndct['edad'] = 36 \nprint(dct) # Output: {'name': 'Jack', 'age': 36}\n\n# adherir llave salario\ndct['salario'] = 4342.4\nprint(dct) # Output: {'name': 'Jack', 'age': 36, 'salary': 4342.4}\n\n\n# borrar llave edad\ndel dct['edad']\nprint(dct) # Output: {'name': 'Jack', 'salary': 4342.4}\n\n# borrar diccionario\ndel dct\n</pre> dct = {'nombre':'Jack', 'edad': 26}  # cambiar edad dct['edad'] = 36  print(dct) # Output: {'name': 'Jack', 'age': 36}  # adherir llave salario dct['salario'] = 4342.4 print(dct) # Output: {'name': 'Jack', 'age': 36, 'salary': 4342.4}   # borrar llave edad del dct['edad'] print(dct) # Output: {'name': 'Jack', 'salary': 4342.4}  # borrar diccionario del dct <pre>{'nombre': 'Jack', 'edad': 36}\n{'nombre': 'Jack', 'edad': 36, 'salario': 4342.4}\n{'nombre': 'Jack', 'salario': 4342.4}\n</pre> In\u00a0[21]: Copied! <pre>print(range(1, 10)) # Output: range(1, 10)\n</pre> print(range(1, 10)) # Output: range(1, 10) <pre>range(1, 10)\n</pre> <p>El resultado es iterable y puede convertirlo en list, tuple, set, etc. Por ejemplo:</p> In\u00a0[22]: Copied! <pre>numeros = range(1, 6)\n\nprint(list(numeros)) # Output: [1, 2, 3, 4, 5]\nprint(tuple(numeros)) # Output: (1, 2, 3, 4, 5)\nprint(set(numeros)) # Output: {1, 2, 3, 4, 5}\n\n# Output: {1: 99, 2: 99, 3: 99, 4: 99, 5: 99} \nprint(dict.fromkeys(numeros, 99))\n</pre> numeros = range(1, 6)  print(list(numeros)) # Output: [1, 2, 3, 4, 5] print(tuple(numeros)) # Output: (1, 2, 3, 4, 5) print(set(numeros)) # Output: {1, 2, 3, 4, 5}  # Output: {1: 99, 2: 99, 3: 99, 4: 99, 5: 99}  print(dict.fromkeys(numeros, 99)) <pre>[1, 2, 3, 4, 5]\n(1, 2, 3, 4, 5)\n{1, 2, 3, 4, 5}\n{1: 99, 2: 99, 3: 99, 4: 99, 5: 99}\n</pre> <p>Hemos omitido el par\u00e1metro de step opcional para range() en los ejemplos anteriores. Cuando se omite, el paso predeterminado es 1. Probemos algunos ejemplos con el par\u00e1metro de paso.</p> In\u00a0[23]: Copied! <pre>numero1 = range(1, 6 , 1)\nprint(list(numero1)) # Output: [1, 2, 3, 4, 5]\n\nnumero2 = range(1, 6, 2)\nprint(list(numero2)) # Output: [1, 3, 5]\n\nnumero3 = range(5, 0, -1)\nprint(list(numero3)) # Output: [5, 4, 3, 2, 1]\n</pre> numero1 = range(1, 6 , 1) print(list(numero1)) # Output: [1, 2, 3, 4, 5]  numero2 = range(1, 6, 2) print(list(numero2)) # Output: [1, 3, 5]  numero3 = range(5, 0, -1) print(list(numero3)) # Output: [5, 4, 3, 2, 1] <pre>[1, 2, 3, 4, 5]\n[1, 3, 5]\n[5, 4, 3, 2, 1]\n</pre> <p></p> In\u00a0[24]: Copied! <pre>num = -1\n\nif num &gt; 0:\n    print(\"numero positivo\")\nelif num == 0:\n    print(\"cero\")\nelse:\n    print(\"numero negativo\")\n    \n# Output: numero negativo\n</pre> num = -1  if num &gt; 0:     print(\"numero positivo\") elif num == 0:     print(\"cero\") else:     print(\"numero negativo\")      # Output: numero negativo <pre>numero negativo\n</pre> <p>Puede haber cero o m\u00e1s partes elif, y la parte else es opcional. La mayor\u00eda de los lenguajes de programaci\u00f3n usan {} para especificar el bloque de c\u00f3digo. Python usa sangr\u00eda.</p> <p>Un bloque de c\u00f3digo comienza con sangr\u00eda y termina con la primera l\u00ednea sin sangr\u00eda. La cantidad de sangr\u00eda depende de usted, pero debe ser consistente a lo largo de ese bloque. En general, se utilizan cuatro espacios en blanco para la sangr\u00eda y se prefieren a las pesta\u00f1as.</p> <p>Probemos con otro ejemplo:</p> In\u00a0[25]: Copied! <pre>if False:\n    print(\"Hola\")\n    print(\"mundo\")\nprint(\"!!!\")\n\n# Output: !!!\n</pre> if False:     print(\"Hola\")     print(\"mundo\") print(\"!!!\")  # Output: !!! <pre>!!!\n</pre> In\u00a0[26]: Copied! <pre>n = 100\n\n# inicializar contador\nsum = 0\ni = 1\n\nwhile i &lt;= n:\n    sum = sum + i\n    i = i+1    # actualizar contador\n\nprint(\"La suma es\", sum)\n\n# Output: La suma es\n</pre> n = 100  # inicializar contador sum = 0 i = 1  while i &lt;= n:     sum = sum + i     i = i+1    # actualizar contador  print(\"La suma es\", sum)  # Output: La suma es <pre>La suma es 5050\n</pre> In\u00a0[27]: Copied! <pre>numbers = [6, 5, 3, 8, 4, 2]\n\nsum = 0\n\n# iterar sobre la lista\nfor val in numbers:\n    sum = sum+val\n\nprint(\"La suma es\", sum) # Output: La suma es 28\n</pre> numbers = [6, 5, 3, 8, 4, 2]  sum = 0  # iterar sobre la lista for val in numbers:     sum = sum+val  print(\"La suma es\", sum) # Output: La suma es 28 <pre>La suma es 28\n</pre> In\u00a0[28]: Copied! <pre>for val in \"string\":\n    if val == \"r\":\n        break\n    print(val)\n\nprint(\"Fin\")\n</pre> for val in \"string\":     if val == \"r\":         break     print(val)  print(\"Fin\") <pre>s\nt\nFin\n</pre> In\u00a0[29]: Copied! <pre>for val in \"string\":\n    if val == \"r\":\n        continue\n    print(val)\n\nprint(\"Fin\")\n</pre> for val in \"string\":     if val == \"r\":         continue     print(val)  print(\"Fin\") <pre>s\nt\ni\nn\ng\nFin\n</pre> In\u00a0[30]: Copied! <pre>sequence = {'p', 'a', 's', 's'}\nfor val in sequence:\n    pass\n</pre> sequence = {'p', 'a', 's', 's'} for val in sequence:     pass <p></p> In\u00a0[31]: Copied! <pre>def imprimir_lineas():\n    print(\"linea 1\")\n    print(\"linea 2\")\n    \n# llamar funcion\nimprimir_lineas()\n</pre> def imprimir_lineas():     print(\"linea 1\")     print(\"linea 2\")      # llamar funcion imprimir_lineas() <pre>linea 1\nlinea 2\n</pre> <p>Una funci\u00f3n puede aceptar argumentos.</p> In\u00a0[32]: Copied! <pre>def sumar(a, b):\n    sum = a + b\n    return sum\n\nresultado = sumar(4, 5)\nprint(resultado)\n\n# Output: 9\n</pre> def sumar(a, b):     sum = a + b     return sum  resultado = sumar(4, 5) print(resultado)  # Output: 9 <pre>9\n</pre> In\u00a0[33]: Copied! <pre># funcion factorial (recursivo)\n\ndef factorial(x):\n\n    if x == 1:\n        return 1\n    else:\n        return (x * factorial(x-1))\n\nnum = 6\nprint(\"El factorial de \", num, \"es\", factorial(num)) \n\n# Output: El factorial de  6 es 720\n</pre> # funcion factorial (recursivo)  def factorial(x):      if x == 1:         return 1     else:         return (x * factorial(x-1))  num = 6 print(\"El factorial de \", num, \"es\", factorial(num))   # Output: El factorial de  6 es 720 <pre>El factorial de  6 es 720\n</pre> In\u00a0[34]: Copied! <pre>cuadrado = lambda x: x ** 2\nprint(cuadrado(5))\n\n# Output: 25\n</pre> cuadrado = lambda x: x ** 2 print(cuadrado(5))  # Output: 25 <pre>25\n</pre> In\u00a0[35]: Copied! <pre>%%writefile modulo_01.py\ndef sumar(a, b):\n    return a + b\n</pre> %%writefile modulo_01.py def sumar(a, b):     return a + b <pre>Overwriting modulo_01.py\n</pre> <p>Para usar este m\u00f3dulo, usamos la palabra clave import.</p> In\u00a0[36]: Copied! <pre># importar modulo \nimport modulo_01 \n# acceder a las funciones del modulo\nmodulo_01.sumar(4, 5.5)\n</pre> # importar modulo  import modulo_01  # acceder a las funciones del modulo modulo_01.sumar(4, 5.5)  Out[36]: <pre>9.5</pre> <p>Tambi\u00e9n se pueden exportar m\u00f3dulos nativos de python.</p> In\u00a0[37]: Copied! <pre>import math\n\nresultado = math.log2(5) # retorna logaritmo base 2\nprint(resultado) # Output: 2.321928094887362\n</pre> import math  resultado = math.log2(5) # retorna logaritmo base 2 print(resultado) # Output: 2.321928094887362 <pre>2.321928094887362\n</pre> <p>Python tiene una tonelada de m\u00f3dulos est\u00e1ndar f\u00e1cilmente disponibles para su uso. Por ejemplo:</p> In\u00a0[38]: Copied! <pre>from math import pi\nprint(\"El valor de pi es\", pi)\n\n# Output: The value of pi is 3.141592653589793\n</pre> from math import pi print(\"El valor de pi es\", pi)  # Output: The value of pi is 3.141592653589793 <pre>El valor de pi es 3.141592653589793\n</pre> <p></p> In\u00a0[39]: Copied! <pre>class Mi_clase:\n    \"Esta es mi clase\"\n    a = 10\n    def func(self):\n        print('hola')\n\n# Output: 10\nprint(Mi_clase.a)\n\n# Output: &lt;function 0x0000000003079bf8=\"\" at=\"\" myclass.func=\"\"&gt;\nprint(Mi_clase.func)\n\n# Output: 'Esta es mi clase'\nprint(Mi_clase.__doc__)\n</pre> class Mi_clase:     \"Esta es mi clase\"     a = 10     def func(self):         print('hola')  # Output: 10 print(Mi_clase.a)  # Output:  print(Mi_clase.func)  # Output: 'Esta es mi clase' print(Mi_clase.__doc__) <pre>10\n&lt;function Mi_clase.func at 0x10643dca0&gt;\nEsta es mi clase\n</pre> <p>Es posible que haya notado el par\u00e1metro self en la definici\u00f3n de la funci\u00f3n dentro de la clase, pero llamamos al m\u00e9todo simplemente como ob.func() sin ning\u00fan argumento. A\u00fan funcion\u00f3.</p> <p>Esto se debe a que, cada vez que un objeto llama a su m\u00e9todo, el objeto mismo se pasa como primer argumento. Entonces, ob.func() se traduce en Mi_clase.func(ob).</p> In\u00a0[40]: Copied! <pre>class Mi_clase:\n    \"Esta es mi clase\"\n    a = 10\n    def func(self):\n        print('hola')\n\nobj1 = Mi_clase()\nprint(obj1.a)        # Output: 10\n \nobj2 = Mi_clase()\nprint(obj1.a + 5)    # Output: 15\n</pre> class Mi_clase:     \"Esta es mi clase\"     a = 10     def func(self):         print('hola')  obj1 = Mi_clase() print(obj1.a)        # Output: 10   obj2 = Mi_clase() print(obj1.a + 5)    # Output: 15 <pre>10\n15\n</pre> In\u00a0[41]: Copied! <pre>class NumerosComplejos:\n    def __init__(self,r = 0,i = 0):  # constructor\n        self.real = r\n        self.imag = i\n\n    def obtener_datos(self):\n        print(\"{0}+{1}j\".format(self.real,self.imag))\n\n\nc1 = NumerosComplejos(2,3) # crear el objeto NumerosComplejos\nc1.obtener_datos() # Output: 2+3j\n\nc2 = NumerosComplejos() # crear un nuevo objeto NumerosComplejos\nc2.obtener_datos() # Output: 0+0j\n</pre> class NumerosComplejos:     def __init__(self,r = 0,i = 0):  # constructor         self.real = r         self.imag = i      def obtener_datos(self):         print(\"{0}+{1}j\".format(self.real,self.imag))   c1 = NumerosComplejos(2,3) # crear el objeto NumerosComplejos c1.obtener_datos() # Output: 2+3j  c2 = NumerosComplejos() # crear un nuevo objeto NumerosComplejos c2.obtener_datos() # Output: 0+0j <pre>2+3j\n0+0j\n</pre> In\u00a0[42]: Copied! <pre>class Mamifero:\n    def caracteristicas(self):\n        print ('Mam\u00edfero es un animal de sangre caliente')\n</pre> class Mamifero:     def caracteristicas(self):         print ('Mam\u00edfero es un animal de sangre caliente') <p>Derivemos una nueva clase Perro de esta clase Mamifero.</p> In\u00a0[43]: Copied! <pre>class Mamifero:\n    def caracteristicas_mamifero(self):\n        print ('Mam\u00edfero es un animal de sangre caliente')\n\nclass Perro(Mamifero):\n    def caracteristicas_perro(self):\n        print('El perro ladra')\n\nd = Perro()\nd.caracteristicas_perro()\nd.caracteristicas_mamifero()\n</pre> class Mamifero:     def caracteristicas_mamifero(self):         print ('Mam\u00edfero es un animal de sangre caliente')  class Perro(Mamifero):     def caracteristicas_perro(self):         print('El perro ladra')  d = Perro() d.caracteristicas_perro() d.caracteristicas_mamifero() <pre>El perro ladra\nMam\u00edfero es un animal de sangre caliente\n</pre> In\u00a0[44]: Copied! <pre>def debug(f):\n    def nueva_funcion(a, b):\n        print(\"La funcion Sumar es llamada!!!\")\n        return f(a, b)\n    return nueva_funcion\n\n\n@debug # decorador\ndef Sumar(a, b):\n    return a + b\nprint(Sumar(7, 5))\n</pre> def debug(f):     def nueva_funcion(a, b):         print(\"La funcion Sumar es llamada!!!\")         return f(a, b)     return nueva_funcion   @debug # decorador def Sumar(a, b):     return a + b print(Sumar(7, 5)) <pre>La funcion Sumar es llamada!!!\n12\n</pre>"},{"location":"lectures/toolkit/bt_01/#python","title":"Python\u00b6","text":"<p>Python es un lenguaje de programaci\u00f3n interpretado y de alto nivel. Fue creado por Guido van Rossum y lanzado por primera vez en 1991. Python se destaca por su legibilidad y sintaxis clara, lo que lo hace ideal tanto para principiantes como para programadores experimentados.</p> <p>Una de las caracter\u00edsticas m\u00e1s destacadas de Python es su filosof\u00eda de dise\u00f1o, que enfatiza la legibilidad del c\u00f3digo y la facilidad de uso. El lenguaje utiliza una sintaxis limpia y utiliza el espaciado en blanco de manera significativa para estructurar el c\u00f3digo, lo que lo hace f\u00e1cil de leer y entender.</p>"},{"location":"lectures/toolkit/bt_01/#nomenclatura-basica","title":"Nomenclatura b\u00e1sica\u00b6","text":""},{"location":"lectures/toolkit/bt_01/#hola-mundo","title":"Hola mundo!\u00b6","text":"<p>Escribamos nuestro primer programa de Python, \"\u00a1Hola, mundo!\". Es un programa simple que imprime Hello World! en el dispositivo de salida est\u00e1ndar (pantalla).</p>"},{"location":"lectures/toolkit/bt_01/#variables","title":"Variables\u00b6","text":"<p>Una variable es una ubicaci\u00f3n con nombre utilizada para almacenar datos en la memoria. Aqu\u00ed hay un ejemplo:</p>"},{"location":"lectures/toolkit/bt_01/#operadores-basico","title":"Operadores b\u00e1sico\u00b6","text":"<p>Los operadores son s\u00edmbolos especiales que realizan operaciones en operandos (variables y valores). Hablemos de operadores aritm\u00e9ticos y de asignaci\u00f3n en esta parte.</p>"},{"location":"lectures/toolkit/bt_01/#operadores-de-asignacion","title":"Operadores de asignaci\u00f3n\u00b6","text":"<p>Los operadores de asignaci\u00f3n se utilizan para asignar valores a las variables. Ya has visto el uso de = operator. Probemos algunos operadores de asignaci\u00f3n m\u00e1s.</p>"},{"location":"lectures/toolkit/bt_01/#inputs-por-el-usuario","title":"Inputs por el usuario\u00b6","text":"<p>En Python, puede usar la funci\u00f3n input() para tomar la entrada del usuario. Por ejemplo:</p>"},{"location":"lectures/toolkit/bt_01/#conversion-de-tipo","title":"Conversi\u00f3n de tipo\u00b6","text":"<p>The process of converting the value of one data type (integer, string, float, etc.) to another is called type conversion. Python has two types of type conversion.</p>"},{"location":"lectures/toolkit/bt_01/#implicita","title":"Impl\u00edcita\u00b6","text":""},{"location":"lectures/toolkit/bt_01/#explicito","title":"Expl\u00edcito\u00b6","text":""},{"location":"lectures/toolkit/bt_01/#python-tipos-numericos","title":"Python tipos num\u00e9ricos\u00b6","text":"<p>Python admite enteros, n\u00fameros de coma flotante y n\u00fameros complejos. Se definen como int, float y clase compleja en Python. Adem\u00e1s de eso, los booleanos: verdadero y falso son un subtipo de enteros.</p>"},{"location":"lectures/toolkit/bt_01/#manejo-de-excepciones-de-python","title":"Manejo de excepciones de Python\u00b6","text":"<p>Los errores que se producen en tiempo de ejecuci\u00f3n se denominan excepciones. Ocurren, por ejemplo, cuando un archivo que intentamos abrir no existe FileNotFoundError, dividiendo un n\u00famero por cero ZeroDivisionError, etc.</p> <p>Si no se manejan las excepciones, se escupe un mensaje de error y nuestro programa se detiene repentinamente e inesperadamente.</p> <p>En Python, las excepciones se pueden manejar usando la declaraci\u00f3n try. Cuando se detectan excepciones, depende de usted qu\u00e9 operador realizar.</p>"},{"location":"lectures/toolkit/bt_01/#aserciones-en-python","title":"Aserciones en python\u00b6","text":"<p>Las aserciones  son expresiones booleanas que comprueban si las condiciones devuelven verdaderas o no. Si es cierto, el programa no hace nada y pasa a la siguiente l\u00ednea de c\u00f3digo. Sin embargo, si es falso, el programa se detiene y arroja un error.</p> <p>Las aserciones son importantes al momento de realizar tests unitarios o asegurar que un resultado siempre sea el mismo.</p> <p></p>"},{"location":"lectures/toolkit/bt_01/#estructura-de-datos","title":"Estructura de datos\u00b6","text":""},{"location":"lectures/toolkit/bt_01/#listas","title":"Listas\u00b6","text":"<p>Se crea una lista colocando todos los elementos (elementos) dentro de un corchete [] separados por comas.</p>"},{"location":"lectures/toolkit/bt_01/#tuplas","title":"Tuplas\u00b6","text":"<p>La tupla es similar a una lista, excepto que no puede cambiar los elementos de una tupla una vez que est\u00e1 definida. Mientras que en una lista, los elementos se pueden modificar.</p>"},{"location":"lectures/toolkit/bt_01/#conjuntos","title":"Conjuntos\u00b6","text":"<p>Un conjunto es una colecci\u00f3n desordenada de elementos donde cada elemento es \u00fanico (sin duplicados).</p>"},{"location":"lectures/toolkit/bt_01/#diccionarios","title":"Diccionarios\u00b6","text":"<p>Los diccionarios en Python son un tipo de estructuras de datos que permite guardar un conjunto no ordenado de pares clave-valor, siendo las claves \u00fanicas dentro de un mismo diccionario (es decir que no pueden existir dos elementos con una misma clave)</p>"},{"location":"lectures/toolkit/bt_01/#python-range","title":"Python range()\u00b6","text":"<p><code>range()</code> devuelve una secuencia inmutable de n\u00fameros entre el entero de inicio dado al entero de parada.</p>"},{"location":"lectures/toolkit/bt_01/#control-de-flujo","title":"Control de Flujo\u00b6","text":""},{"location":"lectures/toolkit/bt_01/#if-elif-else","title":"if, elif,..., else\u00b6","text":"<p>La instrucci\u00f3n if ... else se usa si desea realizar una acci\u00f3n diferente (ejecutar un c\u00f3digo diferente) en diferentes condiciones. Por ejemplo:</p>"},{"location":"lectures/toolkit/bt_01/#while-loop","title":"While Loop\u00b6","text":"<p>Al igual que la mayor\u00eda de los lenguajes de programaci\u00f3n, el loop se usa para iterar sobre un bloque de c\u00f3digo siempre que la expresi\u00f3n de prueba (condici\u00f3n) sea verdadera. Aqu\u00ed hay un ejemplo para encontrar la suma de n\u00fameros naturales:</p>"},{"location":"lectures/toolkit/bt_01/#for-loop","title":"For Loop\u00b6","text":"<p>En Python, for loop se usa para iterar sobre una secuencia (lista, tupla, cadena) u otros objetos iterables. Iterar sobre una secuencia se llama transversal.</p> <p>Aqu\u00ed hay un ejemplo para encontrar la suma de todos los n\u00fameros almacenados en una lista.</p>"},{"location":"lectures/toolkit/bt_01/#break","title":"Break\u00b6","text":"<p>La declaraci\u00f3n break termina el ciclo que lo contiene. El control del programa fluye a la declaraci\u00f3n inmediatamente despu\u00e9s del cuerpo del bucle. Por ejemplo:</p>"},{"location":"lectures/toolkit/bt_01/#continue","title":"Continue\u00b6","text":"<p>La instrucci\u00f3n continue se usa para omitir el resto del c\u00f3digo dentro de un bucle solo para la iteraci\u00f3n actual. El bucle no termina pero contin\u00faa con la siguiente iteraci\u00f3n. Por ejemplo:</p>"},{"location":"lectures/toolkit/bt_01/#pass","title":"Pass\u00b6","text":"<p>Supongamos que tiene un bucle o una funci\u00f3n que a\u00fan no est\u00e1 implementada, pero desea implementarla en el futuro. No pueden tener un cuerpo vac\u00edo. El int\u00e9rprete se quejar\u00eda. Por lo tanto, utiliza la instrucci\u00f3n pass para construir un cuerpo que no hace nada.</p>"},{"location":"lectures/toolkit/bt_01/#funciones","title":"Funciones\u00b6","text":"<p>Una funci\u00f3n es un grupo de declaraciones relacionadas que realizan una tarea espec\u00edfica. Utiliza la palabra clave def para crear funciones en Python.</p>"},{"location":"lectures/toolkit/bt_01/#recursion","title":"Recursion\u00b6","text":"<p>Una funci\u00f3n que se llama a s\u00ed misma se conoce como funci\u00f3n recursiva y este proceso se llama recursividad. Cada funci\u00f3n recursiva debe tener una condici\u00f3n base que detenga la recursividad o, de lo contrario, la funci\u00f3n se llama a s\u00ed misma infinitamente.</p>"},{"location":"lectures/toolkit/bt_01/#lambda","title":"Lambda\u00b6","text":"<p>En Python, puede definir funciones sin nombre. Estas funciones se denominan lambda o funci\u00f3n an\u00f3nima. Para crear una funci\u00f3n lambda, se utiliza la palabra clave lambda.</p>"},{"location":"lectures/toolkit/bt_01/#modulos","title":"M\u00f3dulos\u00b6","text":"<p>Los m\u00f3dulos se refieren a un archivo que contiene declaraciones y definiciones de Python. Un archivo que contiene c\u00f3digo Python, por ejemplo: <code>modulo_01.py</code>, se llama m\u00f3dulo y su nombre de m\u00f3dulo ser\u00eda un ejemplo.</p> <p>Perm\u00edtanos crearlo y guardarlo como modulo_01.py.</p>"},{"location":"lectures/toolkit/bt_01/#programacion-orientada-a-objetos-oop","title":"Programaci\u00f3n orientada a objetos (OOP)\u00b6","text":"<p>Todo en Python es un objeto que incluye enteros, flotantes, funciones, clases y Ninguno. No nos centremos en por qu\u00e9 todo en Python es un objeto. Para eso, visite esta p\u00e1gina. M\u00e1s bien, esta secci\u00f3n se enfoca en crear sus propias clases y objetos.</p>"},{"location":"lectures/toolkit/bt_01/#clase-y-objetos","title":"Clase y objetos\u00b6","text":"<p>El objeto es simplemente una colecci\u00f3n de datos (variables) y m\u00e9todos (funciones) que act\u00faan sobre los datos. Y, la clase es un modelo para el objeto.</p> <p>Tan pronto como defina una clase, se crea un nuevo objeto de clase con el mismo nombre. Este objeto de clase nos permite acceder a los diferentes atributos, as\u00ed como crear instancias de nuevos objetos de esa clase.</p>"},{"location":"lectures/toolkit/bt_01/#creando-objetos","title":"Creando objetos\u00b6","text":"<p>Tambi\u00e9n puede crear objetos de la clase usted mismo.</p>"},{"location":"lectures/toolkit/bt_01/#constructores-de-python","title":"Constructores de Python\u00b6","text":"<p>En Python, un m\u00e9todo con el nombre __init () __  es un constructor. Este m\u00e9todo se llama autom\u00e1ticamente cuando se instancia un objeto.</p>"},{"location":"lectures/toolkit/bt_01/#herencia-de-python","title":"Herencia de Python\u00b6","text":"<p>La herencia se refiere a definir una nueva clase con poca o ninguna modificaci\u00f3n a una clase existente. Tomemos un ejemplo:</p>"},{"location":"lectures/toolkit/bt_01/#decoradores","title":"Decoradores\u00b6","text":"<p>Python tiene una caracter\u00edstica interesante llamada decoradores para agregar funcionalidad a un c\u00f3digo existente. Esto tambi\u00e9n se llama metaprogramaci\u00f3n ya que una parte del programa intenta modificar otra parte del programa en tiempo de compilaci\u00f3n.</p>"},{"location":"lectures/toolkit/bt_01/#referencias","title":"Referencias\u00b6","text":"<ol> <li>Introducci\u00f3n al lenguaje Python</li> <li>Introducci\u00f3n \u2014 Tutorial de Python 3.6.3 documentation</li> </ol>"},{"location":"lectures/toolkit/bt_02/","title":"Jupyter Notebook","text":"<ul> <li>Jupyter Notebook es una herramienta de desarrollo para facilitar la programaci\u00f3n.</li> <li>Est\u00e1 orientada a la Computaci\u00f3n Cient\u00edfica y a la Ciencia de Datos.</li> <li>Es un entorno de computaci\u00f3n interactivo agn\u00f3stico, se puede ocupar en R, python o Julia.</li> <li>Jupyter notebook ofrece muchas herramientas que ser\u00e1n de gran utilidad a lo largo de este curso.</li> </ul> <p>$$p(x) = 3x^2 + 5y^2 + x^2y^2$$</p> <p>$$e^{\\pi i} - 1 = 0$$</p> <p>$$\\lim_{x \\rightarrow \\infty} 3x+1$$</p> <p>$$\\sum_{n=1}^\\infty\\frac{1}{n^2}$$</p> <p>$$\\int_0^\\infty\\frac{\\sin x}{x}\\,\\mathrm{d}x=\\frac{\\pi}{2}$$</p> <p>$$R^2 =  \\begin{pmatrix} c &amp; s \\end{pmatrix}  \\begin{pmatrix} 1 &amp; 0\\\\ 0 &amp; 1 \\end{pmatrix} \\begin{pmatrix} c \\\\ s \\end{pmatrix}  = c^2 + s^2$$</p> In\u00a0[2]: Copied! <pre>import collections\n</pre> import collections In\u00a0[\u00a0]: Copied! <pre>collections. # aprete la tecla &lt;\ud835\udc47\ud835\udc34\ud835\udc35&gt;\n</pre> collections. # aprete la tecla &lt;\ud835\udc47\ud835\udc34\ud835\udc35&gt; In\u00a0[3]: Copied! <pre>import numpy as np\nnp.sum?\n</pre> import numpy as np np.sum? <p>Jupyter posee varias funciones m\u00e1gicas predefinidas que sirven para simplificar tareas comunes.</p> <p>Hay dos tipos de magias:</p> <ul> <li><p>Magias por linea (line magics): son comandos que empiezan con el caracter <code>%</code> y que toman como argumentos valores escritos en la misma l\u00ednea.</p> </li> <li><p>Magias por celda (cell magics): son comandos que empiezan con los caracteres <code>%%</code>, y que reciben argumentos en la misma l\u00ednea y en toda la celda.</p> </li> </ul> <p>En general solo se puede usar una sola m\u00e1gias por celda en cada celda y debe ser escrita en la primer linea de la celda.</p> <p>Un buen ejemplo de m\u00e1gia es <code>%lsmagic</code> que lista todas las magias disponibles:</p> In\u00a0[4]: Copied! <pre>%lsmagic\n</pre> %lsmagic Out[4]: <pre>Available line magics:\n%alias  %alias_magic  %autoawait  %autocall  %automagic  %autosave  %bookmark  %cd  %clear  %cls  %colors  %conda  %config  %connect_info  %copy  %ddir  %debug  %dhist  %dirs  %doctest_mode  %echo  %ed  %edit  %env  %gui  %hist  %history  %killbgscripts  %ldir  %less  %load  %load_ext  %loadpy  %logoff  %logon  %logstart  %logstate  %logstop  %ls  %lsmagic  %macro  %magic  %matplotlib  %mkdir  %more  %notebook  %page  %pastebin  %pdb  %pdef  %pdoc  %pfile  %pinfo  %pinfo2  %pip  %popd  %pprint  %precision  %prun  %psearch  %psource  %pushd  %pwd  %pycat  %pylab  %qtconsole  %quickref  %recall  %rehashx  %reload_ext  %ren  %rep  %rerun  %reset  %reset_selective  %rmdir  %run  %save  %sc  %set_env  %store  %sx  %system  %tb  %time  %timeit  %unalias  %unload_ext  %who  %who_ls  %whos  %xdel  %xmode\n\nAvailable cell magics:\n%%!  %%HTML  %%SVG  %%bash  %%capture  %%cmd  %%debug  %%file  %%html  %%javascript  %%js  %%latex  %%markdown  %%perl  %%prun  %%pypy  %%python  %%python2  %%python3  %%ruby  %%script  %%sh  %%svg  %%sx  %%system  %%time  %%timeit  %%writefile\n\nAutomagic is ON, % prefix IS NOT needed for line magics.</pre> <p>En varias situaciones resulta necesario medir el tiempo de ejecuci\u00f3n de una porci\u00f3n de c\u00f3digo. Para ello podemos usar la magia <code>%timeit</code>. Esta magia est\u00e1 disponible tanto para l\u00ednea como para celda:</p> In\u00a0[8]: Copied! <pre>%%timeit \n1+1 # timeit repite (adaptativamente) la medici\u00f3n a fin de reducir el error.\n</pre> %%timeit  1+1 # timeit repite (adaptativamente) la medici\u00f3n a fin de reducir el error. <pre>15.3 ns \u00b1 0.578 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000000 loops each)\n</pre> <p>Jupyter permite tambi\u00e9n mezclar varios lenguajes de programaci\u00f3n en una misma notebook. Por ejemplo, podr\u00edamos escribir en bash lo siguiente:</p> In\u00a0[10]: Copied! <pre>%%bash\nfor i in {3..1}; do\n    echo $i\ndone\necho \"Hola desde $BASH\"\n</pre> %%bash for i in {3..1}; do     echo $i done echo \"Hola desde $BASH\" <pre>3\n2\n1\nHola desde /bin/bash\n</pre>"},{"location":"lectures/toolkit/bt_02/#jupyter-notebook","title":"Jupyter Notebook\u00b6","text":""},{"location":"lectures/toolkit/bt_02/#funciones-basicas","title":"Funciones B\u00e1sicas\u00b6","text":""},{"location":"lectures/toolkit/bt_02/#toolbox","title":"Toolbox\u00b6","text":"<p>Jupyter notebook nos ofrece el siguiente toolbox:</p> <ul> <li><p>File: En \u00e9l, puede crear un nuevo cuaderno o abrir uno preexistente. Aqu\u00ed es tambi\u00e9n a donde ir\u00eda para cambiar el nombre de un Cuaderno. Creo que el elemento de men\u00fa m\u00e1s interesante es la opci\u00f3n Guardar y Checkpoint. Esto le permite crear puntos de control a los que puede retroceder si lo necesita.</p> </li> <li><p>Edit: Aqu\u00ed puede cortar, copiar y pegar celdas. Aqu\u00ed tambi\u00e9n es donde ir\u00edas si quisieras eliminar, dividir o fusionar una celda. Puede reordenar celdas aqu\u00ed tambi\u00e9n.</p> </li> <li><p>View: es \u00fatil para alternar la visibilidad del encabezado y la barra de herramientas. Tambi\u00e9n puede activar o desactivar los n\u00fameros de l\u00ednea dentro de las celdas. Aqu\u00ed tambi\u00e9n es donde ir\u00edas si quieres meterte con la barra de herramientas de la celda.</p> </li> <li><p>Insert: es solo para insertar celdas encima o debajo de la celda seleccionada actualmente.</p> </li> <li><p>Cell: le permite ejecutar una celda, un grupo de celdas o todas las celdas. Tambi\u00e9n puede ir aqu\u00ed para cambiar el tipo de celda, aunque personalmente considero que la barra de herramientas es m\u00e1s intuitiva para eso.</p> </li> <li><p>Kernel: es para trabajar con el kernel que se ejecuta en segundo plano. Aqu\u00ed puede reiniciar el kernel, volver a conectarlo, apagarlo o incluso cambiar el kernel que est\u00e1 utilizando su computadora port\u00e1til.</p> </li> <li><p>Widgets: es para guardar y borrar el estado del widget. Los widgets son b\u00e1sicamente widgets de JavaScript que puede agregar a sus celdas para crear contenido din\u00e1mico utilizando Python (u otro Kernel).</p> </li> <li><p>Help: es donde debe aprender sobre los atajos de teclado del Notebook, un recorrido por la interfaz de usuario y mucho material de referencia.</p> </li> </ul> <p>A medida que se avance en el cuso se entrar\u00e1 en detalle en cada una de estas herramientas. Ahora se ense\u00f1ar\u00e1 las virtudes de los notebooks en aspectos de presentaci\u00f3n tipo latex (markdown) y algunos atajos importantes que nos ayudar\u00e1n en el desarrollo de c\u00f3digos.</p>"},{"location":"lectures/toolkit/bt_02/#markdown","title":"Markdown\u00b6","text":"<p>Jupyter Notebook permite que escribamos texto formateado, es decir, texto con cursiva, negritas, t\u00edtulos de distintos tama\u00f1os, etc., de forma simple. Para ello Jupyter nos permite usar Markdown, que es un lenguaje de marcado (markup) muy popular.</p> <p>Los lenguajes de markup son lenguajes ideados para procesar texto, algunos de los m\u00e1s conocidos son HTML y $\\LaTeX$. Markdown tiene como objetivo ser un lenguaje de sintaxis minimalista, simple de aprender y usar; de esa forma uno puede dar formato al texto pero sin perder demasiado tiempo en los detalles.</p> <p>La cantidad de tutoriales en la red sobre Markdown es inmenso, por lo que nos centraremos en indicar las opciones que m\u00e1s se utilizan.</p> <ul> <li><p>Texto en negrita/cursiva: El texto en negrita se indica entre dos pares de asteriscos. De este modo <code>**palabra**</code> aparecer\u00e1 como palabra. Por otro lado, el texto en cursiva se indica entre dos asteriscos simples; es decir <code>*palabra*</code> aparecer\u00e1 como palabra.</p> </li> <li><p>Listas: Las listas en Markdown se realizan indicando un asterisco o un n\u00famero seguido de un punto si se desean listas numeradas. Markdown organiza autom\u00e1ticamente los items asign\u00e1ndoles el n\u00famero correcto.</p> </li> <li><p>Inclusi\u00f3n de im\u00e1genes: La sintaxis para incluir im\u00e1genes en Markdown es <code>![nombre alternativo](direcci\u00f3n de la imagen)</code> en donde el nombre alternativo aparecer\u00e1 en caso de que no se pueda cargar la im\u00e1gen y la direcci\u00f3n puede referirse a una imagen local o un enlace en Internet.</p> </li> <li><p>Inclusi\u00f3n de c\u00f3digo HTML: El lenguaje Markdown es un subconjunto del lenguaje HTML y en donde se necesite un mayor control del formato, se puede incluir directamente el c\u00f3digo HTML.</p> </li> <li><p>Enlaces: Las celdas de texto pueden contener enlaces, tanto a otras partes del documento, como a p\u00e1ginas en internet u otros archivos locales. Su sintaxis es <code>[texto](direcci\u00f3n del enlace)</code>.</p> </li> <li><p>F\u00f3rmulas matem\u00e1ticas: Gracias al uso de MathJax, se puede incluir c\u00f3digo en $\\LaTeX$ para mostrar todo tipo de f\u00f3rmulas y expresiones matem\u00e1ticas. Las f\u00f3rmulas dentro de una l\u00ednea de texto se escriben entre s\u00edmbolos de d\u00f3lar <code>$...$</code>, mientras que las expresiones separadas del texto utilizan s\u00edmbolos de d\u00f3lar dobles <code>$$...$$</code>. Los siguientes son ejemplos de f\u00f3rmulas matem\u00e1ticas escritas en $\\LaTeX$:</p> </li> </ul>"},{"location":"lectures/toolkit/bt_02/#algunos-atajos-importantes","title":"Algunos atajos importantes\u00b6","text":""},{"location":"lectures/toolkit/bt_02/#completado-mediantes-tabs","title":"Completado mediantes Tabs.\u00b6","text":"<p>La completaci\u00f3n mediante tabs, especialmente para los atributos, es una forma conveniente de explorar la estructura de cualquier objeto con el que est\u00e9 tratando.</p> <p>Simplemente escriba object_name. <code>&lt;TAB&gt;</code> para ver los atributos del objeto. Adem\u00e1s de los objetos y palabras clave de Python, la finalizaci\u00f3n de pesta\u00f1as tambi\u00e9n funciona en nombres de archivos y directorios.</p>"},{"location":"lectures/toolkit/bt_02/#buscando-ayuda","title":"Buscando ayuda\u00b6","text":"<p>En caso de necesitar ayuda sobre cualquier comando de Python, Jupyter nos ofrece una funci\u00f3n llamada <code>help</code>.</p> <p>En resumen, \u00a1suele ser m\u00e1s importante saber como buscar informaci\u00f3n que memorizarla! Por todo esto, Jupyter nos ofrece ayuda sobre cualquier comando agregando un signo de interrogaci\u00f3n <code>?</code> luego del nombre del comando (y luego ejecutar la celda con la combinaci\u00f3n de teclas SHIFT + ENTER).</p>"},{"location":"lectures/toolkit/bt_02/#magias-magics","title":"Magias (magics).\u00b6","text":""},{"location":"lectures/toolkit/bt_02/#tiempo-por-celda","title":"Tiempo por celda\u00b6","text":""},{"location":"lectures/toolkit/bt_02/#comandos-bash","title":"Comandos bash\u00b6","text":""},{"location":"lectures/toolkit/bt_02/#referencias","title":"Referencias\u00b6","text":"<ol> <li>Notebook Basics</li> <li>Running the Notebook</li> </ol>"},{"location":"lectures/toolkit/bt_03/","title":"Google Colab","text":"<p>Nota: Para ocupar un notebook de Google Colab, debes preseionar el siguiente icono (a menudo se encuentra en la cabezera del proyecto): </p> In\u00a0[1]: Copied! <pre>seconds_in_a_day = 24 * 60 * 60\nseconds_in_a_day\n</pre> seconds_in_a_day = 24 * 60 * 60 seconds_in_a_day Out[1]: <pre>86400</pre> <p>Si quieres ejecutar el c\u00f3digo de la celda anterior, haz clic para seleccionarlo y pulsa el bot\u00f3n de reproducir situado a la izquierda del c\u00f3digo o usa la combinaci\u00f3n de teclas \"Comando/Ctrl\u00a0+\u00a0Intro\". Para editar el c\u00f3digo, solo tienes que hacer clic en la celda.</p> <p>Las variables que definas en una celda se pueden usar despu\u00e9s en otras celdas:</p> In\u00a0[4]: Copied! <pre>seconds_in_a_week = 7 * seconds_in_a_day\nseconds_in_a_week\n</pre> seconds_in_a_week = 7 * seconds_in_a_day seconds_in_a_week Out[4]: <pre>604800</pre> <p>Los cuadernos de Colab te permiten combinar c\u00f3digo ejecutable y texto enriquecido en un mismo documento, adem\u00e1s de im\u00e1genes, HTML, LaTeX y mucho m\u00e1s. Los cuadernos que creas en Colab se almacenan en tu cuenta de Google\u00a0Drive. Puedes compartir tus cuadernos de Colab f\u00e1cilmente con compa\u00f1eros de trabajo o amigos, lo que les permite comentarlos o incluso editarlos. Consulta m\u00e1s informaci\u00f3n en Informaci\u00f3n general sobre Colab. Para crear un cuaderno de Colab, puedes usar el men\u00fa Archivo que aparece arriba o bien acceder al enlace para crear un cuaderno de Colab.</p> <p>Los cuadernos de Colab son cuadernos de Jupyter alojados en Colab. Para obtener m\u00e1s informaci\u00f3n sobre el proyecto Jupyter, visita jupyter.org.</p> In\u00a0[1]: Copied! <pre>import numpy as np\nfrom matplotlib import pyplot as plt\n\nys = 200 + np.random.randn(100)\nx = [x for x in range(len(ys))]\n\nplt.plot(x, ys, '-')\nplt.fill_between(x, ys, 195, where=(ys &gt; 195), facecolor='g', alpha=0.6)\n\nplt.title(\"Sample Visualization\")\nplt.show()\n</pre> import numpy as np from matplotlib import pyplot as plt  ys = 200 + np.random.randn(100) x = [x for x in range(len(ys))]  plt.plot(x, ys, '-') plt.fill_between(x, ys, 195, where=(ys &gt; 195), facecolor='g', alpha=0.6)  plt.title(\"Sample Visualization\") plt.show() <p>Puedes importar tus propios datos a los cuadernos de Colab desde tu cuenta de Google Drive, incluidas las hojas de c\u00e1lculo, y tambi\u00e9n desde GitHub y muchas fuentes m\u00e1s. Para obtener m\u00e1s informaci\u00f3n sobre c\u00f3mo importar datos y c\u00f3mo se puede usar Colab en la ciencia de datos, consulta los enlaces que aparecen en la secci\u00f3n Trabajar con datos m\u00e1s abajo.</p> <p>Colab es una herramienta muy utilizada en la comunidad de aprendizaje autom\u00e1tico. Estos son algunos ejemplos de las aplicaciones que tiene Colab:</p> <ul> <li>Dar los primeros pasos con TensorFlow</li> <li>Desarrollar y entrenar redes neuronales</li> <li>Experimentar con TPUs</li> <li>Divulgar datos de investigaci\u00f3n sobre IA</li> <li>Crear tutoriales</li> </ul> <p>Para ver cuadernos de Colab que demuestran las aplicaciones del aprendizaje autom\u00e1tico, consulta los ejemplos de aprendizaje autom\u00e1tico de abajo.</p> <p></p>"},{"location":"lectures/toolkit/bt_03/#google-colab","title":"Google Colab\u00b6","text":"<p>Si ya conoces Colab, echa un vistazo a este v\u00eddeo para obtener informaci\u00f3n sobre las tablas interactivas, la vista del historial de c\u00f3digo ejecutado y la paleta de comandos.</p>"},{"location":"lectures/toolkit/bt_03/#que-es-colaboratory","title":"\u00bfQu\u00e9 es Colaboratory?\u00b6","text":"<p>Colab, tambi\u00e9n conocido como \"Colaboratory\", te permite programar y ejecutar Python en tu navegador con las siguientes ventajas:</p> <ul> <li>No requiere configuraci\u00f3n</li> <li>Da acceso gratuito a GPUs</li> <li>Permite compartir contenido f\u00e1cilmente</li> </ul> <p>Colab puede facilitar tu trabajo, ya seas estudiante, cient\u00edfico de datos o investigador de IA. No te pierdas el v\u00eddeo de Introducci\u00f3n a Colab para obtener m\u00e1s informaci\u00f3n. O simplemente empieza con los pasos descritos m\u00e1s abajo.</p>"},{"location":"lectures/toolkit/bt_03/#primeros-pasos","title":"Primeros pasos\u00b6","text":"<p>El documento que est\u00e1s leyendo no es una p\u00e1gina web est\u00e1tica, sino un entorno interactivo denominado cuaderno de Colab que te permite escribir y ejecutar c\u00f3digo.</p> <p>Por ejemplo, a continuaci\u00f3n se muestra una celda de c\u00f3digo con una breve secuencia de comandos de Python que calcula un valor, lo almacena en una variable e imprime el resultado:</p>"},{"location":"lectures/toolkit/bt_03/#ciencia-de-datos","title":"Ciencia de datos\u00b6","text":"<p>Con Colab, puedes aprovechar toda la potencia de las bibliotecas m\u00e1s populares de Python para analizar y visualizar datos. La celda de c\u00f3digo de abajo utiliza NumPy para generar datos aleatorios y Matplotlib para visualizarlos. Para editar el c\u00f3digo, solo tienes que hacer clic en la celda.</p>"},{"location":"lectures/toolkit/bt_03/#aprendizaje-automatico","title":"Aprendizaje autom\u00e1tico\u00b6","text":"<p>Con Colab, puedes importar un conjunto de datos de im\u00e1genes, entrenar un clasificador de im\u00e1genes con dicho conjunto de datos y evaluar el modelo con tan solo usar unas pocas l\u00edneas de c\u00f3digo. Los cuadernos de Colab ejecutan c\u00f3digo en los servidores en la nube de Google, lo que te permite aprovechar la potencia del hardware de Google, incluidas las GPU y TPU, independientemente de la potencia de tu equipo. Lo \u00fanico que necesitas es un navegador.</p>"},{"location":"lectures/toolkit/bt_03/#mas-recursos","title":"M\u00e1s recursos\u00b6","text":""},{"location":"lectures/toolkit/bt_03/#trabajar-con-cuadernos-en-colab","title":"Trabajar con cuadernos en Colab\u00b6","text":"<ul> <li>Informaci\u00f3n general sobre Colaboratory</li> <li>Gu\u00eda de Markdown</li> <li>Importar bibliotecas e instalar dependencias</li> <li>Guardar y cargar cuadernos en GitHub</li> <li>Formularios interactivos</li> <li>Widgets interactivos</li> <li>TensorFlow\u00a02 en Colab (Nuevo)</li> </ul>"},{"location":"lectures/toolkit/bt_03/#trabajar-con-datos","title":"Trabajar con datos\u00b6","text":"<ul> <li>Cargar datos: Drive, Hojas de c\u00e1lculo y Google Cloud Storage</li> <li>Gr\u00e1ficos: visualizaci\u00f3n de datos</li> <li>Primeros pasos con BigQuery</li> </ul>"},{"location":"lectures/toolkit/bt_03/#curso-intensivo-de-aprendizaje-automatico","title":"Curso intensivo de aprendizaje autom\u00e1tico\u00b6","text":"<p>A continuaci\u00f3n, se muestran algunos cuadernos del curso online de Google sobre aprendizaje autom\u00e1tico. Para obtener m\u00e1s informaci\u00f3n, consulta el sitio web del curso completo.</p> <ul> <li>Introducci\u00f3n a Pandas DataFrame</li> <li>Regresi\u00f3n lineal con tf.keras usando datos sint\u00e9ticos</li> </ul> <p></p>"},{"location":"lectures/toolkit/bt_03/#uso-de-hardware-acelerado","title":"Uso de hardware acelerado\u00b6","text":"<ul> <li>TensorFlow con GPUs</li> <li>TensorFlow con TPUs</li> </ul>"},{"location":"lectures/toolkit/bt_03/#ejemplos-destacados","title":"Ejemplos destacados\u00b6","text":"<ul> <li><p>Reemplaza voces con NeMo: usa NeMo, el kit de herramientas de IA conversacional de Nvidia, para sustituir una voz de un fragmento de audio por otra generada por ordenador.</p> </li> <li><p>Reentrenamiento de un clasificador de im\u00e1genes: crea un modelo de Keras sobre un clasificador de im\u00e1genes preparado previamente para que distinga flores.</p> </li> <li><p>Clasificaci\u00f3n de textos: clasifica las rese\u00f1as de pel\u00edculas de IMDb en positivas o negativas.</p> </li> <li><p>Transferencia de estilo: utiliza el aprendizaje profundo para transferir el estilo de una imagen a otra.</p> </li> <li><p>Codificador universal de frases multiling\u00fce para preguntas y respuestas: utiliza un modelo de aprendizaje autom\u00e1tico para contestar preguntas con el conjunto de datos SQuAD.</p> </li> <li><p>Interpolaci\u00f3n de v\u00eddeo: predice lo que ocurre entre el primer y el \u00faltimo fotograma de un v\u00eddeo.</p> </li> </ul>"},{"location":"lectures/toolkit/bt_intro/","title":"Introducci\u00f3n","text":"<ul> <li>Personalmente recomiendo Linux, en particular distribuciones como Ubuntu, Mint o Fedora por su facilidad a la hora de instalar.</li> <li>En ocasiones las implementaciones en Windows no est\u00e1n completamente integradas e inclusive en ocasiones no est\u00e1n disponibles.<ul> <li>Una alternativa es Windows Subsystem for Linux, pero lamentablemente no se asegura un 100% de compatibilidad.</li> </ul> </li> <li>En el caso que poseas un equipo con macOS no deber\u00eda haber problema.</li> </ul> <p>Problemas recurrentes:</p> <ul> <li>Dependencias de librer\u00edas (packages) incompatibles.</li> <li>Dificultad a la hora de compartir y reproducir resultados, e.g. no conocer las versiones de las librer\u00edas instaladas.</li> <li>Tener una m\u00e1quina virtual para cada desarrollo es tedioso y costoso.</li> <li>Miedo constante a instalar algo nuevo y tu script vuelva a funcionar.</li> </ul> <p>Soluci\u00f3n</p> <p>Aislar el desarrollo con tal de mejorar la compatibilidad y reproducibilidad de resultados.</p> <p>Para el curso (es recomendable)</p> <p></p> <p>Package, dependency and environment management for any language\u2014Python, R, Ruby, Lua, Scala, Java, JavaScript, C/ C++, FORTRAN.</p> <p>\u00bfPor qu\u00e9 Conda?</p> <ul> <li>Open Source</li> <li>Gestor de librer\u00edas y entornos virtuales.</li> <li>Compatible con Linux, Windows y macOS.</li> <li>Es agn\u00f3stico al lenguaje de programaci\u00f3n (inicialmente fue desarrollado para Python).</li> <li>Es de f\u00e1cil instalaci\u00f3n y uso.</li> </ul> <p>Otras alternativas</p> <ul> <li><code>pip + virtualenv</code>: el primero es el gestor favorito de librer\u00edas de Python y el segundo es un gestos de entornos virtuales, el contra es que es exclusivo de Python.</li> <li><code>Pipenv</code> o  <code>Poetry</code>: librer\u00edas enfocadas al manejo de dependencias (muy recomendables!)</li> </ul>"},{"location":"lectures/toolkit/bt_intro/#introduccion","title":"Introducci\u00f3n\u00b6","text":""},{"location":"lectures/toolkit/bt_intro/#sistema-operativo","title":"Sistema Operativo\u00b6","text":""},{"location":"lectures/toolkit/bt_intro/#interfaz-de-linea-de-comandos-command-line-interface-cli","title":"Interfaz de L\u00ednea de Comandos (Command Line Interface / CLI)\u00b6","text":"<ul> <li>Es un m\u00e9todo que permite a los usuarios interactuar con alg\u00fan programa inform\u00e1tico por medio de l\u00edneas de texto.</li> <li>T\u00edpicamente se hace uso de una terminal/shell (ver imagen).</li> <li>En el d\u00eda a d\u00eda dentro de la oficina facilita flujo de trabajo.</li> <li>Permite moverse entre manipular directorios y ficheros, instalar/actualizar herramientas, aplicaciones, softwares, etc.</li> </ul>"},{"location":"lectures/toolkit/bt_intro/#python","title":"Python\u00b6","text":"<p>Python es un lenguaje de programaci\u00f3n interpretado cuya filosof\u00eda hace hincapi\u00e9 en la legibilidad de su c\u00f3digo.</p> <p>Se trata de un lenguaje de programaci\u00f3n multiparadigma, ya que soporta orientaci\u00f3n a objetos, programaci\u00f3n imperativa y, en menor medida, programaci\u00f3n funcional. Es un lenguaje interpretado, din\u00e1mico y multiplataforma.</p> <p>Las principales librer\u00edas cient\u00edficas a instalar y que ocuparemos durante el curso son:</p> <ul> <li>Numpy: Computaci\u00f3n cient\u00edfica.</li> <li>Pandas: An\u00e1lisis de datos.</li> <li>Matplotlib: Visualizaci\u00f3n.</li> <li>Scikit-Learn: Machine Learning</li> </ul> <p>Durante el curso se ocupar\u00e1n m\u00e1s librer\u00edas a modo de complementaci\u00f3n (ejemplo, scipy, seaborn, statsmodels ,etc.)</p>"},{"location":"lectures/toolkit/bt_intro/#entorno-virtual","title":"Entorno Virtual\u00b6","text":""},{"location":"lectures/toolkit/bt_intro/#entorno-de-desarrollo-integrado","title":"Entorno de desarrollo integrado\u00b6","text":"<ul> <li><p>Un entorno de desarrollo integrado, en ingl\u00e9s Integrated Development Environment (IDE), es una aplicaci\u00f3n inform\u00e1tica que proporciona servicios integrales para facilitarle al desarrollador o programador el desarrollo de software.</p> </li> <li><p>Normalmente, un IDE consiste de un editor de c\u00f3digo fuente, herramientas de construcci\u00f3n autom\u00e1ticas y un depurador. La mayor\u00eda de los IDE tienen auto-completado inteligente de c\u00f3digo (IntelliSense).</p> </li> <li><p>Algunos IDE contienen un compilador, un int\u00e9rprete, o ambos, tales como NetBeans y Eclipse; otros no, tales como SharpDevelop y Lazarus.</p> </li> </ul> <p>Existen varios IDE populares que sirven para varios lenguajes de programaci+on. En python, el m\u00e1s recomendable es Pycharm.</p>"},{"location":"lectures/toolkit/bt_intro/#pycharm","title":"Pycharm\u00b6","text":"<p>PyCharm es un IDE para desarrolladores profesionales. Fue creado por JetBrains, una empresa conocida por crear excelentes herramientas de desarrollo de software.</p> <p>Hay dos versiones de PyCharm:</p> <ul> <li>Community: versi\u00f3n gratuita de c\u00f3digo abierto, ligera, buena para Python y desarrollo cient\u00edfico</li> <li>Professional: versi\u00f3n de pago, IDE con todas las funciones con soporte para desarrollo web tambi\u00e9n</li> </ul>"},{"location":"lectures/toolkit/bt_intro/#observacion","title":"Observaci\u00f3n\u00b6","text":"<p>Se recomienda que puedan descargar Pycharm (en su versi\u00f3n gratuita) para poder familiarizarse con este tipo de herramientas, aunque el curso est\u00e1 orientado a trabajar sobre la terminal y con jupyter notebook.</p>"},{"location":"lectures/toolkit/bt_intro/#project-jupyter","title":"Project Jupyter\u00b6","text":"<p>Project Jupyter exists to develop open-source software, open-standards, and services for interactive computing across dozens of programming languages.*</p> <p></p>"},{"location":"lectures/toolkit/bt_intro/#jupyter-notebook","title":"Jupyter Notebook\u00b6","text":"<p>Es una aplicaci\u00f3n web que permite crear y compartir documentos que contienen c\u00f3digo, ecuaciones, visualizaciones y texto. Entre sus usos se encuentra:</p> <ul> <li>Limpieza de datos</li> <li>Transformaci\u00f3n de datos</li> <li>Simulaciones num\u00e9ricas</li> <li>Modelamiendo Estad\u00edstico</li> <li>Visualizaci\u00f3n de Datos</li> <li>Machine Learning</li> <li>Mucho m\u00e1s.</li> </ul>"},{"location":"lectures/toolkit/bt_intro/#jupyter-lab","title":"Jupyter Lab\u00b6","text":"<ul> <li>Es la siguiente generaci\u00f3n de la interfaz de usuario de Project Jupyter.</li> <li>Similar a Jupyter Notebook cuenta con la facilidad de editar archivos .ipynb (notebooks) y heramientas como una terminal, editor de texto, explorador de archivos, etc.</li> <li>Eventualmente Jupyter Lab reemplazar\u00e1 a Jupyter Notebok (aunque la versi\u00f3n estable fue liberada hace algunos meses).</li> <li>Cuenta con una serie de extensiones que puedes instalar (y desarrollar inclurisve.</li> <li>M\u00e1s informaci\u00f3n en: https://github.com/jupyterlab/jupyterlab-demo</li> </ul>"},{"location":"lectures/toolkit/bt_intro/#otros-proyectos","title":"Otros Proyectos\u00b6","text":"<p>Entre los m\u00e1s conocidos se encuentran:</p> <ul> <li>JupyterHub: Distribuir Jupyter Noterbooks a m\u00faltiples usuarios.</li> <li>nbviewer: Compartir Jupyter Notebooks.</li> <li>Jupyter Book: Construir y publicar libros de t\u00f3picos computacionales.</li> <li>Jupyter Docker Stacks: Im\u00e1genes de Jupyter para utilizar en Docker.</li> </ul>"},{"location":"lectures/toolkit/bt_intro/#versionamiento-de-codigo","title":"Versionamiento de C\u00f3digo\u00b6","text":"<ul> <li><p>Permite compartir el c\u00f3digo fuente de nuestros desarrollos y a la vez mantener un registro de los cambios por los que va pasando.</p> </li> <li><p>Herramienta m\u00e1s importante y fundamental dentro del desarrollo.</p> </li> <li><p>Tipos de versionadores de c\u00f3digo:</p> </li> <li><p>Sistemas Centralizados: Son los m\u00e1s \"tradicionales\", por ejemplo SVN, CVS, etc.</p> </li> <li><p>Sistemas Distribuidos: son los que est\u00e1n en auge actualmente como: Git, Mercurial, Bazaar, etc.</p> </li> </ul>"},{"location":"lectures/toolkit/bt_intro/#git","title":"Git\u00b6","text":"<p>Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency.</p> <p>Es importante comprender que Git es la herramienta que permite versionar tus proyectos, sin embargo, a la hora de querer aprovechar m\u00e1s funcionalidades, como compartir o sincronizar tus trabajos se hace necesario utilizar servicios externos. Los m\u00e1s famosos son:</p> <ul> <li>GitHub</li> <li>GitLab</li> <li>Bitbucket</li> </ul> <p>Piensa lo siguiente, cualquiera podr\u00eda implementar un correo electr\u00f3nico entre dos computadoras conectadas entre ellas por LAN pero no conectadas a Internet. Sin embargo la gente utiliza servicios como Gmail, Outlook, etc. con tal de aprovechar de mejor manera las funcionalidades que ofrece la tecnolog\u00eda del correo electr\u00f3nico. Esta es una analog\u00eda perfecta entre las diferencias de Git y los servicios como GitHub o GitLab.</p>"},{"location":"lectures/toolkit/bt_intro/#github","title":"GitHub\u00b6","text":"<p>GitHub is a development platform inspired by the way you work. From open source to business, you can host and review code, manage projects, and build software alongside 30 million developers.M</p>"},{"location":"lectures/toolkit/bt_intro/#gitlab","title":"Gitlab\u00b6","text":"<p>Gitlab is an open source end-to-end software development platform with built-in version control, issue tracking, code review, CI/CD, and more. Self-host GitLab on your own servers, in a container, or on a cloud provider.</p>"},{"location":"lectures/toolkit/bt_intro/#resumen","title":"Resumen\u00b6","text":"<ul> <li>Sistema operativo: Cualquiera, sin embargo se recomiendan alternativas basadas en Unix.</li> <li>Lenguaje de programaci\u00f3n: Python</li> <li>Entorno virtual: Conda, preferentemetne a trav\u00e9s de miniconda.</li> <li>Entorno de trabajo: Jupyter Lab.</li> <li>Versionamiento: Git &amp; GitHub.</li> </ul>"},{"location":"lectures/toolkit/bt_intro/#configuraciones","title":"Configuraciones\u00b6","text":""},{"location":"lectures/toolkit/bt_intro/#git","title":"Git\u00b6","text":"<p>Puede descargar el instalador en la p\u00e1gina oficial (escoger el sistema operativo correspondiente).</p> <p>Para validar si tu instalaci\u00f3n fue correcta, debes ejecutar en la terminal:</p> <pre><code>git --version\n</code></pre> <p>Usuarios de Windows que no agregaron Git al <code>PATH</code> tendr\u00e1n que utilizar la terminal <code>Git Bash</code>.</p>"},{"location":"lectures/toolkit/bt_intro/#github","title":"GitHub\u00b6","text":"<p>Crear una cuenta directamente en el sitio oficial. * Utilizando tu correo institucional puedes registrarte a trav\u00e9s de GitHub Student Developer Pack, con el cual puedes acceder a repositorios privados, entre otras cosas.</p>"},{"location":"lectures/toolkit/bt_intro/#google-colab","title":"Google Colab\u00b6","text":"<p>Google Colab ser\u00e1 la herramienta oficial que utilizaremos en este curso. Google Colab proporciona un entorno en l\u00ednea para ejecutar y desarrollar c\u00f3digo Python, lo que significa que no es necesario configurar nada en tu computadora local. Simplemente necesitar\u00e1s una conexi\u00f3n a Internet y una cuenta de Google para aprovechar al m\u00e1ximo este curso.</p> <p>Con Google Colab, podr\u00e1s seguir las lecciones, completar ejercicios interactivos y trabajar en proyectos pr\u00e1cticos, todo dentro de un entorno de programaci\u00f3n en l\u00ednea. Adem\u00e1s, Colab ofrece la capacidad de colaborar con otros estudiantes en tiempo real, lo que fomenta el aprendizaje colaborativo y la resoluci\u00f3n conjunta de problemas.</p>"},{"location":"lectures/toolkit/bt_intro/#portafolio-personal","title":"Portafolio Personal\u00b6","text":"<p>Para los entregables del curso, utilizaremos la plantilla de MAT281-Portfolio.</p> <p>Nota: Las instrucciones de uso se encuentran en el siguiente archivo.</p>"},{"location":"lectures/toolkit/git/","title":"Tutorial de Git","text":""},{"location":"lectures/toolkit/git/#introduccion-a-git","title":"Introducci\u00f3n a Git","text":"<p>Git es un sistema de control de versiones distribuido que permite a los desarrolladores llevar un registro de los cambios en sus proyectos de software. Facilita la colaboraci\u00f3n, el seguimiento de versiones y la gesti\u00f3n de c\u00f3digo de manera eficiente.</p>"},{"location":"lectures/toolkit/git/#instalacion-de-git","title":"Instalaci\u00f3n de Git","text":""},{"location":"lectures/toolkit/git/#windows","title":"Windows","text":"<ol> <li>Descarga el instalador desde git-scm.com.</li> <li>Ejecuta el instalador y sigue las instrucciones del asistente de instalaci\u00f3n.</li> <li>Abre el terminal de Git Bash para usar Git.</li> </ol>"},{"location":"lectures/toolkit/git/#macos","title":"macOS","text":"<ol> <li> <p>Instala Git a trav\u00e9s de Homebrew:    <pre><code>brew install git\n</code></pre></p> </li> <li> <p>Verifica la instalaci\u00f3n:    <pre><code>git --version\n</code></pre></p> </li> </ol>"},{"location":"lectures/toolkit/git/#linux-ubuntudebian","title":"Linux (Ubuntu/Debian)","text":"<ol> <li> <p>Instala Git desde el repositorio oficial:    <pre><code>sudo apt update\nsudo apt install git\n</code></pre></p> </li> <li> <p>Verifica la instalaci\u00f3n:    <pre><code>git --version\n</code></pre></p> </li> </ol>"},{"location":"lectures/toolkit/git/#configuracion-de-git","title":"Configuraci\u00f3n de Git","text":"<p>Antes de comenzar a usar Git, configura tu nombre de usuario y direcci\u00f3n de correo electr\u00f3nico.</p> <pre><code>git config --global user.name \"Tu Nombre\"\ngit config --global user.email \"tu@email.com\"\n</code></pre>"},{"location":"lectures/toolkit/git/#conceptos-basicos-de-git","title":"Conceptos B\u00e1sicos de Git","text":""},{"location":"lectures/toolkit/git/#inicializacion-de-un-repositorio","title":"Inicializaci\u00f3n de un Repositorio","text":"<p>Para iniciar un nuevo repositorio Git en tu proyecto existente:</p> <pre><code>cd /ruta/al/proyecto\ngit init\n</code></pre>"},{"location":"lectures/toolkit/git/#ciclo-basico-de-trabajo","title":"Ciclo B\u00e1sico de Trabajo","text":"<ol> <li>Agregar Archivos al \u00c1rea de Preparaci\u00f3n</li> </ol> <p>Para agregar todos los archivos modificados y nuevos al \u00e1rea de preparaci\u00f3n:</p> <pre><code>git add .\n</code></pre> <ol> <li>Confirmar Cambios</li> </ol> <p>Para confirmar los cambios en tu repositorio:</p> <pre><code>git commit -m \"Mensaje del commit\"\n</code></pre>"},{"location":"lectures/toolkit/git/#revision-de-historial-y-estado","title":"Revisi\u00f3n de Historial y Estado","text":"<ol> <li>Ver el Estado del Repositorio</li> </ol> <p>Para ver qu\u00e9 archivos han cambiado y est\u00e1n listos para ser confirmados:</p> <pre><code>git status\n</code></pre> <ol> <li>Ver el Historial de Confirmaciones</li> </ol> <p>Para ver el historial de confirmaciones:</p> <pre><code>git log\n</code></pre>"},{"location":"lectures/toolkit/git/#ramas-en-git","title":"Ramas en Git","text":"<ol> <li>Crear una Nueva Rama</li> </ol> <p>Para crear una nueva rama y cambiarte a ella:</p> <pre><code>git branch nombre_rama\ngit checkout nombre_rama\n</code></pre> <ol> <li>Fusionar Ramas</li> </ol> <p>Para fusionar una rama espec\u00edfica en la rama actual:</p> <pre><code>git merge nombre_rama\n</code></pre>"},{"location":"lectures/toolkit/git/#repositorios-remotos","title":"Repositorios Remotos","text":"<ol> <li>Agregar un Repositorio Remoto</li> </ol> <p>Para agregar un repositorio remoto:</p> <pre><code>git remote add origin url_del_repositorio\n</code></pre> <ol> <li>Enviar Cambios al Repositorio Remoto</li> </ol> <p>Para enviar tus cambios al repositorio remoto:</p> <pre><code>git push -u origin nombre_rama\n</code></pre> <ol> <li>Clonar un Repositorio Remoto</li> </ol> <p>Para clonar un repositorio existente a tu m\u00e1quina local:</p> <pre><code>git clone url_del_repositorio\n</code></pre>"},{"location":"lectures/toolkit/git/#resolucion-de-conflictos","title":"Resoluci\u00f3n de Conflictos","text":"<p>Si hay conflictos durante la fusi\u00f3n de ramas, Git te informar\u00e1. Deber\u00e1s editar los archivos para resolver los conflictos manualmente y luego confirmar los cambios.</p>"},{"location":"lectures/toolkit/git/#conclusiones","title":"Conclusiones","text":"<p>\u00a1Has completado el tutorial completo de Git! Ahora tienes una comprensi\u00f3n b\u00e1sica de c\u00f3mo usar Git para gestionar tu c\u00f3digo de manera efectiva, colaborar con otros desarrolladores y mantener un historial de versiones de tus proyectos.</p>"},{"location":"lectures/toolkit/github/","title":"Tutorial de GitHub","text":""},{"location":"lectures/toolkit/github/#que-es-github","title":"\u00bfQu\u00e9 es GitHub?","text":"<p>GitHub es una plataforma en l\u00ednea que utiliza el sistema de control de versiones Git para alojar proyectos de software y facilitar la colaboraci\u00f3n entre desarrolladores. Permite a los equipos trabajar juntos en proyectos, realizar un seguimiento de los cambios en el c\u00f3digo, proponer y revisar modificaciones, y gestionar el desarrollo de software de manera efectiva.</p>"},{"location":"lectures/toolkit/github/#registro-y-creacion-de-una-cuenta","title":"Registro y Creaci\u00f3n de una Cuenta","text":"<ol> <li> <p>Reg\u00edstrate en GitHub</p> </li> <li> <p>Ve a github.com y haz clic en \"Sign up\" para crear una cuenta.</p> </li> <li> <p>Completa el formulario de registro con tu nombre de usuario, direcci\u00f3n de correo electr\u00f3nico y contrase\u00f1a.</p> </li> <li> <p>Verificaci\u00f3n de Correo Electr\u00f3nico</p> </li> <li> <p>Despu\u00e9s de registrarte, recibir\u00e1s un correo electr\u00f3nico de verificaci\u00f3n. Haz clic en el enlace para verificar tu direcci\u00f3n de correo electr\u00f3nico.</p> </li> <li> <p>Configura tu Perfil</p> </li> <li> <p>Una vez que hayas verificado tu correo electr\u00f3nico, inicia sesi\u00f3n en GitHub y configura tu perfil a\u00f1adiendo una foto y una breve descripci\u00f3n.</p> </li> </ol>"},{"location":"lectures/toolkit/github/#creacion-de-un-repositorio","title":"Creaci\u00f3n de un Repositorio","text":"<ol> <li> <p>Inicia Sesi\u00f3n en GitHub</p> </li> <li> <p>Accede a tu cuenta de GitHub iniciando sesi\u00f3n en github.com.</p> </li> <li> <p>Crea un Nuevo Repositorio</p> </li> <li> <p>Haz clic en el bot\u00f3n \"New\" en la esquina superior derecha de tu panel de control.</p> </li> <li>Completa el nombre del repositorio, una descripci\u00f3n opcional y selecciona la visibilidad (p\u00fablico o privado).</li> <li> <p>Opcionalmente, puedes inicializar el repositorio con un archivo README, un archivo .gitignore y/o una licencia.</p> </li> <li> <p>Confirma la Creaci\u00f3n del Repositorio</p> </li> <li> <p>Haz clic en \"Create repository\" para crear el repositorio.</p> </li> </ol>"},{"location":"lectures/toolkit/github/#clonacion-de-un-repositorio","title":"Clonaci\u00f3n de un Repositorio","text":"<ol> <li> <p>Obt\u00e9n la URL del Repositorio</p> </li> <li> <p>En la p\u00e1gina del repositorio, haz clic en el bot\u00f3n \"Code\" y copia la URL del repositorio.</p> </li> <li> <p>Clona el Repositorio en tu M\u00e1quina Local</p> </li> <li> <p>Abre la terminal o Git Bash en tu ordenador.</p> </li> <li>Navega al directorio donde quieres clonar el repositorio.</li> <li>Ejecuta el siguiente comando para clonar el repositorio:      <pre><code>git clone url_del_repositorio\n</code></pre></li> </ol>"},{"location":"lectures/toolkit/github/#contribucion-a-un-repositorio","title":"Contribuci\u00f3n a un Repositorio","text":"<ol> <li> <p>Realiza Cambios en tu Repositorio Local</p> </li> <li> <p>Navega al directorio clonado en tu m\u00e1quina.</p> </li> <li> <p>Realiza las modificaciones necesarias en los archivos del proyecto.</p> </li> <li> <p>Agrega y Confirma los Cambios</p> </li> <li> <p>Agrega los archivos modificados al \u00e1rea de preparaci\u00f3n:      <pre><code>git add .\n</code></pre></p> </li> <li> <p>Confirma los cambios:      <pre><code>git commit -m \"Descripci\u00f3n breve de los cambios\"\n</code></pre></p> </li> <li> <p>Env\u00eda los Cambios al Repositorio Remoto</p> </li> <li> <p>Sube tus cambios al repositorio remoto en GitHub:      <pre><code>git push origin nombre_de_tu_rama\n</code></pre></p> </li> </ol>"},{"location":"lectures/toolkit/github/#creacion-de-issues-y-pull-requests","title":"Creaci\u00f3n de Issues y Pull Requests","text":"<ol> <li> <p>Creaci\u00f3n de un Issue</p> </li> <li> <p>Ve a la pesta\u00f1a \"Issues\" en la p\u00e1gina de tu repositorio en GitHub.</p> </li> <li> <p>Haz clic en \"New issue\" y describe el problema o la tarea que quieres abordar.</p> </li> <li> <p>Creaci\u00f3n de un Pull Request (Solicitud de Extracci\u00f3n)</p> </li> <li> <p>Haz clic en la pesta\u00f1a \"Pull requests\" en la p\u00e1gina de tu repositorio.</p> </li> <li>Haz clic en \"New pull request\" y selecciona la rama que quieres fusionar con la rama principal.</li> <li>Proporciona una descripci\u00f3n detallada de los cambios y solicita la revisi\u00f3n de otros colaboradores.</li> </ol>"},{"location":"lectures/toolkit/github/#colaboracion-y-gestion-de-proyectos","title":"Colaboraci\u00f3n y Gesti\u00f3n de Proyectos","text":"<ol> <li> <p>Agrega Colaboradores</p> </li> <li> <p>Ve a la configuraci\u00f3n de tu repositorio y haz clic en \"Manage access\".</p> </li> <li> <p>Invita a otros usuarios como colaboradores para permitirles contribuir al proyecto.</p> </li> <li> <p>Gestiona Proyectos con GitHub Projects</p> </li> <li> <p>Utiliza GitHub Projects para crear tableros Kanban y gestionar tareas, problemas y pull requests de manera efectiva.</p> </li> </ol>"},{"location":"lectures/toolkit/github/#exploracion-de-proyectos-y-contribucion-a-proyectos-de-codigo-abierto","title":"Exploraci\u00f3n de Proyectos y Contribuci\u00f3n a Proyectos de C\u00f3digo Abierto","text":"<ol> <li> <p>Explora Proyectos en GitHub</p> </li> <li> <p>Utiliza la barra de b\u00fasqueda para encontrar proyectos interesantes relacionados con tus intereses o habilidades.</p> </li> <li> <p>Contribuye a Proyectos de C\u00f3digo Abierto</p> </li> <li> <p>Busca proyectos de c\u00f3digo abierto y encuentra formas de contribuir, como corregir errores, agregar caracter\u00edsticas o mejorar la documentaci\u00f3n.</p> </li> </ol>"},{"location":"lectures/toolkit/github/#conclusiones","title":"Conclusiones","text":"<p>\u00a1Felicidades! Has completado el tutorial b\u00e1sico de GitHub. Ahora tienes una comprensi\u00f3n s\u00f3lida de c\u00f3mo utilizar GitHub para alojar proyectos de software, colaborar con otros desarrolladores y contribuir a proyectos de c\u00f3digo abierto.</p>"},{"location":"lectures/visualization/vi_01/","title":"Matplotlib","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n\nfrom scipy.stats import multivariate_normal\n\n%matplotlib inline\n</pre> import numpy as np import matplotlib.pyplot as plt import matplotlib.cm as cm  from scipy.stats import multivariate_normal  %matplotlib inline In\u00a0[2]: Copied! <pre># grafico simple\n\n# datos\nx = np.linspace(0, 2, 100)\n\n# grafico\n\n# tamano del grafico\nfig = plt.figure(figsize=(10, 5)) \n\n# graficar\nplt.plot(\n    x, # eje x\n    x, # eje y\n    label='linea', # etiquetado\n    color=\"black\", # color\n    linewidth=1 # tamano de la curva\n)\nplt.legend() # agregar etiquetado\nplt.title(\"grafico simple\") # agregar titulo\nplt.xlabel('x') # nombre eje x\nplt.ylabel('y') # nombre eje y\nplt.grid() # agregar grillado\nplt.show() # mostrar grafico\n</pre> # grafico simple  # datos x = np.linspace(0, 2, 100)  # grafico  # tamano del grafico fig = plt.figure(figsize=(10, 5))   # graficar plt.plot(     x, # eje x     x, # eje y     label='linea', # etiquetado     color=\"black\", # color     linewidth=1 # tamano de la curva ) plt.legend() # agregar etiquetado plt.title(\"grafico simple\") # agregar titulo plt.xlabel('x') # nombre eje x plt.ylabel('y') # nombre eje y plt.grid() # agregar grillado plt.show() # mostrar grafico In\u00a0[3]: Copied! <pre># grafico compuesto\n\n# datos\nx = np.linspace(0, 2, 100)\n\n# grafico\n# tamano del grafico\nfig = plt.figure(figsize=(10, 5)) \n\n# graficar\n\n# a) lineal\nplt.plot(\n    x, # eje x\n    x, # eje y\n    label='linea', # etiquetado\n    color=\"black\", # color\n    linewidth=1 # tamano de la curva\n)\n# b) cuadratica\nplt.plot(\n    x, # eje x\n    x**2, # eje y\n    label='cuadratica', # etiquetado\n    color=\"b\", # color\n    linewidth=1 # tamano de la curva\n)\n# c) cubica \nplt.plot(\n    x, # eje x\n    x**3, # eje y\n    label='cubica', # etiquetado\n    color=\"r\", # color\n    linewidth=1 # tamano de la curva\n)\n\nplt.legend() # agregar etiquetado\nplt.title(\"grafico compuesto\") # agregar titulo\nplt.xlabel('x') # nombre eje x\nplt.ylabel('y') # nombre eje y\nplt.grid() # agregar grillado\nplt.show() # mostrar grafico\n</pre> # grafico compuesto  # datos x = np.linspace(0, 2, 100)  # grafico # tamano del grafico fig = plt.figure(figsize=(10, 5))   # graficar  # a) lineal plt.plot(     x, # eje x     x, # eje y     label='linea', # etiquetado     color=\"black\", # color     linewidth=1 # tamano de la curva ) # b) cuadratica plt.plot(     x, # eje x     x**2, # eje y     label='cuadratica', # etiquetado     color=\"b\", # color     linewidth=1 # tamano de la curva ) # c) cubica  plt.plot(     x, # eje x     x**3, # eje y     label='cubica', # etiquetado     color=\"r\", # color     linewidth=1 # tamano de la curva )  plt.legend() # agregar etiquetado plt.title(\"grafico compuesto\") # agregar titulo plt.xlabel('x') # nombre eje x plt.ylabel('y') # nombre eje y plt.grid() # agregar grillado plt.show() # mostrar grafico <p>\u00bfCu\u00e1ndo utilizar gr\u00e1fico de l\u00edneas?</p> <ul> <li>x: Debe ser datos del tipo ordinal o cuantitativo.</li> <li>y: Debe ser datos de tipo ordinal, posicional o cuantitativo.</li> </ul> In\u00a0[4]: Copied! <pre># datos\nnp.random.seed(0) # fijar semilla\npeople = ('Tom', 'Dick', 'Harry', 'Slim', 'Jim')\ny_pos = np.arange(len(people))\nperformance = 3 + 10 * np.random.rand(len(people))\nerror = np.random.rand(len(people))\n\n# grafico\nfig = plt.figure(figsize=(10, 5))\nplt.bar(\n    y_pos, # eje x\n    performance, # eje y \n    yerr=error,  # # error mostrado en eje y\n    align='center', # centrar nombre eje x\n    color=\"blue\", # color \n    alpha=0.6 # intensidad del color\n)\nplt.xticks(y_pos, people)\nplt.xlabel('People')\nplt.show()\n</pre> # datos np.random.seed(0) # fijar semilla people = ('Tom', 'Dick', 'Harry', 'Slim', 'Jim') y_pos = np.arange(len(people)) performance = 3 + 10 * np.random.rand(len(people)) error = np.random.rand(len(people))  # grafico fig = plt.figure(figsize=(10, 5)) plt.bar(     y_pos, # eje x     performance, # eje y      yerr=error,  # # error mostrado en eje y     align='center', # centrar nombre eje x     color=\"blue\", # color      alpha=0.6 # intensidad del color ) plt.xticks(y_pos, people) plt.xlabel('People') plt.show() <p>Ahora para realizar el mismo gr\u00e1fico pero con los ejes invertidos, se debe graficar con <code>plt.barh</code></p> In\u00a0[5]: Copied! <pre># datos\nnp.random.seed(0) # fijar semilla\npeople = ('Tom', 'Dick', 'Harry', 'Slim', 'Jim')\ny_pos = np.arange(len(people))\nperformance = 3 + 10 * np.random.rand(len(people))\nerror = np.random.rand(len(people))\n\n# grafico\nfig = plt.figure(figsize=(10, 5))\nplt.barh(\n    y_pos, # eje x\n    performance, # eje y \n    xerr=error,  # error mostrado en eje x\n    align='center', # centrar nombre eje y\n    color=\"blue\", # color \n    alpha=0.4 # intensidad del color\n)\nplt.yticks(y_pos, people)\nplt.xlabel('People')\nplt.show()\n</pre> # datos np.random.seed(0) # fijar semilla people = ('Tom', 'Dick', 'Harry', 'Slim', 'Jim') y_pos = np.arange(len(people)) performance = 3 + 10 * np.random.rand(len(people)) error = np.random.rand(len(people))  # grafico fig = plt.figure(figsize=(10, 5)) plt.barh(     y_pos, # eje x     performance, # eje y      xerr=error,  # error mostrado en eje x     align='center', # centrar nombre eje y     color=\"blue\", # color      alpha=0.4 # intensidad del color ) plt.yticks(y_pos, people) plt.xlabel('People') plt.show() <p>Ahora, si queremos poner ambos gr\u00e1ficos en una sola vista, debemos ejecutar la siguiente rutina:</p> In\u00a0[6]: Copied! <pre># datos\nnp.random.seed(0) # fijar semilla\npeople = ('Tom', 'Dick', 'Harry', 'Slim', 'Jim')\ny_pos = np.arange(len(people))\nperformance = 3 + 10 * np.random.rand(len(people))\nerror = np.random.rand(len(people))\n\n# grafico\nfig = plt.figure(figsize=(15, 5)) # ventana\n\n# grafico lado izquierdo\nplt.subplot(1, 2, 1) # sub-ventana\nplt.barh(y_pos, performance, xerr=error, align='center', color=\"blue\", alpha=0.4)\nplt.yticks(y_pos, people)\nplt.xlabel('Performance')\n\n# grafico lado derecho\nplt.subplot(1, 2, 2) # sub-ventana\nplt.bar(y_pos, performance, yerr=error, align='center', color=\"blue\", alpha=0.6)\nplt.xticks(y_pos, people)\nplt.xlabel('People')\nplt.ylabel('Performance')\n\nplt.show()\n</pre> # datos np.random.seed(0) # fijar semilla people = ('Tom', 'Dick', 'Harry', 'Slim', 'Jim') y_pos = np.arange(len(people)) performance = 3 + 10 * np.random.rand(len(people)) error = np.random.rand(len(people))  # grafico fig = plt.figure(figsize=(15, 5)) # ventana  # grafico lado izquierdo plt.subplot(1, 2, 1) # sub-ventana plt.barh(y_pos, performance, xerr=error, align='center', color=\"blue\", alpha=0.4) plt.yticks(y_pos, people) plt.xlabel('Performance')  # grafico lado derecho plt.subplot(1, 2, 2) # sub-ventana plt.bar(y_pos, performance, yerr=error, align='center', color=\"blue\", alpha=0.6) plt.xticks(y_pos, people) plt.xlabel('People') plt.ylabel('Performance')  plt.show() <p>\u00bfCu\u00e1ndo utilizar gr\u00e1fico de barras?</p> <ul> <li>x: Debe ser datos del tipo nominal o ordinal.</li> <li>y: Debe ser datos de tipo ordinal, posicional o cuantitativo.</li> </ul> <p>Evitar: gr\u00e1fico de nominal vs nominal.</p> In\u00a0[7]: Copied! <pre># datos\nnp.random.seed(42)\nx = np.arange(0.0, 50.0, 2.0)\ny = x ** 1.3 + np.random.rand(*x.shape) * 30.0\ns = np.random.rand(*x.shape) * 800 + 500\n\n# grafico\nfig = plt.figure(figsize=(10, 5)) # ventana\nplt.scatter(\n    x, # eje x\n    y, # eje y\n    s, # tamano de los puntos\n    c=\"g\", # color\n    alpha=0.7, # intensidad color\n    marker=r'$\\clubsuit$', # forma de los puntos\n    label=\"Suerte\" # etiquetdo fijando posicion\n)\nplt.xlabel(\"Duende\")\nplt.ylabel(\"Oro\")\nplt.legend(loc='upper left')\nplt.show()\n</pre> # datos np.random.seed(42) x = np.arange(0.0, 50.0, 2.0) y = x ** 1.3 + np.random.rand(*x.shape) * 30.0 s = np.random.rand(*x.shape) * 800 + 500  # grafico fig = plt.figure(figsize=(10, 5)) # ventana plt.scatter(     x, # eje x     y, # eje y     s, # tamano de los puntos     c=\"g\", # color     alpha=0.7, # intensidad color     marker=r'$\\clubsuit$', # forma de los puntos     label=\"Suerte\" # etiquetdo fijando posicion ) plt.xlabel(\"Duende\") plt.ylabel(\"Oro\") plt.legend(loc='upper left') plt.show() <p>Ejercicio: Realizar un gr\u00e1fico que cumpla las siguientes restricciones:</p> <ul> <li>Valores de los ejes: $x,y \\in [0,1]$</li> <li>Gr\u00e1fico de l\u00ednea de una circunferencia de radio $r_0$</li> <li>Los puntos que se encuentren dentro de la circunferencia tengan forma de c\u00edrculos con color naranja y aquellos utnos que se encuentren fuera tengan forma de tri\u00e1ngulos con color azul.</li> <li>Los puntos graficados deben estar escalado por tama\u00f1o.</li> </ul> In\u00a0[8]: Copied! <pre># datos\nN = 100\nr0 = 0.6 # radio inicial\nx = 0.9 * np.random.rand(N) # puntos aleatorios eje x\ny = 0.9 * np.random.rand(N) # puntos aleatorios eje y\n\nr = np.sqrt(x ** 2 + y ** 2) # radio sacado de los puntos\n\narea = np.pi * (10 * np.random.rand(N)) ** 2  # tamano\narea1 = np.ma.masked_where(r &lt; r0, area) # dentro del radio objetivo\narea2 = np.ma.masked_where(r &gt;= r0, area) # fuera del radio objetivo\n</pre> # datos N = 100 r0 = 0.6 # radio inicial x = 0.9 * np.random.rand(N) # puntos aleatorios eje x y = 0.9 * np.random.rand(N) # puntos aleatorios eje y  r = np.sqrt(x ** 2 + y ** 2) # radio sacado de los puntos  area = np.pi * (10 * np.random.rand(N)) ** 2  # tamano area1 = np.ma.masked_where(r &lt; r0, area) # dentro del radio objetivo area2 = np.ma.masked_where(r &gt;= r0, area) # fuera del radio objetivo In\u00a0[9]: Copied! <pre># grafico\n\n# a) circunferencia\nplt.figure(figsize=(8, 8))\ntheta = np.arange(0, np.pi / 2, 0.01)\nplt.plot(r0 * np.cos(theta), r0 * np.sin(theta), \"k--\", lw=1.0)\n\n# b) figuras dentro de la circuenferencia\nsc1 = plt.scatter(x, y, s=area2, marker='o', c = \"orange\", label=\"interior\" )\n\n# b) figuras fuera de la circuenferencia\nsc2 = plt.scatter(x, y, s=area1, marker='^', c = \"b\", label=\"exterior\")\n\n\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend(loc='upper left')\nplt.show()\n</pre> # grafico  # a) circunferencia plt.figure(figsize=(8, 8)) theta = np.arange(0, np.pi / 2, 0.01) plt.plot(r0 * np.cos(theta), r0 * np.sin(theta), \"k--\", lw=1.0)  # b) figuras dentro de la circuenferencia sc1 = plt.scatter(x, y, s=area2, marker='o', c = \"orange\", label=\"interior\" )  # b) figuras fuera de la circuenferencia sc2 = plt.scatter(x, y, s=area1, marker='^', c = \"b\", label=\"exterior\")   plt.xlabel(\"x\") plt.ylabel(\"y\") plt.legend(loc='upper left') plt.show() <p>\u00bfCu\u00e1ndo utilizar scatter plot?</p> <ul> <li>x: Dato del tipo posicional o cuantitativo.</li> <li>y: Dato del tipo posicional o cuantitativo.</li> <li>z: Dato del tipo nominal u ordinal (opcional)</li> </ul> <p>OBSERVACION: Si hay pocos puntos, tambi\u00e9n puede usarse para z datos de tipo posicional o cuantitativo.</p> In\u00a0[10]: Copied! <pre># datos\nvegetables = [\"cucumber\", \"tomato\", \"lettuce\", \"asparagus\",\n              \"potato\", \"wheat\", \"barley\"]\nfarmers = [\"Farmer Joe\", \"Upland Bros.\", \"Smith Gardening\",\n           \"Agrifun\", \"Organiculture\", \"BioGoods Ltd.\", \"Cornylee Corp.\"]\n\nharvest = np.array([[0.8, 2.4, 2.5, 3.9, 0.0, 4.0, 0.0],\n                    [2.4, 0.0, 4.0, 1.0, 2.7, 0.0, 0.0],\n                    [1.1, 2.4, 0.8, 4.3, 1.9, 4.4, 0.0],\n                    [0.6, 0.0, 0.3, 0.0, 3.1, 0.0, 0.0],\n                    [0.7, 1.7, 0.6, 2.6, 2.2, 6.2, 0.0],\n                    [1.3, 1.2, 0.0, 0.0, 0.0, 3.2, 5.1],\n                    [0.1, 2.0, 0.0, 1.4, 0.0, 1.9, 6.3]])\n</pre> # datos vegetables = [\"cucumber\", \"tomato\", \"lettuce\", \"asparagus\",               \"potato\", \"wheat\", \"barley\"] farmers = [\"Farmer Joe\", \"Upland Bros.\", \"Smith Gardening\",            \"Agrifun\", \"Organiculture\", \"BioGoods Ltd.\", \"Cornylee Corp.\"]  harvest = np.array([[0.8, 2.4, 2.5, 3.9, 0.0, 4.0, 0.0],                     [2.4, 0.0, 4.0, 1.0, 2.7, 0.0, 0.0],                     [1.1, 2.4, 0.8, 4.3, 1.9, 4.4, 0.0],                     [0.6, 0.0, 0.3, 0.0, 3.1, 0.0, 0.0],                     [0.7, 1.7, 0.6, 2.6, 2.2, 6.2, 0.0],                     [1.3, 1.2, 0.0, 0.0, 0.0, 3.2, 5.1],                     [0.1, 2.0, 0.0, 1.4, 0.0, 1.9, 6.3]]) In\u00a0[11]: Copied! <pre>fig, ax = plt.subplots(figsize=(8, 8))\n\nim = ax.imshow(harvest,cmap='gray')\n\n\n# Show all ticks and label them with the respective list entries\nax.set_xticks(np.arange(len(farmers)), labels=farmers)\nax.set_yticks(np.arange(len(vegetables)), labels=vegetables)\n\n# Rotate the tick labels and set their alignment.\nplt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n         rotation_mode=\"anchor\")\n\n# Loop over data dimensions and create text annotations.\nfor i in range(len(vegetables)):\n    for j in range(len(farmers)):\n        text = ax.text(j, i, harvest[i, j],\n                       ha=\"center\", va=\"center\", color=\"w\")\n\nax.set_title(\"Harvest of local farmers (in tons/year)\")\nfig.tight_layout()\nplt.show()\n</pre> fig, ax = plt.subplots(figsize=(8, 8))  im = ax.imshow(harvest,cmap='gray')   # Show all ticks and label them with the respective list entries ax.set_xticks(np.arange(len(farmers)), labels=farmers) ax.set_yticks(np.arange(len(vegetables)), labels=vegetables)  # Rotate the tick labels and set their alignment. plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",          rotation_mode=\"anchor\")  # Loop over data dimensions and create text annotations. for i in range(len(vegetables)):     for j in range(len(farmers)):         text = ax.text(j, i, harvest[i, j],                        ha=\"center\", va=\"center\", color=\"w\")  ax.set_title(\"Harvest of local farmers (in tons/year)\") fig.tight_layout() plt.show() In\u00a0[12]: Copied! <pre># datos\nx = np.arange(0.1, 4, 0.5)\ny = np.exp(-x)\n\n# graficos\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(20, 10))\nx_error = 0.1 + 0.2*np.random.rand(len(x))\nax1.errorbar(x, y, xerr=x_error)\ny_error = 0.1 + 0.2*np.random.rand(len(x))\nax2.errorbar(x, y, yerr=y_error)\nfig.show()\n</pre> # datos x = np.arange(0.1, 4, 0.5) y = np.exp(-x)  # graficos fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(20, 10)) x_error = 0.1 + 0.2*np.random.rand(len(x)) ax1.errorbar(x, y, xerr=x_error) y_error = 0.1 + 0.2*np.random.rand(len(x)) ax2.errorbar(x, y, yerr=y_error) fig.show() <pre>C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_14816\\2000223557.py:11: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n  fig.show()\n</pre> <p>\u00bfCu\u00e1ndo utilizar gr\u00e1fico de barra de error?</p> <ul> <li>x: Dato del tipo posicional o cuantitativo.</li> <li>y: Dato del tipo posicional o cuantitativo.</li> <li>z: Dato del tipo posicional o cuantitativo. Los valores de z tienen que tener las mismas unidades y.</li> </ul> In\u00a0[13]: Copied! <pre># datos\nx, y = np.mgrid[-3:3:.025, -2:2:.025]\npos = np.empty(x.shape + (2,))\npos[:, :, 0] = x\npos[:, :, 1] = y\nz1 = multivariate_normal.pdf(\n    pos,\n    mean=[-1.0, -1.0],\n    cov=[[1.0, 0.0], [0.0, 0.1]]\n)\nz2 = multivariate_normal.pdf(\n    pos, \n    mean=[1.0, 1.0],\n    cov=[[1.5, 0.0], [0.0, 0.5]]\n)\nz = 10 * (z1 - z2)\n\n# grafico\nfig, axs = plt.subplots(ncols=2, figsize=(20, 10), sharex=True, sharey=True)\ncmaps = [cm.rainbow, cm.autumn, cm.coolwarm, cm.gray]\ncountour_styles = [\n    {\"colors\": \"k\", \"linestyles\": \"solid\"},\n    {\"colors\": \"k\", \"linestyles\": \"dashed\"},\n]\n\nfor i, ax in zip(range(len(cmaps)), axs.ravel()):\n    cs = ax.contour(x, y, z, 11, **countour_styles[i])\n    if i &gt; 0:\n        ax.clabel(cs, fontsize=9, inline=1)\n        ax.grid(alpha=0.5)\nfig.show()\n</pre> # datos x, y = np.mgrid[-3:3:.025, -2:2:.025] pos = np.empty(x.shape + (2,)) pos[:, :, 0] = x pos[:, :, 1] = y z1 = multivariate_normal.pdf(     pos,     mean=[-1.0, -1.0],     cov=[[1.0, 0.0], [0.0, 0.1]] ) z2 = multivariate_normal.pdf(     pos,      mean=[1.0, 1.0],     cov=[[1.5, 0.0], [0.0, 0.5]] ) z = 10 * (z1 - z2)  # grafico fig, axs = plt.subplots(ncols=2, figsize=(20, 10), sharex=True, sharey=True) cmaps = [cm.rainbow, cm.autumn, cm.coolwarm, cm.gray] countour_styles = [     {\"colors\": \"k\", \"linestyles\": \"solid\"},     {\"colors\": \"k\", \"linestyles\": \"dashed\"}, ]  for i, ax in zip(range(len(cmaps)), axs.ravel()):     cs = ax.contour(x, y, z, 11, **countour_styles[i])     if i &gt; 0:         ax.clabel(cs, fontsize=9, inline=1)         ax.grid(alpha=0.5) fig.show() <pre>C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_14816\\108196608.py:31: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n  fig.show()\n</pre> <p>\u00bfCu\u00e1ndo se debe utiliar countour plot?</p> <ul> <li>x: Dato del tipo posicional o cuantitativo.</li> <li>y: Dato de tipo posicional o cuantitativo.</li> <li>z: Dato de tipo posicional o cuantitativo.</li> </ul> <p>OBSERVACION: Se debe tener suficiente densidad/regularidad de puntos como para poder obtener superficies de nivel.</p> In\u00a0[14]: Copied! <pre>def my_vector_field():\n    \"\"\"\n    You can even define a new function.\n    \"\"\"\n    X, Y = np.meshgrid(np.arange(0, 2 * np.pi, .2), np.arange(0, 2 * np.pi, .2))\n    U = np.cos(X)\n    V = np.sin(Y)\n\n    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(20, 10))\n\n    Q1 = ax1.quiver(U, V)\n    qk1 = ax1.quiverkey(\n        Q1,\n        0.5,\n        0.92,\n        2,\n        r'$2 \\frac{m}{s}$',\n        labelpos='W',\n        fontproperties={'weight': 'bold'}\n    )\n    \n    Q2 = ax2.quiver(\n        X[::3, ::3],\n        Y[::3, ::3],\n        U[::3, ::3],\n        V[::3, ::3],\n        pivot='mid',\n        color='r',\n        units='inches'\n    )\n    qk2 = ax2.quiverkey(\n        Q2,\n        0.5,\n        0.03,\n        1,\n        r'$1 \\frac{m}{s}$',\n        fontproperties={'weight': 'bold'}\n    )\n    ax2.plot(X[::3, ::3], Y[::3, ::3], 'k.')\n    ax2.set_title(\"pivot='mid'; every third arrow; units='inches'\")\n    \n    fig.show()\n\nmy_vector_field()\n</pre> def my_vector_field():     \"\"\"     You can even define a new function.     \"\"\"     X, Y = np.meshgrid(np.arange(0, 2 * np.pi, .2), np.arange(0, 2 * np.pi, .2))     U = np.cos(X)     V = np.sin(Y)      fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(20, 10))      Q1 = ax1.quiver(U, V)     qk1 = ax1.quiverkey(         Q1,         0.5,         0.92,         2,         r'$2 \\frac{m}{s}$',         labelpos='W',         fontproperties={'weight': 'bold'}     )          Q2 = ax2.quiver(         X[::3, ::3],         Y[::3, ::3],         U[::3, ::3],         V[::3, ::3],         pivot='mid',         color='r',         units='inches'     )     qk2 = ax2.quiverkey(         Q2,         0.5,         0.03,         1,         r'$1 \\frac{m}{s}$',         fontproperties={'weight': 'bold'}     )     ax2.plot(X[::3, ::3], Y[::3, ::3], 'k.')     ax2.set_title(\"pivot='mid'; every third arrow; units='inches'\")          fig.show()  my_vector_field() <pre>C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_14816\\1378192203.py:42: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n  fig.show()\n</pre> <p>\u00bfCu\u00e1ndo utilizar campos de vectores?</p> <ul> <li>x: Debe ser datos del tipo posicional o cuantitativo.</li> <li>y: Debe ser datos de tipo posicional o cuantitativo.</li> <li>z: Pendiente debe ser dato de tipo posicional o cuantitativo.</li> </ul> <p>Evitar: gr\u00e1fico de campo de vectores si no es posible la interpretaci\u00f3n correspondiente.</p>"},{"location":"lectures/visualization/vi_01/#matplotlib","title":"Matplotlib\u00b6","text":""},{"location":"lectures/visualization/vi_01/#visualizacion-imperativa","title":"Visualizaci\u00f3n Imperativa\u00b6","text":"<p>Este paradigma se focaliza en las instrucciones recibidas, ya que no abstrae las operaciones o codificaciones visuales. Algunas de sus caracter\u00edsticas son:</p> <ul> <li>Se especifica C\u00f3mo se debe hacer algo.</li> <li>Se deben especificar manualmente los pasos del trazado.</li> <li>Especificaci\u00f3n y ejecuci\u00f3n entrelazadas.</li> </ul> <p>Coloquialmente se puede entender como que se debe decidir pixel a pixel lo que se desea mostrar.</p>"},{"location":"lectures/visualization/vi_01/#acerca-de-matplotlib","title":"Acerca de Matplotlib\u00b6","text":"<p>Matplotlib es una biblioteca para la generaci\u00f3n de gr\u00e1ficos a partir de datos contenidos en listas o arrays en el lenguaje de programaci\u00f3n Python y su extensi\u00f3n matem\u00e1tica NumPy. Proporciona una API, pylab, dise\u00f1ada para recordar a la de MATLAB.</p> <p>En matplotlib todo est\u00e1 organizado en una jerarqu\u00eda:</p> <ul> <li><p>En la parte superior se encuentra el m\u00f3dulo <code>matplotlib.pyplot</code>. En este nivel, se utilizan funciones simples para agregar elementos de trazado (l\u00edneas, im\u00e1genes, texto, etc.) a los ejes actuales en la figura actual.</p> </li> <li><p>El siguiente nivel en la jerarqu\u00eda es el primer nivel de la interfaz orientada a objetos, en la que pyplot se usa solo para algunas funciones, como la creaci\u00f3n de figuras, y el usuario crea y realiza un seguimiento expl\u00edcito de los objetos de figuras y ejes. En este nivel, el usuario usa pyplot para crear figuras, y a trav\u00e9s de esas figuras, se pueden crear uno o m\u00e1s objetos de ejes.</p> </li> </ul>"},{"location":"lectures/visualization/vi_01/#componentes-de-un-grafico","title":"Componentes de un gr\u00e1fico\u00b6","text":""},{"location":"lectures/visualization/vi_01/#figure","title":"Figure\u00b6","text":"<p>Es la visualizaci\u00f3n completa. Figure realiza un seguimiento de todos los Axes hijos y el Canvas. Una figura puede tener cualquier n\u00famero de Axes, pero para ser \u00fatil debe tener al menos uno.</p> <p>La forma m\u00e1s f\u00e1cil de crear una nueva Figure es con pyplot:</p> <pre>fig = plt.figure()  # an empty figure with no axes\n\nfig, ax_lst = plt.subplots(2, 2)  # a figure with a 2x2 grid of Axes\n</pre>"},{"location":"lectures/visualization/vi_01/#axes","title":"Axes\u00b6","text":"<p>Esto es lo que se puede pensar como 'un gr\u00e1fico', es la regi\u00f3n de la imagen con el espacio de datos. Un Figure dada puede contener muchos Axes, pero un objeto Axe dado solo puede estar en un Figure. Axes contiene dos (o tres en el caso de 3D) objetos Axis que se ocupan de los l\u00edmites de datos. Cada Axe tiene un t\u00edtulo, una etiqueta para el eje horizonal y una etiqueta para el eje vertical.</p> <p>La clase Axes y sus funciones son el punto de entrada principal para trabajar con la interfaz orientada a objetos.</p>"},{"location":"lectures/visualization/vi_01/#axis","title":"Axis\u00b6","text":"<p>Corresponden a los ejes, algo as\u00ed como l\u00edneas rectas. Se encargan de establecer los l\u00edmites del gr\u00e1fico y generar los ticks (las marcas en el eje) y los ticklabels (strings que etiquetan los ticks).</p>"},{"location":"lectures/visualization/vi_01/#grafico-a-grafico","title":"Gr\u00e1fico a Gr\u00e1fico\u00b6","text":"<p>A continuaci\u00f3n, mostraremos un amplia gama de gr\u00e1ficos que pueden ser desplegados con <code>Matplotlib</code>. Lo primero ser\u00e1 cargar las librerias para este m\u00f3dulo.</p>"},{"location":"lectures/visualization/vi_01/#grafico-de-lineas","title":"Gr\u00e1fico de l\u00edneas\u00b6","text":""},{"location":"lectures/visualization/vi_01/#grafico-de-barras","title":"Gr\u00e1fico de Barras\u00b6","text":""},{"location":"lectures/visualization/vi_01/#scatter-plot","title":"Scatter Plot\u00b6","text":""},{"location":"lectures/visualization/vi_01/#mapa-de-calor","title":"Mapa de calor\u00b6","text":""},{"location":"lectures/visualization/vi_01/#otros-graficos-de-interes","title":"Otros gr\u00e1ficos de inter\u00e9s\u00b6","text":""},{"location":"lectures/visualization/vi_01/#grafico-de-barra-de-error","title":"Gr\u00e1fico de Barra de Error\u00b6","text":""},{"location":"lectures/visualization/vi_01/#countor-plot","title":"Countor Plot\u00b6","text":""},{"location":"lectures/visualization/vi_01/#campos-de-vectores","title":"Campos de Vectores\u00b6","text":"<p>\u00bfPorqu\u00e9 se llama quiver al campo de vectores en ingl\u00e9s?</p>"},{"location":"lectures/visualization/vi_01/#referencias","title":"Referencias\u00b6","text":"<ol> <li>Gallery-matplotlib</li> </ol>"},{"location":"lectures/visualization/vi_02/","title":"Seaborn","text":"In\u00a0[1]: Copied! <pre># librerias\n \nimport os\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \npd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes\n\n# Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab\n%matplotlib inline\n</pre> # librerias   import os import numpy as np import pandas as pd  import matplotlib.pyplot as plt import seaborn as sns  pd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes  # Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab %matplotlib inline In\u00a0[2]: Copied! <pre># cargar datos\niris_df = pd.read_csv(\n    \"https://raw.githubusercontent.com/fralfaro/MAT281_2022/main/docs/lectures/data_manipulation/visualization/data/iris.csv\"\n    \n)\niris_df.columns = ['sepalLength',\n                  'sepalWidth',\n                  'petalLength',\n                  'petalWidth',\n                  'species']\n\niris_df.head()\n</pre> # cargar datos iris_df = pd.read_csv(     \"https://raw.githubusercontent.com/fralfaro/MAT281_2022/main/docs/lectures/data_manipulation/visualization/data/iris.csv\"      ) iris_df.columns = ['sepalLength',                   'sepalWidth',                   'petalLength',                   'petalWidth',                   'species']  iris_df.head()  Out[2]: sepalLength sepalWidth petalLength petalWidth species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa <p>El ejemplo cl\u00e1sico consiste en graficar sepalWidth versus petalLength y colorear por especie.</p> <p>Imperativo</p> <p>En <code>matplotlib</code> ser\u00eda algo as\u00ed:</p> In\u00a0[3]: Copied! <pre>color_map = dict(zip(iris_df[\"species\"].unique(), \n                     [\"blue\", \"green\", \"red\"]))\n\nplt.figure(figsize=(10, 6))\n\nfor species, group in iris_df.groupby(\"species\"):\n    plt.scatter(group[\"petalLength\"], \n                group[\"sepalWidth\"],\n                color=color_map[species],\n                alpha=0.3,\n                edgecolor=None,\n                label=species,\n               )\n    \nplt.legend(frameon=True, title=\"species\")\nplt.xlabel(\"petalLength\")\nplt.ylabel(\"sepalWidth\")\nplt.show()\n</pre> color_map = dict(zip(iris_df[\"species\"].unique(),                       [\"blue\", \"green\", \"red\"]))  plt.figure(figsize=(10, 6))  for species, group in iris_df.groupby(\"species\"):     plt.scatter(group[\"petalLength\"],                  group[\"sepalWidth\"],                 color=color_map[species],                 alpha=0.3,                 edgecolor=None,                 label=species,                )      plt.legend(frameon=True, title=\"species\") plt.xlabel(\"petalLength\") plt.ylabel(\"sepalWidth\") plt.show() <p>Declarativo</p> <p>En <code>seaborn</code> ser\u00eda algo as\u00ed:</p> In\u00a0[4]: Copied! <pre>sns.set(rc={'figure.figsize':(10,8)})\n\nsns.scatterplot(\n        x='petalLength',\n        y='sepalWidth',\n        data=iris_df,\n        hue='species',\n        palette = ['blue', 'green', 'red']\n    \n)\nplt.show()\n</pre> sns.set(rc={'figure.figsize':(10,8)})  sns.scatterplot(         x='petalLength',         y='sepalWidth',         data=iris_df,         hue='species',         palette = ['blue', 'green', 'red']      ) plt.show() In\u00a0[5]: Copied! <pre># librerias\n \nimport os\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \npd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes\n\n# Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab\n%matplotlib inline\n</pre> # librerias   import os import numpy as np import pandas as pd  import matplotlib.pyplot as plt import seaborn as sns  pd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes  # Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab %matplotlib inline In\u00a0[6]: Copied! <pre># cargar datos\nurl='https://drive.google.com/file/d/1uVJcfTBWemOTfxmjWTqALfUpIMdH3LVV/view?usp=drive_link'\nurl='https://drive.google.com/uc?id=' + url.split('/')[-2]\n\npokemon_data = pd.read_csv(url, sep=\",\")\npokemon_data.head()\n</pre> # cargar datos url='https://drive.google.com/file/d/1uVJcfTBWemOTfxmjWTqALfUpIMdH3LVV/view?usp=drive_link' url='https://drive.google.com/uc?id=' + url.split('/')[-2]  pokemon_data = pd.read_csv(url, sep=\",\") pokemon_data.head() Out[6]: # Name Type 1 Type 2 HP Attack Defense Sp. Atk Sp. Def Speed Generation Legendary 0 1 Bulbasaur Grass Poison 45 49 49 65 65 45 1 False 1 2 Ivysaur Grass Poison 60 62 63 80 80 60 1 False 2 3 Venusaur Grass Poison 80 82 83 100 100 80 1 False 3 4 Mega Venusaur Grass Poison 80 100 123 122 120 80 1 False 4 5 Charmander Fire NaN 39 52 43 60 50 65 1 False In\u00a0[7]: Copied! <pre># grafico de linea\nplt.figure(figsize=(10, 6))\npalette = sns.color_palette(\"hls\", 6)\n\nsns.lineplot(\n    x='Attack',\n    y='Defense',\n    hue='Generation',# color por Generation\n    data=pokemon_data,\n    ci = None,\n    palette=palette\n)   \nplt.show()\n</pre> # grafico de linea plt.figure(figsize=(10, 6)) palette = sns.color_palette(\"hls\", 6)  sns.lineplot(     x='Attack',     y='Defense',     hue='Generation',# color por Generation     data=pokemon_data,     ci = None,     palette=palette )    plt.show() <pre>C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_22240\\3612574168.py:5: FutureWarning: \n\nThe `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n\n  sns.lineplot(\n</pre> In\u00a0[8]: Copied! <pre># grafico de puntos\nplt.figure(figsize=(10, 6))\nsns.scatterplot(\n    x='Attack',\n    y='Defense',\n    hue='Generation',# color por Generation\n    data=pokemon_data,\n    palette=palette\n)   \nplt.show()\n</pre> # grafico de puntos plt.figure(figsize=(10, 6)) sns.scatterplot(     x='Attack',     y='Defense',     hue='Generation',# color por Generation     data=pokemon_data,     palette=palette )    plt.show() In\u00a0[9]: Copied! <pre># Pre-format DataFrame\nstats_df = pokemon_data.drop(['#', 'Generation', 'Legendary'], axis=1)\n \n# New boxplot using stats_df\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=stats_df)\nplt.show()\n</pre> # Pre-format DataFrame stats_df = pokemon_data.drop(['#', 'Generation', 'Legendary'], axis=1)   # New boxplot using stats_df plt.figure(figsize=(10, 6)) sns.boxplot(data=stats_df) plt.show() In\u00a0[10]: Copied! <pre># Calculate correlations\ncorr = stats_df.corr()\n \n# Heatmap\nplt.figure(figsize=(10, 6))\nsns.heatmap(corr)\nplt.show()\n</pre> # Calculate correlations corr = stats_df.corr()   # Heatmap plt.figure(figsize=(10, 6)) sns.heatmap(corr) plt.show() In\u00a0[11]: Copied! <pre># Distribution Plot (a.k.a. Histogram)\nplt.figure(figsize=(10, 6))\nsns.histplot(pokemon_data.Attack)\nplt.show()\n</pre> # Distribution Plot (a.k.a. Histogram) plt.figure(figsize=(10, 6)) sns.histplot(pokemon_data.Attack) plt.show() In\u00a0[12]: Copied! <pre># realizar conteo de manera manual\ndf_generation = pokemon_data.groupby('Generation').apply(lambda x: len(x)).reset_index()\ndf_generation.columns = ['Generation','Count']\ndf_generation.head()\n</pre> # realizar conteo de manera manual df_generation = pokemon_data.groupby('Generation').apply(lambda x: len(x)).reset_index() df_generation.columns = ['Generation','Count'] df_generation.head() Out[12]: Generation Count 0 1 166 1 2 106 2 3 160 3 4 121 4 5 165 In\u00a0[13]: Copied! <pre># plot seaborn: barplot\nplt.figure(figsize=(10, 6))\nsns.barplot(\n    x='Generation',\n    y='Count',\n    data=df_generation \n)\nplt.show()\n</pre> # plot seaborn: barplot plt.figure(figsize=(10, 6)) sns.barplot(     x='Generation',     y='Count',     data=df_generation  ) plt.show() In\u00a0[14]: Copied! <pre># Count Plot (a.k.a. Bar Plot)\n\nplt.figure(figsize=(10, 6))\npkmn_type_colors = ['#78C850',  # Grass\n                    '#F08030',  # Fire\n                    '#6890F0',  # Water\n                    '#A8B820',  # Bug\n                    '#A8A878',  # Normal\n                    '#A040A0',  # Poison\n                    '#F8D030',  # Electric\n                    '#E0C068',  # Ground\n                    '#EE99AC',  # Fairy\n                    '#C03028',  # Fighting\n                    '#F85888',  # Psychic\n                    '#B8A038',  # Rock\n                    '#705898',  # Ghost\n                    '#98D8D8',  # Ice\n                    '#7038F8',  # Dragon\n                   ]\nsns.countplot(x='Type 1', \n              data=pokemon_data,\n              palette=pkmn_type_colors)\n \n# Rotate x-labels\nplt.xticks(rotation=-45)\nplt.show()\n</pre> # Count Plot (a.k.a. Bar Plot)  plt.figure(figsize=(10, 6)) pkmn_type_colors = ['#78C850',  # Grass                     '#F08030',  # Fire                     '#6890F0',  # Water                     '#A8B820',  # Bug                     '#A8A878',  # Normal                     '#A040A0',  # Poison                     '#F8D030',  # Electric                     '#E0C068',  # Ground                     '#EE99AC',  # Fairy                     '#C03028',  # Fighting                     '#F85888',  # Psychic                     '#B8A038',  # Rock                     '#705898',  # Ghost                     '#98D8D8',  # Ice                     '#7038F8',  # Dragon                    ] sns.countplot(x='Type 1',                data=pokemon_data,               palette=pkmn_type_colors)   # Rotate x-labels plt.xticks(rotation=-45) plt.show() In\u00a0[15]: Copied! <pre># Factor Plot\nplt.figure(figsize=(16, 12))\ng = sns.catplot(x='Type 1', \n                   y='Attack', \n                   data=pokemon_data, \n                   hue='Legendary',  # Color by stage\n                   col='Legendary',  # Separate by stage\n                   kind='swarm', # Swarmplot\n                   s = 1.5,\n                   palette = ['black','blue'])\n \n# Rotate x-axis labels\ng.set_xticklabels(rotation=-45)\n \n# Doesn't work because only rotates last plot\n# plt.xticks(rotation=-45)\nplt.show()\n</pre> # Factor Plot plt.figure(figsize=(16, 12)) g = sns.catplot(x='Type 1',                     y='Attack',                     data=pokemon_data,                     hue='Legendary',  # Color by stage                    col='Legendary',  # Separate by stage                    kind='swarm', # Swarmplot                    s = 1.5,                    palette = ['black','blue'])   # Rotate x-axis labels g.set_xticklabels(rotation=-45)   # Doesn't work because only rotates last plot # plt.xticks(rotation=-45) plt.show() <pre>&lt;Figure size 1600x1200 with 0 Axes&gt;</pre> In\u00a0[16]: Copied! <pre># plot seaborn\nplt.figure(figsize=(10, 6))\nsns.lmplot(x='Attack',\n           y='Defense', \n           data=pokemon_data,\n           fit_reg=False, \n           height=8,\n           hue='Generation')\n \n# usar Matplotlib\nplt.ylim(0, None)\nplt.xlim(0, None)\nplt.show()\n</pre> # plot seaborn plt.figure(figsize=(10, 6)) sns.lmplot(x='Attack',            y='Defense',             data=pokemon_data,            fit_reg=False,             height=8,            hue='Generation')   # usar Matplotlib plt.ylim(0, None) plt.xlim(0, None) plt.show() <pre>&lt;Figure size 1000x600 with 0 Axes&gt;</pre>"},{"location":"lectures/visualization/vi_02/#seaborn","title":"Seaborn\u00b6","text":""},{"location":"lectures/visualization/vi_02/#visualizacion-declarativa","title":"Visualizaci\u00f3n Declarativa\u00b6","text":"<p>Es un paradigma de visualizaci\u00f3n en donde se busca preocuparse de los datos y sus relaciones, m\u00e1s que en detalles sin mayor importancia. Algunas caracter\u00edsticas son:</p> <ul> <li>Se especifica lo que se desea hacer.</li> <li>Los detalles se determinan autom\u00e1ticamente.</li> <li>Especificaci\u00f3n y Ejecuci\u00f3n est\u00e1n separadas.</li> </ul> <p>A modo de resumen, se refiere a construir visualizaciones a partir de los siguientes elementos:</p> <ul> <li>Data</li> <li>Transformation</li> <li>Marks</li> <li>Encoding</li> <li>Scale</li> <li>Guides</li> </ul> <p>Diferencias entre enfoques</p> Imperativa Declarativa Especificar c\u00f3mo se debe hacer algo Especificar qu\u00e9 se quiere hacer Especificaci\u00f3n y ejecuci\u00f3n entrelazadas Separar especificaci\u00f3n de ejecuci\u00f3n Colocar un c\u00edrculo rojo aqu\u00ed y un c\u00edrculo azul ac\u00e1 Mapear <code>x</code> como posici\u00f3n e <code>y</code> como el color <p>Ejemplo</p> <p>El Iris dataset es un conjunto de datos que contine una  muestras de tres especies de Iris (Iris setosa, Iris virginica e Iris versicolor). Se midi\u00f3 cuatro rasgos de cada muestra: el largo y ancho del s\u00e9palo y p\u00e9talo, en cent\u00edmetros.</p> <p>Este ejemplo  servir\u00e1 para mostrar una de las mayores diferencias entre una visualizaci\u00f3n imperativa (como <code>matplotlib</code>) versus una declarativa (como <code>seaborn</code>).</p> <p></p>"},{"location":"lectures/visualization/vi_02/#acerca-de-seaborn","title":"Acerca de Seaborn\u00b6","text":"<p><code>Matplotlib</code> ha demostrado ser una herramienta de visualizaci\u00f3n incre\u00edblemente \u00fatil y popular, pero incluso los usuarios entusiastas admitir\u00e1n que a menudo deja mucho que desear. Hay varias quejas v\u00e1lidas sobre Matplotlib que a menudo surgen:</p> <ul> <li><p>Antes de la versi\u00f3n 2.0, los valores predeterminados de Matplotlib no son exactamente las mejores opciones. Se bas\u00f3 en MATLAB alrededor de 1999, y esto a menudo se nota.</p> </li> <li><p>La API de Matplotlib es de nivel relativamente bajo. Es posible realizar una visualizaci\u00f3n estad\u00edstica sofisticada, pero a menudo requiere mucho c\u00f3digo repetitivo. Matplotlib fue anterior a Pandas en m\u00e1s de una d\u00e9cada y, por lo tanto, no est\u00e1 dise\u00f1ado para su uso con Pandas DataFrames. Para visualizar datos de un Pandas DataFrame, debe extraer cada Serie y, a menudo, concatenarlas juntas en el formato correcto. Ser\u00eda mejor tener una biblioteca de trazado que pueda usar inteligentemente las etiquetas de DataFrame en un trazado.</p> </li> </ul> <p>Una respuesta a estos problemas es <code>Seaborn</code>. Seaborn proporciona una API sobre Matplotlib que ofrece opciones sensatas para el estilo de trazado y los valores predeterminados de color, define funciones simples de alto nivel para tipos de trazado estad\u00edsticos comunes, y se integra con la funcionalidad proporcionada por Pandas DataFrames.</p>"},{"location":"lectures/visualization/vi_02/#grafico-a-grafico","title":"Gr\u00e1fico a Gr\u00e1fico\u00b6","text":"<p>Para mostrar el funcionamiento de seaborn, se ocupa el conjunto de datos: pokemon.csv. Para el caso de <code>seaborn</code> se los gr\u00e1ficos ser\u00e1n generados directamente desde el dataframe.</p> <p></p>"},{"location":"lectures/visualization/vi_02/#grafico-de-linea-y-puntos","title":"Gr\u00e1fico de l\u00ednea y puntos\u00b6","text":"<p>Realizar un gr\u00e1fico de l\u00ednea y otro de  puntos para analizar el ataque vs defensa de todos los pokemones separados por generaci\u00f3n.</p>"},{"location":"lectures/visualization/vi_02/#boxplot","title":"Boxplot\u00b6","text":"<p>Realizar un gr\u00e1fico box plot sobre los stats de los pokemones.</p>"},{"location":"lectures/visualization/vi_02/#mapas-de-calor","title":"Mapas de calor\u00b6","text":"<p>Realizar un mapa de calor sobre los stats de los pokemones.</p>"},{"location":"lectures/visualization/vi_02/#histogramas","title":"Histogramas\u00b6","text":"<p>Realizar un histograma del stat attack.</p>"},{"location":"lectures/visualization/vi_02/#barplot","title":"Barplot\u00b6","text":"<p>Realizar un bar plot  sobre la cantidad de pokemones que hay por generaci\u00f3n</p>"},{"location":"lectures/visualization/vi_02/#countplot","title":"countplot\u00b6","text":"<p>Realizar un conteo sobre los distintos tipos Type 1 de pokemones.</p>"},{"location":"lectures/visualization/vi_02/#factor-plot","title":"Factor plot\u00b6","text":"<p>Realizar un catplot de los distintos tipos de pokemones para la generaci\u00f3n Type 1, analizando si el pokem\u00f3n es legendario o no.</p>"},{"location":"lectures/visualization/vi_02/#customizando-con-matplotlib","title":"Customizando con Matplotlib.\u00b6","text":"<p>Seaborn es una interfaz de alto nivel para Matplotlib. Seg\u00fan nuestra experiencia, Seaborn lo llevar\u00e1 a la mayor parte del camino, pero a veces necesitar\u00e1 traer Matplotlib.</p> <p>Establecer los l\u00edmites de los ejes es uno de esos momentos, pero el proceso es bastante simple:</p> <p>Primero, usar la funci\u00f3n lmplotde  Seaborn de manera normal. Luego, use las funciones de customizaci\u00f3n de Matplotlib. En este caso, usaremos sus funciones ylim () y xlim (). Aqu\u00ed est\u00e1 nuestro nuevo diagrama de dispersi\u00f3n con l\u00edmites de ejes sensibles:</p>"},{"location":"lectures/visualization/vi_02/#referencias","title":"Referencias\u00b6","text":"<ol> <li>Gallery-seaborn</li> </ol>"},{"location":"lectures/visualization/vi_intro/","title":"Introducci\u00f3n","text":"<p>Mapa del c\u00f3lera (John Snow, 1855)</p> <p>Gr\u00e1fico que muestra los casos de c\u00f3lera durante la epidemia en Londres de 1854 y Las cruces la ubicaci\u00f3n de las bombas de agua.</p> <p></p> In\u00a0[1]: { \"tags\": [ \"hide-input\" ] } Copied! <pre>from IPython.display import display_html\n\ndef display_side_by_side(*args):\n    html_str = ''\n    for df in args:\n        html_str += df.to_html()\n        html_str += '&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;'\n    display_html(\n        html_str.replace('table','table style=\"display:inline\"'), \n        raw=True\n    )\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Cargar los datos del cuarteto de Anscombe\nanscombe = sns.load_dataset(\"anscombe\")\n\n# Dividir los datos en cuatro conjuntos seg\u00fan el valor de \"dataset\"\nds1 = anscombe[anscombe['dataset'] == 'I']\nds2 = anscombe[anscombe['dataset'] == 'II']\nds3 = anscombe[anscombe['dataset'] == 'III']\nds4 = anscombe[anscombe['dataset'] == 'IV']\n\n# Mostrar estadisticas para cada conjunto\nprint(\"Estadisticas Basicas:\\n\")\nx=\" \"\nprint(f\"{10*x}Dataset I {13*x}Dataset II\")\ndisplay_side_by_side(ds1.describe(),ds2.describe())\nprint()\nprint(f\"{10*x}Dataset III {13*x}Dataset IV\")\ndisplay_side_by_side(ds3.describe(),ds4.describe())\n\n# Mostrar  correlaciones para cada conjunto\nprint(\"Correlaciones:\\n\")\n\nprint(f\"{5*x}Dataset I {10*x}Dataset II {10*x}Dataset III {10*x}Dataset IV\")\ndisplay_side_by_side(ds1.corr(),ds2.corr(),ds3.corr(),ds4.corr())\n\n\nprint(\"Graficos:\\n\")\n\n# Crear una figura con cuatro subplots\nfig, axs = plt.subplots(ncols=4, figsize=(16, 4))\n\n# Graficar cada conjunto de datos en su respectivo subplot\nsns.regplot(x='x', y='y', data=ds1, ax=axs[0], ci=None)\nsns.regplot(x='x', y='y', data=ds2, ax=axs[1], ci=None)\nsns.regplot(x='x', y='y', data=ds3, ax=axs[2], ci=None)\nsns.regplot(x='x', y='y', data=ds4, ax=axs[3], ci=None)\n\n# Agregar t\u00edtulos a los subplots\naxs[0].set_title('Dataset I')\naxs[1].set_title('Dataset II')\naxs[2].set_title('Dataset III')\naxs[3].set_title('Dataset IV')\n\n# Mostrar el gr\u00e1fico\nplt.show()\n</pre> from IPython.display import display_html  def display_side_by_side(*args):     html_str = ''     for df in args:         html_str += df.to_html()         html_str += '\u00a0\u00a0\u00a0\u00a0'     display_html(         html_str.replace('table','table style=\"display:inline\"'),          raw=True     )  import seaborn as sns import matplotlib.pyplot as plt  # Cargar los datos del cuarteto de Anscombe anscombe = sns.load_dataset(\"anscombe\")  # Dividir los datos en cuatro conjuntos seg\u00fan el valor de \"dataset\" ds1 = anscombe[anscombe['dataset'] == 'I'] ds2 = anscombe[anscombe['dataset'] == 'II'] ds3 = anscombe[anscombe['dataset'] == 'III'] ds4 = anscombe[anscombe['dataset'] == 'IV']  # Mostrar estadisticas para cada conjunto print(\"Estadisticas Basicas:\\n\") x=\" \" print(f\"{10*x}Dataset I {13*x}Dataset II\") display_side_by_side(ds1.describe(),ds2.describe()) print() print(f\"{10*x}Dataset III {13*x}Dataset IV\") display_side_by_side(ds3.describe(),ds4.describe())  # Mostrar  correlaciones para cada conjunto print(\"Correlaciones:\\n\")  print(f\"{5*x}Dataset I {10*x}Dataset II {10*x}Dataset III {10*x}Dataset IV\") display_side_by_side(ds1.corr(),ds2.corr(),ds3.corr(),ds4.corr())   print(\"Graficos:\\n\")  # Crear una figura con cuatro subplots fig, axs = plt.subplots(ncols=4, figsize=(16, 4))  # Graficar cada conjunto de datos en su respectivo subplot sns.regplot(x='x', y='y', data=ds1, ax=axs[0], ci=None) sns.regplot(x='x', y='y', data=ds2, ax=axs[1], ci=None) sns.regplot(x='x', y='y', data=ds3, ax=axs[2], ci=None) sns.regplot(x='x', y='y', data=ds4, ax=axs[3], ci=None)  # Agregar t\u00edtulos a los subplots axs[0].set_title('Dataset I') axs[1].set_title('Dataset II') axs[2].set_title('Dataset III') axs[3].set_title('Dataset IV')  # Mostrar el gr\u00e1fico plt.show() <pre>Estadisticas Basicas:\n\n          Dataset I              Dataset II\n</pre> x y count 11.000000 11.000000 mean 9.000000 7.500909 std 3.316625 2.031568 min 4.000000 4.260000 25% 6.500000 6.315000 50% 9.000000 7.580000 75% 11.500000 8.570000 max 14.000000 10.840000 x y count 11.000000 11.000000 mean 9.000000 7.500909 std 3.316625 2.031657 min 4.000000 3.100000 25% 6.500000 6.695000 50% 9.000000 8.140000 75% 11.500000 8.950000 max 14.000000 9.260000 <pre>\n          Dataset III              Dataset IV\n</pre> x y count 11.000000 11.000000 mean 9.000000 7.500000 std 3.316625 2.030424 min 4.000000 5.390000 25% 6.500000 6.250000 50% 9.000000 7.110000 75% 11.500000 7.980000 max 14.000000 12.740000 x y count 11.000000 11.000000 mean 9.000000 7.500909 std 3.316625 2.030579 min 8.000000 5.250000 25% 8.000000 6.170000 50% 8.000000 7.040000 75% 8.000000 8.190000 max 19.000000 12.500000 <pre>Correlaciones:\n\n     Dataset I           Dataset II           Dataset III           Dataset IV\n</pre> x y x 1.000000 0.816421 y 0.816421 1.000000 x y x 1.000000 0.816237 y 0.816237 1.000000 x y x 1.000000 0.816287 y 0.816287 1.000000 x y x 1.000000 0.816521 y 0.816521 1.000000 <pre>Graficos:\n\n</pre> In\u00a0[2]: { \"tags\": [ \"hide-input\" ] } Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Crear un DataFrame con los datos\ndata = {'Grupo': ['Grupo 1', 'Grupo 2', 'Grupo 3'], \n        'Tasa de \u00e9xito': [0.9, 0.85, 0.7], \n        'N\u00famero de casos': [100, 200, 300]}\ndf = pd.DataFrame(data)\n\n# Calcular la tasa de \u00e9xito general\ngeneral_success_rate = df['Tasa de \u00e9xito'].mean()\nprint('Tasa de \u00e9xito general:', general_success_rate)\n\n# Graficar los datos\nfig, ax = plt.subplots()\ndf.plot(x='Grupo', y='Tasa de \u00e9xito', kind='bar', ax=ax)\nax.axhline(y=general_success_rate, color='r', linestyle='--')\nax.set_ylabel('Tasa de \u00e9xito')\nax.set_title('Paradoja de Simpson')\nplt.show()\n</pre> import pandas as pd import matplotlib.pyplot as plt  # Crear un DataFrame con los datos data = {'Grupo': ['Grupo 1', 'Grupo 2', 'Grupo 3'],          'Tasa de \u00e9xito': [0.9, 0.85, 0.7],          'N\u00famero de casos': [100, 200, 300]} df = pd.DataFrame(data)  # Calcular la tasa de \u00e9xito general general_success_rate = df['Tasa de \u00e9xito'].mean() print('Tasa de \u00e9xito general:', general_success_rate)  # Graficar los datos fig, ax = plt.subplots() df.plot(x='Grupo', y='Tasa de \u00e9xito', kind='bar', ax=ax) ax.axhline(y=general_success_rate, color='r', linestyle='--') ax.set_ylabel('Tasa de \u00e9xito') ax.set_title('Paradoja de Simpson') plt.show() <pre>Tasa de \u00e9xito general: 0.8166666666666668\n</pre> <p>Otro ejemplo ser\u00eda aplicar un modelo de regresi\u00f3n l\u00edneal al conjunto de datos completos y luego aplicarlo al conjunto de datos separado por categor\u00edas.</p> In\u00a0[3]: { \"tags\": [ \"hide-input\" ] } Copied! <pre>import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n# Generate 'random' data\nnp.random.seed(0)\nX = 2.5 * np.random.randn(100) + 1.5   # Array of 100 values with mean = 1.5, stddev = 2.5\nres = 0.5 * np.random.randn(100)       # Generate 100 residual terms\ny = 2 + 0.3 * X + res                  # Actual values of Y\n\n# Create pandas dataframe to store our X and y values\nds1 = pd.DataFrame({\n    'x': X+10,\n    'y': y+10}\n).assign(label = 'Dataset I')\n\nds2 = pd.DataFrame({\n    'x': X,\n    'y': y}\n).assign(label = 'Dataset II')\n\ndf = pd.concat([ds1,ds2])\n</pre> import pandas as pd import numpy as np from matplotlib import pyplot as plt  # Generate 'random' data np.random.seed(0) X = 2.5 * np.random.randn(100) + 1.5   # Array of 100 values with mean = 1.5, stddev = 2.5 res = 0.5 * np.random.randn(100)       # Generate 100 residual terms y = 2 + 0.3 * X + res                  # Actual values of Y  # Create pandas dataframe to store our X and y values ds1 = pd.DataFrame({     'x': X+10,     'y': y+10} ).assign(label = 'Dataset I')  ds2 = pd.DataFrame({     'x': X,     'y': y} ).assign(label = 'Dataset II')  df = pd.concat([ds1,ds2]) In\u00a0[4]: { \"tags\": [ \"hide-input\" ] } Copied! <pre>print(\"Grafico para todo el conjunto de datos\")\nsns.lmplot (x='x', y='y',  data=df,   ci=None)\nplt.show()\n</pre> print(\"Grafico para todo el conjunto de datos\") sns.lmplot (x='x', y='y',  data=df,   ci=None) plt.show() <pre>Grafico para todo el conjunto de datos\n</pre> In\u00a0[5]: { \"tags\": [ \"hide-input\" ] } Copied! <pre>print(\"Grafico para los grupos por separados\")\nsns.lmplot (x='x', y='y', data=df,    hue='label', ci=None)\nplt.show()\n</pre> print(\"Grafico para los grupos por separados\") sns.lmplot (x='x', y='y', data=df,    hue='label', ci=None) plt.show() <pre>Grafico para los grupos por separados\n</pre> In\u00a0[6]: { \"tags\": [ \"hide-input\" ] } Copied! <pre># Pie chart, where the slices will be ordered and plotted counter-clockwise:\nlabels = 'Frogs', 'Hogs', 'Dogs', 'Logs'\nsizes = [15, 30, 45, 10]\nexplode = (0, 0.1, 0, 0)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n\nfig1, ax1 = plt.subplots(figsize=(8, 8))\nax1.pie(\n    sizes,\n    explode=explode,\n    labels=labels,\n    autopct='%1.1f%%',\n    shadow=True,\n    startangle=90\n)\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\nplt.show()\n</pre> # Pie chart, where the slices will be ordered and plotted counter-clockwise: labels = 'Frogs', 'Hogs', 'Dogs', 'Logs' sizes = [15, 30, 45, 10] explode = (0, 0.1, 0, 0)  # only \"explode\" the 2nd slice (i.e. 'Hogs')  fig1, ax1 = plt.subplots(figsize=(8, 8)) ax1.pie(     sizes,     explode=explode,     labels=labels,     autopct='%1.1f%%',     shadow=True,     startangle=90 ) ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.  plt.show() In\u00a0[7]: { \"tags\": [ \"hide-input\" ] } Copied! <pre>np.random.seed(42)\nN = 31\nx = np.arange(N)\ny1 = 80 + 20 *x / N + 5 * np.random.rand(N)\ny2 = 75 + 25 *x / N + 5 * np.random.rand(N)\nfig, axs = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(16,8))\n\naxs[0][0].plot(x, y1, 'ok')\naxs[0][0].plot(x, y2, 'sk')\n\naxs[0][1].plot(x, y1, 'ob')\naxs[0][1].plot(x, y2, 'or')\n\naxs[1][0].plot(x, y1, 'ob')\naxs[1][0].plot(x, y2, '*k')\n\naxs[1][1].plot(x, y1, 'sr')\naxs[1][1].plot(x, y2, 'ob')\n\nplt.show()\n</pre> np.random.seed(42) N = 31 x = np.arange(N) y1 = 80 + 20 *x / N + 5 * np.random.rand(N) y2 = 75 + 25 *x / N + 5 * np.random.rand(N) fig, axs = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(16,8))  axs[0][0].plot(x, y1, 'ok') axs[0][0].plot(x, y2, 'sk')  axs[0][1].plot(x, y1, 'ob') axs[0][1].plot(x, y2, 'or')  axs[1][0].plot(x, y1, 'ob') axs[1][0].plot(x, y2, '*k')  axs[1][1].plot(x, y1, 'sr') axs[1][1].plot(x, y2, 'ob')  plt.show()"},{"location":"lectures/visualization/vi_intro/#introduccion","title":"Introducci\u00f3n\u00b6","text":""},{"location":"lectures/visualization/vi_intro/#introduccion","title":"Introducci\u00f3n\u00b6","text":"<p>Aprender sobre visualizaci\u00f3n es importante por varias razones:</p> <ul> <li><p>Comunicar informaci\u00f3n compleja de manera clara y efectiva: Al presentar datos de una manera visual, es m\u00e1s f\u00e1cil identificar patrones y tendencias, as\u00ed como tambi\u00e9n hacer comparaciones y contrastes. Esto es especialmente importante cuando se trabaja con grandes conjuntos de datos o cuando se trata de presentar informaci\u00f3n a un p\u00fablico diverso.</p> </li> <li><p>Descubrir informaci\u00f3n oculta o desconocida: A menudo, los datos pueden contener patrones o relaciones que no son obvios a simple vista, pero que pueden ser descubiertos mediante la exploraci\u00f3n y la visualizaci\u00f3n. La visualizaci\u00f3n tambi\u00e9n puede ayudar a identificar errores y anomal\u00edas en los datos, lo que puede ser importante para la toma de decisiones y la planificaci\u00f3n.</p> </li> <li><p>Mejorar la capacidad de an\u00e1lisis de datos: Al comprender c\u00f3mo presentar datos de manera efectiva, se puede desarrollar una mejor comprensi\u00f3n de los datos y las relaciones que existen entre ellos. Esto puede ayudar a tomar decisiones informadas basadas en datos y a identificar tendencias y oportunidades que de otra manera podr\u00edan haber pasado desapercibidas.</p> </li> </ul>"},{"location":"lectures/visualization/vi_intro/#malos-graficos","title":"Malos Gr\u00e1ficos\u00b6","text":""},{"location":"lectures/visualization/vi_intro/#buenos-graficos","title":"Buenos Gr\u00e1ficos\u00b6","text":""},{"location":"lectures/visualization/vi_intro/#primeras-visualizaciones","title":"Primeras visualizaciones\u00b6","text":"<p>Campa\u00f1a de Napole\u00f3n a Mosc\u00fa (Charles Minard, 1889)</p> <p>Gr\u00e1fico que muestra el n\u00famero de las fuerzas francesas en su marcha hacia Mosc\u00fa y durante la retirada, por Charles Minard. Tambi\u00e9n contiene informaci\u00f3n ambiental como la temperatura por fecha.</p> <p></p>"},{"location":"lectures/visualization/vi_intro/#por-que-utilizar-graficos","title":"\u00bfPor qu\u00e9 utilizar gr\u00e1ficos?\u00b6","text":"<ul> <li><p>El 70 % de los receptores sensoriales del cuerpo humano est\u00e1 dedicado a la visi\u00f3n.</p> </li> <li><p>Cerebro ha sido entrenado evolutivamente para interpretar la informaci\u00f3n visual de manera masiva.</p> <p>\u201cThe eye and the visual cortex of the brain form a massively parallel processor that provides the highest bandwidth channel into human cognitive centers\u201d \u2014 Colin Ware, Information Visualization, 2004.</p> </li> </ul> <p>Nota: En lo siguientes ejemplos utilizaremos c\u00f3digo de <code>matplotlib</code> y <code>seaborn</code>, sin embargo, no entraremos en detalles debido a que cada una de estas librer\u00edas ser\u00e1 estudiada en los siguientes cap\u00edtulos.</p>"},{"location":"lectures/visualization/vi_intro/#cuarteto-de-anscombe","title":"Cuarteto de ANSCOMBE\u00b6","text":"<p>El Cuarteto de Anscombe es un conjunto de cuatro conjuntos de datos que tienen las mismas estad\u00edsticas descriptivas (medias, varianzas, correlaciones y regresiones), pero que se ven muy diferentes cuando se visualizan. Fueron presentados por el estad\u00edstico Francis Anscombe en 1973 para demostrar la importancia de la visualizaci\u00f3n en el an\u00e1lisis de datos.</p> <p>Los cuatro conjuntos de datos consisten en pares de variables x e y, y cada conjunto representa un tipo diferente de relaci\u00f3n entre las variables. A simple vista, los cuatro conjuntos parecen tener distribuciones y relaciones completamente diferentes entre s\u00ed, pero cuando se analizan las estad\u00edsticas descriptivas, todas son id\u00e9nticas.</p>"},{"location":"lectures/visualization/vi_intro/#paradoja-de-simpson","title":"Paradoja de Simpson\u00b6","text":"<p>La Paradoja de Simpson es un fen\u00f3meno en la visualizaci\u00f3n de datos que puede ocurrir cuando se analizan datos de diferentes subgrupos o categor\u00edas de una poblaci\u00f3n. La paradoja se produce cuando una tendencia o patr\u00f3n que aparece en cada subgrupo se invierte o desaparece cuando se combinan los datos de todos los subgrupos.</p> <p>Supongamos que tienes datos de la tasa de \u00e9xito de un tratamiento m\u00e9dico para tres grupos de pacientes: uno con edad joven, uno con edad media y uno con edad avanzada. Los datos se presentan en la siguiente tabla:</p> <p>|                 \t| Grupo 1 \t| Grupo 2 \t| Grupo 3 \t| |-----------------\t|---------\t|---------\t|---------\t| | Tasa de \u00e9xito   \t| 90%     \t| 85%     \t| 70%     \t| | N\u00famero de casos \t| 100     \t| 200     \t| 300     \t|</p> <p>En la tabla anterior, se puede observar que la tasa de \u00e9xito general del tratamiento es del 80%. Sin embargo, si se examina cada grupo de forma individual, la tasa de \u00e9xito para cada grupo es mayor en comparaci\u00f3n con el promedio general. Esto es un ejemplo de la Paradoja de Simpson.</p> <p>A continuaci\u00f3n se presenta un c\u00f3digo en Python que ilustra este ejemplo:</p>"},{"location":"lectures/visualization/vi_intro/#teoria-de-visualizacion","title":"Teor\u00eda de visualizaci\u00f3n\u00b6","text":"<p>La teor\u00eda de visualizaci\u00f3n se refiere a la investigaci\u00f3n y el estudio de c\u00f3mo las personas procesan, interpretan y comprenden informaci\u00f3n visual. La visualizaci\u00f3n puede involucrar cualquier tipo de informaci\u00f3n que pueda ser representada visualmente, incluyendo gr\u00e1ficos, diagramas, mapas, fotograf\u00edas y videos.</p> <p>Algunos de los conceptos y principios importantes en la teor\u00eda de visualizaci\u00f3n incluyen:</p> <ul> <li><p>Percepci\u00f3n visual: C\u00f3mo procesamos y entendemos la informaci\u00f3n visual a trav\u00e9s de nuestros sentidos.</p> </li> <li><p>Cognici\u00f3n visual: C\u00f3mo procesamos y entendemos la informaci\u00f3n visual a trav\u00e9s de nuestros procesos mentales, como la atenci\u00f3n, la memoria y la toma de decisiones.</p> </li> <li><p>Dise\u00f1o visual: C\u00f3mo se pueden crear visualizaciones efectivas y atractivas para comunicar informaci\u00f3n de manera clara y efectiva.</p> </li> <li><p>Interactividad visual: C\u00f3mo las visualizaciones interactivas pueden ayudar a los usuarios a explorar y comprender mejor la informaci\u00f3n visual.</p> </li> </ul>"},{"location":"lectures/visualization/vi_intro/#consejos-generales","title":"Consejos generales\u00b6","text":"<p>Noah Iliinsky es un experto en visualizaci\u00f3n de datos y ha identificado cuatro pilares fundamentales de la visualizaci\u00f3n.</p> <p>Estos pilares son:</p> <ul> <li><p>Contenido: El contenido se refiere a la informaci\u00f3n que se est\u00e1 visualizando. Para que la visualizaci\u00f3n sea efectiva, es importante tener una comprensi\u00f3n clara del contenido y c\u00f3mo se relaciona con el objetivo de la visualizaci\u00f3n.</p> </li> <li><p>Funci\u00f3n: La funci\u00f3n se refiere al prop\u00f3sito de la visualizaci\u00f3n. \u00bfQu\u00e9 se espera que haga la visualizaci\u00f3n? \u00bfDebe mostrar una tendencia, comparar datos o explorar patrones? Es importante tener en cuenta la funci\u00f3n de la visualizaci\u00f3n para asegurarse de que se est\u00e1 dise\u00f1ando de manera efectiva.</p> </li> <li><p>Forma: La forma se refiere a la apariencia visual de la visualizaci\u00f3n. Esto incluye cosas como el tipo de gr\u00e1fico o diagrama utilizado, la paleta de colores y la tipograf\u00eda. La forma debe ser coherente y legible para que la visualizaci\u00f3n sea f\u00e1cil de entender.</p> </li> <li><p>Audiencia: La audiencia se refiere a las personas que ver\u00e1n la visualizaci\u00f3n. La comprensi\u00f3n de la audiencia es esencial para determinar el nivel de detalle y complejidad adecuados para la visualizaci\u00f3n. La visualizaci\u00f3n debe ser accesible y comprensible para su audiencia objetivo.</p> </li> </ul> <p>Nota: Se recomienda ver el siguiente video para profundizar estos conceptos</p>"},{"location":"lectures/visualization/vi_intro/#mas-aspectos-de-la-visualizacion","title":"M\u00e1s Aspectos de la Visualizaci\u00f3n\u00b6","text":""},{"location":"lectures/visualization/vi_intro/#honestidad","title":"Honestidad\u00b6","text":"<p>El ojo humano no tiene la misma precisi\u00f3n al estimar distintas atribuciones:</p> <ul> <li>Largo: Bien estimado y sin sesgo, con un factor multiplicativo de 0.9 a 1.1.</li> <li>\u00c1rea: Subestimado y con sesgo, con un factor multiplicativo de 0.6 a 0.9.</li> <li>Volumen: Muy subestimado y con sesgo, con un factor multiplicativo de 0.5 a 0.8.</li> </ul> <p>Resulta inadecuado realizar gr\u00e1ficos de datos utilizando \u00e1reas o vol\u00famenes si no queda claro la atribuci\u00f3n utilizada.</p> <p></p> <p>Una pseudo-excepci\u00f3n la constituyen los pie-chart o gr\u00e1ficos circulares, porque el ojo humano distingue bien \u00e1ngulos y segmentos de c\u00edrculo, y porque es posible indicar los porcentajes respectivos.</p>"},{"location":"lectures/visualization/vi_intro/#priorizacion","title":"Priorizaci\u00f3n\u00b6","text":"<p>Dato m\u00e1s importante debe utilizar elemento de mejor percepci\u00f3n.</p>"},{"location":"lectures/visualization/vi_intro/#percepcion","title":"Percepci\u00f3n\u00b6","text":"<p>No todos los elementos tienen la misma percepci\u00f3n a nivel del sistema visual. En particular, el color y la forma son elementos preatentivos: un color distinto o una forma distinta se reconocen de manera no conciente.</p> <p>Ejemplos de elementos preatentivos.</p> <p></p> <p></p> <p>El sistema visual humano puede estimar con precisi\u00f3n siguientes atributos visuales:</p> <ol> <li>Posici\u00f3n</li> <li>Largo</li> <li>Pendiente</li> <li>\u00c1ngulo</li> <li>\u00c1rea</li> <li>Volumen</li> <li>Color</li> </ol> <p>Utilice el atributo que se estima con mayor precisi\u00f3n cuando sea posible.</p>"},{"location":"lectures/visualization/vi_intro/#colormaps","title":"Colormaps\u00b6","text":"<p>Puesto que la percepci\u00f3n del color tiene muy baja precisi\u00f3n, resulta inadecuado tratar de representar un valor num\u00e9rico con colores.</p> <ul> <li>\u00bfQu\u00e9 diferencia num\u00e9rica existe entre el verde y el rojo?</li> <li>\u00bfQue asociaci\u00f3n preexistente posee el color rojo, el amarillo y el verde?</li> <li>\u00bfCon cu\u00e1nta precisi\u00f3n podemos distinguir valores en una escala de grises?</li> </ul> <p></p>"},{"location":"lectures/visualization/vi_intro/#python-landscape","title":"Python Landscape\u00b6","text":"<p>Para empezar, PyViz es un sitio web que se dedica a ayudar a los usuarios a decidir dentro de las mejores herramientas de visualizaci\u00f3n open-source implementadas en Python, dependiendo de sus necesidades y objetivos. Mucho de lo que se menciona en esta secci\u00f3n est\u00e1 en detalle en la p\u00e1gina web del proyecto PyViz.</p> <p>Algunas de las librer\u00edas de visualizaci\u00f3n de Python m\u00e1s conocidas son:</p> <p></p> <p>Este esquema es una adaptaci\u00f3n de uno presentado en la charla The Python Visualization Landscape realizada por Jake VanderPlas en la PyCon 2017.</p> <p>Cada una de estas librer\u00edas fue creada para satisfacer diferentes necesidades, algunas han ganado m\u00e1s adeptos que otras por uno u otro motivo. Tal como avanza la tecnolog\u00eda, estas librer\u00edas se actualizan o se crean nuevas, la importancia no recae en ser un experto en una, si no en saber adaptarse a las situaciones, tomar la mejor decicisi\u00f3n y escoger seg\u00fan nuestras necesidades y preferencias. Por ejemplo, <code>matplotlib</code> naci\u00f3 como una soluci\u00f3n para imitar los gr\u00e1ficos de <code>MATLAB</code> (puedes ver la historia completa aqu\u00ed), manteniendo una sintaxis similar y con ello poder crear gr\u00e1ficos est\u00e1ticos de muy buen nivel.</p> <p>Debido al \u00e9xito de <code>matplotlib</code> en la comunidad, nacen librer\u00edas basadas ella. Algunos ejemplos son:</p> <ul> <li><code>seaborn</code> se basa en <code>matp\u013aotlib</code> pero su nicho corresponde a las visualizaciones estad\u00edsticas.</li> <li><code>ggpy</code> una suerte de copia a <code>ggplot2</code> perteneciente al lenguaje de programaci\u00f3n <code>R</code>.</li> <li><code>networkx</code> visualizaciones de grafos.</li> <li><code>pandas</code> no es una librer\u00eda de visualizaci\u00f3n propiamente tal, pero utiliza a <code>matplotplib</code> como bakcned en los m\u00e9todos con tal de crear gr\u00e1ficos de manera muy r\u00e1pida, e.g. <code>pandas.DataFrame.plot.bar()</code></li> </ul> <p>Por otro lado, con tal de crear visualizaciones interactivas aparecen librer\u00edas basadas en <code>javascript</code>, algunas de las m\u00e1s conocidas en Python son:</p> <ul> <li><code>bokeh</code> tiene como objetivo proporcionar gr\u00e1ficos vers\u00e1tiles, elegantes e incluso interactivos, teniendo una gran performance con grandes datasets o incluso streaming de datos.</li> <li><code>plotly</code> visualizaciones interactivas que en conjunto a <code>Dash</code> (de la misma empresa) permite crear aplicaciones webs, similar a <code>shiny</code> de <code>R</code>.</li> <li><code>D3.js</code> a pesar de estar basado en <code>javascript</code> se ha ganado un lugar en el coraz\u00f3n de toda la comunidad, debido a la ilimitada cantidad de visualizaciones que son posibles de hacer, por ejemplo, la malla interactiva que hizo un estudiante de la UTFSM est\u00e1 hecha en <code>D3.js</code>.</li> </ul> <p>De las librer\u00edas m\u00e1s recientes est\u00e1 <code>Altair</code>, que consiste en visualizaciones declarativas (ya lo veremos en el pr\u00f3ximo laboratorio). Constru\u00edda sobre <code>Vega-Lite</code>, a su vez que est\u00e9 est\u00e1 sobre <code>Vega</code> y este finalmente sobre <code>D3.js</code>. <code>Altair</code> permite crear visualizaciones est\u00e1ticas e interactivas con pocas l\u00edneas de c\u00f3digo, sin embargo, al ser relativamente nueva, a\u00fan existen funcionalidades en desarrollo o que simplemente a\u00fan no existen en esta librer\u00eda pero en otras si.</p>"},{"location":"projects/project_definition/","title":"Proyecto Final","text":""},{"location":"projects/project_definition/#introduccion","title":"Introducci\u00f3n","text":"<p>La finalidad de este proyecto es enfrentar a los estudiantes a problemas de Machine Learning con todas las etapas (t\u00edpicas) que eso implica, bas\u00e1ndose en cada uno de los m\u00f3dulos aprendidos a lo largo del curso, dando pie a la investigaci\u00f3n y a la soluci\u00f3n de problemas operacionales del mundo real.</p>"},{"location":"projects/project_definition/#descripcion-del-proyecto","title":"Descripci\u00f3n del Proyecto","text":"<p>Bienvenido al a\u00f1o 2912, donde se necesitan tus habilidades de ciencia de datos  para resolver un misterio c\u00f3smico. Hemos recibido una transmisi\u00f3n desde cuatro a\u00f1os luz de distancia y las cosas no pintan bien.</p> <p>La nave espacial Titanic fue un transatl\u00e1ntico de  pasajeros interestelar lanzado hace un mes.  Con casi 13.000 pasajeros a bordo, la nave emprendi\u00f3  su viaje inaugural transportando emigrantes de nuestro sistema solar a tres exoplanetas recientemente habitables  que orbitan estrellas cercanas.</p> <p>Mientras rodeaba Alpha Centauri en ruta hacia su primer destino,  el t\u00f3rrido 55 Cancri E, la desprevenida nave espacial Titanic choc\u00f3 con una anomal\u00eda del espacio-tiempo escondida dentro de una nube de polvo. Lamentablemente, tuvo un destino similar al de su hom\u00f3nimo de 1000 a\u00f1os antes. Aunque la nave permaneci\u00f3 intacta, \u00a1casi la mitad de los pasajeros fueron transportados a una dimensi\u00f3n alternativa!</p> <p></p>"},{"location":"projects/project_definition/#evaluacion","title":"Evaluaci\u00f3n","text":"<p>El proyecto final consta de dos parte:</p> <ul> <li>Parte T\u00e9cnica: Desarrollar una soluci\u00f3n end to end del proyecto utilizando Jupyter Notebook.</li> <li>Presentaci\u00f3n de Resultados: Realizar una presentaci\u00f3n de 10-20 minutos de sus hallazgos.  </li> </ul>"},{"location":"projects/project_definition/#parte-tecnica","title":"Parte T\u00e9cnica","text":"<p>La Parte t\u00e9cnica debe cumplir con la siguiente r\u00fabrica de trabajo:</p> <ol> <li>Definici\u00f3n del problema </li> <li>Estad\u00edstica descriptiva </li> <li>Visualizaci\u00f3n descriptiva </li> <li>Preprocesamiento</li> <li>Selecci\u00f3n de modelo<ol> <li>Por lo menos debe comparar cuatro modelos</li> <li>Al menos tres de estos modelos tienen que tener hiperpar\u00e1metros.</li> <li>Realizar optimizaci\u00f3n de hiperpar\u00e1metros.</li> </ol> </li> <li>M\u00e9tricas y an\u00e1lisis de resultados</li> <li>Visualizaciones del modelo </li> <li>Conclusiones </li> </ol> <p>Observaci\u00f3n: Tendr\u00e1 una mejor puntaci\u00f3n si tambi\u00e9n incorpora modelos de redes neuronales. Se deja un tutorial de tensorflow a modo de ejemplo.</p> <p>La soluci\u00f3n debe alojarse en su Portafolio Personal del curso (<code>.ipynb</code>).</p>"},{"location":"projects/project_definition/#presentacion-de-resultados","title":"Presentaci\u00f3n de Resultados","text":"<ul> <li>La presentaci\u00f3n consta de 10-20 minutos. </li> <li>Debe grabar su presentaci\u00f3n y subirla al siguiente espacio de trabajo.</li> <li>Utilizar diapositivas con BEAMER. Se deja el siguiente el siguiente tutorial a modo de ejemplo.</li> <li>La presentaci\u00f3n debe alojarse en su Portafolio Personal del curso (<code>.pdf</code>).</li> </ul>"},{"location":"projects/project_definition/#informacion-importante","title":"Informaci\u00f3n Importante","text":"<ul> <li>Plazo: 01 de Diciembre del 2023 (hasta las 11:59 PM) </li> <li>Esto corresponde a un desafio de Kaggle (link).</li> <li>La informaci\u00f3n respecto a los datos, lo pueden encontrar en el siguiente link.</li> <li>A modo de inspiraci\u00f3n, pueden ocupar algunos gr\u00e1ficos de otros participantes del desaf\u00edo (link).</li> </ul>"}]}