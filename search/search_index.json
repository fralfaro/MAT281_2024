{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Bienvenidos a MAT281!","text":""},{"location":"#contenidos-del-curso","title":"Contenidos del Curso","text":"<p>Toolkit del Curso</p><p>Python, Git/GitHub</p> <p>Manipulaci\u00f3n de Datos</p><p>Numpy, Pandas</p> <p>Visualizaci\u00f3n de Datos</p><p>Matplotlib, Seaborn</p> <p>Machine Learning</p><p>Sklearn, XGBoost</p>"},{"location":"#evaluaciones-del-curso","title":"Evaluaciones del Curso","text":"<p>Laboratorios</p><p>Labs del curso</p> <p>Tareas</p><p>Tareas del curso</p> <p>Proyecto</p><p>Proyecto del curso</p>"},{"location":"__init__/","title":"init","text":""},{"location":"evaluation/","title":"Tabla de Contenidos","text":"<p>Laboratorios</p><p>Labs del curso</p> <p>Tareas</p><p>Tareas del curso</p> <p>Proyecto</p><p>Proyecto del curso</p>"},{"location":"mat281_01/","title":"Sobre el curso","text":""},{"location":"mat281_01/#descripcion-de-la-asignatura","title":"Descripci\u00f3n de la Asignatura","text":"<p>La asignatura tiene como objetivo proporcionar a los estudiantes las competencias y destrezas b\u00e1sicas necesarias para desempe\u00f1arse como Data Scientist utilizando el lenguaje de  programaci\u00f3n Python.</p>"},{"location":"mat281_01/#requisitos-de-entrada","title":"Requisitos de entrada","text":"<ul> <li>Fundamentos de C\u00e1lculo y \u00c1lgebra.</li> <li>Conceptos b\u00e1sicos de Probabilidad y Estad\u00edstica.</li> <li>Conocimientos en Optimizaci\u00f3n.</li> <li>Familiaridad con el lenguaje de programaci\u00f3n Python.</li> </ul>"},{"location":"mat281_01/#contenidos-tematicos","title":"Contenidos tem\u00e1ticos","text":"<p>El curso se organiza en torno a los siguientes temas principales:</p> <ul> <li>Toolkit B\u00e1sico: Introducci\u00f3n a las herramientas esenciales para el an\u00e1lisis de datos.</li> <li>Manipulaci\u00f3n de Datos: T\u00e9cnicas para la manipulaci\u00f3n eficiente de conjuntos de datos.</li> <li>Visualizaci\u00f3n: M\u00e9todos para visualizar datos de manera efectiva.</li> <li>Machine Learning: Introducci\u00f3n a los conceptos b\u00e1sicos y aplicaciones del aprendizaje autom\u00e1tico.</li> </ul>"},{"location":"mat281_01/#recursos-para-el-aprendizaje","title":"Recursos para el Aprendizaje","text":""},{"location":"mat281_01/#1-textos-guia-principales","title":"1. Textos Gu\u00eda Principales","text":"<p>Estos textos son esenciales para desarrollar una comprensi\u00f3n s\u00f3lida de la ciencia de datos y el aprendizaje autom\u00e1tico utilizando Python:</p> <ul> <li>Python Data Science Handbook por Jake VanderPlas: Un recurso integral que cubre desde las bases de Python hasta t\u00e9cnicas avanzadas de ciencia de datos, con un enfoque pr\u00e1ctico utilizando librer\u00edas como NumPy, Pandas, Matplotlib, Scikit-Learn, entre otras.</li> </ul>"},{"location":"mat281_01/#2-lecturas-complementarias","title":"2. Lecturas Complementarias","text":"<p>Estos libros ofrecen una profundizaci\u00f3n en \u00e1reas espec\u00edficas y son recomendados para aquellos que desean ampliar su conocimiento m\u00e1s all\u00e1 de los conceptos b\u00e1sicos:</p> <ul> <li> <p>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow por Aur\u00e9lien G\u00e9ron: Una gu\u00eda pr\u00e1ctica para el aprendizaje autom\u00e1tico utilizando herramientas poderosas como Scikit-Learn, Keras y TensorFlow. Ideal para quienes quieren aplicar t\u00e9cnicas de machine learning a proyectos reales.</p> </li> <li> <p>Data Science from Scratch por Joel Grus: Este libro es excelente para aquellos que prefieren aprender los fundamentos de la ciencia de datos desde cero, sin depender de bibliotecas, lo que permite un entendimiento m\u00e1s profundo de los algoritmos.</p> </li> <li> <p>Python for Data Analysis por Wes McKinney: Escrito por el creador de Pandas, este libro es una referencia obligatoria para cualquiera que quiera dominar la manipulaci\u00f3n y el an\u00e1lisis de datos en Python.</p> </li> </ul>"},{"location":"mat281_01/#3-repositorios","title":"3. Repositorios","text":"<p>Estos repositorios de GitHub contienen material pr\u00e1ctico y ejemplos que complementan los textos y permiten poner en pr\u00e1ctica lo aprendido:</p> <ul> <li> <p>python_intro: Un curso introductorio de Python que cubre los conceptos b\u00e1sicos de programaci\u00f3n y los fundamentos del lenguaje.</p> </li> <li> <p>python_eda: Un curso enfocado en la manipulaci\u00f3n y an\u00e1lisis exploratorio de datos (EDA) en Python, utilizando librer\u00edas como Pandas y Matplotlib.</p> </li> <li> <p>python_ml: Un curso sobre aprendizaje autom\u00e1tico (machine learning) en Python, que abarca desde los algoritmos m\u00e1s simples hasta t\u00e9cnicas m\u00e1s avanzadas.</p> </li> </ul>"},{"location":"mat281_01/#evaluacion","title":"Evaluaci\u00f3n","text":""},{"location":"mat281_01/#laboratorios","title":"Laboratorios","text":"<ul> <li>Semanal.</li> <li>Individual.</li> <li>Notas: 0, 25, 50, 75, 100.</li> <li>Plazo: final del d\u00eda de clases.</li> <li>Entregas fuera del plazo tienen nota cero (0).  </li> </ul>"},{"location":"mat281_01/#tareas","title":"Tareas","text":"<ul> <li>Mensual.</li> <li>Individual.</li> <li>Plazo:</li> <li>\\(T_1\\): 30 de Septiembre 2024.</li> <li>\\(T_2\\): 25 de Noviembre 2024.</li> <li>Entregas fuera del plazo descuentan 25 puntos por d\u00eda (parte entera). <ul> <li>Por ejemplo, un retraso de 15 minutos cuenta como un d\u00eda y descuenta 25 puntos.  </li> </ul> </li> </ul>"},{"location":"mat281_01/#proyecto","title":"Proyecto","text":"<ul> <li>Semestral.</li> <li>Plazo: final del curso (05 de Diciembre 2024).</li> <li>Grupal: m\u00e1ximo 4 integrantes.</li> <li>Entregas fuera del plazo descuentan 25 puntos por d\u00eda (parte entera).  </li> </ul>"},{"location":"mat281_01/#nota-final","title":"Nota Final","text":"<p>La nota final ser\u00e1 el promedio ponderado entre los laboratorios, tareas y el proyecto final del curso.</p> \\[ N_f = 0.3\\bar{n_l} + 0.35\\bar{n_t} + 0.35n_p \\] <p>\u00a1Importante!: Todos los entregables se deben subir al repositorio personal del estudiante (en GitHub). Las notas se trataran de actualizar al final de cada mes.</p>"},{"location":"mat281_02/","title":"Historia del curso","text":""},{"location":"mat281_02/#inicios-del-curso","title":"Inicios del Curso","text":"<p>El curso MAT281 - Aplicaciones de las Matem\u00e1ticas se concibi\u00f3 con la intenci\u00f3n de sumergirse en el mundo pr\u00e1ctico de las matem\u00e1ticas. Con la participaci\u00f3n de diversos profesores, cada uno con su especializaci\u00f3n \u00fanica, se exploraban diferentes  \u00e1reas desde perspectivas variadas. </p> <p>Por ejemplo, si un profesor era experto en sistemas din\u00e1micos,  el curso se adentraba en fen\u00f3menos como la propagaci\u00f3n de enfermedades. Por otro lado, si el profesor destacaba en ecuaciones diferenciales,  se aplicaban t\u00e9cnicas de resoluci\u00f3n para problemas como la ecuaci\u00f3n del calor.  Esto significaba que no hab\u00eda un horizonte com\u00fan definido de antemano, ya que cada a\u00f1o  el curso tomaba un rumbo distinto seg\u00fan la experiencia y enfoque del profesor a cargo. </p> <p>Adem\u00e1s, en sus inicios, el curso no se centraba en la programaci\u00f3n, a diferencia de lo que  ocurre en la actualidad.</p> <p>La versi\u00f3n actual del curso, tal como la conocemos, se remonta al a\u00f1o 2014, cuando el profesor Sebastian Flores reformul\u00f3 el enfoque de MAT281, orient\u00e1ndolo hacia aplicaciones m\u00e1s pr\u00e1cticas y did\u00e1cticas, particularmente en programaci\u00f3n (utilizando Python).</p> <p>Aunque las primeras versiones abordaban temas como an\u00e1lisis num\u00e9rico y an\u00e1lisis de datos ,  con un toque de aprendizaje autom\u00e1tico, desde 2017, el curso ha evolucionado hacia una  introducci\u00f3n al data science, gracias a los valiosos aportes de ayudantes y alumnos,  especialmente de Francisco Alfaro, Alonso Ogueda y Alberto Rubio.</p> <p>Es relevante mencionar que hasta el a\u00f1o 2019, el curso se dictaba exclusivamente en la sede de Casa Central. Sin embargo, a partir de ese a\u00f1o, el profesor Francisco Alfaro inici\u00f3 la ense\u00f1anza del mismo en la sede de San Joaqu\u00edn.</p> <p>Esta transici\u00f3n marc\u00f3 el inicio de importantes modificaciones tanto en el contenido del  curso como en su metodolog\u00eda de ense\u00f1anza. Entre los cambios m\u00e1s destacados se incluy\u00f3  la integraci\u00f3n de herramientas como GitHub y Google Colab, as\u00ed como la adopci\u00f3n de GitHub Actions y GitHub Pages para la documentaci\u00f3n del curso.</p>"},{"location":"mat281_02/#estado-del-curso","title":"Estado del Curso","text":"<p>En la actualidad, el curso MAT281 cumple con los m\u00e1s altos est\u00e1ndares de calidad en su presentaci\u00f3n. Aqu\u00ed se detalla brevemente c\u00f3mo funciona:</p> <ul> <li>GitHub se utiliza como repositorio principal para el curso.</li> <li>Google Colab permite trabajar con Python de manera colaborativa en la web.</li> <li>Mkdocs, junto con plugins adicionales, se emplea para crear la documentaci\u00f3n est\u00e1tica del curso.</li> <li>GitHub Actions automatiza la generaci\u00f3n de documentaci\u00f3n y su publicaci\u00f3n mediante GitHub Pages.</li> </ul> <p>En cuanto al contenido, se busca mejorar continuamente tanto los temas como las evaluaciones. El curso se centra en los siguientes puntos:</p> <p>Toolkit B\u00e1sico: * Manipulaci\u00f3n de Datos: T\u00e9cnicas eficientes para manipular conjuntos de datos. * Visualizaci\u00f3n: M\u00e9todos efectivos para representar gr\u00e1ficamente los datos. * Machine Learning: Introducci\u00f3n a los fundamentos y aplicaciones del aprendizaje autom\u00e1tico.</p> <p>Se solicita constantemente retroalimentaci\u00f3n del curso para su mejora, tanto de profesores,  colegas como de los propios alumnos.</p>"},{"location":"mat281_02/#agradecimientos","title":"Agradecimientos","text":"<p>Se agradece enormemente a todas las personas que han contribuido a  mejorar el curso, especialmente a los estudiantes que a\u00f1o tras a\u00f1o han  brindado valiosos comentarios. A continuaci\u00f3n, se mencionan los principales colaboradores  en la versi\u00f3n actual del curso:</p> <p>Profesores</p> <ul> <li>Sebastian Flores</li> <li>Alonso Ogueda</li> <li>Francisco Alfaro</li> </ul> <p>Ayudantes</p> <ul> <li>Alberto Rubio</li> <li>Eric Zepeda</li> </ul> <p>Alumnos</p> <p>A todas las generaciones de ingenieros matem\u00e1ticos que han formado parte de este curso.  Su participaci\u00f3n ha sido fundamental en su desarrollo y mejora continua.</p>"},{"location":"homeworks/hw_01/","title":"MAT281 - Tarea N\u00b001","text":"<p>Para efectos pr\u00e1cticos del curso, es posible representar cada pixel de una imagen con un array de 3 dimensiones, cada valor representa a una de las capas RGB. Por lo tanto, una imagen de $n \\times m$ pixeles se representa como un arreglo de dimension $(n, m , 3)$ En <code>Python</code> una de las librer\u00edas de procesamiento de im\u00e1genes m\u00e1s utilizada es <code>Pillow</code>.</p> <p>Abrir una imagen es tan f\u00e1cil como:</p> In\u00a0[1]: Copied! <pre># librerias\n\nimport numpy as np\nfrom PIL import Image\n</pre> # librerias  import numpy as np from PIL import Image In\u00a0[2]: Copied! <pre>gatito = Image.open(\"images/gatito.png\")\n</pre> gatito = Image.open(\"images/gatito.png\") <p>Notar que la variable anterior es de una clase espec\u00edfica de la librer\u00eda.</p> In\u00a0[3]: Copied! <pre>type(gatito)\n</pre> type(gatito) Out[3]: <pre>PIL.PngImagePlugin.PngImageFile</pre> <p>Para ver la imagen en Jupyter puedes utilizar la misma t\u00e9cnica que con los <code>pd.DataFrames</code>, es decir:</p> In\u00a0[4]: Copied! <pre>gatito\n</pre> gatito Out[4]: <p>Para tener su representaci\u00f3n en un array podemos utilizar el constructor <code>np.array</code> con argumento la imagen.</p> In\u00a0[5]: Copied! <pre>gatito_np = np.array(gatito)\nprint(f\"Dimension de la imagen gatito: {gatito_np.shape}.\\n\")\nprint(f\"Al convertir a np.ndarry el tipo de elementos es {gatito_np.dtype}.\\n\")\nprint(gatito_np)\n</pre> gatito_np = np.array(gatito) print(f\"Dimension de la imagen gatito: {gatito_np.shape}.\\n\") print(f\"Al convertir a np.ndarry el tipo de elementos es {gatito_np.dtype}.\\n\") print(gatito_np) <pre>Dimension de la imagen gatito: (2160, 1280, 3).\n\nAl convertir a np.ndarry el tipo de elementos es uint8.\n\n[[[179 211 215]\n  [179 211 215]\n  [179 209 215]\n  ...\n  [171 201 209]\n  [171 201 209]\n  [169 199 207]]\n\n [[181 213 217]\n  [181 213 217]\n  [181 211 217]\n  ...\n  [173 203 211]\n  [173 203 211]\n  [171 201 209]]\n\n [[179 211 215]\n  [179 211 215]\n  [181 211 217]\n  ...\n  [173 203 211]\n  [173 203 211]\n  [173 203 211]]\n\n ...\n\n [[125 179 190]\n  [123 177 188]\n  [129 183 194]\n  ...\n  [ 41  36  33]\n  [ 43  38  35]\n  [ 45  40  37]]\n\n [[127 181 192]\n  [125 179 190]\n  [127 181 192]\n  ...\n  [ 43  38  37]\n  [ 43  38  37]\n  [ 45  40  37]]\n\n [[125 179 190]\n  [123 177 188]\n  [123 177 188]\n  ...\n  [ 49  44  43]\n  [ 45  40  37]\n  [ 43  38  35]]]\n</pre> <p>1.1 Crear una lista vac\u00eda declarada como <code>secret_list</code>.</p> In\u00a0[\u00a0]: Copied! <pre>secret_list = ## FIX ME ##\n</pre> secret_list = ## FIX ME ## <p>1.2 Iterar por cada uno de los canales RGB (<code>gatito_np.shape[2]</code>) y en cada iteraci\u00f3n:</p> <ul> <li>Crear un arreglo temporal llamado <code>secret_aux</code> de dos dimensiones, de la misma dimension de pixeles de la imagen <code>gatito</code> y que tenga valores enteros, <code>0</code> si el valor de la capa de <code>gatito_np</code> es par y <code>1</code> si es impar.</li> </ul> <ul> <li>No iterar por filas y columnas.</li> <li>Utilizar la operaci\u00f3n m\u00f3dulo <code>%</code>.</li> <li>En la i-\u00e9sima iteraci\u00f3n de los canales la capa de <code>gatito_np</code> es <code>gatito_np[:, :, i]</code>.</li> </ul> <ul> <li><p>Escalar <code>secret_aux</code> a valores 0 y 255.</p> </li> <li><p>Cambiar el <code>dtype</code> de <code>secret_aux</code> a <code>np.uint8</code> (utilize el m\u00e9teodo <code>astype()</code>).</p> </li> <li><p>Agregue <code>secret_aux</code> a <code>secret_list</code>.</p> </li> </ul> <p>Al final de la iteraci\u00f3n <code>secret_list</code> debe tener solo tres arreglos.</p> <p>Observaci\u00f3n: recuerde que puede aplicar operaciones directo a un arreglo de numpy.</p> In\u00a0[\u00a0]: Copied! <pre>for channel in ## FIX ME ##:\n    secret_aux = ## FIX ME ##\n    secret_aux = ## FIX ME ##\n    secret_aux = ## FIX ME ##\n    secret_list.append(secret_aux)\n</pre> for channel in ## FIX ME ##:     secret_aux = ## FIX ME ##     secret_aux = ## FIX ME ##     secret_aux = ## FIX ME ##     secret_list.append(secret_aux) In\u00a0[\u00a0]: Copied! <pre>print(f\"secret_list tiene {len(secret_list)} elementos\")\n</pre> print(f\"secret_list tiene {len(secret_list)} elementos\") <p>1.3 Crear la variable <code>secret_np</code> concatenando horizontalmente los elementos de <code>secret_list</code>.</p> In\u00a0[\u00a0]: Copied! <pre>secret_np = ## FIX ME ##\nsecret_np.shape\n</pre> secret_np = ## FIX ME ## secret_np.shape <p>1.4 Crear el objeto <code>secret_img</code> utilizando el arreglo <code>secret_np</code>, asegurar que los valores est\u00e9n entre 0 y 255, y que el dtype sea <code>np.uint8</code>, con el m\u00e9todo <code>Image.fromarray</code> con argumento <code>mode=\"L\"</code></p> In\u00a0[\u00a0]: Copied! <pre>np.unique(secret_np)\n</pre> np.unique(secret_np) In\u00a0[\u00a0]: Copied! <pre>secret_np.dtype\n</pre> secret_np.dtype In\u00a0[\u00a0]: Copied! <pre>secret_img = Image.fromarray(## FIX ME ##)\n</pre> secret_img = Image.fromarray(## FIX ME ##) <p>Ahora puedes ver el resultado!</p> In\u00a0[\u00a0]: Copied! <pre>secret_img\n</pre> secret_img <p>2.1 Selecciona una imagen de 2160 x 3840 pixeles (a.k.a resoluci\u00f3n 4k), lo importante es que sea solo en blanco y negro, en la carpeta <code>images</code> se disponibiliza como ejemplo la imagen <code>black_and_white_example.jpg</code> y crea una variable llamada <code>my_img</code> leyendo la imagen seleccionada con <code>Image.open()</code>.</p> In\u00a0[\u00a0]: Copied! <pre>my_img = Image.open(## FIX ME ##)\n</pre> my_img = Image.open(## FIX ME ##) <p>2.2 Crea un arreglo llamado <code>my_img_np</code> utilizando <code>my_img</code> y el m\u00e9todo <code>np.array()</code>. * Es importante que <code>my_img_np.shape</code> sea <code>(2160, 3840)</code>, es decir, que solo sea de dos dimensiones. Esto porque es una imagen en blanco y negro, no necesitando el modelo RGB.</p> In\u00a0[\u00a0]: Copied! <pre>my_img_np = np.array(my_img)\nprint(my_img_np.shape)\n</pre> my_img_np = np.array(my_img) print(my_img_np.shape) <p>2.3 Crear la variable <code>my_img_np_aux</code> utilizando un umbral con tal de que: - 1: Si el valor del pixel es mayor  al umbral. - 0: Si el valor del pixel es menor o igual al umbral. - El <code>dtype</code> debe ser <code>np.uint8</code>. - Para <code>black_and_white_example.jpg</code> un umbral adecuado es <code>20</code>.</p> In\u00a0[\u00a0]: Copied! <pre>umbral = 20\nmy_img_np_aux = ## FIX ME ##\n</pre> umbral = 20 my_img_np_aux = ## FIX ME ## <p>Puedes probar que tan bien qued\u00f3 la imagen con la siguiente linea. Si crees que no se ve bien, puedes cambiar el umbral.</p> In\u00a0[\u00a0]: Copied! <pre>Image.fromarray(my_img_np_aux * 255)\n</pre> Image.fromarray(my_img_np_aux * 255) <p>2.4 Dividir la imagen en tres arreglos de tama\u00f1o (2160, 1280) y guardarlos en una lista con el nombre <code>my_img_split</code>. Hint: Revisa en la documentaci\u00f3n de <code>numpy</code>.</p> In\u00a0[\u00a0]: Copied! <pre>my_img_split = ## FIX ME ##\n</pre> my_img_split = ## FIX ME ## <p>Revisa utilizando la siguiente iteraci\u00f3n.</p> In\u00a0[\u00a0]: Copied! <pre>for img_array in my_img_split:\n    print(img_array.shape)\n</pre> for img_array in my_img_split:     print(img_array.shape) <p>2.5 La imagen donde se esconder\u00e1 tu imagen selecionada est\u00e1 en la carpeta <code>images</code> con el nombre <code>gatito_original.png</code>, que sospechosamente es de 2160 x 1280 pixeles. Carga la imagen en la variable <code>cat</code> y luego crea arreglo <code>cat_np</code> utilizando <code>cat</code>. Verifica que <code>cat_np.shape = (2160, 1280, 3)</code>.</p> In\u00a0[\u00a0]: Copied! <pre>cat = Image.open(## FIX ME ##)\ncat_np = ## FIX ME ##\nprint(cat_np.shape)\n</pre> cat = Image.open(## FIX ME ##) cat_np = ## FIX ME ## print(cat_np.shape) <p>2.6 Convierte todos los valores de <code>cat_np</code> a valores pares. Esto lo puedes hacer sumando 1 a cada valor de arreglo si es impar</p> In\u00a0[\u00a0]: Copied! <pre>cat_np ## FIX ME ##\n</pre> cat_np ## FIX ME ## <p>2.7 Itera por canal RGB de <code>cat_np</code> y en cada capa suma los valores de uno de los arreglos de <code>my_img_split</code>.</p> In\u00a0[\u00a0]: Copied! <pre>for channel in ## FIX ME ##:\n    cat_np[## FIX ME ##] += ## FIX ME ##\n</pre> for channel in ## FIX ME ##:     cat_np[## FIX ME ##] += ## FIX ME ## <p>2.8 Crea una variable llamada <code>cat_secret_im</code> con <code>Image.fromarray</code> y la variable <code>cat_np</code> (que ya ha sido modificada). Luego guarda la imagen en la carpeta <code>images</code> con el nombre <code>my_secret.png</code>.</p> In\u00a0[\u00a0]: Copied! <pre>cat_secret_im = Image.fromarray(## FIX ME ##)\ncat_secret_im.save(## FIX ME ##)\n</pre> cat_secret_im = Image.fromarray(## FIX ME ##) cat_secret_im.save(## FIX ME ##) <p>2.9 Crea una funci\u00f3n llamada <code>imagenception()</code> que como argumento tenga la ruta de la imagen que quieres descifrar y que descifre la imagen secreta recientemente creada. Hint: Utiliza todos los pasos de la primera parte.</p> In\u00a0[\u00a0]: Copied! <pre>def imagenception(filepath):\n   ## FIX ME ##\n    return secret_img\n</pre> def imagenception(filepath):    ## FIX ME ##     return secret_img In\u00a0[\u00a0]: Copied! <pre>my_secret_img = imagenception(\"images/my_secret.png\")\nmy_secret_img\n</pre> my_secret_img = imagenception(\"images/my_secret.png\") my_secret_img In\u00a0[\u00a0]: Copied! <pre># libraries\nimport pandas as pd\npd.set_option(\"display.max_columns\", 999)  # Permite mostrar hasta 999 columnas de un DataFrame en Jupyter.\n</pre> # libraries import pandas as pd pd.set_option(\"display.max_columns\", 999)  # Permite mostrar hasta 999 columnas de un DataFrame en Jupyter. <p>En la carpeta <code>data/world-happiness</code> se disponen de tres archivos, uno por cada reporte anual (a\u00f1os 2015, 2016 y 2017). No es de sorprender que env\u00eden un archivo por a\u00f1o (podr\u00eda ser mensual, semestral, etc.), lo imortante es ser capaces de leer una cantidad variable de archivos al mismo tiempo. Una buena pr\u00e1ctica es crear un diccionario de dataframes.</p> In\u00a0[\u00a0]: Copied! <pre># Comprehension dictionary\ndf_dict = {\n    year: pd.read_csv(f\"data/world-happiness/{year}.csv\").assign(Year=year)\n    for year in [2015, 2016, 2017]\n}\n</pre> # Comprehension dictionary df_dict = {     year: pd.read_csv(f\"data/world-happiness/{year}.csv\").assign(Year=year)     for year in [2015, 2016, 2017] } <p>Por ejemplo, se puede acceder al DataFrame asociado al archivo <code>data/world-happiness/2016.csv</code> de la siguiente manera:</p> <p>Una peque\u00f1a descripci\u00f3n de las columnas</p> <ul> <li><code>Country</code> Name of the country.</li> <li><code>Region</code> Region the country belongs to.</li> <li><code>Happiness Rank</code> Rank of the country based on the Happiness Score.</li> <li><code>Happiness Score</code> A metric measured in 2015 by asking the sampled people the question: \"How would you rate your happiness on a scale of 0 to 10 where 10 is the happiest.\"</li> <li><code>Standard Error</code> The standard error of the happiness score.</li> <li><code>Economy (GDP per Capita)</code> The extent to which GDP contributes to the calculation of the Happiness Score.</li> <li><code>Family</code> The extent to which Family contributes to the calculation of the Happiness Score</li> <li><code>Health (Life Expectancy)</code> The extent to which Life expectancy contributed to the calculation of the Happiness Score</li> <li><code>Freedom</code> The extent to which Freedom contributed to the calculation of the Happiness Score.</li> <li><code>Trust (Government Corruption)</code> The extent to which Perception of Corruption contributes to Happiness Score.</li> <li><code>Generosity</code> The extent to which Generosity contributed to the calculation of the Happiness Score.</li> <li><code>Dystopia Residual</code> The extent to which Dystopia Residual contributed to the calculation of the Happiness Score.</li> </ul> <p>Notar que los conjuntos de datos no poseen las mismas columnas, por lo tanto, solo se trabajar\u00e1n con las columnas en com\u00fan y posteriormente agregaremos el a\u00f1o con tal de concatenar los tres conjuntos.</p> In\u00a0[\u00a0]: Copied! <pre>from functools import reduce\nintersection_columns = reduce(np.intersect1d, [df_i.columns.values for df_i in df_dict.values()]).tolist() \nprint(intersection_columns)\n</pre> from functools import reduce intersection_columns = reduce(np.intersect1d, [df_i.columns.values for df_i in df_dict.values()]).tolist()  print(intersection_columns) <p>Explica con tus palabras las operaciones que se realizaron para obtener la variable <code>intersection_columns</code>.</p> <p>Respuesta: &lt; RESPONDER AQU\u00cd &gt;</p> <p>Define el DataFrame <code>happiness</code> tal que:</p> <ul> <li>Sea la concatenaci\u00f3n de los dataframes de <code>df_dict</code></li> <li>Resetea los \u00edndices.</li> <li>Selecciona solo las columnas de la lista <code>intersection_columns</code>.</li> <li>Los nombres de las columnas deben estar en min\u00edsculas, reemplazar espacios por guiones bajos (<code>_</code>) y elimina los par\u00e9ntesis.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>happiness = (\n    pd.concat(## FIX ME ##)\n    .droplevel(## FIX ME ##)\n    .## FIX ME ##\n    .## FIX ME ##\n    .## FIX ME ##\n)\nhappiness.head()\n</pre> happiness = (     pd.concat(## FIX ME ##)     .droplevel(## FIX ME ##)     .## FIX ME ##     .## FIX ME ##     .## FIX ME ## ) happiness.head() <p>Como siempre, partimos con un an\u00e1lisis descriptivo simple.</p> In\u00a0[\u00a0]: Copied! <pre>happiness.describe(include=\"all\").fillna(\"\").T\n</pre> happiness.describe(include=\"all\").fillna(\"\").T <p>\u00bfCu\u00e1ntos pa\u00edses no tienen mediciones de felicidad en los tres a\u00f1os del estudio? \u00bfCu\u00e1les son?</p> In\u00a0[\u00a0]: Copied! <pre>## FIX ME ##\n</pre> ## FIX ME ## In\u00a0[\u00a0]: Copied! <pre>## FIX ME ##\n</pre> ## FIX ME ## <p>Respuesta: &lt; RESPONDER AQU\u00cd &gt;</p> <p>Note que la lista de pa\u00edses proveniente de la pregunta anterior tiene errores de consistencia, por ejemplo est\u00e1n los registros de <code>Hong Kong</code> y <code>Hong Kong S.A.R., China</code> que escencialmente son el mismo. Lo mismo ocurre con <code>Taiwan</code> y <code>Somaliland Region</code>.</p> <p>Modifique la columna <code>country</code> del dataframe <code>happiness</code> con tal de reparar los errores de <code>Hong Kong</code>, <code>Taiwan</code> y <code>Somaliland Region</code>.</p> In\u00a0[\u00a0]: Copied! <pre>bad_country_names_dict = {\"Hong Kong S.A.R., China\": \"Hong Kong\", ## FIX ME ##}\nhappiness = happiness.assign(## FIX ME ##)\n</pre> bad_country_names_dict = {\"Hong Kong S.A.R., China\": \"Hong Kong\", ## FIX ME ##} happiness = happiness.assign(## FIX ME ##) <p>Luego de la modificaci\u00f3n, \u00bfCu\u00e1ntos pa\u00edses no tienen mediciones en los tres a\u00f1os de estudio?</p> In\u00a0[\u00a0]: Copied! <pre>## FIX ME ##\n</pre> ## FIX ME ## <p>Pivotea el dataframe <code>happines</code> tal que los \u00edndices sean los a\u00f1os, las columnas los pa\u00edses y el valor su <code>happiness_score</code>. LLena los valores nulos con un string vac\u00edo <code>\"\"</code>. Un pa\u00eds no puede tener m\u00e1s de un registro por a\u00f1o, por lo que puedes utilizar directamente el m\u00e9doto <code>pd.DataFrame.pivot()</code>.</p> In\u00a0[\u00a0]: Copied! <pre>## FIX ME ##\n</pre> ## FIX ME ## <p>\u00bfQu\u00e9 informaci\u00f3n podr\u00edas sacar r\u00e1pidamente de esta tabla pivoteada? \u00bfPodr\u00edas decir que siempre es \u00fatil pivotear una tabla?</p> <p>Respuesta:  &lt; RESPONDER AQU\u00cd &gt;</p> <p>En promedio, \u00bfCu\u00e1les son los tres pa\u00edses con el mayor score de felicidad?</p> In\u00a0[\u00a0]: Copied! <pre>## FIX ME ##\n</pre> ## FIX ME ## <p>Respuesta:  &lt; RESPONDER AQU\u00cd &gt;</p> <p>Calcula el promedio anual de todas las columnas factores de felicidad, es decir, todas las variables num\u00e9ricas excepto <code>happiness_score</code> y <code>happiness_rank</code>.</p> In\u00a0[\u00a0]: Copied! <pre>hap_mean_factors = ## FIX ME ##\nhap_mean_factors\n</pre> hap_mean_factors = ## FIX ME ## hap_mean_factors <p>Respecto al c\u00e1lculo anterior, para cada uno de los a\u00f1os, \u00bfCu\u00e1l es el factor que m\u00e1s contribuye (en promedio) al score de la felicidad y en qu\u00e9 medida?</p> In\u00a0[\u00a0]: Copied! <pre>## FIX ME ##\n</pre> ## FIX ME ## <p>Respuesta:  &lt; RESPONDER AQU\u00cd &gt;</p> <p>A continuaci\u00f3n, agregaremos un nuevo conjunto de datos, el que contiene estad\u00edsticas de suicidio por a\u00f1os, pa\u00edses y rangos et\u00e1reos. Se encuentra disponible en el siguiente link.</p> In\u00a0[\u00a0]: Copied! <pre>suicide = pd.read_csv(\"data/suicide_rates.csv\")\nsuicide.head()\n</pre> suicide = pd.read_csv(\"data/suicide_rates.csv\") suicide.head() <p>La mayor\u00eda de las columnas son autoexplicativas.</p> <ul> <li><code>country</code></li> <li><code>year</code></li> <li><code>sex</code></li> <li><code>age</code></li> <li><code>suicides_no</code></li> <li><code>population</code></li> <li><code>suicides/100k pop</code></li> <li><code>country-year</code></li> <li><code>HDI for year</code> Human Development Index</li> <li><code>gdp_for_year ($)</code> Gross Domestic Product</li> <li><code>gdp_per_capita ($)</code></li> <li><code>generation</code> based on age grouping average</li> </ul> <p>Un poco de estad\u00edstica descriptiva.</p> In\u00a0[\u00a0]: Copied! <pre>suicide.describe(include=\"all\").fillna(\"\").T\n</pre> suicide.describe(include=\"all\").fillna(\"\").T <p>Crea un nuevo DataFrame llamado suicide_agg siguiendo las siguientes instrucciones:</p> <ul> <li>Agrupa por pa\u00eds y a\u00f1o.</li> <li>Suma la poblaci\u00f3n y el n\u00famero de suicidios.</li> <li>Resetea los \u00edndices.</li> <li>Agrega una nueva columna llamada <code>suicides_ratio_100k</code> formada por la divisi\u00f3n de <code>suicides_no</code> y <code>population</code>, para posteriormente muliplicarla por 100,000.</li> <li>Agrega una nuevale columna llamada <code>suicides_rank</code> similar a <code>happiness_rank</code>, es decir, que asigne un orden por a\u00f1o a cada pa\u00eds seg\u00fan la columna <code>suicides_ratio_100k</code> tal que el rank 1 corresponda al que tenga mayor <code>suicides_ratio_100k</code>. Hint: Usa el m\u00e9todo <code>rank()</code>.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># Es posible hacer todas las operaciones encadenadas!\nsuicides_agg = (\n    suicide.groupby(## FIX ME ##])\n    .agg(\n      ## FIX ME ##\n    )\n    .## FIX ME ##\n    .assign(\n        suicides_ratio_100k=## FIX ME ##,\n        suicides_rank=## FIX ME ##\n    )\n)\n</pre> # Es posible hacer todas las operaciones encadenadas! suicides_agg = (     suicide.groupby(## FIX ME ##])     .agg(       ## FIX ME ##     )     .## FIX ME ##     .assign(         suicides_ratio_100k=## FIX ME ##,         suicides_rank=## FIX ME ##     ) ) <p>Crea un nuevo DataFrame con el nombre <code>hap_sui</code> al unir <code>happiness</code> y <code>suicides_agg</code> tal que coincidan pa\u00eds y a\u00f1o, qu\u00e9date con solo los registros que coincidan en ambas DataFrames.</p> In\u00a0[\u00a0]: Copied! <pre>hap_sui = ## FIX ME ##\nhap_sui.head()\n</pre> hap_sui = ## FIX ME ## hap_sui.head() <p>\u00bfQu\u00e9 tipo de correlaci\u00f3n lineal hay entre las variables <code>happiness_rank</code> y <code>suicides_rank</code>?</p> In\u00a0[\u00a0]: Copied! <pre>hap_sui.loc[:, [\"happiness_rank\", \"suicides_rank\"]].## FIX ME ##\n</pre> hap_sui.loc[:, [\"happiness_rank\", \"suicides_rank\"]].## FIX ME ## <p>\u00bfQu\u00e9 tipo de correlaci\u00f3n lineal hay entre las variables <code>happiness_rank</code> y <code>suicides_rank</code> por cada a\u00f1o?</p> In\u00a0[\u00a0]: Copied! <pre>## FIX ME ##\n</pre> ## FIX ME ## <p>Respuesta:  &lt; RESPONDER AQU\u00cd &gt;</p> <p>\u00bfLa respuesta de las dos preguntas anteriores cambia si se utilizan las variables <code>happiness_score</code> y <code>suicides_ratio_100k</code>?</p> In\u00a0[\u00a0]: Copied! <pre>## FIX ME ##\n</pre> ## FIX ME ## <p>Respuesta:  &lt; RESPONDER AQU\u00cd &gt;</p> <p>Estos \u00edndices est\u00e1n ajustados a la Ciudad de Nueva York (NYC). Lo que significa que para la Ciudad de Nueva York, cada \u00edndice deber\u00eda marcar 100(%). Si otra ciudad tiene, por ejemplo, un \u00edndice de alquiler de 120, significa que en esa ciudad se paga de media por el alquiler un 20% m\u00e1s que en Nueva York. Si una ciudad tiene un \u00edndice de alquiler de 70, significa que en esa ciudad los alquileres son de media un 30% m\u00e1s baratos que en Nueva York.</p> <ul> <li><p>El \u00cdndice de Costo de Vida (Sin Alquiler) es un indicador relativo de los precios de bienes de consumo, incluyendo comestibles, restaurantes, transporte y servicios. El \u00cdndice de Costo de Vida no incluye gastos de residencia como alquileres o hipotecas. Si una ciudad tiene un Costo de Vida de 120, significa que Numbeo estima que es un 20% m\u00e1s cara que Nueva York (sin contar alquiler).</p> </li> <li><p>El \u00cdndice de Alquiler es una estimaci\u00f3n de precios de alquiler de apartamentos de una ciudad comparada con Nueva York. Si el \u00cdndice de Alquiler es 80, Numbeo estima que el precio de los alquileres en esa ciudad es de media un 20% m\u00e1s barato que en Nueva York.</p> </li> <li><p>El \u00cdndice de Comestibles es una estimaci\u00f3n de los precios de la compra de una ciudad en comparaci\u00f3n con Nueva York. Para calcular esta secci\u00f3n, Numbeo utiliza el peso de los art\u00edculos en la secci\u00f3n \"Mercados\" por cada ciudad.</p> </li> <li><p>El \u00cdndice de Restaurantes es una comparaci\u00f3n de precios de comidas y bebidas en bares y restaurantes en comparaci\u00f3n con NY.</p> </li> <li><p>El \u00cdndice de Costo de Vida m\u00e1s Alquiler es una estimaci\u00f3n de precios de consumo incluyendo alquiler en comparaci\u00f3n con la Ciudad de Nueva York.</p> </li> <li><p>El Poder Adquisitivo Local muestra la capacidad adquisitiva relativa a la hora de comprar bienes y servicios en una ciudad determinada, con relaci\u00f3n al salario medio de la ciudad. Si el poder adquisitivo dom\u00e9stico es 40, significa que los habitantes de dicha ciudad con salario medio pueden permitirse comprar una media de 60% menos bienes y servicios que los habitantes de Nueva York con salario medio.</p> </li> </ul> <p>Para m\u00e1s informaci\u00f3n sobre los pesos utilizados (f\u00f3rmula completa) puedes visitar: motivaci\u00f3n y metodolog\u00eda.</p> <p>Para comenzar es necesario instalar el paquete <code>lxml</code> en tu entorno virtual de conda para poder descargar los datos. Basta con ejecutar:</p> In\u00a0[\u00a0]: Copied! <pre># instalar lxml\n!pip install lxml\n</pre> # instalar lxml !pip install lxml <p>Se disponibiliza a continuaci\u00f3n la carga de datos de un dataframe.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\npd.set_option('display.max_columns', 999)\n%matplotlib inline\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt  pd.set_option('display.max_columns', 999) %matplotlib inline In\u00a0[\u00a0]: Copied! <pre>years = [2015, 2016, 2017, 2018, 2019, 2020]\nlife_cost = (\n    pd.concat(\n        {\n            year: (\n                pd.read_html(f\"https://www.numbeo.com/cost-of-living/rankings.jsp?title={year}\")[1]\n                .rename(columns=lambda x: x.lower().replace(\" \", \"_\"))\n                .assign(rank=lambda x: x.index + 1)\n                .set_index(\"rank\")\n            ) for year in years\n        }\n    )\n    .rename_axis([\"year\", \"rank\"])\n    .reset_index()\n)\nlife_cost.head()\n</pre> years = [2015, 2016, 2017, 2018, 2019, 2020] life_cost = (     pd.concat(         {             year: (                 pd.read_html(f\"https://www.numbeo.com/cost-of-living/rankings.jsp?title={year}\")[1]                 .rename(columns=lambda x: x.lower().replace(\" \", \"_\"))                 .assign(rank=lambda x: x.index + 1)                 .set_index(\"rank\")             ) for year in years         }     )     .rename_axis([\"year\", \"rank\"])     .reset_index() ) life_cost.head() In\u00a0[\u00a0]: Copied! <pre>## FIX ME PLEASE ##\n</pre> ## FIX ME PLEASE ## In\u00a0[\u00a0]: Copied! <pre>rol_seed = 201110002  # Escribe tu rol UTFSM sin n\u00famero verificador\nmy_cities = life_cost[\"city\"].drop_duplicates().sample(n=10, random_state=rol_seed).values\n</pre> rol_seed = 201110002  # Escribe tu rol UTFSM sin n\u00famero verificador my_cities = life_cost[\"city\"].drop_duplicates().sample(n=10, random_state=rol_seed).values In\u00a0[\u00a0]: Copied! <pre>## FIX ME PLEASE ##\n</pre> ## FIX ME PLEASE ##"},{"location":"homeworks/hw_01/#mat281-tarea-n01","title":"MAT281 - Tarea N\u00b001\u00b6","text":""},{"location":"homeworks/hw_01/#instrucciones","title":"Instrucciones\u00b6","text":"<p>1.- Completa tus datos personales (nombre y rol USM) en siguiente celda.</p> <ul> <li><p>Nombre:</p> </li> <li><p>Rol:</p> </li> </ul> <p>2.- Debes subir este archivo con tus cambios a tu repositorio personal del curso, incluyendo datos, im\u00e1genes, scripts, etc.</p> <p>3.- Se evaluar\u00e1:</p> <ul> <li>Soluciones</li> <li>C\u00f3digo</li> <li>Al presionar  <code>Kernel -&gt; Restart Kernel and Run All Cells</code> deben ejecutarse todas las celdas sin error.</li> </ul>"},{"location":"homeworks/hw_01/#i-imagenception","title":"I.-  Imagenception\u00b6","text":"<p>Desde Wikipedia, RGB (sigla en ingl\u00e9s de red, green, blue) es un modelo de color basado en la s\u00edntesis aditiva, con el que es posible representar un color mediante la mezcla por adici\u00f3n de los tres colores de luz primarios. El modelo de color RGB no define por s\u00ed mismo lo que significa exactamente rojo, verde o azul, por lo que los mismos valores RGB pueden mostrar colores notablemente diferentes en distintos dispositivos que usen este modelo de color. Aunque utilicen un mismo modelo de color, sus espacios de color pueden variar considerablemente.</p> <p>Para indicar con qu\u00e9 proporci\u00f3n es mezclado cada color, se asigna un valor a cada uno de los colores primarios, de manera que el valor \"0\" significa que no interviene en la mezcla y, a medida que ese valor aumenta, se entiende que aporta m\u00e1s intensidad a la mezcla. Aunque el intervalo de valores podr\u00eda ser cualquiera (valores reales entre 0 y 1, valores enteros entre 0 y 37, etc.), es frecuente que cada color primario se codifique con un byte (8 bits).</p> <p>As\u00ed, de manera usual, la intensidad de cada una de las componentes se mide seg\u00fan una escala que va del 0 al 255 y cada color es definido por un conjunto de valores escritos entre par\u00e9ntesis (correspondientes a valores \"R\", \"G\" y \"B\") y separados por comas.</p> <p>El conjunto de todos los colores tambi\u00e9n se puede representar en forma de cubo. Cada color es un punto de la superficie o del interior de \u00e9ste. La escala de grises estar\u00eda situada en la diagonal que une al color blanco con el negro.</p> <p></p>"},{"location":"homeworks/hw_01/#1-encontrando-la-imagen-oculta","title":"1.- Encontrando la imagen oculta\u00b6","text":"<p>La imagen anterior tiene una imagen oculta, el ejercicio corresponde en descifrarlo. Las instrucciones son las siguientes:</p>"},{"location":"homeworks/hw_01/#2-escondiendo-una-nueva-imagen","title":"2.- Escondiendo una nueva imagen\u00b6","text":"<p>Es tu turno, ahora tu esconder\u00e1s una imagen. Las instrucciones son las siguientes:</p>"},{"location":"homeworks/hw_01/#ii-analizando-la-felicidad","title":"II.- Analizando la Felicidad\u00b6","text":"<p>Este ejercicio es netamente an\u00e1lisis de datos, tratando de abarcar problemas t\u00edpicos como la lectura de datos, correcci\u00f3n de errores, m\u00e9tricas agrupadas, uni\u00f3n de datos, etc. Utilizaremos un conjunto de datos llamado World Happiness Report disponible en el siguiente link, de donde se puede obtener informaci\u00f3n al respecto.</p>"},{"location":"homeworks/hw_01/#context","title":"Context\u00b6","text":"<p>The World Happiness Report is a landmark survey of the state of global happiness. The first report was published in 2012, the second in 2013, the third in 2015, and the fourth in the 2016 Update. The World Happiness 2017, which ranks 155 countries by their happiness levels, was released at the United Nations at an event celebrating International Day of Happiness on March 20th. The report continues to gain global recognition as governments, organizations and civil society increasingly use happiness indicators to inform their policy-making decisions. Leading experts across fields \u2013 economics, psychology, survey analysis, national statistics, health, public policy and more \u2013 describe how measurements of well-being can be used effectively to assess the progress of nations. The reports review the state of happiness in the world today and show how the new science of happiness explains personal and national variations in happiness.</p>"},{"location":"homeworks/hw_01/#content","title":"Content\u00b6","text":"<p>The happiness scores and rankings use data from the Gallup World Poll. The scores are based on answers to the main life evaluation question asked in the poll. This question, known as the Cantril ladder, asks respondents to think of a ladder with the best possible life for them being a 10 and the worst possible life being a 0 and to rate their own current lives on that scale. The scores are from nationally representative samples for the years 2013-2016 and use the Gallup weights to make the estimates representative. The columns following the happiness score estimate the extent to which each of six factors \u2013 economic production, social support, life expectancy, freedom, absence of corruption, and generosity \u2013 contribute to making life evaluations higher in each country than they are in Dystopia, a hypothetical country that has values equal to the world\u2019s lowest national averages for each of the six factors. They have no impact on the total score reported for each country, but they do explain why some countries rank higher than others.</p>"},{"location":"homeworks/hw_01/#21-lectura-de-datos","title":"2.1 Lectura de datos\u00b6","text":""},{"location":"homeworks/hw_01/#22-concatenacion-y-procesado","title":"2.2 Concatenaci\u00f3n y procesado\u00b6","text":""},{"location":"homeworks/hw_01/#23-analisis","title":"2.3 An\u00e1lisis\u00b6","text":""},{"location":"homeworks/hw_01/#24-agregando-mas-datos","title":"2.4 Agregando m\u00e1s datos\u00b6","text":""},{"location":"homeworks/hw_01/#iii-indices-de-costos-de-vida","title":"III.- \u00cdndices de Costos de Vida\u00b6","text":""},{"location":"homeworks/hw_01/#ejercicio-31","title":"Ejercicio 3.1\u00b6","text":"<p>Explique lo que se hizo en la celda anterior detalladamente.</p>"},{"location":"homeworks/hw_01/#ejercicio-32","title":"Ejercicio 3.2\u00b6","text":"<p>Genera un histograma del \u00edndice del costo de vida (sin alquiler) para cada a\u00f1o (es decir, 6 histogramas).</p> <p>\u00bfQu\u00e9 conclusi\u00f3n puedes sacar de estos gr\u00e1ficos?</p>"},{"location":"homeworks/hw_01/#ejercicio-33","title":"Ejercicio 3.3\u00b6","text":"<p>Grafica el \u00edndice de restaurantes a trav\u00e9s de los a\u00f1os para diez ciudades escogidas pseudo-aleatoriamente (variable <code>my_cities</code> de la celda siguiente) en un mismo gr\u00e1fico. Recuerda escoger el tipo de gr\u00e1fico adecuadamente.</p> <p>\u00bfVes alguna relaci\u00f3n? \u00bfQu\u00e9 podr\u00edas decir del gr\u00e1fico? \u00bfPor qu\u00e9 no graficar todas las ciudades en lugar de solo escoger algunas?</p>"},{"location":"homeworks/hw_02/","title":"MAT281 - Tarea N\u00b002","text":"In\u00a0[\u00a0]: Copied! <pre>## FIX ME PLEASE ##\n</pre> ## FIX ME PLEASE ## <p>El desaf\u00edo Titanic - Machine Learning from Disaster en Kaggle invita a predecir qu\u00e9 pasajeros sobrevivieron al naufragio del Titanic mediante un modelo de machine learning. Utiliza datos reales de los pasajeros, como su nombre, edad, g\u00e9nero y clase socioecon\u00f3mica, para explorar patrones de supervivencia y construir un modelo predictivo. Este es uno de los desaf\u00edos m\u00e1s populares de Kaggle y un excelente punto de partida para aprender sobre machine learning y an\u00e1lisis de datos.</p> In\u00a0[\u00a0]: Copied! <pre>## FIX ME PLEASE ##\n</pre> ## FIX ME PLEASE ##"},{"location":"homeworks/hw_02/#mat281-tarea-n02","title":"MAT281 - Tarea N\u00b002\u00b6","text":""},{"location":"homeworks/hw_02/#instrucciones","title":"Instrucciones\u00b6","text":"<p>1.- Completa tus datos personales (nombre y rol USM) en siguiente celda.</p> <ul> <li><p>Nombre:</p> </li> <li><p>Rol:</p> </li> </ul> <p>2.- Debes subir este archivo con tus cambios a tu repositorio personal del curso, incluyendo datos, im\u00e1genes, scripts, etc.</p> <p>3.- Se evaluar\u00e1:</p> <ul> <li>Soluciones</li> <li>C\u00f3digo</li> <li>Al presionar  <code>Kernel -&gt; Restart Kernel and Run All Cells</code> deben ejecutarse todas las celdas sin error.</li> </ul> <p>4.- Esta Tarea debe ser entregada en Dos Jupyter Notebooks Distinto.</p> <ul> <li>Ejemplo: <code>hw_02_part_01.ipynb</code>, <code>hw_02_part_02.ipynb</code>.</li> </ul>"},{"location":"homeworks/hw_02/#i-learnplatform","title":"I.- LearnPlatform\u00b6","text":""},{"location":"homeworks/hw_02/#introduccion","title":"Introducci\u00f3n\u00b6","text":"<p>Nelson Mandela cre\u00eda que la educaci\u00f3n era el arma m\u00e1s poderosa para cambiar el mundo. Pero no todos los estudiantes tienen las mismas oportunidades de aprender. Es necesario promulgar pol\u00edticas y planes efectivos para que la educaci\u00f3n sea m\u00e1s equitativa, y tal vez su innovador an\u00e1lisis de datos ayude a revelar la soluci\u00f3n.</p> <p>La investigaci\u00f3n actual muestra que los resultados educativos est\u00e1n lejos de ser equitativos. El desequilibrio se vio agravado por la pandemia de COVID-19. Existe una necesidad urgente de comprender y medir mejor el alcance y el impacto de la pandemia en estas inequidades.</p> <p>La empresa de tecnolog\u00eda educativa LearnPlatform se fund\u00f3 en 2014 con la misi\u00f3n de ampliar el acceso equitativo a la tecnolog\u00eda educativa para todos los estudiantes y profesores. Los distritos y estados utilizan el sistema integral de efectividad de la tecnolog\u00eda educativa de LearnPlatform para mejorar continuamente la seguridad, la equidad y la efectividad de su tecnolog\u00eda educativa. LearnPlatform lo hace generando una base de evidencia de lo que est\u00e1 funcionando y promulg\u00e1ndola en beneficio de los estudiantes, los profesores y los presupuestos.</p> <p>En esta competencia de an\u00e1lisis, trabajar\u00e1 para descubrir tendencias en el aprendizaje digital. Logre esto con un an\u00e1lisis de datos sobre c\u00f3mo el compromiso con el aprendizaje digital se relaciona con factores como la demograf\u00eda del distrito, el acceso a la banda ancha y las pol\u00edticas y eventos a nivel estatal / nacional. Luego, env\u00ede un notebook de Kaggle para proponer su mejor soluci\u00f3n a estas desigualdades educativas.</p> <p>Sus presentaciones informar\u00e1n las pol\u00edticas y pr\u00e1cticas que cierran la brecha digital. Con una mejor comprensi\u00f3n de las tendencias de aprendizaje digital, puede ayudar a revertir la p\u00e9rdida de aprendizaje a largo plazo entre los m\u00e1s vulnerables de Estados Unidos, haciendo que la educaci\u00f3n sea m\u00e1s equitativa.</p>"},{"location":"homeworks/hw_02/#planteamiento-del-problema","title":"Planteamiento del problema\u00b6","text":"<p>La pandemia COVID-19 ha interrumpido el aprendizaje de m\u00e1s de 56 millones de estudiantes en los Estados Unidos. En la primavera de 2020, la mayor\u00eda de los gobiernos estatales y locales de los EE. UU. Cerraron las instituciones educativas para detener la propagaci\u00f3n del virus. En respuesta, las escuelas y los maestros han intentado llegar a los estudiantes de forma remota a trav\u00e9s de herramientas de aprendizaje a distancia y plataformas digitales. Hasta el d\u00eda de hoy, las preocupaciones sobre la exacerbaci\u00f3n de la brecha digital y la p\u00e9rdida de aprendizaje a largo plazo entre los estudiantes m\u00e1s vulnerables de Estados Unidos contin\u00faan creciendo.</p>"},{"location":"homeworks/hw_02/#desafio","title":"Desaf\u00edo\u00b6","text":"<p>Los estudiantes deben explorar (1) el estado del aprendizaje digital en 2020 y (2) c\u00f3mo la participaci\u00f3n del aprendizaje digital se relaciona con factores como la demograf\u00eda del distrito, el acceso a banda ancha y las pol\u00edticas y eventos a nivel estatal/nacional.</p> <p>Le recomendamos que oriente el an\u00e1lisis con preguntas relacionadas con los temas descritos anteriormente (en negrita). A continuaci\u00f3n se muestran algunos ejemplos de preguntas que se relacionan con el planteamiento de nuestro problema:</p> <ul> <li>\u00bfCu\u00e1l es el panorama de la conectividad y el compromiso digitales en 2020?</li> <li>\u00bfCu\u00e1l es el efecto de la pandemia de COVID-19 en el aprendizaje en l\u00ednea y a distancia, y c\u00f3mo podr\u00eda evolucionar tambi\u00e9n en el futuro?</li> <li>\u00bfC\u00f3mo cambia la participaci\u00f3n de los estudiantes con los diferentes tipos de tecnolog\u00eda educativa durante el transcurso de la pandemia?</li> <li>\u00bfC\u00f3mo se relaciona la participaci\u00f3n de los estudiantes con las plataformas de aprendizaje en l\u00ednea con las diferentes geograf\u00edas? \u00bfContexto demogr\u00e1fico (por ejemplo, raza/etnia, ESL, discapacidad de aprendizaje)? Contexto de aprendizaje? \u00bfEstatus socioecon\u00f3mico?</li> <li>\u00bfSe correlacionan ciertas intervenciones, pr\u00e1cticas o pol\u00edticas estatales (por ejemplo, est\u00edmulo, reapertura, moratoria de desalojo) con el aumento o la disminuci\u00f3n de la participaci\u00f3n en l\u00ednea?</li> </ul>"},{"location":"homeworks/hw_02/#evaluacion","title":"Evaluaci\u00f3n\u00b6","text":""},{"location":"homeworks/hw_02/#claridad","title":"Claridad\u00b6","text":"<ul> <li>\u00bfEl autor present\u00f3 un hilo claro de preguntas o temas que motivaron su an\u00e1lisis?</li> <li>\u00bfEl autor document\u00f3 por qu\u00e9/c\u00f3mo se eligi\u00f3 y utiliz\u00f3 un conjunto de m\u00e9todos para su an\u00e1lisis?</li> <li>\u00bfEst\u00e1 documentado el notebook de una manera que sea f\u00e1cilmente reproducible (p. Ej., C\u00f3digo, fuentes de datos adicionales, citas)?</li> <li>\u00bfEl notebook contiene visualizaciones de datos claras que ayuden a comunicar de manera eficaz los hallazgos del autor tanto a expertos como a no expertos?</li> </ul>"},{"location":"homeworks/hw_02/#precision","title":"Precisi\u00f3n\u00b6","text":"<ul> <li>\u00bfEl autor proces\u00f3 los datos (por ejemplo, fusionando) y/o fuentes de datos adicionales con precisi\u00f3n?</li> <li>\u00bfLa metodolog\u00eda utilizada en el an\u00e1lisis es apropiada y razonable?</li> <li>\u00bfSon razonables y convincentes las interpretaciones basadas en el an\u00e1lisis y la visualizaci\u00f3n?</li> </ul>"},{"location":"homeworks/hw_02/#creatividad","title":"Creatividad\u00b6","text":"<ul> <li>\u00bfEl notebook ayuda al lector a aprender algo nuevo o lo desaf\u00eda a pensar de una manera nueva?</li> <li>\u00bfEl notebook aprovecha m\u00e9todos novedosos y/o visualizaciones que ayudan a revelar informaci\u00f3n a partir de datos y/o comunicar hallazgos?</li> <li>\u00bfEl autor utiliz\u00f3 fuentes de datos p\u00fablicas adicionales en su an\u00e1lisis?</li> </ul>"},{"location":"homeworks/hw_02/#hints","title":"Hints\u00b6","text":"<ul> <li>Esto corresponde a un desafio de Kaggle (link).</li> <li>La informaci\u00f3n respecto a los datos, lo pueden encontrar en el siguiente link.</li> <li>A modo de inspiraci\u00f3n, pueden ocupar algunos gr\u00e1ficos de otros participantes del desaf\u00edo (link).</li> </ul>"},{"location":"homeworks/hw_02/#ii-titanic-machine-learning-from-disaster","title":"II.- Titanic - Machine Learning from Disaster\u00b6","text":""},{"location":"homeworks/hw_02/#pasos-para-participar","title":"Pasos para participar:\u00b6","text":"<ol> <li><p>Unirse a la competencia:</p> <ul> <li>Crea una cuenta o inicia sesi\u00f3n en Kaggle y acepta las reglas para acceder a los datos de la competencia.</li> </ul> </li> <li><p>Descargar y explorar los datos:</p> <ul> <li>Descarga los archivos <code>train.csv</code> y <code>test.csv</code> desde la p\u00e1gina de datos.</li> <li><code>train.csv</code> contiene informaci\u00f3n de 891 pasajeros, incluyendo si sobrevivieron o no (columna <code>Survived</code>). En <code>test.csv</code>, se oculta esta columna para que tu modelo prediga la supervivencia de 418 pasajeros adicionales.</li> </ul> </li> <li><p>Desarrollar el modelo:</p> <ul> <li>Usa <code>train.csv</code> para explorar y descubrir patrones, luego entrena un modelo de machine learning que pueda predecir la supervivencia en <code>test.csv</code>. Un recurso \u00fatil para aprender es el tutorial de Alexis Cook, que explica paso a paso c\u00f3mo hacer tu primera predicci\u00f3n.</li> <li>Puedes explorar notebooks de otros participantes para inspiraci\u00f3n y t\u00e9cnicas avanzadas en la secci\u00f3n de notebooks.</li> </ul> </li> <li><p>Realizar una predicci\u00f3n y enviar tu archivo:</p> <ul> <li>El archivo CSV de predicciones debe tener dos columnas: <code>PassengerId</code> y <code>Survived</code>. Puedes consultar un ejemplo en el archivo <code>gender_submission.csv</code> disponible en la p\u00e1gina de datos.</li> <li>Sube tu archivo en la secci\u00f3n de env\u00edos y revisa tu puntaje de precisi\u00f3n, que mide el porcentaje de pasajeros que tu modelo predijo correctamente.</li> </ul> </li> <li><p>Revisar el leaderboard y mejorar el modelo:</p> <ul> <li>Ve tu posici\u00f3n en el leaderboard y mejora tu modelo bas\u00e1ndote en ideas de los foros o pruebas adicionales.</li> </ul> </li> </ol>"},{"location":"homeworks/hw_02/#ayuda-y-recursos-adicionales","title":"Ayuda y recursos adicionales:\u00b6","text":"<ul> <li>Foro de discusi\u00f3n del Titanic: Un espacio donde puedes hacer preguntas y ver consejos de otros participantes.</li> <li>V\u00eddeo sobre la jerga de Kaggle por Dr. Rachael Tatman, para entender mejor los t\u00e9rminos comunes en Kaggle.</li> <li>Notebooks de la competencia: Revisa notebooks compartidos para ver c\u00f3mo otros abordan el desaf\u00edo.</li> </ul> <p>Este desaf\u00edo es ideal para principiantes en machine learning y permite practicar desde la limpieza de datos hasta el desarrollo y evaluaci\u00f3n de modelos.</p>"},{"location":"labs/lab_01/","title":"MAT281 - Laboratorio N\u00b001","text":"In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME   In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME"},{"location":"labs/lab_01/#mat281-laboratorio-n01","title":"MAT281 - Laboratorio N\u00b001\u00b6","text":""},{"location":"labs/lab_01/#problema-01","title":"Problema 01\u00b6","text":"<p>En los siglos XVII y XVIII, James Gregory y Gottfried Leibniz descubrieron una serie infinita que sirve para calcular $\\pi$:</p> <p>$$\\displaystyle \\pi = 4 \\sum_{k=1}^{\\infty}\\dfrac{(-1)^{k+1}}{2k-1} = 4(1-\\dfrac{1}{3}+\\dfrac{1}{5}-\\dfrac{1}{7} + ...) $$</p> <p>Desarolle un programa para estimar el valor de $\\pi$ ocupando el m\u00e9todo de Leibniz, donde la entrada del programa debe ser un n\u00famero entero $n$ que indique cu\u00e1ntos t\u00e9rminos de la suma se utilizar\u00e1.</p> <ul> <li>Ejemplo:<ul> <li>calcular_pi(3) = 3.466666666666667</li> <li>calcular_pi(1000) = 3.140592653839794</li> </ul> </li> </ul>"},{"location":"labs/lab_01/#problema-02","title":"Problema 02\u00b6","text":"<p>Euler realiz\u00f3 varios aportes en relaci\u00f3n a $e$, pero no fue hasta 1748 cuando public\u00f3 su Introductio in analysin infinitorum que dio un tratamiento definitivo a las ideas sobre $e$. All\u00ed mostr\u00f3 que:</p> <p>En los siglos XVII y XVIII, James Gregory y Gottfried Leibniz descubrieron una serie infinita que sirve para calcular \u03c0:</p> <p>$$\\displaystyle e = \\sum_{k=0}^{\\infty}\\dfrac{1}{k!} = 1+\\dfrac{1}{2!}+\\dfrac{1}{3!}+\\dfrac{1}{4!} + ... $$</p> <p>Desarolle un programa para estimar el valor de $e$ ocupando el m\u00e9todo de Euler, donde la entrada del programa debe ser un n\u00famero entero $n$ que indique cu\u00e1ntos t\u00e9rminos de la suma se utilizar\u00e1.</p> <p>Para esto:</p> <ul> <li><p>a) Defina la funci\u00f3n <code>factorial</code>, donde la entrada sea un n\u00famero natural  $n$ y la salida sea el factorial de dicho n\u00famero.</p> <ul> <li>Ejemplo: factorial(3) =3, factorial(5) = 120</li> </ul> </li> <li><p>b) Ocupe la funci\u00f3n <code>factorial</code> dentro de la funci\u00f3n <code>calcular_e</code>.</p> <ul> <li>Ejemplo: calcular_e(3) = 2.6666666666666665, calcular_e(1000) = 2.7182818284590455</li> </ul> </li> </ul>"},{"location":"labs/lab_01/#problema-03","title":"Problema 03\u00b6","text":"<p>Sea $\\sigma(n)$ definido como la suma de los divisores propios de $n$ (n\u00fameros menores que n que se dividen en $n$).</p> <p>Los n\u00fameros amigos son  enteros positivos $n_1$ y $n_2$ tales que la suma de los divisores propios de uno es igual al otro n\u00famero y viceversa, es decir, $\\sigma(n_1)=\\sigma(n_2)$ y $\\sigma(n_2)=\\sigma(n_1)$.</p> <p>Por ejemplo, los n\u00fameros 220 y 284 son n\u00fameros amigos.</p> <ul> <li>los divisores propios de 220 son 1, 2, 4, 5, 10, 11, 20, 22, 44, 55 y 110; por lo tanto $\\sigma(220) = 284$.</li> <li>los divisores propios de 284 son 1, 2, 4, 71 y 142; entonces $\\sigma(284) = 220$.</li> </ul> <p>Implemente una funci\u00f3n llamada <code>amigos</code> cuyo input sean dos n\u00fameros naturales $n_1$ y $n_2$, cuyo output sea verifique si los n\u00fameros son amigos o no.</p> <p>Para esto:</p> <ul> <li><p>a) Defina la funci\u00f3n <code>divisores_propios</code>, donde la entrada sea un n\u00famero natural $n$ y la salida sea una lista con los divisores propios de dicho n\u00famero.</p> <ul> <li>Ejemplo: divisores_propios(220) = [1, 2, 4, 5, 10, 11, 20, 22, 44, 55 y 110], divisores_propios(284) = [1, 2, 4, 71 y 142]</li> </ul> </li> <li><p>b) Ocupe la funci\u00f3n <code>divisores_propios</code> dentro de la funci\u00f3n <code>amigos</code>.</p> <ul> <li>Ejemplo: amigos(220,284) = True, amigos(6,5) = False</li> </ul> </li> </ul>"},{"location":"labs/lab_01/#problema-04","title":"Problema 04\u00b6","text":"<p>La conjetura de Collatz, conocida tambi\u00e9n como conjetura $3n+1$ o conjetura de Ulam (entre otros nombres), fue enunciada por el matem\u00e1tico Lothar Collatz en 1937, y a la fecha no se ha resuelto.</p> <p>Sea la siguiente operaci\u00f3n, aplicable a cualquier n\u00famero entero positivo:</p> <ul> <li>Si el n\u00famero es par, se divide entre 2.</li> <li>Si el n\u00famero es impar, se multiplica por 3 y se suma 1.</li> </ul> <p>La conjetura dice que siempre alcanzaremos el 1 (y por tanto el ciclo 4, 2, 1) para cualquier n\u00famero con el que comencemos.</p> <p>Implemente una funci\u00f3n llamada <code>collatz</code> cuyo input sea un n\u00famero natural positivo $N$ y como output devulva la secuencia de n\u00fameros hasta llegar a 1.</p> <ul> <li>Ejemplo: collatz(9) = [9, 28, 14, 7, 22, 11, 34, 17, 52, 26, 13, 40, 20, 10, 5, 16, 8, 4, 2, 1]</li> </ul>"},{"location":"labs/lab_01/#problema-05","title":"Problema 05\u00b6","text":"<p>La conjetura de Goldbach es uno de los problemas abiertos m\u00e1s antiguos en matem\u00e1ticas. Concretamente, G.H. Hardy, en 1921, en su famoso discurso pronunciado en la Sociedad Matem\u00e1tica de Copenhague, coment\u00f3 que probablemente la conjetura de Goldbach no es solo uno de los problemas no resueltos m\u00e1s dif\u00edciles de la teor\u00eda de n\u00fameros, sino de todas las matem\u00e1ticas. Su enunciado es el siguiente:</p> <p>Todo n\u00famero par mayor que 2 puede escribirse como suma de dos n\u00fameros primos - Christian Goldbach (1742)</p> <p>Implemente una funci\u00f3n llamada <code>goldbach</code> cuyo input sea un n\u00famero natural positivo $n$ y como output devuelva la suma de dos primos ($n_1$ y $n_2$) tal que: $n_1+n_2=n$.</p> <p>Para esto:</p> <ul> <li><p>a) Defina la funci\u00f3n <code>es_primo</code>, donde la entrada sea un n\u00famero natural $n$ y la salida sea True si el n\u00famero es primo y False en otro caso.</p> <ul> <li>Ejemplo: es_primo(3) = True, es_primo(4) = False</li> </ul> </li> <li><p>b)  Defina la funci\u00f3n <code>lista_de_primos</code>, donde la entrada sea un n\u00famero natural par $n$ mayor que dos y la salida sea una lista con todos los n\u00famero primos entre 2 y $n$.</p> <ul> <li>Ejemplo: lista_de_primos(4) = [2,3], lista_de_primos(6) = [2,3,5], lista_de_primos(8) = [2,3,5,7]</li> </ul> </li> <li><p>c) Ocupe la funci\u00f3n <code>lista_de_primos</code> dentro de la funci\u00f3n <code>goldbash</code>.</p> </li> <li><p>Ejemplo: goldbash(4) = (2,2), goldbash(6) = (3,3) , goldbash(8) = (3,5)</p> </li> </ul>"},{"location":"labs/lab_02/","title":"MAT281 - Laboratorio N\u00b002","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nYearsExperience = np.array([\n 1.1,1.3,1.5,2.0,2.2,\n 2.9,3.0,3.2,3.2,3.7,\n 3.9,4.0,4.0,4.1,4.5,\n 4.9,5.1,5.3,5.9,6.0,\n 6.8,7.1,7.9,8.2,8.7,\n 9.0,9.5,9.6,10.3,10.5\n ])\n\nSalary =  np.array([\n 39343.0,46205.0,37731.0,43525.0,39891.0,\n 56642.0,60150.0,54445.0,64445.0,57189.0,\n 63218.0,55794.0,56957.0,57081.0,61111.0,\n 67938.0,66029.0,83088.0,81363.0,93940.0,\n 91738.0,98273.0,101302.0,113812.0,109431.0,\n 105582.0,116969.0,112635.0,122391.0,121872.0\n])\n</pre> import numpy as np import matplotlib.pyplot as plt  YearsExperience = np.array([  1.1,1.3,1.5,2.0,2.2,  2.9,3.0,3.2,3.2,3.7,  3.9,4.0,4.0,4.1,4.5,  4.9,5.1,5.3,5.9,6.0,  6.8,7.1,7.9,8.2,8.7,  9.0,9.5,9.6,10.3,10.5  ])  Salary =  np.array([  39343.0,46205.0,37731.0,43525.0,39891.0,  56642.0,60150.0,54445.0,64445.0,57189.0,  63218.0,55794.0,56957.0,57081.0,61111.0,  67938.0,66029.0,83088.0,81363.0,93940.0,  91738.0,98273.0,101302.0,113812.0,109431.0,  105582.0,116969.0,112635.0,122391.0,121872.0 ])  <p>Buscamos encontrar la regresi\u00f3n lineal simple:</p> <p>$$Salary_i=\\beta_0+\\beta_1YearsExperience_i+\\epsilon_i$$</p> <p>Para esto debe resolver las siguientes preguntas:</p> <ol> <li>Defina la funci\u00f3n <code>estimate_coef(x,y)</code> para encontrar los coeficientes de regresi\u00f3n lineal $b = (b_0,b_1)$.</li> </ol> In\u00a0[\u00a0]: Copied! <pre>def estimate_coef(x, y):\n    \"\"\"\n    Encontrar los coeficientes del modelo de \n    regresion lineal: beta = (beta_0,beta_1)\n    \"\"\"\n    beta_0 = 0\n    beta_1 = 0\n    \n    return (beta_0, beta_1)\n</pre> def estimate_coef(x, y):     \"\"\"     Encontrar los coeficientes del modelo de      regresion lineal: beta = (beta_0,beta_1)     \"\"\"     beta_0 = 0     beta_1 = 0          return (beta_0, beta_1) In\u00a0[\u00a0]: Copied! <pre># imprimir valores del beta estimado\nbeta_estimado = estimate_coef(YearsExperience, Salary)\nprint(f\"Coeficientes estimados:\\nb_0 = {beta_estimado[0]} \\nb_1 = {beta_estimado[1]}\")\n</pre> # imprimir valores del beta estimado beta_estimado = estimate_coef(YearsExperience, Salary) print(f\"Coeficientes estimados:\\nb_0 = {beta_estimado[0]} \\nb_1 = {beta_estimado[1]}\") <ol> <li>Grafique su soluci\u00f3n ocupando la funci\u00f3n <code>plot_regression_line(x,y,yhat)</code>.</li> </ol> In\u00a0[\u00a0]: Copied! <pre>def plot_regression_line(x, y, yhat):\n    plt.figure(figsize=(10,4))\n    # plotting the actual points as scatter plot\n    plt.scatter(x, y, color = \"m\",marker = \"o\", s = 30)\n\n\n    # plotting the regression line\n    plt.plot(x, yhat, color = \"g\")\n\n    # putting labels\n    plt.xlabel('YearsExperience')\n    plt.ylabel('Salary')\n    plt.title(\"Plot YearsExperience vs Salary\")\n    \n    # function to show plot\n    plt.show()\n</pre> def plot_regression_line(x, y, yhat):     plt.figure(figsize=(10,4))     # plotting the actual points as scatter plot     plt.scatter(x, y, color = \"m\",marker = \"o\", s = 30)       # plotting the regression line     plt.plot(x, yhat, color = \"g\")      # putting labels     plt.xlabel('YearsExperience')     plt.ylabel('Salary')     plt.title(\"Plot YearsExperience vs Salary\")          # function to show plot     plt.show() In\u00a0[\u00a0]: Copied! <pre># mostrar resultados del ajuste lineal\nprediccion = beta_estimado[0] + beta_estimado[1]*YearsExperience\nplot_regression_line(YearsExperience, Salary, prediccion)\n</pre> # mostrar resultados del ajuste lineal prediccion = beta_estimado[0] + beta_estimado[1]*YearsExperience plot_regression_line(YearsExperience, Salary, prediccion) <ol> <li>Calcule el estad\u00edstico r-cuadrado ($r^2$) y las siguientes m\u00e9tricas de error:<ul> <li>mae</li> <li>rmse</li> <li>mape</li> <li>smape</li> </ul> </li> </ol> In\u00a0[\u00a0]: Copied! <pre>def mae(y, yhat) -&gt; float:\n    \"\"\"\n    Calculo de la metrica: mean absolute error (MAE)\n    \"\"\"\n    # agregar codigo \n    metrica = 0\n    return metrica\n</pre> def mae(y, yhat) -&gt; float:     \"\"\"     Calculo de la metrica: mean absolute error (MAE)     \"\"\"     # agregar codigo      metrica = 0     return metrica In\u00a0[\u00a0]: Copied! <pre>def rmse(y, yhat) -&gt; float:\n    \"\"\"\n    Calculo de la metrica: root mean squared error (RMSE)\n    \"\"\"\n    # agregar codigo \n    metrica = 0\n    return metrica\n</pre> def rmse(y, yhat) -&gt; float:     \"\"\"     Calculo de la metrica: root mean squared error (RMSE)     \"\"\"     # agregar codigo      metrica = 0     return metrica In\u00a0[\u00a0]: Copied! <pre>def mape(y, yhat) -&gt; float:\n    \"\"\"\n    Calculo de la metrica: mean absolute percentage error (MAPE)\n    \"\"\"\n    # agregar codigo \n    metrica = 0\n    return metrica\n</pre> def mape(y, yhat) -&gt; float:     \"\"\"     Calculo de la metrica: mean absolute percentage error (MAPE)     \"\"\"     # agregar codigo      metrica = 0     return metrica In\u00a0[\u00a0]: Copied! <pre>def smape(y, yhat) -&gt; float:\n    \"\"\"\n    Calculo de la metrica: symmetric mean absolute percentage error (SMAPE)\n    \"\"\"\n    # agregar codigo \n    metrica = 0\n    return metrica\n</pre> def smape(y, yhat) -&gt; float:     \"\"\"     Calculo de la metrica: symmetric mean absolute percentage error (SMAPE)     \"\"\"     # agregar codigo      metrica = 0     return metrica In\u00a0[\u00a0]: Copied! <pre>def rsquared(y, yhat) -&gt; float:\n    \"\"\"\n    Calculo del r-cuadrado\n    \"\"\"\n    # agregar codigo \n    metrica = 0\n    return metrica\n</pre> def rsquared(y, yhat) -&gt; float:     \"\"\"     Calculo del r-cuadrado     \"\"\"     # agregar codigo      metrica = 0     return metrica In\u00a0[\u00a0]: Copied! <pre># calcular resultados\ncalcular_mae = round(mae(Salary,prediccion), 4)\ncalcular_rmse = round(rmse(Salary,prediccion), 4)\ncalcular_mape = round(mape(Salary,prediccion), 4)\ncalcular_smape = round(smape(Salary,prediccion), 4)\ncalcular_rsquared = round(rsquared(Salary,prediccion), 4)\n</pre> # calcular resultados calcular_mae = round(mae(Salary,prediccion), 4) calcular_rmse = round(rmse(Salary,prediccion), 4) calcular_mape = round(mape(Salary,prediccion), 4) calcular_smape = round(smape(Salary,prediccion), 4) calcular_rsquared = round(rsquared(Salary,prediccion), 4) In\u00a0[\u00a0]: Copied! <pre># imprimir resultados\nprint(f\"mae:   {calcular_mae}\")\nprint(f\"rmse:  {calcular_rmse}\")\nprint(f\"mape:  {calcular_mape}\")\nprint(f\"smape: {calcular_smape}\")\nprint(f\"r^2:   {calcular_rsquared}\")\n</pre> # imprimir resultados print(f\"mae:   {calcular_mae}\") print(f\"rmse:  {calcular_rmse}\") print(f\"mape:  {calcular_mape}\") print(f\"smape: {calcular_smape}\") print(f\"r^2:   {calcular_rsquared}\") <ol> <li>Conclusiones del caso de estudio (evaluar si la regresi\u00f3n lineal se ajusta correctamente a los datos o no).</li> </ol> <p>Respuesta:</p>"},{"location":"labs/lab_02/#mat281-laboratorio-n02","title":"MAT281 - Laboratorio N\u00b002\u00b6","text":""},{"location":"labs/lab_02/#problema-01","title":"Problema 01\u00b6","text":"<p>El objetivo de este laboratorio es aplicar un modelo de regresi\u00f3n lineal simple.</p>"},{"location":"labs/lab_02/#definicion","title":"Definici\u00f3n\u00b6","text":"<p>El modelo de regresio\u0301n lineal general o modelo de regresi\u00f3n multiple,  supone que, $\\boldsymbol{Y} =  \\boldsymbol{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon},$ donde:</p> <ul> <li>$\\boldsymbol{X} = (x_1,...,x_n)^{T}$: variable explicativa</li> <li>$\\boldsymbol{Y} = (y_1,...,y_n)^{T}$: variable respuesta</li> <li>$\\boldsymbol{\\epsilon} = (\\epsilon_1,...,\\epsilon_n)^{T}$: error se asume un ruido blanco, es decir, $\\epsilon \\sim \\mathcal{N}( \\boldsymbol{0},\\sigma^2I)$</li> <li>$\\boldsymbol{\\beta} = (\\beta_1,...,\\beta_n)^{T}$: coeficientes de regresio\u0301n.</li> </ul> <p>La idea es tratar de establecer la relaci\u00f3n entre las variables independientes y dependientes por medio de ajustar el mejor hyper plano con respecto a los puntos.</p> <p>Por ejemplo, para el caso de la regresi\u00f3n lineal simple, se tiene la siguiente estructura: $y_i=\\beta_0+\\beta_1x_i+\\epsilon_i.$ En este caso, la regresi\u00f3n lineal corresponder\u00e1 a la recta que mejor pasa por los puntos observados.</p> <p></p> <p>Existen algunas situaciones donde los modelos lineales no son apropiados:</p> <ul> <li>El rango de valores de $Y$ est\u00e1 restringido (ejemplo: datos binarios o de conteos).</li> <li>La varianza de $Y$ depende de la media.</li> </ul>"},{"location":"labs/lab_02/#mejores-paremetros-metodo-de-minimos-cudrados","title":"Mejores par\u00e9metros: M\u00e9todo de minimos cudrados\u00b6","text":"<p>El m\u00e9todo de m\u00ednimos cudrados es un m\u00e9todo de optimizaci\u00f3n que busca encontrar la mejor aproximaci\u00f3n mediante la minimizaci\u00f3n de los residuos al cuadrado, es decir, se buscar encontrar:</p> <p>$$(P)\\ \\min \\sum_{i=1}^n e_{i}^2 =\\sum_{i=1}^n (y_{i}-f_{i}(x;\\beta))^2   $$</p> <p>Para el caso de la regresi\u00f3n lineal simple, se busca una funci\u00f3n $$f(x;\\beta) = \\beta_{0} + \\beta_{1}x,$$</p> <p>por lo tanto el problema que se debe resolver es el siguiente:</p> <p>$$(P)\\ \\min \\sum_{i=1}^n e_{i}^2 =\\dfrac{1}{n}\\sum_{i=1}^{n}\\left ( y_{i}-(\\beta_{0} + \\beta_{1}x_{i})\\right )^2$$</p> <p>Lo que significa, que para este problema, se debe encontrar $\\beta = (\\beta_{0},\\beta_{1})$ que minimicen el problema de optimizaci\u00f3n. En este caso la soluci\u00f3n viene dada por:</p> <p>$$\\hat{\\beta}_{1} = \\dfrac{\\sum(x-\\bar{x})(y-\\bar{y})}{\\sum(x-\\bar{x})^2} = \\rho (x,y)\\ ; \\  \\hat{\\beta}_{0} = \\bar{y}-\\hat{\\beta}_{1} \\bar{x} $$</p>"},{"location":"labs/lab_02/#seleccion-de-modelos","title":"Selecci\u00f3n de modelos\u00b6","text":"<p>R-cuadrado</p> <p>El coeficiente de determinaci\u00f3n o R-cuadrado ($r^2$ ) , es un estad\u00edstico usado en el contexto de un modelo estad\u00edstico cuyo principal prop\u00f3sito es predecir futuros resultados o probar una hip\u00f3tesis. El coeficiente determina la calidad del modelo para replicar los resultados, y la proporci\u00f3n de variaci\u00f3n de los resultados que puede explicarse por el modelo.</p> <p>El valor del $r^2$ habitualmente entre 0 y 1, donde 0 significa una mala calidad de ajuste en el modelo y 1 corresponde a un ajuste lineal perfecto. A menudo, este estad\u00edstico es ocupado para modelos lineales.</p> <p>Se define por la f\u00f3rmula:</p> <p>$$r^2 = \\dfrac{SS_{reg}}{SS_{tot}} = 1 - \\dfrac{SS_{res}}{SS_{tot}},$$</p> <p>donde:</p> <ul> <li><p>$SS_{reg}$ ( suma explicada de cuadrados (ESS)): $\\sum_{i}(\\hat{y}-\\bar{y})^2$</p> </li> <li><p>$SS_{res}$: ( suma residual de cuadrados (RSS)): $\\sum_{i}(y_{i}-\\hat{y})^2 = \\sum_{i}e_{i}^2$</p> </li> <li><p>$SS_{tot}$: ( varianza): $\\sum_{i}(y_{i}-\\bar{y})$, donde: $SS_{tot}=SS_{reg}+SS_{res}$</p> </li> </ul> <p>En una forma general, se puede ver que $r^2$ est\u00e1 relacionado con la fracci\u00f3n de varianza inexplicada (FVU), ya que el segundo t\u00e9rmino compara la varianza inexplicada (varianza de los errores del modelo) con la varianza total (de los datos).</p> <p></p> <ul> <li><p>Las \u00e1reas de los cuadrados azules representan los residuos cuadrados con respecto a la regresi\u00f3n lineal ($SS_{tot}$).</p> </li> <li><p>Las \u00e1reas de los cuadrados rojos representan los residuos al cuadrado con respecto al valor promedio ($SS_{res}$).</p> </li> </ul>"},{"location":"labs/lab_02/#error-de-un-modelo","title":"Error de un modelo\u00b6","text":""},{"location":"labs/lab_02/#definicion","title":"Definici\u00f3n\u00b6","text":"<p>El error corresponde a la diferencia entre el valor original y el valor predicho,es decir:</p> <p>$$e_{i}=y_{i}-\\hat{y}_{i} $$</p> <p></p>"},{"location":"labs/lab_02/#formas-de-medir-el-error-de-un-modelo","title":"Formas de medir el error de un modelo\u00b6","text":"<p>Para medir el ajuste de un modelo se ocupan las denominadas funciones de distancias o m\u00e9tricas. Existen varias m\u00e9tricas, dentro de las cuales encontramos:</p> <ol> <li><p>M\u00e9tricas absolutas: Las m\u00e9tricas absolutas o no escalada miden el error sin escalar los valores. Las m\u00e9trica absolutas m\u00e1s ocupadas son:</p> <ul> <li>Mean Absolute Error (MAE)</li> </ul> <p>$$\\textrm{MAE}(y,\\hat{y}) = \\dfrac{1}{n}\\sum_{t=1}^{n}\\left | y_{t}-\\hat{y}_{t}\\right |$$</p> <ul> <li>Mean squared error (MSE):</li> </ul> <p>$$\\textrm{MSE}(y,\\hat{y}) =\\dfrac{1}{n}\\sum_{t=1}^{n}\\left ( y_{t}-\\hat{y}_{t}\\right )^2$$</p> </li> <li><p>M\u00e9tricas Porcentuales: Las m\u00e9tricas porcentuales o escaladas miden el error de manera escalada, es decir, se busca acotar el error entre valores de 0 a 1, donde 0 significa que el ajuste es perfecto, mientras que 1 ser\u00eda un mal ajuste. Cabe destacar que muchas veces las m\u00e9tricas porcentuales puden tener valores mayores a 1.Las m\u00e9trica Porcentuales m\u00e1s ocupadas son:</p> <ul> <li>Mean absolute percentage error (MAPE):</li> </ul> <p>$$\\textrm{MAPE}(y,\\hat{y}) = \\dfrac{1}{n}\\sum_{t=1}^{n}\\left | \\frac{y_{t}-\\hat{y}_{t}}{y_{t}} \\right |$$</p> <ul> <li>Symmetric mean absolute percentage error (sMAPE):</li> </ul> <p>$$\\textrm{sMAPE}(y,\\hat{y}) = \\dfrac{1}{n}\\sum_{t=1}^{n} \\frac{\\left |y_{t}-\\hat{y}_{t}\\right |}{(\\left | y_{t} \\right |^2+\\left | \\hat{y}_{t} \\right |^2)/2}$$</p> </li> </ol>"},{"location":"labs/lab_02/#problema-a-resolver","title":"Problema a resolver\u00b6","text":"<p>En este art\u00edculo, utilizaremos un conjunto de datos de salarios. Nuestro conjunto de datos tendr\u00e1 2 columnas:</p> <ul> <li>a\u00f1os de experiencia (YearsExperience) - variable explicativa</li> <li>salario (Salary) - variable de respuesta</li> </ul> <p>A coninuaci\u00f3n, mostramos expl\u00edcitamente el conjunto de datos.</p>"},{"location":"labs/lab_031/","title":"MAT281 - Laboratorio N\u00b0031","text":"In\u00a0[3]: Copied! <pre>import pandas as pd\n\n# load data\nurl='https://raw.githubusercontent.com/fralfaro/MAT281_2024/main/docs/labs/data/ocupation.csv'\n\ndf = pd.read_csv(url, sep=\"|\" )\ndf.head()\n</pre> import pandas as pd  # load data url='https://raw.githubusercontent.com/fralfaro/MAT281_2024/main/docs/labs/data/ocupation.csv'  df = pd.read_csv(url, sep=\"|\" ) df.head() Out[3]: user_id age gender occupation zip_code 0 1 24 M technician 85711 1 2 53 F other 94043 2 3 23 M writer 32067 3 4 24 M technician 43537 4 5 33 F other 15213 <p>El objetivo es tratar de obtener la mayor informaci\u00f3n posible de este conjunto de datos. Para cumplir este objetivo debe resolver las siguientes problem\u00e1ticas:</p> In\u00a0[\u00a0]: Copied! <pre>#FIXME\n</pre> #FIXME In\u00a0[4]: Copied! <pre>#FIXME\n</pre> #FIXME In\u00a0[5]: Copied! <pre>#FIXME\n</pre> #FIXME In\u00a0[6]: Copied! <pre>#FIXME\n</pre> #FIXME"},{"location":"labs/lab_031/#mat281-laboratorio-n031","title":"MAT281 - Laboratorio N\u00b0031\u00b6","text":""},{"location":"labs/lab_031/#problema-01","title":"Problema 01\u00b6","text":"<p>El conjunto de datos <code>occupation.csv</code> consiste en informaci\u00f3n detallada sobre distintos usuarios, abarcando aspectos como edad, g\u00e9nero, profesi\u00f3n y ubicaci\u00f3n geogr\u00e1fica. Este conjunto de datos proporciona una visi\u00f3n rica sobre la diversidad de ocupaciones y demograf\u00eda de una poblaci\u00f3n espec\u00edfica.</p> <p>Descripci\u00f3n de las Columnas</p> <p>El DataFrame contiene las siguientes columnas, cada una representando un aspecto clave de la informaci\u00f3n del usuario:</p> <ul> <li><code>user_id</code>: Un identificador \u00fanico para cada usuario.</li> <li><code>age</code>: La edad del usuario.</li> <li><code>gender</code>: El g\u00e9nero del usuario, indicado como 'M' (masculino) o 'F' (femenino).</li> <li><code>occupation</code>: La profesi\u00f3n u ocupaci\u00f3n del usuario.</li> <li><code>zip_code</code>: El c\u00f3digo postal del \u00e1rea de residencia del usuario, que puede ser \u00fatil para an\u00e1lisis geogr\u00e1ficos.</li> </ul> <p>Este conjunto de datos permite realizar una variedad de an\u00e1lisis estad\u00edsticos y descriptivos, desde demogr\u00e1ficos b\u00e1sicos hasta exploraciones m\u00e1s complejas de las relaciones entre ocupaci\u00f3n, edad y ubicaci\u00f3n.</p>"},{"location":"labs/lab_031/#descripcion-general","title":"Descripci\u00f3n General\u00b6","text":"<ul> <li>Total de Observaciones: \u00bfCu\u00e1ntas filas tiene el conjunto de datos?</li> <li>Total de Columnas: \u00bfCu\u00e1ntas columnas tiene el conjunto de datos?</li> <li>Nombres de Columnas: Listar todos los nombres de las columnas.</li> <li>\u00cdndice del DataFrame: Describir c\u00f3mo est\u00e1 indexado el DataFrame.</li> <li>Tipos de Datos: Detallar los tipos de datos de cada columna.</li> </ul>"},{"location":"labs/lab_031/#estadisticas-descriptivas","title":"Estad\u00edsticas Descriptivas\u00b6","text":"<ul> <li>Resumen Estad\u00edstico: Utilizar .describe() para obtener un resumen estad\u00edstico de las columnas pertinentes.</li> <li>Columna de Ocupaci\u00f3n: Visualizar los datos de la columna occupation.</li> </ul>"},{"location":"labs/lab_031/#analisis-de-ocupaciones","title":"An\u00e1lisis de Ocupaciones\u00b6","text":"<ul> <li>Cantidad de Ocupaciones \u00danicas: \u00bfCu\u00e1ntas ocupaciones diferentes existen en el conjunto de datos?</li> <li>Ocupaci\u00f3n M\u00e1s Frecuente: \u00bfCu\u00e1l es la ocupaci\u00f3n que aparece con mayor frecuencia?</li> <li>Distribuci\u00f3n de Ocupaciones: Mostrar la distribuci\u00f3n de las 10 ocupaciones m\u00e1s comunes con un gr\u00e1fico de barras.</li> </ul>"},{"location":"labs/lab_031/#analisis-demografico","title":"An\u00e1lisis Demogr\u00e1fico\u00b6","text":"<ul> <li>Edad Media de los Usuarios: Calcular la edad promedio.</li> <li>Edad con Menos Ocurrencia: Identificar las edades menos comunes.</li> <li>Distribuci\u00f3n de la Edad de los Usuarios: Identificar la distribuci\u00f3n de edades.</li> <li>Proporci\u00f3n de G\u00e9neros: Calcular la proporci\u00f3n de usuarios por g\u00e9nero.</li> </ul>"},{"location":"labs/lab_032/","title":"MAT281 - Laboratorio N\u00b0032","text":"<p>Esta semana revisaremos datos del \u00cdndice de Libertad de Prensa que confecciona cada a\u00f1o la asociaci\u00f3n de Reporteros Sin Fronteras.</p> In\u00a0[19]: Copied! <pre>import numpy as np \nimport pandas as pd\n\n# lectura de datos\narchivos_anio = [\n    'https://raw.githubusercontent.com/fralfaro/MAT281_2024/main/docs/labs/data/libertad_prensa_01.csv',\n    'https://raw.githubusercontent.com/fralfaro/MAT281_2024/main/docs/labs/data/libertad_prensa_02.csv'\n ] \ndf_codigos = pd.read_csv('https://raw.githubusercontent.com/fralfaro/MAT281_2024/main/docs/labs/data/libertad_prensa_codigo.csv')\n</pre> import numpy as np  import pandas as pd  # lectura de datos archivos_anio = [     'https://raw.githubusercontent.com/fralfaro/MAT281_2024/main/docs/labs/data/libertad_prensa_01.csv',     'https://raw.githubusercontent.com/fralfaro/MAT281_2024/main/docs/labs/data/libertad_prensa_02.csv'  ]  df_codigos = pd.read_csv('https://raw.githubusercontent.com/fralfaro/MAT281_2024/main/docs/labs/data/libertad_prensa_codigo.csv') <p>El objetivo es tratar de obtener la mayor informaci\u00f3n posible de este conjunto de datos. Para cumplir este objetivo debe resolver las siguientes problem\u00e1ticas:</p> <ol> <li>Lo primero ser\u00e1 juntar toda la informaci\u00f3n en un solo archivo, para ello necesitamos seguir los siguientes pasos:</li> </ol> <ul> <li>a) Crear el archivo df_anio, que contenga la informaci\u00f3n de libertad_prensa_anio.csv para cada a\u00f1o. Luego, normalice el nombre de las columnas a min\u00fascula.</li> <li>b) Encuentre y elimine el dato que esta duplicado en el archivo df_codigo.</li> <li>c) Crear el archivo df que junte la informaci\u00f3n del archivo df_anio con df_codigo por la columna codigo_iso.</li> </ul> <p>Hint: Para juntar por anio ocupe la funci\u00f3n pd.concat. Para juntar informaci\u00f3n por columna ocupe pd.merge.</p> In\u00a0[25]: Copied! <pre># FIXME\n\ndf = pd.DataFrame()\n</pre> # FIXME  df = pd.DataFrame() <ol> <li>Encontrar:<ul> <li>\u00bfCu\u00e1l es el n\u00famero de observaciones en el conjunto de datos?</li> <li>\u00bfCu\u00e1l es el n\u00famero de columnas en el conjunto de datos?</li> <li>Imprime el nombre de todas las columnas</li> <li>\u00bfCu\u00e1l es el tipo de datos de cada columna?</li> <li>Describir el conjunto de datos (hint: .describe())</li> </ul> </li> </ol> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME <ol> <li>Desarrolle una funci\u00f3n <code>resumen_df(df)</code> para encontrar el total de elementos distintos y vac\u00edos por columnas.</li> </ol> In\u00a0[28]: Copied! <pre># respuesta\ndef resumen_df(df):\n    \"\"\"\n    Funci\u00f3n para generar un resumen de un DataFrame que incluye\n    el n\u00famero de elementos distintos y vac\u00edos por columna.\n\n    Args:\n    df (pd.DataFrame): DataFrame a resumir.\n\n    Returns:\n    pd.DataFrame: DataFrame resumen con el nombre de las columnas,\n                  cantidad de elementos distintos y cantidad de elementos vac\u00edos.\n    \"\"\"\n    # Crear un DataFrame de resultado con los nombres de las columnas\n    nombres = df.columns\n    result = pd.DataFrame({'nombres': nombres})\n    \n    # Calcular el n\u00famero de elementos distintos por columna\n    result['elementos_distintos'] = 0\n    \n    # Calcular el n\u00famero de elementos vac\u00edos (NaN) por columna\n    result['elementos_vacios'] = 0\n    \n    return result\n</pre> # respuesta def resumen_df(df):     \"\"\"     Funci\u00f3n para generar un resumen de un DataFrame que incluye     el n\u00famero de elementos distintos y vac\u00edos por columna.      Args:     df (pd.DataFrame): DataFrame a resumir.      Returns:     pd.DataFrame: DataFrame resumen con el nombre de las columnas,                   cantidad de elementos distintos y cantidad de elementos vac\u00edos.     \"\"\"     # Crear un DataFrame de resultado con los nombres de las columnas     nombres = df.columns     result = pd.DataFrame({'nombres': nombres})          # Calcular el n\u00famero de elementos distintos por columna     result['elementos_distintos'] = 0          # Calcular el n\u00famero de elementos vac\u00edos (NaN) por columna     result['elementos_vacios'] = 0          return result In\u00a0[29]: Copied! <pre># retornar \nresumen_df(df)\n</pre> # retornar  resumen_df(df) Out[29]: nombres elementos_distintos elementos_vacios <ol> <li>Para los paises latinoamericano, encuentre por a\u00f1o  el pa\u00eds con mayor y menor <code>indice</code>.</li> </ol> <ul> <li>a) Mediante un ciclo for.</li> <li>b) Mediante un  groupby.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># respuesta\namerica = ['ARG', 'ATG', 'BLZ', 'BOL', 'BRA', 'CAN', 'CHL', 'COL', 'CRI',\n       'CUB', 'DOM', 'ECU', 'GRD', 'GTM', 'GUY', 'HND', 'HTI', 'JAM',\n       'MEX', 'NIC', 'PAN', 'PER', 'PRY', 'SLV', 'SUR', 'TTO', 'URY',\n       'USA', 'VEN']\n\ndf_america =  pd.DataFrame() # FIX ME\n</pre> # respuesta america = ['ARG', 'ATG', 'BLZ', 'BOL', 'BRA', 'CAN', 'CHL', 'COL', 'CRI',        'CUB', 'DOM', 'ECU', 'GRD', 'GTM', 'GUY', 'HND', 'HTI', 'JAM',        'MEX', 'NIC', 'PAN', 'PER', 'PRY', 'SLV', 'SUR', 'TTO', 'URY',        'USA', 'VEN']  df_america =  pd.DataFrame() # FIX ME <ol> <li>Para cada pa\u00eds, muestre el indice m\u00e1ximo que alcanzo por anio. Para los datos nulos, rellene con el valor 0.</li> </ol> <p>Hint: Utilice la funci\u00f3n pd.pivot_table.</p> In\u00a0[\u00a0]: Copied! <pre># FIX ME\n</pre> # FIX ME"},{"location":"labs/lab_032/#mat281-laboratorio-n032","title":"MAT281 - Laboratorio N\u00b0032\u00b6","text":""},{"location":"labs/lab_032/#diccionario-de-datos","title":"Diccionario de datos\u00b6","text":"Variable Clase Descripci\u00f3n codigo_iso caracter C\u00f3digo ISO del pa\u00eds pais caracter Pa\u00eds anio entero A\u00f1o del resultado indice entero Puntaje \u00cdndice Libertad de Prensa (menor puntaje = mayor libertad de prensa) ranking entero Ranking Libertad de Prensa"},{"location":"labs/lab_032/#fuente-original-y-adaptacion","title":"Fuente original y adaptaci\u00f3n\u00b6","text":"<p>Los datos fueron extra\u00eddos de The World Bank. La fuente original es Reporteros sin Fronteras.</p> <p>Por otro lado, estos archivos han sido modificado intencionalmente para ocupar todo lo aprendido en clases. A continuaci\u00f3n, una breve descripci\u00f3n de cada uno de los data frames:</p> <ul> <li>libertad_prensa_codigo.csv: contiene la informaci\u00f3n codigo_iso/pais. Existe un c\u00f3digo que tiene dos valores.</li> <li>libertad_prensa_01.csv: contiene la informaci\u00f3n pais/anio/indice/ranking. Los nombres de las columnas estan en may\u00fascula (antes del a\u00f1o 2010).</li> <li>libertad_prensa_02.csv: contiene la informaci\u00f3n pais/anio/indice/ranking. Los nombres de las columnas estan en may\u00fascula (despu\u00e9s del a\u00f1o 2010).</li> </ul>"},{"location":"labs/lab_04/","title":"MAT281 - Laboratorio N\u00b004","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set(rc={'figure.figsize':(10,8)})\n</pre> import pandas as pd import seaborn as sns import matplotlib.pyplot as plt  sns.set(rc={'figure.figsize':(10,8)}) In\u00a0[2]: Copied! <pre># cargar datos\ndf = pd.read_csv(\"https://raw.githubusercontent.com/fralfaro/MAT281_2024/main/docs/labs/data/company_sales_data.csv\")\ndf.head()\n</pre> # cargar datos df = pd.read_csv(\"https://raw.githubusercontent.com/fralfaro/MAT281_2024/main/docs/labs/data/company_sales_data.csv\") df.head() Out[2]: month_number facecream facewash toothpaste bathingsoap shampoo moisturizer total_units total_profit 0 1 2500 1500 5200 9200 1200 1500 21100 211000 1 2 2630 1200 5100 6100 2100 1200 18330 183300 2 3 2140 1340 4550 9550 3550 1340 22470 224700 3 4 3400 1130 5870 8870 1870 1130 22270 222700 4 5 3600 1740 4560 7760 1560 1740 20960 209600 <p>El objetivo principal es extraer la mayor cantidad de informaci\u00f3n posible de este conjunto de datos. Para lograrlo, se deben abordar las siguientes tareas. Para cada pregunta, deber\u00e1 obtener una conclusi\u00f3n basada en los gr\u00e1ficos generados. Al finalizar todas las preguntas, se espera una conclusi\u00f3n general del an\u00e1lisis.</p> <p>Nota: Se permite el uso de las librer\u00edas Matplotlib o Seaborn para la visualizaci\u00f3n y an\u00e1lisis de los datos.</p> <ol> <li>Lea la columna <code>total_profit</code> correspondiente a todos los meses y represente los datos utilizando un gr\u00e1fico lineal y un gr\u00e1fico de dispersi\u00f3n.</li> </ol> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME <ol> <li>Lea los datos de ventas de todos los productos y repres\u00e9ntelos en un gr\u00e1fico de l\u00edneas m\u00faltiples.</li> </ol> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME <ol> <li>Lea los datos de ventas de <code>facecream</code> y <code>facewash</code> y repres\u00e9ntelos utilizando un gr\u00e1fico de barras.</li> </ol> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME <ol> <li>Lea los datos de ventas de todos los productos y repres\u00e9ntelos utilizando un gr\u00e1fico de caja (box-plot).</li> </ol> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME <ol> <li>Calcule las ventas totales del a\u00f1o pasado para cada producto y repres\u00e9ntelas utilizando un gr\u00e1fico circular.</li> </ol> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME"},{"location":"labs/lab_04/#mat281-laboratorio-n04","title":"MAT281 - Laboratorio N\u00b004\u00b6","text":""},{"location":"labs/lab_04/#problema-01","title":"Problema 01\u00b6","text":"<p>El conjunto de datos, denominado <code>company_sales_data.csv</code>, recoge las ventas mensuales de distintos productos de una empresa, ofreciendo una visi\u00f3n general de las unidades vendidas y los beneficios generados. Entre los productos incluidos se encuentran cremas faciales, jabones de ba\u00f1o y art\u00edculos para el cuidado capilar. Este conjunto de datos es \u00fatil para analizar el rendimiento de ventas y el comportamiento del mercado a lo largo del tiempo.</p>"},{"location":"labs/lab_04/#descripcion-de-los-datos","title":"Descripci\u00f3n de los Datos:\u00b6","text":"<ol> <li>month_number: N\u00famero correspondiente al mes del a\u00f1o (1 para enero, 2 para febrero, etc.).</li> <li>facecream: Cantidad de unidades de crema facial vendidas durante el mes.</li> <li>facewash: Cantidad de unidades de limpiador facial vendidas durante el mes.</li> <li>toothpaste: Cantidad de unidades de pasta de dientes vendidas durante el mes.</li> <li>bathingsoap: Cantidad de unidades de jab\u00f3n de ba\u00f1o vendidas durante el mes.</li> <li>shampoo: Cantidad de unidades de champ\u00fa vendidas durante el mes.</li> <li>moisturizer: Cantidad de unidades de crema hidratante vendidas durante el mes.</li> <li>total_units: Total de unidades vendidas de todos los productos en un mes dado.</li> <li>total_profit: Ganancia total obtenida por la venta de todos los productos durante el mes, expresada en la moneda correspondiente.</li> </ol>"},{"location":"labs/lab_05/","title":"MAT281 - Laboratorio N\u00b005","text":"<p>El Iris dataset es un conjunto de datos que contiene muestras de tres especies de Iris (Iris setosa, Iris virginica e Iris versicolor). Se midieron cuatro caracter\u00edsticas en cada muestra: el largo y ancho del s\u00e9palo y del p\u00e9talo, ambos en cent\u00edmetros. Este conjunto de datos es com\u00fanmente utilizado para an\u00e1lisis de clasificaci\u00f3n y para demostrar algoritmos de machine learning en la ciencia de datos.</p> In\u00a0[\u00a0]: Copied! <pre># librerias\n \nimport os\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \npd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes\n\n\n# Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab\n%matplotlib inline\n</pre> # librerias   import os import numpy as np import pandas as pd  import matplotlib.pyplot as plt import seaborn as sns  pd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes   # Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab %matplotlib inline In\u00a0[\u00a0]: Copied! <pre># cargar datos\ndata= pd.read_csv(\"https://raw.githubusercontent.com/fralfaro/MAT281_2024/main/docs/labs/data/iris_contaminados.csv\")\n\ndata.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n\ndata.head()\n</pre> # cargar datos data= pd.read_csv(\"https://raw.githubusercontent.com/fralfaro/MAT281_2024/main/docs/labs/data/iris_contaminados.csv\")  data.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']  data.head()  <p>Su objetivo es realizar un An\u00e1lisis Exploratorio de Datos (EDA) completo y riguroso. Para lograrlo, siga las instrucciones proporcionadas a continuaci\u00f3n, asegur\u00e1ndose de extraer conclusiones intermedias despu\u00e9s de cada pregunta, y de elaborar una conclusi\u00f3n general al finalizar todas las preguntas.</p> <p>Esto le permitir\u00e1 identificar patrones, anomal\u00edas y relaciones clave entre las variables antes de proceder con un an\u00e1lisis m\u00e1s profundo.</p> <ol> <li>Realice un conteo de los elementos en la columna species y ajuste cualquier valor que considere incorrecto. Reemplace los valores <code>NaN</code> con \"default\".</li> </ol> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME <ol> <li>Genere un gr\u00e1fico de box-plot para visualizar la distribuci\u00f3n del largo y ancho de los p\u00e9talos y s\u00e9palos. Antes de crear el gr\u00e1fico, reemplace los valores <code>NaN</code> por 0.</li> </ol> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME <ol> <li>Utilizando los rangos de valores v\u00e1lidos previamente definidos para el largo y ancho de los p\u00e9talos y s\u00e9palos, agregue una nueva columna denominada label. Esta columna debe identificar si alg\u00fan valor se encuentra fuera del rango permitido. Marque los valores fuera de rango con \"fuera de rango\" y los valores dentro del rango con \"dentro del rango\".</li> </ol> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME <ol> <li><p>Genere dos gr\u00e1ficos de dispersi\u00f3n:</p> <ul> <li>El primero, mostrando la relaci\u00f3n entre sepal_length y petal_length.</li> <li>El segundo, mostrando la relaci\u00f3n entre sepal_width y petal_width.</li> </ul> </li> </ol> <p>En ambos gr\u00e1ficos, los puntos deben estar categorizados por la columna label (es decir, \"dentro del rango\" o \"fuera del rango\").</p> <p>Concluya los resultados evaluando la relaci\u00f3n entre las variables y si los valores fuera de rango afectan significativamente la distribuci\u00f3n o comportamiento de los datos.</p> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME <ol> <li>Filtre los datos para conservar solo aquellos que est\u00e9n dentro del rango v\u00e1lido. Luego, genere un gr\u00e1fico de dispersi\u00f3n mostrando la relaci\u00f3n entre sepal_length y petal_length, categorizando los puntos por la columna species. Esto permitir\u00e1 visualizar c\u00f3mo var\u00edan estas dimensiones entre las diferentes especies de Iris.</li> </ol> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME"},{"location":"labs/lab_05/#mat281-laboratorio-n05","title":"MAT281 - Laboratorio N\u00b005\u00b6","text":""},{"location":"labs/lab_05/#problema-01","title":"Problema 01\u00b6","text":""},{"location":"labs/lab_05/#descripcion-de-los-datos","title":"Descripci\u00f3n de los Datos:\u00b6","text":"<ol> <li>sepal_length: Largo del s\u00e9palo en cent\u00edmetros.</li> <li>sepal_width: Ancho del s\u00e9palo en cent\u00edmetros.</li> <li>petal_length: Largo del p\u00e9talo en cent\u00edmetros.</li> <li>petal_width: Ancho del p\u00e9talo en cent\u00edmetros.</li> <li>species: La especie de la flor (Iris setosa, Iris virginica, Iris versicolor).</li> </ol> <p>Lo primero es cargar el conjunto de datos y ver las primeras filas que lo componen:</p>"},{"location":"labs/lab_05/#bases-del-experimento","title":"Bases del Experimento\u00b6","text":"<p>El primer paso en el an\u00e1lisis es identificar las variables clave del estudio y definir su naturaleza:</p> <ul> <li>species:<ul> <li>Descripci\u00f3n: Nombre de la especie de Iris a la que pertenece cada observaci\u00f3n.</li> <li>Tipo de dato: string.</li> <li>Restricciones: Solo se consideran tres especies (Iris setosa, Iris virginica, e Iris versicolor).</li> </ul> </li> <li>sepalLength:<ul> <li>Descripci\u00f3n: Longitud del s\u00e9palo en cent\u00edmetros.</li> <li>Tipo de dato: float.</li> <li>Rango: Los valores oscilan entre 4.0 cm y 7.0 cm.</li> </ul> </li> <li>sepalWidth:<ul> <li>Descripci\u00f3n: Ancho del s\u00e9palo en cent\u00edmetros.</li> <li>Tipo de dato: float.</li> <li>Rango: Los valores var\u00edan entre 2.0 cm y 4.5 cm.</li> </ul> </li> <li>petalLength:<ul> <li>Descripci\u00f3n: Longitud del p\u00e9talo en cent\u00edmetros.</li> <li>Tipo de dato: float.</li> <li>Rango: Los valores est\u00e1n comprendidos entre 1.0 cm y 7.0 cm.</li> </ul> </li> <li>petalWidth:<ul> <li>Descripci\u00f3n: Ancho del p\u00e9talo en cent\u00edmetros.</li> <li>Tipo de dato: float.</li> <li>Rango: Los valores se encuentran entre 0.1 cm y 2.5 cm.</li> </ul> </li> </ul> <p>Esta descripci\u00f3n clara de las variables y sus limitaciones es esencial para establecer una base s\u00f3lida en el an\u00e1lisis de datos.</p>"},{"location":"labs/lab_06/","title":"MAT281 - Laboratorio N\u00b006","text":"<p>El cuarteto de Anscombe es un ejemplo cl\u00e1sico en estad\u00edstica que ilustra c\u00f3mo diferentes conjuntos de datos pueden compartir las mismas propiedades estad\u00edsticas, como media, varianza y correlaci\u00f3n, pero presentan comportamientos muy distintos cuando se visualizan gr\u00e1ficamente. Cada uno de los cuatro conjuntos consiste en once puntos (x, y) y fue creado por el estad\u00edstico F. J. Anscombe en 1973. Esta herramienta resalta la importancia de la visualizaci\u00f3n de datos para evitar interpretaciones err\u00f3neas basadas \u00fanicamente en an\u00e1lisis num\u00e9ricos.</p> <p>Descripci\u00f3n del conjunto</p> <ol> <li>Propiedades estad\u00edsticas comunes: Todos los conjuntos tienen el mismo valor promedio para las variables (x) e (y), la misma varianza para (x) e (y), y una correlaci\u00f3n lineal id\u00e9ntica.</li> <li>Diferencias gr\u00e1ficas: A pesar de sus similitudes estad\u00edsticas, los cuatro conjuntos presentan gr\u00e1ficos muy distintos:<ul> <li>El primer conjunto muestra una relaci\u00f3n lineal simple.</li> <li>El segundo conjunto tiene una relaci\u00f3n no lineal, con una curva clara.</li> <li>El tercer conjunto tiene una relaci\u00f3n lineal clara, pero con un punto at\u00edpico que influye significativamente.</li> <li>El cuarto conjunto tiene la mayor\u00eda de los puntos alineados verticalmente, con un punto at\u00edpico que afecta la correlaci\u00f3n.</li> </ul> </li> </ol> <p>Este cuarteto enfatiza que las estad\u00edsticas descriptivas por s\u00ed solas pueden no capturar la esencia completa de los datos, subrayando la necesidad de utilizar visualizaciones en cualquier an\u00e1lisis exploratorio de datos.</p> In\u00a0[1]: Copied! <pre># Importar las bibliotecas necesarias\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Configuraci\u00f3n de los gr\u00e1ficos\n%matplotlib inline\nsns.set_theme(style=\"whitegrid\")  # Establece un tema general para los gr\u00e1ficos\nsns.set_palette(\"deep\", desat=0.6)\nplt.rcParams['figure.figsize'] = (12, 8)  # Ajuste del tama\u00f1o de las figuras\n\n# Cargar los datos del cuarteto de Anscombe\ndata = sns.load_dataset(\"anscombe\")\n\n# Mostrar las primeras filas del conjunto de datos\ndata.head()\n</pre> # Importar las bibliotecas necesarias import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error, r2_score  # Configuraci\u00f3n de los gr\u00e1ficos %matplotlib inline sns.set_theme(style=\"whitegrid\")  # Establece un tema general para los gr\u00e1ficos sns.set_palette(\"deep\", desat=0.6) plt.rcParams['figure.figsize'] = (12, 8)  # Ajuste del tama\u00f1o de las figuras  # Cargar los datos del cuarteto de Anscombe data = sns.load_dataset(\"anscombe\")  # Mostrar las primeras filas del conjunto de datos data.head() Out[1]: dataset x y 0 I 10.0 8.04 1 I 8.0 6.95 2 I 13.0 7.58 3 I 9.0 8.81 4 I 11.0 8.33 <p>Con base en la informaci\u00f3n presentada y el an\u00e1lisis realizado, les invitamos a reflexionar y responder las siguientes preguntas. Estas preguntas est\u00e1n dise\u00f1adas para profundizar en su comprensi\u00f3n del cuarteto de Anscombe y fomentar un an\u00e1lisis cr\u00edtico de los datos:</p> <ol> <li>Cree un gr\u00e1fico de dispersi\u00f3n (scatter plot) para cada uno de los cuatro grupos del cuarteto de Anscombe. A partir de la visualizaci\u00f3n, \u00bfpuede identificar diferencias significativas entre los grupos? \u00bfQu\u00e9 caracter\u00edsticas particulares observa en cada uno que sugieren comportamientos distintos?</li> </ol> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME <ol> <li>Utilice el comando <code>describe</code> para generar un resumen de las medidas estad\u00edsticas m\u00e1s relevantes para cada uno de los grupos del cuarteto de Anscombe. A partir de estos resultados, interprete las estad\u00edsticas obtenidas, destacando las caracter\u00edsticas m\u00e1s significativas de cada grupo y c\u00f3mo pueden influir en la comprensi\u00f3n de sus respectivas distribuciones.</li> </ol> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME <ol> <li>Ajuste un modelo de regresi\u00f3n lineal para cada grupo utilizando sklearn. Calcule las m\u00e9tricas de evaluaci\u00f3n, como el error cuadr\u00e1tico medio (MSE) y R\u00b2, y grafique los resultados de la regresi\u00f3n. Interprete los resultados y su impacto en la calidad del ajuste.</li> </ol> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME <ol> <li>Es evidente que el ajuste lineal no es adecuado para algunos grupos. Existen diversas estrategias para abordar este problema, como eliminar outliers o emplear diferentes modelos de regresi\u00f3n. Identifique una estrategia que podr\u00eda mejorar el ajuste del modelo de regresi\u00f3n lineal y, si lo considera necesario, implemente otros modelos alternativos para aquellos casos donde el ajuste lineal resulte inadecuado.</li> </ol> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME"},{"location":"labs/lab_06/#mat281-laboratorio-n06","title":"MAT281 - Laboratorio N\u00b006\u00b6","text":""},{"location":"labs/lab_071/","title":"MAT281 - Laboratorio N\u00b0071","text":"<p>Los datos utilizados corresponden a propiedades localizadas en un distrito espec\u00edfico de California, junto con algunas estad\u00edsticas resumen extra\u00eddas del censo de 1990. Es importante tener en cuenta que los datos no han sido procesados previamente, por lo que ser\u00e1 necesario realizar algunas etapas de limpieza y preprocesamiento.</p> <p>Las columnas incluidas en el conjunto de datos son las siguientes (sus nombres son descriptivos):</p> <ul> <li>longitude: Longitud geogr\u00e1fica de la propiedad.</li> <li>latitude: Latitud geogr\u00e1fica de la propiedad.</li> <li>housingmedianage: Edad media de las viviendas en la zona.</li> <li>total_rooms: N\u00famero total de habitaciones.</li> <li>total_bedrooms: N\u00famero total de dormitorios.</li> <li>population: Poblaci\u00f3n en el \u00e1rea circundante.</li> <li>households: N\u00famero de hogares en la zona.</li> <li>median_income: Ingreso medio por hogar.</li> <li>medianhousevalue: Valor medio de la propiedad.</li> <li>ocean_proximity: Proximidad al oc\u00e9ano.</li> </ul> <p>El objetivo de este an\u00e1lisis es predecir el valor medio de las propiedades utilizando las caracter\u00edsticas proporcionadas.</p> <p>Para completar este laboratorio, se recomienda seguir la siguiente r\u00fabrica de trabajo:</p> <ol> <li>Definici\u00f3n del problema: Clarificar el objetivo del an\u00e1lisis y los resultados esperados.</li> <li>Estad\u00edstica descriptiva: Resumir las principales caracter\u00edsticas de los datos a trav\u00e9s de medidas estad\u00edsticas.</li> <li>Visualizaci\u00f3n descriptiva: Utilizar gr\u00e1ficos para explorar los datos y sus relaciones.</li> <li>Preprocesamiento: Realizar los pasos necesarios de limpieza y transformaci\u00f3n de los datos.</li> <li>Selecci\u00f3n de modelo: Comparar al menos cuatro modelos de predicci\u00f3n diferentes.</li> <li>M\u00e9tricas y an\u00e1lisis de resultados: Evaluar el rendimiento de los modelos utilizando m\u00e9tricas apropiadas.</li> <li>Visualizaci\u00f3n de resultados del modelo: Crear gr\u00e1ficos que muestren el desempe\u00f1o de los modelos.</li> <li>Conclusiones: Resumir los hallazgos del an\u00e1lisis.</li> </ol> <p>Nota: Se anima a los estudiantes a desarrollar un an\u00e1lisis m\u00e1s profundo si lo desean. Pueden consultar como referencia el siguiente enlace.</p> In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns In\u00a0[\u00a0]: Copied! <pre>from sklearn.metrics import mean_absolute_error, mean_squared_error\n\ndef mean_absolute_percentage_error(y_true, y_pred):\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\ndef regression_metrics(df):\n    \"\"\"\n    Aplicar las distintas m\u00e9tricas definidas\n    :param df: DataFrame con las columnas: ['y', 'yhat']\n    :return: DataFrame con las m\u00e9tricas especificadas\n    \"\"\"\n    df_result = pd.DataFrame()\n\n    y_true = df['y']\n    y_pred = df['yhat']\n\n    df_result['mae'] = [round(mean_absolute_error(y_true, y_pred), 4)]\n    df_result['mse'] = [round(mean_squared_error(y_true, y_pred), 4)]\n    df_result['rmse'] = [round(np.sqrt(mean_squared_error(y_true, y_pred)), 4)]\n    df_result['mape'] = [round(mean_absolute_percentage_error(y_true, y_pred), 4)]\n    df_result['smape'] = [round(2 * mean_absolute_percentage_error(y_true, y_pred) / (mean_absolute_percentage_error(y_true, y_pred) + 100), 4)]\n\n    return df_result\n</pre> from sklearn.metrics import mean_absolute_error, mean_squared_error  def mean_absolute_percentage_error(y_true, y_pred):     return np.mean(np.abs((y_true - y_pred) / y_true)) * 100  def regression_metrics(df):     \"\"\"     Aplicar las distintas m\u00e9tricas definidas     :param df: DataFrame con las columnas: ['y', 'yhat']     :return: DataFrame con las m\u00e9tricas especificadas     \"\"\"     df_result = pd.DataFrame()      y_true = df['y']     y_pred = df['yhat']      df_result['mae'] = [round(mean_absolute_error(y_true, y_pred), 4)]     df_result['mse'] = [round(mean_squared_error(y_true, y_pred), 4)]     df_result['rmse'] = [round(np.sqrt(mean_squared_error(y_true, y_pred)), 4)]     df_result['mape'] = [round(mean_absolute_percentage_error(y_true, y_pred), 4)]     df_result['smape'] = [round(2 * mean_absolute_percentage_error(y_true, y_pred) / (mean_absolute_percentage_error(y_true, y_pred) + 100), 4)]      return df_result In\u00a0[\u00a0]: Copied! <pre>from sklearn.datasets import fetch_california_housing\n\n# Cargar los datos de housing\nhousing_data = fetch_california_housing(as_frame=True)\n\n# Convertir los datos en un DataFrame de pandas\nhousing = housing_data['data']\nhousing['target'] = housing_data['target']\n\n# Visualizar las primeras filas del DataFrame\nhousing.head()\n</pre> from sklearn.datasets import fetch_california_housing  # Cargar los datos de housing housing_data = fetch_california_housing(as_frame=True)  # Convertir los datos en un DataFrame de pandas housing = housing_data['data'] housing['target'] = housing_data['target']  # Visualizar las primeras filas del DataFrame housing.head() Out[\u00a0]: MedInc HouseAge AveRooms AveBedrms Population AveOccup Latitude Longitude target 0 8.3252 41.0 6.984127 1.023810 322.0 2.555556 37.88 -122.23 4.526 1 8.3014 21.0 6.238137 0.971880 2401.0 2.109842 37.86 -122.22 3.585 2 7.2574 52.0 8.288136 1.073446 496.0 2.802260 37.85 -122.24 3.521 3 5.6431 52.0 5.817352 1.073059 558.0 2.547945 37.85 -122.25 3.413 4 3.8462 52.0 6.281853 1.081081 565.0 2.181467 37.85 -122.25 3.422 In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME"},{"location":"labs/lab_071/#mat281-laboratorio-n071","title":"MAT281 - Laboratorio N\u00b0071\u00b6","text":""},{"location":"labs/lab_071/#i-problema-01","title":"I.- Problema 01\u00b6","text":""},{"location":"labs/lab_072/","title":"MAT281 - Laboratorio N\u00b0072","text":"<p>El objetivo de este proyecto es realizar la mejor predicci\u00f3n posible de las im\u00e1genes a partir de los datos disponibles. Para lograrlo, es necesario seguir los pasos habituales en un proyecto de Machine Learning, que incluyen la exploraci\u00f3n estad\u00edstica, la visualizaci\u00f3n y el preprocesamiento de los datos.</p> <p>Se solicita lo siguiente:</p> <ul> <li><p>Ajustar al menos tres modelos de clasificaci\u00f3n:</p> <ul> <li>Regresi\u00f3n log\u00edstica.</li> <li>K-Nearest Neighbors.</li> <li>Un algoritmo adicional o m\u00e1s, de libre elecci\u00f3n. Puedes consultar esta lista de algoritmos para obtener ideas.</li> </ul> </li> <li><p>Realizar predicciones con cada uno de los tres modelos utilizando el conjunto de datos de test y calcular sus respectivas puntuaciones (score).</p> </li> <li><p>Evaluar y analizar los resultados de los modelos utilizando las siguientes m\u00e9tricas de rendimiento:</p> <ul> <li>Accuracy</li> <li>Precision</li> <li>Recall</li> <li>F1-score</li> </ul> </li> </ul> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\nfrom sklearn import datasets\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n</pre> import numpy as np import pandas as pd from sklearn import datasets import matplotlib.pyplot as plt  %matplotlib inline In\u00a0[\u00a0]: Copied! <pre>digits_dict = datasets.load_digits()\nprint(digits_dict[\"DESCR\"])\n</pre> digits_dict = datasets.load_digits() print(digits_dict[\"DESCR\"]) In\u00a0[\u00a0]: Copied! <pre># informacion de las columnas\ndigits_dict.keys()\n</pre> # informacion de las columnas digits_dict.keys() In\u00a0[\u00a0]: Copied! <pre># informacion del target\ndigits_dict[\"target\"]\n</pre> # informacion del target digits_dict[\"target\"] <p>A continuaci\u00f3n se crea dataframe declarado como <code>digits</code> con los datos de <code>digits_dict</code> tal que tenga 65 columnas, las 6 primeras a la representaci\u00f3n de la imagen en escala de grises (0-blanco, 255-negro) y la \u00faltima correspondiente al d\u00edgito (<code>target</code>) con el nombre target.</p> In\u00a0[\u00a0]: Copied! <pre># leer datos\ndigits = (\n    pd.DataFrame(\n        digits_dict[\"data\"],\n    )\n    .rename(columns=lambda x: f\"c{x:02d}\")\n    .assign(target=digits_dict[\"target\"])\n    .astype(int)\n)\n\ndigits.head()\n</pre> # leer datos digits = (     pd.DataFrame(         digits_dict[\"data\"],     )     .rename(columns=lambda x: f\"c{x:02d}\")     .assign(target=digits_dict[\"target\"])     .astype(int) )  digits.head() In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME In\u00a0[\u00a0]: Copied! <pre>digits_dict[\"images\"][0]\n</pre> digits_dict[\"images\"][0] <p>Visualiza im\u00e1genes de los d\u00edgitos utilizando la llave <code>images</code> de <code>digits_dict</code>.</p> <p>Sugerencia: Utiliza <code>plt.subplots</code> y el m\u00e9todo <code>imshow</code>. Puedes hacer una grilla de varias im\u00e1genes al mismo tiempo!</p> In\u00a0[\u00a0]: Copied! <pre>nx, ny = 5, 5\nfig, axs = plt.subplots(nx, ny, figsize=(12, 12))\n## FIXME\n</pre> nx, ny = 5, 5 fig, axs = plt.subplots(nx, ny, figsize=(12, 12)) ## FIXME In\u00a0[\u00a0]: Copied! <pre># features, target\n\nX = digits.drop(columns=\"target\").values\ny = digits[\"target\"].values\n</pre> # features, target  X = digits.drop(columns=\"target\").values y = digits[\"target\"].values In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME In\u00a0[\u00a0]: Copied! <pre>def mostrar_resultados(digits, model, nx=5, ny=5, label=\"correctos\"):\n    \"\"\"\n    Muestra los resultados de las predicciones de un modelo de clasificaci\u00f3n en particular.\n    Se toman aleatoriamente los valores de los resultados.\n\n    - label == 'correctos': muestra los valores en los que el modelo acierta.\n    - label == 'incorrectos': muestra los valores en los que el modelo no acierta.\n\n    Observaci\u00f3n: El modelo que se recibe como argumento no debe estar entrenado.\n\n    :param digits: dataset 'digits'\n    :param model: modelo de sklearn\n    :param nx: n\u00famero de filas (subplots)\n    :param ny: n\u00famero de columnas (subplots)\n    :param label: 'correctos' o 'incorrectos'\n    :return: gr\u00e1ficos matplotlib\n    \"\"\"\n\n    X = digits.drop(columns=\"target\").values\n    y = digits[\"target\"].values\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    model.fit(X_train, y_train)  # Ajustar el modelo\n    y_pred = model.predict(X_test)\n\n    # Mostrar los datos correctos\n    if label == \"correctos\":\n        mask = (y_pred == y_test)\n        color = \"green\"\n\n    # Mostrar los datos incorrectos\n    elif label == \"incorrectos\":\n        mask = (y_pred != y_test)\n        color = \"red\"\n\n    else:\n        raise ValueError(\"Valor incorrecto\")\n\n    X_aux = X_test[mask]\n    y_aux_true = y_test[mask]\n    y_aux_pred = y_pred[mask]\n\n    # Mostrar los resultados\n    n_samples = min(nx * ny, len(X_aux))\n    indices = np.random.choice(len(X_aux), n_samples, replace=False)\n    fig, ax = plt.subplots(nx, ny, figsize=(12, 12))\n\n    for i, index in enumerate(indices):\n        data = X_aux[index, :].reshape(8, 8)\n        label_pred = str(int(y_aux_pred[index]))\n        label_true = str(int(y_aux_true[index]))\n        row = i // ny\n        col = i % ny\n        ax[row, col].imshow(data, interpolation='nearest', cmap='gray_r')\n        ax[row, col].text(0, 0, label_pred, horizontalalignment='center', verticalalignment='center', fontsize=10, color=color)\n        ax[row, col].text(7, 0, label_true, horizontalalignment='center', verticalalignment='center', fontsize=10, color='blue')\n        ax[row, col].get_xaxis().set_visible(False)\n        ax[row, col].get_yaxis().set_visible(False)\n\n    plt.show()\n</pre> def mostrar_resultados(digits, model, nx=5, ny=5, label=\"correctos\"):     \"\"\"     Muestra los resultados de las predicciones de un modelo de clasificaci\u00f3n en particular.     Se toman aleatoriamente los valores de los resultados.      - label == 'correctos': muestra los valores en los que el modelo acierta.     - label == 'incorrectos': muestra los valores en los que el modelo no acierta.      Observaci\u00f3n: El modelo que se recibe como argumento no debe estar entrenado.      :param digits: dataset 'digits'     :param model: modelo de sklearn     :param nx: n\u00famero de filas (subplots)     :param ny: n\u00famero de columnas (subplots)     :param label: 'correctos' o 'incorrectos'     :return: gr\u00e1ficos matplotlib     \"\"\"      X = digits.drop(columns=\"target\").values     y = digits[\"target\"].values     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)     model.fit(X_train, y_train)  # Ajustar el modelo     y_pred = model.predict(X_test)      # Mostrar los datos correctos     if label == \"correctos\":         mask = (y_pred == y_test)         color = \"green\"      # Mostrar los datos incorrectos     elif label == \"incorrectos\":         mask = (y_pred != y_test)         color = \"red\"      else:         raise ValueError(\"Valor incorrecto\")      X_aux = X_test[mask]     y_aux_true = y_test[mask]     y_aux_pred = y_pred[mask]      # Mostrar los resultados     n_samples = min(nx * ny, len(X_aux))     indices = np.random.choice(len(X_aux), n_samples, replace=False)     fig, ax = plt.subplots(nx, ny, figsize=(12, 12))      for i, index in enumerate(indices):         data = X_aux[index, :].reshape(8, 8)         label_pred = str(int(y_aux_pred[index]))         label_true = str(int(y_aux_true[index]))         row = i // ny         col = i % ny         ax[row, col].imshow(data, interpolation='nearest', cmap='gray_r')         ax[row, col].text(0, 0, label_pred, horizontalalignment='center', verticalalignment='center', fontsize=10, color=color)         ax[row, col].text(7, 0, label_true, horizontalalignment='center', verticalalignment='center', fontsize=10, color='blue')         ax[row, col].get_xaxis().set_visible(False)         ax[row, col].get_yaxis().set_visible(False)      plt.show() <p>Pregunta</p> <ul> <li><p>Tomando en cuenta el mejor modelo entontrado en el <code>Ejercicio 3</code>, grafique los resultados cuando:</p> </li> <li><p>el valor predicho y original son iguales</p> </li> <li><p>el valor predicho y original son distintos</p> </li> <li><p>Cuando el valor predicho y original son distintos ,  \u00bfPor qu\u00e9 ocurren estas fallas?</p> </li> </ul> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME"},{"location":"labs/lab_072/#mat281-laboratorio-n072","title":"MAT281 - Laboratorio N\u00b0072\u00b6","text":""},{"location":"labs/lab_072/#i-problema-01","title":"I.- Problema 01\u00b6","text":""},{"location":"labs/lab_072/#exploracion-de-los-datos","title":"Exploraci\u00f3n de los datos\u00b6","text":"<p>A continuaci\u00f3n se carga el conjunto de datos a utilizar, a trav\u00e9s del sub-m\u00f3dulo <code>datasets</code> de <code>sklearn</code>.</p>"},{"location":"labs/lab_072/#ejercicio-1","title":"Ejercicio 1\u00b6","text":"<p>An\u00e1lisis exploratorio: Realiza tu an\u00e1lisis exploratorio, no debes olvidar nada! Recuerda, cada an\u00e1lisis debe responder una pregunta.</p> <p>Algunas sugerencias:</p> <ul> <li>\u00bfC\u00f3mo se distribuyen los datos?</li> <li>\u00bfCu\u00e1nta memoria estoy utilizando?</li> <li>\u00bfQu\u00e9 tipo de datos son?</li> <li>\u00bfCu\u00e1ntos registros por clase hay?</li> <li>\u00bfHay registros que no se correspondan con tu conocimiento previo de los datos?</li> </ul>"},{"location":"labs/lab_072/#ejercicio-2","title":"Ejercicio 2\u00b6","text":"<p>Visualizaci\u00f3n: Para visualizar los datos utilizaremos el m\u00e9todo <code>imshow</code> de <code>matplotlib</code>. Resulta necesario convertir el arreglo desde las dimensiones (1,64)  a (8,8) para que la imagen sea cuadrada y pueda distinguirse el d\u00edgito. Superpondremos adem\u00e1s el label correspondiente al d\u00edgito, mediante el m\u00e9todo <code>text</code>. Esto nos permitir\u00e1 comparar la imagen generada con la etiqueta asociada a los valores. Realizaremos lo anterior para los primeros 25 datos del archivo.</p>"},{"location":"labs/lab_072/#ejercicio-3","title":"Ejercicio 3\u00b6","text":"<p>Machine Learning: En esta parte usted debe entrenar los distintos modelos escogidos desde la librer\u00eda de <code>skelearn</code>. Para cada modelo, debe realizar los siguientes pasos:</p> <ul> <li><p>train-test</p> <ul> <li>Crear conjunto de entrenamiento y testeo (usted determine las proporciones adecuadas).</li> <li>Imprimir por pantalla el largo del conjunto de entrenamiento y de testeo.</li> </ul> </li> <li><p>modelo:</p> <ul> <li>Instanciar el modelo objetivo desde la librer\u00eda sklearn.</li> </ul> </li> <li><p>M\u00e9tricas:</p> <ul> <li>Graficar matriz de confusi\u00f3n.</li> <li>Analizar m\u00e9tricas de error.</li> </ul> </li> </ul> <p>Preguntas a responder:</p> <ul> <li>\u00bfCu\u00e1l modelo es mejor basado en sus m\u00e9tricas?</li> <li>\u00bfCu\u00e1l modelo demora menos tiempo en ajustarse?</li> <li>\u00bfQu\u00e9 modelo escoges?</li> </ul>"},{"location":"labs/lab_072/#ejercicio-4","title":"Ejercicio 4\u00b6","text":"<p>Comprensi\u00f3n del modelo: Tomando en cuenta el mejor modelo entontrado en el <code>Ejercicio 3</code>, debe comprender e interpretar minuciosamente los resultados y gr\u00e1ficos asocados al modelo en estudio, para ello debe resolver los siguientes puntos:</p> <ul> <li>Curva AUC\u2013ROC: Replica el ejemplo del siguiente  link pero con el modelo, par\u00e1metros y m\u00e9trica adecuada. Saque conclusiones del gr\u00e1fico.</li> </ul>"},{"location":"labs/lab_072/#ejercicio-5","title":"Ejercicio 5\u00b6","text":"<p>Visualizando Resultados: A continuaci\u00f3n se provee c\u00f3digo para comparar las etiquetas predichas vs las etiquetas reales del conjunto de test.</p>"},{"location":"labs/lab_072/#ejercicio-6","title":"Ejercicio 6\u00b6","text":"<p>Conclusiones: Entrega tu veredicto, responde las preguntas iniciales, visualizaciones, trabajos futuros, dificultades, etc.</p>"},{"location":"labs/lab_08/","title":"MAT281 - Laboratorio N\u00b008","text":"In\u00a0[4]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.cluster import KMeans\n\n\n%matplotlib inline\n\nsns.set_palette(\"deep\", desat=.6)\nsns.set(rc={'figure.figsize':(11.7,8.27)})\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns  from sklearn.preprocessing import MinMaxScaler from sklearn.dummy import DummyClassifier from sklearn.cluster import KMeans   %matplotlib inline  sns.set_palette(\"deep\", desat=.6) sns.set(rc={'figure.figsize':(11.7,8.27)}) In\u00a0[5]: Copied! <pre># cargar datos\ndf = pd.read_csv(\"https://raw.githubusercontent.com/fralfaro/MAT281_2024/main/docs/labs/data/vehiculos_procesado_con_grupos.csv\", sep=\",\")\\\n       .drop(\n            [\"fabricante\", \n             \"modelo\",\n             \"transmision\", \n             \"traccion\", \n             \"clase\", \n             \"combustible\",\n             \"consumo\"], \n    \n          axis=1)\n\ndf.head()\n</pre> # cargar datos df = pd.read_csv(\"https://raw.githubusercontent.com/fralfaro/MAT281_2024/main/docs/labs/data/vehiculos_procesado_con_grupos.csv\", sep=\",\")\\        .drop(             [\"fabricante\",               \"modelo\",              \"transmision\",               \"traccion\",               \"clase\",               \"combustible\",              \"consumo\"],                 axis=1)  df.head() Out[5]: year desplazamiento cilindros co2 clase_tipo traccion_tipo transmision_tipo combustible_tipo tamano_motor_tipo consumo_tipo co2_tipo consumo_litros_milla 0 1984 2.5 4.0 522.764706 Veh\u00edculos Especiales dos Automatica Normal peque\u00f1o alto alto 0.222671 1 1984 4.2 6.0 683.615385 Veh\u00edculos Especiales dos Automatica Normal grande muy alto muy alto 0.291185 2 1985 2.5 4.0 555.437500 Veh\u00edculos Especiales dos Automatica Normal peque\u00f1o alto alto 0.236588 3 1985 4.2 6.0 683.615385 Veh\u00edculos Especiales dos Automatica Normal grande muy alto muy alto 0.291185 4 1987 3.8 6.0 555.437500 Coches Medianos dos Automatica Premium grande alto alto 0.236588 <p>En este caso, no solo se tienen datos num\u00e9ricos, sino que tambi\u00e9n categ\u00f3ricos. Adem\u00e1s, tenemos problemas de datos vac\u00edos (Nan). As\u00ed que para resolver este problema, seguiremos varios pasos:</p> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME <p>Al observar el gr\u00e1fico resultante, se pueden obtener conclusiones sobre el n\u00famero apropiado de clusters. La regla del codo sugiere elegir el n\u00famero de clusters donde la reducci\u00f3n en la inercia se estabiliza significativamente. En otras palabras, se busca el punto en el gr\u00e1fico donde la curva de inercia comienza a aplanarse o forma un codo.</p> In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns\n</pre> import pandas as pd from sklearn.datasets import load_iris from sklearn.decomposition import PCA from sklearn.manifold import TSNE  import matplotlib.pyplot as plt from matplotlib.colors import ListedColormap from mpl_toolkits.mplot3d import Axes3D import seaborn as sns In\u00a0[\u00a0]: Copied! <pre>dataset = load_iris()\n\nfeatures = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\ntarget = 'species'\niris = pd.DataFrame(\n    dataset.data,\n    columns=features)\n\niris['species'] = dataset.target\niris.head()\n</pre> dataset = load_iris()  features = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width'] target = 'species' iris = pd.DataFrame(     dataset.data,     columns=features)  iris['species'] = dataset.target iris.head() <p>El objetivo es aplicar ambos algoritmos de la siguiente manera:</p> <ul> <li>An\u00e1lisis detallado algoritma PCA (tablas, gr\u00e1ficos, etc.)</li> <li>An\u00e1lisis detallado algoritma TSNE (tablas, gr\u00e1ficos, etc.)</li> <li>Comparar ambos algoritmos (conclusiones del caso)</li> </ul> In\u00a0[\u00a0]: Copied! <pre># FIXME\n</pre> # FIXME"},{"location":"labs/lab_08/#mat281-laboratorio-n08","title":"MAT281 - Laboratorio N\u00b008\u00b6","text":""},{"location":"labs/lab_08/#i-problema-01","title":"I.- Problema 01\u00b6","text":"<p>El conjunto de datos, denominado <code>vehiculos_procesado_con_grupos.csv</code>, contiene algunas de las caracter\u00edsticas m\u00e1s importantes de los veh\u00edculos. El objetivo de este ejercicio es clasificar los veh\u00edculos en diferentes categor\u00edas, bas\u00e1ndonos en las caracter\u00edsticas que se describen a continuaci\u00f3n.</p> <p>El desaf\u00edo de este ejercicio radica en la combinaci\u00f3n de variables num\u00e9ricas y categ\u00f3ricas, lo que a\u00f1ade complejidad al an\u00e1lisis.</p> <p>Lo primero que haremos ser\u00e1 cargar el conjunto de datos:</p> <p>Descripci\u00f3n de los Datos:</p> Nombre de la Columna Descripci\u00f3n year El a\u00f1o en que el veh\u00edculo fue fabricado. desplazamiento La capacidad volum\u00e9trica del motor en litros. Indica la cantidad de aire y combustible que puede desplazar el motor durante una revoluci\u00f3n. cilindros El n\u00famero de cilindros que tiene el motor. Los cilindros son las c\u00e1maras donde ocurre la combusti\u00f3n interna en los motores de los veh\u00edculos. co2 Emisiones de di\u00f3xido de carbono del veh\u00edculo, medido en gramos por kil\u00f3metro. Es una medida de las emisiones de gases de efecto invernadero. clase_tipo La clase o tipo de veh\u00edculo, como veh\u00edculos especiales, deportivos, etc. traccion_tipo Tipo de tracci\u00f3n del veh\u00edculo, ya sea tracci\u00f3n en dos ruedas, en cuatro ruedas o en todas las ruedas. transmision_tipo Tipo de transmisi\u00f3n del veh\u00edculo, como autom\u00e1tica, manual, entre otros. combustible_tipo Tipo de combustible que utiliza el veh\u00edculo, como gasolina, di\u00e9sel, el\u00e9ctrico, h\u00edbrido, etc. tamano_motor_tipo Clasificaci\u00f3n del tama\u00f1o del motor (por ejemplo, peque\u00f1o, mediano o grande), que generalmente se basa en la capacidad de desplazamiento. consumo_tipo Clasificaci\u00f3n del nivel de consumo de combustible del veh\u00edculo, indicando si es alto, bajo, o muy alto. co2_tipo Clasificaci\u00f3n de las emisiones de CO2 del veh\u00edculo, indicando si es alto, bajo, o muy alto. consumo_litros_milla El consumo de combustible del veh\u00edculo, medido en litros por milla. Indica la eficiencia del veh\u00edculo en t\u00e9rminos de consumo de combustible."},{"location":"labs/lab_08/#1-normalizar-datos","title":"1.- Normalizar datos\u00b6","text":"<ol> <li>Cree un conjunto de datos con las variables num\u00e9ricas, adem\u00e1s, para cada dato vac\u00eda, rellene con el promedio asociado a esa columna. Finalmente, normalize los datos mediante el procesamiento MinMaxScaler de sklearn.</li> </ol> <p>2.-  Cree un conjunto de datos con las variables categ\u00f3ricas , adem\u00e1s, transforme de variables categoricas a numericas ocupando el comando get_dummies de pandas (referencia). Explique a grande rasgo como se realiza la codificaci\u00f3n de variables num\u00e9ricas a categ\u00f3ricas.</p> <p>3.- Junte ambos dataset en uno, llamado df_procesado.</p>"},{"location":"labs/lab_08/#2-realizar-ajuste-mediante-kmeans","title":"2.- Realizar ajuste mediante kmeans\u00b6","text":"<p>Una vez depurado el conjunto de datos, es momento de aplicar el algoritmo de kmeans.</p> <ol> <li>Ajuste el modelo de kmeans sobre el conjunto de datos, con un total de 8 clusters.</li> <li>Asociar a cada individuo el correspondiente cluster y calcular valor de los centroides de cada cluster.</li> <li>Realizar un resumen de las principales cualidades de cada cluster. Para  esto debe calcular (para cluster) las siguientes medidas de resumen:<ul> <li>Valor promedio de las variables num\u00e9rica</li> <li>Moda para las variables numericas</li> </ul> </li> </ol>"},{"location":"labs/lab_08/#3-elegir-numero-de-cluster","title":"3.- Elegir N\u00famero de cluster\u00b6","text":"<p>Estime mediante la regla del codo, el n\u00famero de cluster apropiados para el caso. Para efectos pr\u00e1cticos, eliga la siguiente secuencia como n\u00famero de clusters a comparar:</p> <p>$$[5, 10, 20, 30, 50, 75, 100, 200, 300]$$</p> <p>Una vez realizado el gr\u00e1fico, saque sus propias conclusiones del caso.</p>"},{"location":"labs/lab_08/#ii-problema-02","title":"II.- Problema 02\u00b6","text":"<p>Para el conjunto de datos de Iris, se pide realizar una reducci\u00f3n de dimensionalidad ocupando las t\u00e9cnicas de PCA y TSNE (vistas en clases).</p>"},{"location":"lectures/data_manipulation/data_intro/","title":"Introducci\u00f3n","text":"<p>La manipulaci\u00f3n de datos es una habilidad fundamental en el campo  de la ciencia de datos y el an\u00e1lisis de datos. Consiste en la transformaci\u00f3n,  limpieza y preparaci\u00f3n de conjuntos de datos para su an\u00e1lisis y visualizaci\u00f3n.</p> <p>A medida que el volumen de datos generados contin\u00faa creciendo exponencialmente  en la era digital, la capacidad de manejar eficientemente estos datos se vuelve  cada vez m\u00e1s crucial.</p>"},{"location":"lectures/data_manipulation/data_intro/#que-es-la-manipulacion-de-datos","title":"\u00bfQu\u00e9 es la Manipulaci\u00f3n de Datos?","text":"<p>La manipulaci\u00f3n de datos implica una serie de acciones que permiten trabajar con datos de manera efectiva. Esto incluye la limpieza de datos, donde se identifican y corrigen errores o inconsistencias en los conjuntos de datos. Adem\u00e1s, implica la transformaci\u00f3n de datos, que puede involucrar la reestructuraci\u00f3n de datos, la creaci\u00f3n de nuevas variables o la combinaci\u00f3n de m\u00faltiples conjuntos de datos.</p>"},{"location":"lectures/data_manipulation/data_intro/#herramientas-comunes","title":"Herramientas Comunes","text":"<p>Existen varias herramientas y lenguajes de programaci\u00f3n dise\u00f1ados espec\u00edficamente para la manipulaci\u00f3n de datos. Algunas de las herramientas m\u00e1s populares incluyen:</p> <ul> <li> <p>Python: Con bibliotecas como Pandas y NumPy, Python es ampliamente utilizado en la manipulaci\u00f3n y an\u00e1lisis de datos debido a su facilidad de uso y su amplia gama de funcionalidades.</p> </li> <li> <p>R: Especialmente popular entre los estad\u00edsticos y analistas de datos, R ofrece una amplia variedad de paquetes para la manipulaci\u00f3n y an\u00e1lisis de datos, incluyendo dplyr y tidyr.</p> </li> <li> <p>SQL: El lenguaje de consulta estructurado se utiliza para manipular datos almacenados en bases de datos relacionales, permitiendo realizar consultas, actualizaciones y modificaciones en los datos.</p> </li> </ul>"},{"location":"lectures/data_manipulation/data_intro/#tipos-de-procesos","title":"Tipos de Procesos","text":"<p>El proceso de manipulaci\u00f3n de datos generalmente sigue una serie de pasos comunes:</p> <ol> <li> <p>Obtenci\u00f3n de Datos: Se recopilan los datos de diversas fuentes, como bases de datos, archivos CSV, API web, entre otros.</p> </li> <li> <p>Exploraci\u00f3n de Datos: Se realiza una exploraci\u00f3n inicial de los datos para comprender su estructura, identificar problemas de calidad de datos y determinar las transformaciones necesarias.</p> </li> <li> <p>Limpieza de Datos: Se eliminan los valores at\u00edpicos, se manejan los valores perdidos y se corrigen los errores en los datos para garantizar su integridad y consistencia.</p> </li> <li> <p>Transformaci\u00f3n de Datos: Se aplican diversas transformaciones a los datos, como la agregaci\u00f3n, la pivoteaci\u00f3n, la creaci\u00f3n de nuevas variables y la normalizaci\u00f3n, para preparar los datos para su an\u00e1lisis.</p> </li> <li> <p>An\u00e1lisis de Datos: Una vez que los datos est\u00e1n limpios y transformados, se pueden realizar an\u00e1lisis estad\u00edsticos, modelado predictivo, visualizaci\u00f3n de datos y otras t\u00e9cnicas para extraer informaci\u00f3n significativa.</p> </li> <li> <p>Comunicaci\u00f3n de Resultados: Finalmente, los resultados del an\u00e1lisis se comunican de manera clara y efectiva a las partes interesadas, utilizando visualizaciones de datos, informes y presentaciones.</p> </li> </ol>"},{"location":"lectures/data_manipulation/data_intro/#importancia","title":"Importancia","text":"<p>La manipulaci\u00f3n de datos es un paso fundamental en el proceso de an\u00e1lisis de datos, ya que la calidad y la integridad de los datos influyen directamente  en la calidad de los resultados del an\u00e1lisis. </p> <p>Una manipulaci\u00f3n efectiva de datos puede ayudar a mejorar la precisi\u00f3n de  los modelos predictivos, identificar patrones ocultos en los datos y tomar decisiones informadas basadas en evidencia.</p>"},{"location":"lectures/data_manipulation/eda_01/","title":"EDA","text":"<p>El conjunto de datos <code>terremotos.csv</code> recoge informaci\u00f3n sobre terremotos ocurridos en diferentes pa\u00edses entre los a\u00f1os 2000 y 2011. Dado que este dataset es relativamente sencillo de manipular, hemos creado una versi\u00f3n con datos contaminados, denominada <code>terremotos_contaminados.csv</code>, que incluye errores y anomal\u00edas en varias de sus columnas.</p> <p>Este nuevo dataset tiene como objetivo ilustrar los desaf\u00edos comunes que pueden surgir durante el an\u00e1lisis exploratorio de datos, como valores at\u00edpicos, datos faltantes y errores tipogr\u00e1ficos, permitiendo practicar t\u00e9cnicas de limpieza y transformaci\u00f3n de datos.</p> <p>\u00bf Por qu\u00e9 no puede ser un terremoto con una intensidad mayor a 9.6?. Esto se debe a que el terremoto con mayor magnitud registrado por la humanidad es de 9.6, ocurrido en Chile (Valdivia) durante el a\u00f1o 1960. Por lo tanto, entre mayor conocimiento se tenga del fen\u00f3meno en estudio, m\u00e1s restrictivo se vulve el an\u00e1lisis exploratorio y m\u00e1s sentido tienen los resultados obtenidos.</p> In\u00a0[1]: Copied! <pre>#!pip install matplotlib==3.4.2\n</pre> #!pip install matplotlib==3.4.2 In\u00a0[2]: Copied! <pre># cargar librerias\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes\n\n# Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab\n%matplotlib inline\n</pre> # cargar librerias  import os import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns  pd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes  # Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab %matplotlib inline In\u00a0[3]: Copied! <pre># cargar datos\npath = 'https://raw.githubusercontent.com/fralfaro/MAT281_2024/main/docs/lectures/data_manipulation/data/terremotos_contaminados.csv'\n\nterremotos_data = pd.read_csv(path, sep=\",\")\nterremotos_data.head()\n</pre> # cargar datos path = 'https://raw.githubusercontent.com/fralfaro/MAT281_2024/main/docs/lectures/data_manipulation/data/terremotos_contaminados.csv'  terremotos_data = pd.read_csv(path, sep=\",\") terremotos_data.head() Out[3]: A\u00f1o Pais Magnitud Informacion 0 2000 Turkey 6 info no valiosa 1 2000 Turkmenistan 7 info no valiosa 2 2000 Azerbaijan 6.5 info no valiosa 3 2000 Azerbaijan 6.8 info no valiosa 4 2000 Papua New Guinea 8 info no valiosa In\u00a0[4]: Copied! <pre># normalizar columnas (minusculas y sin espacios)\n\nterremotos_data.columns = terremotos_data.columns.str.lower().str.strip()\nterremotos_data.head()\n</pre> # normalizar columnas (minusculas y sin espacios)  terremotos_data.columns = terremotos_data.columns.str.lower().str.strip() terremotos_data.head() Out[4]: a\u00f1o pais magnitud informacion 0 2000 Turkey 6 info no valiosa 1 2000 Turkmenistan 7 info no valiosa 2 2000 Azerbaijan 6.5 info no valiosa 3 2000 Azerbaijan 6.8 info no valiosa 4 2000 Papua New Guinea 8 info no valiosa In\u00a0[5]: Copied! <pre>terremotos_data[\"pais\"]\n</pre> terremotos_data[\"pais\"] Out[5]: <pre>0                Turkey\n1          Turkmenistan\n2            Azerbaijan\n3           Azerbaijan \n4      Papua New Guinea\n             ...       \n223              Serbia\n224              Haiti \n225                 NaN\n226                 NaN\n227               arica\nName: pais, Length: 228, dtype: object</pre> In\u00a0[6]: Copied! <pre># resumen de la informacion\n\ndef resumen_por_columna(df,cols):\n    pd_series = df[cols]\n    \n    # elementos distintos \n    l_unique = pd_series.unique()\n    \n    # elementos vacios\n    \n    l_vacios = pd_series[pd_series.isna()]\n    \n    df_info = pd.DataFrame({\n        'columna': [cols],\n        'unicos': [len(l_unique)],\n        'vacios': [len(l_vacios)]\n    })\n    \n    return df_info\n</pre> # resumen de la informacion  def resumen_por_columna(df,cols):     pd_series = df[cols]          # elementos distintos      l_unique = pd_series.unique()          # elementos vacios          l_vacios = pd_series[pd_series.isna()]          df_info = pd.DataFrame({         'columna': [cols],         'unicos': [len(l_unique)],         'vacios': [len(l_vacios)]     })          return df_info In\u00a0[7]: Copied! <pre>frames = []\n\nfor col in terremotos_data.columns:\n    aux_df = resumen_por_columna(terremotos_data,col)\n    frames.append(aux_df)\n    \ndf_info = pd.concat(frames).reset_index(drop=True)\ndf_info['% vacios'] = df_info['vacios']/len(terremotos_data)\ndf_info\n</pre> frames = []  for col in terremotos_data.columns:     aux_df = resumen_por_columna(terremotos_data,col)     frames.append(aux_df)      df_info = pd.concat(frames).reset_index(drop=True) df_info['% vacios'] = df_info['vacios']/len(terremotos_data) df_info Out[7]: columna unicos vacios % vacios 0 a\u00f1o 17 2 0.008772 1 pais 75 2 0.008772 2 magnitud 46 3 0.013158 3 informacion 4 8 0.035088 In\u00a0[8]: Copied! <pre>plt.style.use('default')\nplt.figure(figsize=(7,5))\n\nplotting = sns.barplot(\n    x=\"columna\",\n    y=\"unicos\",\n    data=df_info.sort_values('unicos'),\n    palette=\"Greys_d\",\n    linewidth=3\n)\n\nfor container in plotting.containers:\n    plotting.bar_label(container,fontsize=12)\n\nplt.title(\"Valores Distintos\")\nplt.yticks(fontsize=14)\nplt.xticks(fontsize=14)\n\nplt.show()\n</pre> plt.style.use('default') plt.figure(figsize=(7,5))  plotting = sns.barplot(     x=\"columna\",     y=\"unicos\",     data=df_info.sort_values('unicos'),     palette=\"Greys_d\",     linewidth=3 )  for container in plotting.containers:     plotting.bar_label(container,fontsize=12)  plt.title(\"Valores Distintos\") plt.yticks(fontsize=14) plt.xticks(fontsize=14)  plt.show() <p>\u00bfQu\u00e9 falta en los datos y c\u00f3mo los maneja?</p> <p>En este caso, se tiene la informaci\u00f3n suficiente para poder realizar el experimento, solo falta ver que los datos de la muestra no esten lo suficientemente contaminados.</p> <p>\u00bfQu\u00e9 hacer con los datos faltantes, outliers o informaci\u00f3n mal inputada?</p> <p>Este caso es m\u00e1s interesante, y se necesita ir detallando columna por columna los distintos an\u00e1lisis.</p> In\u00a0[9]: Copied! <pre>terremotos_data['a\u00f1o'].unique()\n</pre> terremotos_data['a\u00f1o'].unique() Out[9]: <pre>array(['2000', '2001', 'dos mil uno', '2002', '2003', '2004', '2005',\n       '2006', '2007', '2008', '2009', '2010', '2011', '1997', '1990',\n       '1999', nan], dtype=object)</pre> In\u00a0[10]: Copied! <pre># bar plot: a\u00f1o\n\nplt.style.use('default')\nplt.figure(figsize=(7,4))\n\nplotting = sns.countplot(\n    y=\"a\u00f1o\",\n    data=terremotos_data,\n    order=terremotos_data['a\u00f1o'].value_counts().index,\n    palette=\"Greys_d\",\n    linewidth=3\n)\n\nfor container in plotting.containers:\n    plotting.bar_label(container,fontsize=12)\n\n\nplt.show()\n</pre> # bar plot: a\u00f1o  plt.style.use('default') plt.figure(figsize=(7,4))  plotting = sns.countplot(     y=\"a\u00f1o\",     data=terremotos_data,     order=terremotos_data['a\u00f1o'].value_counts().index,     palette=\"Greys_d\",     linewidth=3 )  for container in plotting.containers:     plotting.bar_label(container,fontsize=12)   plt.show() <p>Se presentan las siguientes anomalidades:</p> <ul> <li>A\u00f1os sin importancia: Se ha establecido que los a\u00f1os de estudios son desde el a\u00f1o 2000 al 2011.</li> <li>Nombres mal escritos: en este caso sabemos que 'dos mil uno' corresponde a '2001'.</li> <li>Datos vac\u00edo</li> </ul> <p>Ahora la pregunta es, \u00bf qu\u00e9 debemos hacer primero?. Lo primero es corregir la informaci\u00f3n, dar un formato est\u00e1ndar a los datos y luego filtrar.</p> <p>a) Correcci\u00f3n</p> In\u00a0[11]: Copied! <pre>terremotos_data.loc[terremotos_data['a\u00f1o']=='dos mil uno','a\u00f1o'] = '2001'\nterremotos_data.loc[terremotos_data['a\u00f1o'].isnull(),'a\u00f1o'] = '0'\n\nterremotos_data['a\u00f1o'].unique()\n</pre> terremotos_data.loc[terremotos_data['a\u00f1o']=='dos mil uno','a\u00f1o'] = '2001' terremotos_data.loc[terremotos_data['a\u00f1o'].isnull(),'a\u00f1o'] = '0'  terremotos_data['a\u00f1o'].unique() Out[11]: <pre>array(['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007',\n       '2008', '2009', '2010', '2011', '1997', '1990', '1999', '0'],\n      dtype=object)</pre> <p>b) Formato</p> <p>El formato de los a\u00f1os es integer, por lo tanto se le debe dar ese formato.</p> In\u00a0[12]: Copied! <pre>terremotos_data['a\u00f1o'] = terremotos_data['a\u00f1o'].astype(int)\n</pre> terremotos_data['a\u00f1o'] = terremotos_data['a\u00f1o'].astype(int) In\u00a0[13]: Copied! <pre>terremotos_data['a\u00f1o'].unique()\n</pre> terremotos_data['a\u00f1o'].unique() Out[13]: <pre>array([2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010,\n       2011, 1997, 1990, 1999,    0])</pre> <p>c) Filtro</p> <p>Se ha establecido que los a\u00f1os de estudios son desde el a\u00f1o 2000 al 2011, por lo tanto los a\u00f1os en estudios deberian ser:</p> In\u00a0[14]: Copied! <pre>anios_estudio = [x for x in range(2000,2011+1)]\nprint(anios_estudio)\n</pre> anios_estudio = [x for x in range(2000,2011+1)] print(anios_estudio) <pre>[2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011]\n</pre> <p>Por lo tanto ya tenemo nuestro primer filtro:</p> In\u00a0[15]: Copied! <pre>mask_anio = terremotos_data['a\u00f1o'].isin(anios_estudio)\n</pre> mask_anio = terremotos_data['a\u00f1o'].isin(anios_estudio) In\u00a0[16]: Copied! <pre>set(terremotos_data['pais'].unique())\n</pre> set(terremotos_data['pais'].unique()) Out[16]: <pre>{'Afghanistan',\n 'Afghanistan ',\n 'Algeria',\n 'Algeria ',\n 'Argentina',\n 'Azerbaijan',\n 'Azerbaijan ',\n 'Bangladesh',\n 'Burma ',\n 'Chile',\n 'Chile ',\n 'China',\n 'China ',\n 'Colombia',\n 'Costa Rica',\n 'Costa Rica ',\n 'Democratic Republic of the Congo',\n 'Democratic Republic of the Congo ',\n 'Dominican Republic',\n 'Ecuador',\n 'El Salvador ',\n 'Greece',\n 'Greece ',\n 'Guadeloupe',\n 'Guatemala',\n 'Haiti ',\n 'India',\n 'India ',\n 'Indonesia',\n 'Indonesia ',\n 'Iran',\n 'Iran ',\n 'Iran, 2005 Qeshm earthquake',\n 'Italy',\n 'Italy ',\n 'Japan',\n 'Japan ',\n 'Kazakhstan',\n 'Kyrgyzstan ',\n 'Martinique',\n 'Mexico ',\n 'Morocco',\n 'Morocco ',\n 'Mozambique',\n 'New Zealand',\n 'New Zealand ',\n 'Nicaragua',\n 'Pakistan',\n 'Pakistan ',\n 'Panama',\n 'Papua New Guinea',\n 'Peru',\n 'Peru ',\n 'Philippines',\n 'Russian Federation',\n 'Rwanda',\n 'Samoa ',\n 'Serbia',\n 'Slovenia',\n 'Solomon Islands ',\n 'Taiwan',\n 'Taiwan ',\n 'Tajikistan',\n 'Tajikistan ',\n 'Tanzania',\n 'Tanzania ',\n 'Turkey',\n 'Turkey ',\n 'Turkmenistan',\n 'United States ',\n 'Venezuela',\n 'Vietnam',\n 'arica',\n nan,\n 'shile'}</pre> In\u00a0[17]: Copied! <pre># bar plot: pais\n\nplt.style.use('default')\nplt.figure(figsize=(14,14))\n\nplotting = sns.countplot(\n    y=\"pais\",\n    data=terremotos_data,\n    order=terremotos_data.pais.value_counts().index,\n    palette=\"Greys_d\",\n    linewidth=3\n)\n\nfor container in plotting.containers:\n    plotting.bar_label(container,fontsize=12)\n\n\nplt.show()\n</pre> # bar plot: pais  plt.style.use('default') plt.figure(figsize=(14,14))  plotting = sns.countplot(     y=\"pais\",     data=terremotos_data,     order=terremotos_data.pais.value_counts().index,     palette=\"Greys_d\",     linewidth=3 )  for container in plotting.containers:     plotting.bar_label(container,fontsize=12)   plt.show() <p>Se presentan las siguientes anomalidades:</p> <ul> <li>Formato de los nombres: no se le ha aplicado strip() y lower(), por lo cual tenemos casos como: 'Turkey' y 'Turkey ' como elementos diferentes.</li> <li>Nombres mal escritos: en este caso sabemos que 'shile' corresponde a 'Chile' e 'Iran, 2005 Qeshm earthquake' corrsponde a 'Iran'.</li> <li>Datos vac\u00edo</li> </ul> <p>Se solucionar\u00e1 cada uno de estos inconvenientes:</p> <p>Correcci\u00f3n de los nombres</p> In\u00a0[18]: Copied! <pre>terremotos_data.loc[terremotos_data['pais']=='arica','pais'] = 'chile'\nterremotos_data.loc[terremotos_data['pais']=='shile','pais'] = 'chile'\nterremotos_data.loc[terremotos_data['pais']=='Iran, 2005 Qeshm earthquake','pais'] = 'iran'\nterremotos_data.loc[terremotos_data['pais'].isnull(),'pais'] = 'sin_nombre'\n</pre> terremotos_data.loc[terremotos_data['pais']=='arica','pais'] = 'chile' terremotos_data.loc[terremotos_data['pais']=='shile','pais'] = 'chile' terremotos_data.loc[terremotos_data['pais']=='Iran, 2005 Qeshm earthquake','pais'] = 'iran' terremotos_data.loc[terremotos_data['pais'].isnull(),'pais'] = 'sin_nombre' <p>Formato</p> In\u00a0[19]: Copied! <pre># correccion formato de nombre \nterremotos_data['pais'] = terremotos_data['pais'].str.lower().str.strip()\n</pre> # correccion formato de nombre  terremotos_data['pais'] = terremotos_data['pais'].str.lower().str.strip() <p>Filtro</p> In\u00a0[20]: Copied! <pre>mask_pais = terremotos_data['pais']!='sin_nombre'\n</pre> mask_pais = terremotos_data['pais']!='sin_nombre' In\u00a0[21]: Copied! <pre># bar plot: magnitud\n\nplt.style.use('default')\nplt.figure(figsize=(10,7))\n\nplotting = sns.countplot(\n    y=\"magnitud\",\n    data=terremotos_data,\n    order=terremotos_data['magnitud'].value_counts().index,\n    palette=\"Greys_d\",\n    linewidth=3\n)\n\nfor container in plotting.containers:\n    plotting.bar_label(container,fontsize=10)\n\n\nplt.show()\n</pre> # bar plot: magnitud  plt.style.use('default') plt.figure(figsize=(10,7))  plotting = sns.countplot(     y=\"magnitud\",     data=terremotos_data,     order=terremotos_data['magnitud'].value_counts().index,     palette=\"Greys_d\",     linewidth=3 )  for container in plotting.containers:     plotting.bar_label(container,fontsize=10)   plt.show() <p>Se presentan las siguientes anomalidades:</p> <ul> <li>Magnitudes sin importancia: Se ha establecido que las magnitudes de un terremoto son de 5 a 9.6.</li> <li>Datos con informaci\u00f3n comprimida: Debido a una inputaci\u00f3n incorrecta de los datos o a una mala lectura, la informaci\u00f3n se concentra en una celda.</li> <li>Datos vac\u00edo</li> </ul> <p>Correcci\u00f3n de las magnitudes</p> In\u00a0[22]: Copied! <pre># caso: tradicional\nterremotos_data.loc[terremotos_data['magnitud']=='2002-Tanzania-5.8','magnitud'] = '0'\nterremotos_data.loc[terremotos_data['magnitud']=='2003-japan-8.5','magnitud'] = '0'\nterremotos_data.loc[terremotos_data['magnitud'].isnull(),'magnitud'] = '0'\n</pre> # caso: tradicional terremotos_data.loc[terremotos_data['magnitud']=='2002-Tanzania-5.8','magnitud'] = '0' terremotos_data.loc[terremotos_data['magnitud']=='2003-japan-8.5','magnitud'] = '0' terremotos_data.loc[terremotos_data['magnitud'].isnull(),'magnitud'] = '0' In\u00a0[23]: Copied! <pre>terremotos_data.head()\n</pre> terremotos_data.head() Out[23]: a\u00f1o pais magnitud informacion 0 2000 turkey 6 info no valiosa 1 2000 turkmenistan 7 info no valiosa 2 2000 azerbaijan 6.5 info no valiosa 3 2000 azerbaijan 6.8 info no valiosa 4 2000 papua new guinea 8 info no valiosa In\u00a0[24]: Copied! <pre># caso: informacion comprimida\nterremotos_data.loc[-1] = [2002,'tanzania','5.8','-']\nterremotos_data.loc[-2] = [2003,'japan','8.5','-']\n</pre> # caso: informacion comprimida terremotos_data.loc[-1] = [2002,'tanzania','5.8','-'] terremotos_data.loc[-2] = [2003,'japan','8.5','-'] In\u00a0[25]: Copied! <pre>terremotos_data = terremotos_data.reset_index(drop=True)\nterremotos_data.tail()\n</pre> terremotos_data = terremotos_data.reset_index(drop=True) terremotos_data.tail() Out[25]: a\u00f1o pais magnitud informacion 225 0 sin_nombre 0 NaN 226 0 sin_nombre 0 NaN 227 2005 chile 8 valiosa 228 2002 tanzania 5.8 - 229 2003 japan 8.5 - <p>Correcci\u00f3n formato de las magnitudes</p> In\u00a0[26]: Copied! <pre>terremotos_data['magnitud'].unique()\n</pre> terremotos_data['magnitud'].unique() Out[26]: <pre>array(['6', '7', '6.5', '6.8', '8', '5.7', '6.4', '5.5', '6.3', '5.4',\n       '6.1', '6.7', '7.9', '7.2', '7.5', '5.3', '5.9', '9.7', '5.8',\n       '4.7', '7.6', '8.4', '5', '5.6', '6.6', '6.2', '7.1', '7.3', '5.1',\n       '5.2', '8.3', '6.9', '9.1', '4.9', '7.8', '8.6', '7.7', '7.4',\n       '8.5', '8.1', '8.8', '9', '-10', '0'], dtype=object)</pre> In\u00a0[27]: Copied! <pre>terremotos_data['magnitud'] = terremotos_data['magnitud'].astype(float)\nterremotos_data['magnitud'].unique()\n</pre> terremotos_data['magnitud'] = terremotos_data['magnitud'].astype(float) terremotos_data['magnitud'].unique() Out[27]: <pre>array([  6. ,   7. ,   6.5,   6.8,   8. ,   5.7,   6.4,   5.5,   6.3,\n         5.4,   6.1,   6.7,   7.9,   7.2,   7.5,   5.3,   5.9,   9.7,\n         5.8,   4.7,   7.6,   8.4,   5. ,   5.6,   6.6,   6.2,   7.1,\n         7.3,   5.1,   5.2,   8.3,   6.9,   9.1,   4.9,   7.8,   8.6,\n         7.7,   7.4,   8.5,   8.1,   8.8,   9. , -10. ,   0. ])</pre> <p>c) Filtro de las magnitudes</p> In\u00a0[28]: Copied! <pre>mask_mag_inf =  terremotos_data['magnitud']&gt;=5\nmask_mag_sup =  terremotos_data['magnitud']&lt;=9.6\n\nmask_mag = mask_mag_inf &amp; mask_mag_sup\n</pre> mask_mag_inf =  terremotos_data['magnitud']&gt;=5 mask_mag_sup =  terremotos_data['magnitud']&lt;=9.6  mask_mag = mask_mag_inf &amp; mask_mag_sup In\u00a0[29]: Copied! <pre>terremotos_data['informacion'].unique()\n</pre> terremotos_data['informacion'].unique() Out[29]: <pre>array(['info no valiosa', 'info valiosa', nan, 'valiosa', '-'],\n      dtype=object)</pre> <p>Se observa que esta columna no aporta informaci\u00f3n valiosa al estudio, quedando excluida para cualquier an\u00e1lisis importante.</p> In\u00a0[30]: Copied! <pre>#terremotos_data.drop('informacion', axis=1, inplace=True)\n</pre> #terremotos_data.drop('informacion', axis=1, inplace=True) In\u00a0[31]: Copied! <pre>terremotos_data.head()\n</pre> terremotos_data.head() Out[31]: a\u00f1o pais magnitud informacion 0 2000 turkey 6.0 info no valiosa 1 2000 turkmenistan 7.0 info no valiosa 2 2000 azerbaijan 6.5 info no valiosa 3 2000 azerbaijan 6.8 info no valiosa 4 2000 papua new guinea 8.0 info no valiosa <p>5. \u00bfSe puede sacar m\u00e1s provecho a los datos ?</p> <p>Una vez realizado toda la limpieza de datos, debemos filtrar la informaci\u00f3n que se considere importante.</p> <ul> <li>A\u00f1os: A\u00f1os desde el 2000 al 2011.</li> <li>Pa\u00eds: Paises con nombre distinto de sin_nombre</li> <li>Magnitud: Magnitud entre 5 y 9.6.</li> </ul> In\u00a0[32]: Copied! <pre># aplicar filtros\nterremotos_data_filtrado = terremotos_data[mask_anio &amp; mask_pais &amp; mask_mag]\nterremotos_data_filtrado.head()\n</pre> # aplicar filtros terremotos_data_filtrado = terremotos_data[mask_anio &amp; mask_pais &amp; mask_mag] terremotos_data_filtrado.head() Out[32]: a\u00f1o pais magnitud informacion 0 2000 turkey 6.0 info no valiosa 1 2000 turkmenistan 7.0 info no valiosa 2 2000 azerbaijan 6.5 info no valiosa 3 2000 azerbaijan 6.8 info no valiosa 4 2000 papua new guinea 8.0 info no valiosa <p>Veamos cu\u00e1nta informaci\u00f3n se perdio:</p> In\u00a0[33]: Copied! <pre>print('Cantidad de filas dataset sin filtro:',len(terremotos_data))\nprint('Cantidad de filas dataset con filtro:',len(terremotos_data_filtrado))\n</pre> print('Cantidad de filas dataset sin filtro:',len(terremotos_data)) print('Cantidad de filas dataset con filtro:',len(terremotos_data_filtrado)) <pre>Cantidad de filas dataset sin filtro: 230\nCantidad de filas dataset con filtro: 212\n</pre> <p>Ahora veamos el el resumen de la informaci\u00f3n para el nuevo dataset:</p> In\u00a0[34]: Copied! <pre>frames = []\n\nfor col in terremotos_data_filtrado.columns:\n    aux_df = resumen_por_columna(terremotos_data_filtrado,col)\n    frames.append(aux_df)\n    \ndf_info = pd.concat(frames).reset_index(drop=True)\ndf_info\n</pre> frames = []  for col in terremotos_data_filtrado.columns:     aux_df = resumen_por_columna(terremotos_data_filtrado,col)     frames.append(aux_df)      df_info = pd.concat(frames).reset_index(drop=True) df_info Out[34]: columna unicos vacios 0 a\u00f1o 12 0 1 pais 50 0 2 magnitud 39 0 3 informacion 3 0 <p>Finalmente, podemos responder la pregunta del inicio:</p> In\u00a0[35]: Copied! <pre># formato wide\nterremotos_data_filtrado.pivot_table(index=\"pais\", \n                                     columns=\"a\u00f1o\",\n                                     values=\"magnitud\", \n                                     fill_value='', \n                                     aggfunc=pd.np.max)\n</pre> # formato wide terremotos_data_filtrado.pivot_table(index=\"pais\",                                       columns=\"a\u00f1o\",                                      values=\"magnitud\",                                       fill_value='',                                       aggfunc=pd.np.max) <pre>C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_18544\\1854650910.py:6: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n  aggfunc=pd.np.max)\n</pre> Out[35]: a\u00f1o 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 pais afghanistan 6.3 5.0 7.3 5.8 6.5 6.5 algeria 5.7 6.8 5.2 5.5 argentina 7.2 6.1 azerbaijan 6.8 bangladesh 5.6 burma 6.8 chile 6.3 8.0 7.7 8.8 china 5.9 5.6 5.5 6.3 5.3 5.2 5.0 6.1 7.9 5.7 6.9 5.4 colombia 6.5 5.9 costa rica 6.4 6.1 democratic republic of the congo 6.2 5.9 dominican republic 6.4 ecuador 5.5 el salvador 7.6 greece 6.2 6.4 guadeloupe 6.3 guatemala 6.4 haiti 7.0 india 7.6 6.5 5.1 5.3 5.1 6.9 indonesia 7.9 7.5 6.9 9.1 8.6 7.7 8.5 7.3 7.6 iran 5.3 6.5 6.6 6.3 6.4 6.1 italy 5.9 6.2 japan 6.1 6.8 8.3 6.6 6.6 6.7 6.9 6.4 9.0 kazakhstan 6.0 kyrgyzstan 6.9 martinique 7.4 mexico 7.5 morocco 6.3 mozambique 7.0 new zealand 5.4 6.6 6.3 nicaragua 5.4 pakistan 6.3 5.4 7.6 5.2 6.4 panama 6.5 papua new guinea 8.0 7.6 6.1 peru 8.4 7.5 8.0 philippines 7.5 6.5 7.1 5.3 russian federation 7.3 6.2 rwanda 5.3 samoa 8.1 serbia 5.7 slovenia 5.2 solomon islands 8.1 taiwan 6.4 7.1 5.2 7.0 tajikistan 5.2 5.6 5.2 tanzania 6.4 5.5 6.8 turkey 6.0 6.5 6.3 5.6 5.9 6.1 7.1 turkmenistan 7.0 united states 6.8 6.6 venezuela 5.5 vietnam 5.3 <p>\u00bf Podemos sacar m\u00e1s informaci\u00f3n ?. Por supuesto que se puede, no obstante, siempre se debe ser preciso con la informaci\u00f3n que se</p> <p>Conclusi\u00f3n del caso</p> <ul> <li>El an\u00e1lisis exploratorio de datos (EDA) es una metodolog\u00eda que sirve para asegurarse de la calidad de los datos.</li> <li>A medida que se tiene m\u00e1s expertice en el tema, mejor es el an\u00e1lisis de datos y por tanto, mejor son los resultados obtenidos.</li> <li>No existe un procedimiento est\u00e1ndar para realizar el EDA, pero siempre se debe tener una claridad mental con:<ul> <li>Problema que se quiere resolverlo</li> <li>C\u00f3mo resolver el problema</li> <li>Posibles problemas de la muestra (datos perdidos, ouliers, etc.)</li> </ul> </li> </ul>"},{"location":"lectures/data_manipulation/eda_01/#eda","title":"EDA\u00b6","text":"<p>La manipulaci\u00f3n de datos es una habilidad esencial en el \u00e1mbito de la ciencia y el an\u00e1lisis de datos. Implica transformar, limpiar y preparar conjuntos de datos para su posterior an\u00e1lisis y visualizaci\u00f3n.</p> <p>Con el crecimiento exponencial del volumen de datos generado en la era digital, la capacidad para gestionar y procesar eficientemente esta informaci\u00f3n se ha vuelto m\u00e1s importante que nunca. Una correcta manipulaci\u00f3n de datos no solo garantiza la calidad del an\u00e1lisis, sino que tambi\u00e9n permite extraer insights valiosos que impulsan la toma de decisiones informadas.</p>"},{"location":"lectures/data_manipulation/eda_01/#que-es-la-manipulacion-de-datos","title":"\u00bfQu\u00e9 es la Manipulaci\u00f3n de Datos?\u00b6","text":"<p>La manipulaci\u00f3n de datos es un conjunto de t\u00e9cnicas que permiten gestionar y trabajar con datos de manera eficiente. Entre las acciones m\u00e1s comunes se encuentra la limpieza de datos, que consiste en identificar y corregir errores o inconsistencias en los conjuntos de datos para asegurar su calidad. Adem\u00e1s, incluye la transformaci\u00f3n de datos, la cual puede implicar la reestructuraci\u00f3n de la informaci\u00f3n, la creaci\u00f3n de nuevas variables, o la integraci\u00f3n de varios conjuntos de datos. Estos procesos son fundamentales para garantizar que los datos est\u00e9n en un formato adecuado para su an\u00e1lisis y modelado.</p>"},{"location":"lectures/data_manipulation/eda_01/#herramientas-comunes","title":"Herramientas Comunes\u00b6","text":"<p>Existen diversas herramientas y lenguajes de programaci\u00f3n espec\u00edficamente dise\u00f1ados para la manipulaci\u00f3n de datos. Algunas de las m\u00e1s utilizadas incluyen:</p> <ul> <li><p>Python: Gracias a bibliotecas como Pandas y NumPy, Python se ha consolidado como una opci\u00f3n preferida para la manipulaci\u00f3n y an\u00e1lisis de datos. Su facilidad de uso, flexibilidad y amplia gama de funcionalidades lo hacen ideal tanto para principiantes como para profesionales.</p> </li> <li><p>R: Muy popular entre estad\u00edsticos y analistas, R ofrece potentes paquetes como dplyr y tidyr, que facilitan tareas de transformaci\u00f3n y limpieza de datos, siendo especialmente \u00fatil para an\u00e1lisis estad\u00edstico avanzado.</p> </li> <li><p>SQL: El Lenguaje de Consulta Estructurado (SQL) es esencial para la manipulaci\u00f3n de datos en bases de datos relacionales. Permite realizar consultas complejas, actualizaciones, filtrados y modificaciones de grandes vol\u00famenes de datos de manera eficiente.</p> </li> </ul>"},{"location":"lectures/data_manipulation/eda_01/#tipos-de-procesos","title":"Tipos de Procesos\u00b6","text":"<p>La manipulaci\u00f3n de datos sigue una serie de pasos estructurados que aseguran la calidad y utilidad de los datos para el an\u00e1lisis. Estos son los pasos m\u00e1s comunes:</p> <ol> <li><p>Obtenci\u00f3n de Datos: Se recopilan los datos desde diversas fuentes, como bases de datos, archivos CSV, APIs web, o sensores. Es crucial que los datos sean representativos y est\u00e9n actualizados.</p> </li> <li><p>Exploraci\u00f3n de Datos: Se realiza una exploraci\u00f3n inicial para comprender la estructura del conjunto de datos, detectar posibles problemas de calidad, y evaluar las transformaciones necesarias. Aqu\u00ed se identifican patrones, valores at\u00edpicos y distribuciones.</p> </li> <li><p>Limpieza de Datos: Se eliminan valores at\u00edpicos, se gestionan los valores faltantes y se corrigen errores en los datos, garantizando su integridad y consistencia. La limpieza adecuada es esencial para evitar sesgos y errores en los an\u00e1lisis posteriores.</p> </li> <li><p>Transformaci\u00f3n de Datos: En este paso, se aplican diversas transformaciones, como la agregaci\u00f3n, pivoteaci\u00f3n, creaci\u00f3n de nuevas variables, escalamiento y normalizaci\u00f3n, con el fin de preparar los datos para el an\u00e1lisis detallado o modelado.</p> </li> <li><p>An\u00e1lisis de Datos: Con los datos limpios y transformados, se procede a realizar an\u00e1lisis estad\u00edsticos, modelado predictivo, visualizaciones de datos, y otras t\u00e9cnicas avanzadas para extraer informaci\u00f3n relevante y patrones.</p> </li> <li><p>Comunicaci\u00f3n de Resultados: Finalmente, los resultados del an\u00e1lisis se comunican a las partes interesadas mediante visualizaciones efectivas, informes, o presentaciones, con un enfoque claro y accesible para la toma de decisiones informadas.</p> </li> </ol> <p>\ud83d\udd11 Importancia: La manipulaci\u00f3n de datos es un paso crucial en el proceso de an\u00e1lisis, ya que la calidad e integridad de los datos impactan directamente en la precisi\u00f3n y validez de los resultados. Una manipulaci\u00f3n adecuada permite mejorar la precisi\u00f3n de los modelos predictivos, descubrir patrones ocultos en los datos, y tomar decisiones basadas en evidencia s\u00f3lida, garantizando que el an\u00e1lisis refleje la realidad de manera confiable.</p>"},{"location":"lectures/data_manipulation/eda_01/#ejercicio-aplicado","title":"Ejercicio Aplicado\u00b6","text":""},{"location":"lectures/data_manipulation/eda_01/#bases-del-experimento","title":"Bases del experimento\u00b6","text":"<p>Lo primero es identificar las variables que influyen en el estudio y la naturaleza de esta.</p> <ul> <li>Pa\u00eds:<ul> <li>Descripci\u00f3n: Pa\u00eds del evento s\u00edsmico.</li> <li>Tipo de dato: string</li> <li>Limitantes: No pueden haber nombre de ciudades, comunas o pueblos.</li> </ul> </li> <li>A\u00f1o:<ul> <li>Descripci\u00f3n: A\u00f1o del evento s\u00edsmico.</li> <li>Tipo de dato: integer.</li> <li>Limitantes: el a\u00f1o debe estar entre al a\u00f1o 2000 y 2011.</li> </ul> </li> <li>Magnitud:<ul> <li>Descripci\u00f3n: Magnitud del evento s\u00edsmico.</li> <li>Tipo de dato: float.</li> <li>Limitantes: la magnitud puede estar entre 5 y 9.6.</li> </ul> </li> </ul>"},{"location":"lectures/data_manipulation/eda_01/#conjunto-de-datos","title":"Conjunto de datos\u00b6","text":"<p>El conjunto de datos consta de cuatro columnas:</p> <ul> <li>Pa\u00eds</li> <li>A\u00f1o</li> <li>Magnitud</li> <li>Informaci\u00f3n</li> </ul>"},{"location":"lectures/data_manipulation/eda_01/#checklist-del-experimento","title":"Checklist del experimento\u00b6","text":"<p>Dado que conocemos el fen\u00f3meno en estudio, vayamos realizando un checklist de los procesos para hacer un correcto EDA.</p> <p>1. \u00bf Qu\u00e9 pregunta (s) est\u00e1s tratando de resolver (o probar que est\u00e1s equivocado)?</p> <p>El objetivo es encontrar el terremoto de mayor magnitud por pa\u00eds en los distintos a\u00f1os.</p> <p>2. \u00bf Qu\u00e9 tipo de datos tienes ?</p> <p>Los tipos de variables que tiene el conjunto de datos son:</p> <ul> <li>Categ\u00f3ricas: Pa\u00eds, Informaci\u00f3n.</li> <li>Num\u00e9ricas: A\u00f1o, Magnitud.</li> </ul> <p></p>"},{"location":"lectures/data_manipulation/eda_01/#columna-ano","title":"Columna: A\u00f1o\u00b6","text":"<p>Los a\u00f1os distintos en la muestra son:</p>"},{"location":"lectures/data_manipulation/eda_01/#columna-pais","title":"Columna: Pa\u00eds\u00b6","text":"<p>Los paises distintos en la muestra son:</p>"},{"location":"lectures/data_manipulation/eda_01/#columna-magnitud","title":"Columna: Magnitud\u00b6","text":"<p>Las magnitudes distintas en la muestra son:</p>"},{"location":"lectures/data_manipulation/eda_01/#columna-informacion","title":"Columna: Informaci\u00f3n\u00b6","text":"<p>La cantidad de elementos distintos para la columna informaci\u00f3n son:</p>"},{"location":"lectures/data_manipulation/eda_01/#solucion","title":"Soluci\u00f3n\u00b6","text":""},{"location":"lectures/data_manipulation/eda_01/#referencia","title":"Referencia\u00b6","text":"<ol> <li>Detailed exploratory data analysis with python</li> <li>Exploratory Data Analysis (EDA) and Data Visualization with Python</li> </ol>"},{"location":"lectures/data_manipulation/pd_01a/","title":"Pandas I","text":"<p>Series y DataFrames</p> <ul> <li>Las series son  arreglos unidimensionales con etiquetas. Se puede pensar como una generalizaci\u00f3n de los diccionarios de Python.</li> </ul> <p></p> <ul> <li>Los dataframe son arreglos bidimensionales y una extensi\u00f3n natural de las series. Se puede pensar como la generalizaci\u00f3n de un numpy.array.</li> </ul> <p></p> In\u00a0[2]: Copied! <pre>import pandas as pd\nimport numpy as np\n</pre> import pandas as pd import numpy as np In\u00a0[3]: Copied! <pre># empty dataframe\ndf_empty = pd.DataFrame()\ndf_empty\n</pre> # empty dataframe df_empty = pd.DataFrame() df_empty Out[3]: In\u00a0[4]: Copied! <pre># dataframe with list\ndf_list = pd.DataFrame(\n    [\n        [\"nombre_01\", \"apellido_01\", 60],\n        [\"nombre_02\", \"apellido_02\", 14]\n    ],\n    \n    columns = [\"nombre\", \"apellido\", \"edad\"]\n)\ndf_list\n</pre> # dataframe with list df_list = pd.DataFrame(     [         [\"nombre_01\", \"apellido_01\", 60],         [\"nombre_02\", \"apellido_02\", 14]     ],          columns = [\"nombre\", \"apellido\", \"edad\"] ) df_list Out[4]: nombre apellido edad 0 nombre_01 apellido_01 60 1 nombre_02 apellido_02 14 In\u00a0[5]: Copied! <pre># dataframe with dct\ndf_dct =  pd.DataFrame(\n    {\n        \"nombre\": [\"nombre_01\", \"nombre_02\",],\n        \"apellido\": [\"apellido_01\", \"apellido_02\"],\n        \"edad\": np.array([60,14]),\n    }\n)\n\ndf_dct\n</pre> # dataframe with dct df_dct =  pd.DataFrame(     {         \"nombre\": [\"nombre_01\", \"nombre_02\",],         \"apellido\": [\"apellido_01\", \"apellido_02\"],         \"edad\": np.array([60,14]),     } )  df_dct Out[5]: nombre apellido edad 0 nombre_01 apellido_01 60 1 nombre_02 apellido_02 14 <p>\ud83c\udfc0 Ejemplo: El conjunto de datos \"player_info.csv\" es una colecci\u00f3n exhaustiva que proporciona informaci\u00f3n detallada sobre los jugadores de la NBA desde el a\u00f1o 1947 hasta el 2018.</p> <p>En conjunto, estas columnas ofrecen una visi\u00f3n completa de los jugadores de la NBA durante m\u00e1s de medio siglo, permitiendo an\u00e1lisis detallados sobre su trayectoria, caracter\u00edsticas f\u00edsicas y antecedentes educativos.</p> <p></p> <p>\ud83d\udccbDescripci\u00f3n de las columnas</p> Columna Descripci\u00f3n name El nombre completo de cada jugador en el formato \"apellido, nombre\". year_start El a\u00f1o en el que cada jugador comenz\u00f3 su carrera profesional en la NBA. year_end El a\u00f1o en el que la carrera profesional de cada jugador en la NBA lleg\u00f3 a su fin. position La posici\u00f3n principal en la que cada jugador se desempe\u00f1a en el campo de juego (base, escolta, alero, ala-p\u00edvot, p\u00edvot). height La altura de cada jugador en pulgadas. weight El peso de cada jugador en libras. birth_date La fecha de nacimiento de cada jugador. college La universidad a la que asisti\u00f3 cada jugador antes de ingresar a la NBA. In\u00a0[6]: Copied! <pre># cargar datos\npath = 'https://raw.githubusercontent.com/fralfaro/MAT281_2024/main/docs/lectures/data_manipulation/data/player_info.csv'\ndf = pd.read_csv(path, sep=\",\" )\n</pre> # cargar datos path = 'https://raw.githubusercontent.com/fralfaro/MAT281_2024/main/docs/lectures/data_manipulation/data/player_info.csv' df = pd.read_csv(path, sep=\",\" ) In\u00a0[8]: Copied! <pre># mostar resultados\ndf.head()\n</pre> # mostar resultados df.head() Out[8]: name year_start year_end position height weight birth_date college 0 Alaa Abdelnaby 1991 1995 F-C 6-10 NaN June 24, 1968 NaN 1 Alaa Abdelnaby 1991 1995 F-C 6-10 NaN June 24, 1968 NaN 2 Zaid Abdul-Aziz 1969 1978 C-F 6-9 235.0 April 7, 1946 Iowa State University 3 Kareem Abdul-Jabbar 1970 1989 C 7-2 225.0 April 16, 1947 University of California, Los Angeles 4 Mahmoud Abdul-Rauf 1991 2001 G 6-1 162.0 March 9, 1969 Louisiana State University <p>DataFrame</p> In\u00a0[7]: Copied! <pre># valores\ndf.values\n</pre> # valores df.values Out[7]: <pre>array([['Alaa Abdelnaby', 1991, 1995, ..., nan, 'June 24, 1968', nan],\n       ['Alaa Abdelnaby', 1991, 1995, ..., nan, 'June 24, 1968', nan],\n       ['Zaid Abdul-Aziz', 1969, 1978, ..., 235.0, 'April 7, 1946',\n        'Iowa State University'],\n       ...,\n       ['Bill Zopf', 1971, 1971, ..., 170.0, 'June 7, 1948',\n        'Duquesne University'],\n       ['Ivica Zubac', 2017, 2018, ..., 265.0, 'March 18, 1997', nan],\n       ['Matt Zunic', 1949, 1949, ..., 195.0, 'December 19, 1919',\n        'George Washington University']], dtype=object)</pre> In\u00a0[8]: Copied! <pre># indice\ndf.index\n</pre> # indice df.index Out[8]: <pre>RangeIndex(start=0, stop=4551, step=1)</pre> In\u00a0[9]: Copied! <pre># columnas\ndf.columns\n</pre> # columnas df.columns Out[9]: <pre>Index(['name', 'year_start', 'year_end', 'position', 'height', 'weight',\n       'birth_date', 'college'],\n      dtype='object')</pre> In\u00a0[10]: Copied! <pre># tipo\ntype(df)\n</pre> # tipo type(df) Out[10]: <pre>pandas.core.frame.DataFrame</pre> <p>Series</p> In\u00a0[11]: Copied! <pre># elegir columna (pueden ser una o varias)\npd_series = df['name']\n</pre> # elegir columna (pueden ser una o varias) pd_series = df['name'] In\u00a0[12]: Copied! <pre># valores\npd_series.values\n</pre> # valores pd_series.values Out[12]: <pre>array(['Alaa Abdelnaby', 'Alaa Abdelnaby', 'Zaid Abdul-Aziz', ...,\n       'Bill Zopf', 'Ivica Zubac', 'Matt Zunic'], dtype=object)</pre> In\u00a0[13]: Copied! <pre># indice\npd_series.index\n</pre> # indice pd_series.index Out[13]: <pre>RangeIndex(start=0, stop=4551, step=1)</pre> In\u00a0[14]: Copied! <pre># tipo\ntype(pd_series)\n</pre> # tipo type(pd_series) Out[14]: <pre>pandas.core.series.Series</pre> In\u00a0[15]: Copied! <pre># priemras filas \ndf.head()\n</pre> # priemras filas  df.head() Out[15]: name year_start year_end position height weight birth_date college 0 Alaa Abdelnaby 1991 1995 F-C 6-10 NaN June 24, 1968 NaN 1 Alaa Abdelnaby 1991 1995 F-C 6-10 NaN June 24, 1968 NaN 2 Zaid Abdul-Aziz 1969 1978 C-F 6-9 235.0 April 7, 1946 Iowa State University 3 Kareem Abdul-Jabbar 1970 1989 C 7-2 225.0 April 16, 1947 University of California, Los Angeles 4 Mahmoud Abdul-Rauf 1991 2001 G 6-1 162.0 March 9, 1969 Louisiana State University In\u00a0[16]: Copied! <pre># ultimas filas \ndf.tail()\n</pre> # ultimas filas  df.tail() Out[16]: name year_start year_end position height weight birth_date college 4546 Ante Zizic 2018 2018 F-C 6-11 250.0 January 4, 1997 NaN 4547 Jim Zoet 1983 1983 C 7-1 240.0 December 20, 1953 Kent State University 4548 Bill Zopf 1971 1971 G 6-1 170.0 June 7, 1948 Duquesne University 4549 Ivica Zubac 2017 2018 C 7-1 265.0 March 18, 1997 NaN 4550 Matt Zunic 1949 1949 G-F 6-3 195.0 December 19, 1919 George Washington University In\u00a0[17]: Copied! <pre># informacion del dataframe\ndf.info()\n</pre> # informacion del dataframe df.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4551 entries, 0 to 4550\nData columns (total 8 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   name        4551 non-null   object \n 1   year_start  4551 non-null   int64  \n 2   year_end    4551 non-null   int64  \n 3   position    4550 non-null   object \n 4   height      4550 non-null   object \n 5   weight      4543 non-null   float64\n 6   birth_date  4520 non-null   object \n 7   college     4247 non-null   object \ndtypes: float64(1), int64(2), object(5)\nmemory usage: 284.6+ KB\n</pre> In\u00a0[18]: Copied! <pre># tipo de datos por columnas\ndf.dtypes\n</pre> # tipo de datos por columnas df.dtypes Out[18]: <pre>name           object\nyear_start      int64\nyear_end        int64\nposition       object\nheight         object\nweight        float64\nbirth_date     object\ncollege        object\ndtype: object</pre> In\u00a0[19]: Copied! <pre># filas y columnas\ndf.shape\n</pre> # filas y columnas df.shape Out[19]: <pre>(4551, 8)</pre> In\u00a0[20]: Copied! <pre># estadisticas basicas\ndf.describe()\n</pre> # estadisticas basicas df.describe() Out[20]: year_start year_end weight count 4551.000000 4551.000000 4543.000000 mean 1985.077565 1989.273786 208.901167 std 20.972067 21.872522 26.267502 min 1947.000000 1947.000000 114.000000 25% 1969.000000 1973.000000 190.000000 50% 1986.000000 1992.000000 210.000000 75% 2003.000000 2009.000000 225.000000 max 2018.000000 2018.000000 360.000000 In\u00a0[21]: Copied! <pre># cantidad de objetos unicos - columna especifica\ndf['year_start'].nunique()\n</pre> # cantidad de objetos unicos - columna especifica df['year_start'].nunique() Out[21]: <pre>72</pre> In\u00a0[22]: Copied! <pre># objetos unicos por columna especifica\ndf['year_start'].unique()\n</pre> # objetos unicos por columna especifica df['year_start'].unique() Out[22]: <pre>array([1991, 1969, 1970, 1998, 1997, 1977, 1957, 1947, 2017, 2006, 1954,\n       1988, 1968, 2013, 1976, 1971, 1973, 2007, 2015, 1986, 2014, 1987,\n       2018, 2011, 2008, 1982, 2009, 1967, 1960, 1985, 2016, 1996, 2001,\n       1994, 1992, 2012, 2002, 1989, 2005, 1972, 1981, 1995, 2000, 1999,\n       2010, 1975, 1983, 1993, 1979, 1990, 1955, 2004, 1962, 2003, 1951,\n       1949, 1978, 1956, 1964, 1961, 1984, 1974, 1953, 1980, 1950, 1952,\n       1965, 1963, 1966, 1959, 1948, 1958], dtype=int64)</pre> In\u00a0[23]: Copied! <pre># cantidad de objetos unicos - todas las columnas\ndf.nunique()\n</pre> # cantidad de objetos unicos - todas las columnas df.nunique() Out[23]: <pre>name          4500\nyear_start      72\nyear_end        72\nposition         7\nheight          28\nweight         143\nbirth_date    4161\ncollege        473\ndtype: int64</pre> In\u00a0[24]: Copied! <pre># numero de ocurrencias de cada valor en una columna\ndf['year_start'].value_counts()#.sort_index()\n</pre> # numero de ocurrencias de cada valor en una columna df['year_start'].value_counts()#.sort_index() Out[24]: <pre>1968    173\n1947    161\n1950    120\n1971    100\n1949     94\n       ... \n1964     23\n1966     23\n1958     21\n1961     20\n1960     19\nName: year_start, Length: 72, dtype: int64</pre> In\u00a0[25]: Copied! <pre># ordenar valores - por columna especifica, menor a mayor\ndf.sort_values('year_start', ascending = True).head()\n</pre> # ordenar valores - por columna especifica, menor a mayor df.sort_values('year_start', ascending = True).head() Out[25]: name year_start year_end position height weight birth_date college 3483 Irv Rothenberg 1947 1949 C 6-7 215.0 December 31, 1921 Long Island University 3008 Buddy O'Grady 1947 1949 G 5-11 160.0 January 19, 1920 Georgetown University 1008 Bob Dille 1947 1947 F 6-3 190.0 July 2, 1917 Valparaiso University 3020 Garland O'Shields 1947 1947 G-F 6-1 195.0 May 23, 1921 University of Tennessee 3726 Belus Smawley 1947 1952 G-F 6-1 195.0 March 20, 1918 Appalachian State University In\u00a0[26]: Copied! <pre># ordenar valores - por columna especifica, mayor a menor\ndf.sort_values('year_start', ascending = False).head()\n</pre> # ordenar valores - por columna especifica, mayor a menor df.sort_values('year_start', ascending = False).head() Out[26]: name year_start year_end position height weight birth_date college 1227 Terrance Ferguson 2018 2018 G-F 6-7 184.0 May 17, 1998 NaN 2837 Monte Morris 2018 2018 G 6-3 175.0 June 27, 1995 Iowa State University 1334 Markelle Fultz 2018 2018 G 6-4 195.0 May 29, 1998 University of Washington 2676 Alfonzo McKinnie 2018 2018 F 6-8 215.0 September 17, 1992 University of Wisconsin-Green Bay 3088 Marcus Paige 2018 2018 G 6-1 175.0 September 11, 1993 University of North Carolina In\u00a0[27]: Copied! <pre># operaciones aritmeticas\ns1 = df['year_end']\ns2 = df['year_start']\n\nsuma = s1+s2\nresta = s1-s2\nmultiplicacion = s1*s2\ndivision = s1/s2\n\n# suma\nprint(f\"suma: \\n{suma.head()}\\n\")\n\n# resta\nprint(f\"resta: \\n{resta.head()}\\n\")\n\n# multiplicacion\nprint(f\"multiplicacion: \\n{multiplicacion.head()}\\n\")\n\n# division\nprint(f\"division: \\n{division.head()}\")\n</pre> # operaciones aritmeticas s1 = df['year_end'] s2 = df['year_start']  suma = s1+s2 resta = s1-s2 multiplicacion = s1*s2 division = s1/s2  # suma print(f\"suma: \\n{suma.head()}\\n\")  # resta print(f\"resta: \\n{resta.head()}\\n\")  # multiplicacion print(f\"multiplicacion: \\n{multiplicacion.head()}\\n\")  # division print(f\"division: \\n{division.head()}\") <pre>suma: \n0    3986\n1    3986\n2    3947\n3    3959\n4    3992\ndtype: int64\n\nresta: \n0     4\n1     4\n2     9\n3    19\n4    10\ndtype: int64\n\nmultiplicacion: \n0    3972045\n1    3972045\n2    3894682\n3    3918330\n4    3983991\ndtype: int64\n\ndivision: \n0    1.002009\n1    1.002009\n2    1.004571\n3    1.009645\n4    1.005023\ndtype: float64\n</pre> In\u00a0[28]: Copied! <pre># operaciones estadisticas\ns1 = df['year_start']\n\nprint(f\"mean: {s1.mean()}\") # mean\nprint(f\"std:  {s1.std()}\") # std\nprint(f\"min:  {s1.min()}\") # min\nprint(f\"max:  {s1.max()}\") # max\n</pre> # operaciones estadisticas s1 = df['year_start']  print(f\"mean: {s1.mean()}\") # mean print(f\"std:  {s1.std()}\") # std print(f\"min:  {s1.min()}\") # min print(f\"max:  {s1.max()}\") # max <pre>mean: 1985.0775653702483\nstd:  20.97206734360449\nmin:  1947\nmax:  2018\n</pre> In\u00a0[29]: Copied! <pre># correlaciones lineales\ncols = ['year_start', 'year_end']\ndf[cols].corr()\n</pre> # correlaciones lineales cols = ['year_start', 'year_end'] df[cols].corr() Out[29]: year_start year_end year_start 1.000000 0.978784 year_end 0.978784 1.000000 In\u00a0[30]: Copied! <pre># crear una columna constante\ndf['label'] = 'NBA'\ndf.head()\n</pre> # crear una columna constante df['label'] = 'NBA' df.head() Out[30]: name year_start year_end position height weight birth_date college label 0 Alaa Abdelnaby 1991 1995 F-C 6-10 NaN June 24, 1968 NaN NBA 1 Alaa Abdelnaby 1991 1995 F-C 6-10 NaN June 24, 1968 NaN NBA 2 Zaid Abdul-Aziz 1969 1978 C-F 6-9 235.0 April 7, 1946 Iowa State University NBA 3 Kareem Abdul-Jabbar 1970 1989 C 7-2 225.0 April 16, 1947 University of California, Los Angeles NBA 4 Mahmoud Abdul-Rauf 1991 2001 G 6-1 162.0 March 9, 1969 Louisiana State University NBA In\u00a0[31]: Copied! <pre># Eliminar columna\ndf = df.drop('label',axis=1)\ndf.head()\n</pre> # Eliminar columna df = df.drop('label',axis=1) df.head() Out[31]: name year_start year_end position height weight birth_date college 0 Alaa Abdelnaby 1991 1995 F-C 6-10 NaN June 24, 1968 NaN 1 Alaa Abdelnaby 1991 1995 F-C 6-10 NaN June 24, 1968 NaN 2 Zaid Abdul-Aziz 1969 1978 C-F 6-9 235.0 April 7, 1946 Iowa State University 3 Kareem Abdul-Jabbar 1970 1989 C 7-2 225.0 April 16, 1947 University of California, Los Angeles 4 Mahmoud Abdul-Rauf 1991 2001 G 6-1 162.0 March 9, 1969 Louisiana State University In\u00a0[32]: Copied! <pre># nueva columna a partir de otras dos\ndf['duration'] = df['year_end']-df['year_start']\ndf.head()\n</pre> # nueva columna a partir de otras dos df['duration'] = df['year_end']-df['year_start'] df.head() Out[32]: name year_start year_end position height weight birth_date college duration 0 Alaa Abdelnaby 1991 1995 F-C 6-10 NaN June 24, 1968 NaN 4 1 Alaa Abdelnaby 1991 1995 F-C 6-10 NaN June 24, 1968 NaN 4 2 Zaid Abdul-Aziz 1969 1978 C-F 6-9 235.0 April 7, 1946 Iowa State University 9 3 Kareem Abdul-Jabbar 1970 1989 C 7-2 225.0 April 16, 1947 University of California, Los Angeles 19 4 Mahmoud Abdul-Rauf 1991 2001 G 6-1 162.0 March 9, 1969 Louisiana State University 10 In\u00a0[33]: Copied! <pre># mediante funciones 'apply'\ndf['greater_than_10'] = df['duration'].apply(lambda x: 1 if x&gt;10 else 0)\ndf.head()\n</pre> # mediante funciones 'apply' df['greater_than_10'] = df['duration'].apply(lambda x: 1 if x&gt;10 else 0) df.head() Out[33]: name year_start year_end position height weight birth_date college duration greater_than_10 0 Alaa Abdelnaby 1991 1995 F-C 6-10 NaN June 24, 1968 NaN 4 0 1 Alaa Abdelnaby 1991 1995 F-C 6-10 NaN June 24, 1968 NaN 4 0 2 Zaid Abdul-Aziz 1969 1978 C-F 6-9 235.0 April 7, 1946 Iowa State University 9 0 3 Kareem Abdul-Jabbar 1970 1989 C 7-2 225.0 April 16, 1947 University of California, Los Angeles 19 1 4 Mahmoud Abdul-Rauf 1991 2001 G 6-1 162.0 March 9, 1969 Louisiana State University 10 0 In\u00a0[34]: Copied! <pre>df['greater_than_10'].value_counts()\n</pre> df['greater_than_10'].value_counts() Out[34]: <pre>0    3999\n1     552\nName: greater_than_10, dtype: int64</pre> <p>Muchas veces, en un Dataframe se necesita realizar operaciones entre  la fila actual y la fila anterior, lo cual puede ser complejo si no se utilizan las funciones correctas. A continuaci\u00f3n se trabajan algunas de estas funciones:</p> <ol> <li><code>shift()</code>: Se utiliza para mover hacia arriba o hacia abajo los valores de una columna o serie de datos.</li> <li><code>cumsum()</code>: es una funci\u00f3n que se utiliza para calcular la suma acumulativa de valores a lo largo de un eje en un DataFrame o una Serie.</li> <li><code>pct_change()</code>: es una funci\u00f3n que se utiliza para calcular el cambio porcentual entre los elementos de una serie o columna en un DataFrame.</li> <li><code>rank()</code>: es una funci\u00f3n que se utiliza para asignar un rango a los elementos de una serie o columna en un DataFrame.</li> </ol> In\u00a0[35]: Copied! <pre># aplicar funciones\ndf['shift'] = df['duration'].shift() # se muestra el valor de la fila anterior (la primera fila en este caso es NaN)\ndf['cumsum'] = df['duration'].cumsum()  # suma acumulada entre la fila actual y todas las anteriores\ndf['pct_change'] = df['duration'].pct_change() # porcentaje de cambio entre la fila actual y la anterior \ndf['rank'] = df['duration'].rank() # ranking de los valores (donde 1 es el menor valor)\ndf.head()\n</pre> # aplicar funciones df['shift'] = df['duration'].shift() # se muestra el valor de la fila anterior (la primera fila en este caso es NaN) df['cumsum'] = df['duration'].cumsum()  # suma acumulada entre la fila actual y todas las anteriores df['pct_change'] = df['duration'].pct_change() # porcentaje de cambio entre la fila actual y la anterior  df['rank'] = df['duration'].rank() # ranking de los valores (donde 1 es el menor valor) df.head() Out[35]: name year_start year_end position height weight birth_date college duration greater_than_10 shift cumsum pct_change rank 0 Alaa Abdelnaby 1991 1995 F-C 6-10 NaN June 24, 1968 NaN 4 0 NaN 4 NaN 2714.5 1 Alaa Abdelnaby 1991 1995 F-C 6-10 NaN June 24, 1968 NaN 4 0 4.0 8 0.000000 2714.5 2 Zaid Abdul-Aziz 1969 1978 C-F 6-9 235.0 April 7, 1946 Iowa State University 9 0 4.0 17 1.250000 3720.0 3 Kareem Abdul-Jabbar 1970 1989 C 7-2 225.0 April 16, 1947 University of California, Los Angeles 19 1 9.0 36 1.111111 4545.0 4 Mahmoud Abdul-Rauf 1991 2001 G 6-1 162.0 March 9, 1969 Louisiana State University 10 0 19.0 46 -0.473684 3912.5 In\u00a0[36]: Copied! <pre>cols = ['shift', 'cumsum','pct_change', 'rank']\ndf = df.drop(cols,axis=1)\ndf.head()\n</pre> cols = ['shift', 'cumsum','pct_change', 'rank'] df = df.drop(cols,axis=1) df.head() Out[36]: name year_start year_end position height weight birth_date college duration greater_than_10 0 Alaa Abdelnaby 1991 1995 F-C 6-10 NaN June 24, 1968 NaN 4 0 1 Alaa Abdelnaby 1991 1995 F-C 6-10 NaN June 24, 1968 NaN 4 0 2 Zaid Abdul-Aziz 1969 1978 C-F 6-9 235.0 April 7, 1946 Iowa State University 9 0 3 Kareem Abdul-Jabbar 1970 1989 C 7-2 225.0 April 16, 1947 University of California, Los Angeles 19 1 4 Mahmoud Abdul-Rauf 1991 2001 G 6-1 162.0 March 9, 1969 Louisiana State University 10 0 In\u00a0[37]: Copied! <pre># 'fecha' mayor a 2000\ndf_new = df.loc[df['year_start'] &gt;= 2000]\ndf_new.head()\n</pre> # 'fecha' mayor a 2000 df_new = df.loc[df['year_start'] &gt;= 2000] df_new.head() Out[37]: name year_start year_end position height weight birth_date college duration greater_than_10 10 Alex Abrines 2017 2018 G-F 6-6 190.0 August 1, 1993 NaN 1 0 11 Alex Acker 2006 2009 G 6-5 185.0 January 21, 1983 Pepperdine University 3 0 15 Quincy Acy 2013 2018 F 6-7 240.0 October 6, 1990 Baylor University 5 0 19 Hassan Adams 2007 2009 G 6-4 220.0 June 20, 1984 University of Arizona 2 0 20 Jordan Adams 2015 2016 G 6-5 209.0 July 8, 1994 University of California, Los Angeles 1 0 In\u00a0[38]: Copied! <pre># crear condicion\nvalor_objetivo = 2000 \ncondicion = (df['year_start'] &gt;= valor_objetivo)\ncondicion.head()\n</pre> # crear condicion valor_objetivo = 2000  condicion = (df['year_start'] &gt;= valor_objetivo) condicion.head() Out[38]: <pre>0    False\n1    False\n2    False\n3    False\n4    False\nName: year_start, dtype: bool</pre> In\u00a0[39]: Copied! <pre># aplicar condicion\ndf_new = df.loc[condicion]\ndf_new.head()\n</pre> # aplicar condicion df_new = df.loc[condicion] df_new.head() Out[39]: name year_start year_end position height weight birth_date college duration greater_than_10 10 Alex Abrines 2017 2018 G-F 6-6 190.0 August 1, 1993 NaN 1 0 11 Alex Acker 2006 2009 G 6-5 185.0 January 21, 1983 Pepperdine University 3 0 15 Quincy Acy 2013 2018 F 6-7 240.0 October 6, 1990 Baylor University 5 0 19 Hassan Adams 2007 2009 G 6-4 220.0 June 20, 1984 University of Arizona 2 0 20 Jordan Adams 2015 2016 G 6-5 209.0 July 8, 1994 University of California, Los Angeles 1 0 <p>Veamos distintos tipos de filtros:</p> In\u00a0[40]: Copied! <pre># mayor o igual a 2000\ndf_new = df.loc[df['year_start'] &gt;= 2000]\ndf_new.head()\n</pre> # mayor o igual a 2000 df_new = df.loc[df['year_start'] &gt;= 2000] df_new.head() Out[40]: name year_start year_end position height weight birth_date college duration greater_than_10 10 Alex Abrines 2017 2018 G-F 6-6 190.0 August 1, 1993 NaN 1 0 11 Alex Acker 2006 2009 G 6-5 185.0 January 21, 1983 Pepperdine University 3 0 15 Quincy Acy 2013 2018 F 6-7 240.0 October 6, 1990 Baylor University 5 0 19 Hassan Adams 2007 2009 G 6-4 220.0 June 20, 1984 University of Arizona 2 0 20 Jordan Adams 2015 2016 G 6-5 209.0 July 8, 1994 University of California, Los Angeles 1 0 In\u00a0[41]: Copied! <pre># entre 2005-2015\ndf_new = df.loc[df['year_start'].between(2005,2015)]\ndf_new.head()\n</pre> # entre 2005-2015 df_new = df.loc[df['year_start'].between(2005,2015)] df_new.head() Out[41]: name year_start year_end position height weight birth_date college duration greater_than_10 11 Alex Acker 2006 2009 G 6-5 185.0 January 21, 1983 Pepperdine University 3 0 15 Quincy Acy 2013 2018 F 6-7 240.0 October 6, 1990 Baylor University 5 0 19 Hassan Adams 2007 2009 G 6-4 220.0 June 20, 1984 University of Arizona 2 0 20 Jordan Adams 2015 2016 G 6-5 209.0 July 8, 1994 University of California, Los Angeles 1 0 22 Steven Adams 2014 2018 C 7-0 255.0 July 20, 1993 University of Pittsburgh 4 0 In\u00a0[42]: Copied! <pre># solo 2000\ndf_new = df.loc[df['year_start']==2000]\ndf_new.head()\n</pre> # solo 2000 df_new = df.loc[df['year_start']==2000] df_new.head() Out[42]: name year_start year_end position height weight birth_date college duration greater_than_10 67 Rafer Alston 2000 2010 G 6-2 171.0 July 24, 1976 California State University, Fresno 10 0 143 Chucky Atkins 2000 2010 G 5-11 160.0 August 14, 1974 University of South Florida 10 0 154 William Avery 2000 2002 G 6-2 197.0 August 8, 1979 Duke University 2 0 286 Jonathan Bender 2000 2010 F 6-11 202.0 January 30, 1981 NaN 10 0 386 Calvin Booth 2000 2009 C 6-11 230.0 May 7, 1976 Pennsylvania State University 9 0 In\u00a0[43]: Copied! <pre># varias condiciones condiciones\ndf_new = df.loc[(df['year_start']==2000) &amp; (df['duration']&gt;5)]\ndf_new.head()\n</pre> # varias condiciones condiciones df_new = df.loc[(df['year_start']==2000) &amp; (df['duration']&gt;5)] df_new.head() Out[43]: name year_start year_end position height weight birth_date college duration greater_than_10 67 Rafer Alston 2000 2010 G 6-2 171.0 July 24, 1976 California State University, Fresno 10 0 143 Chucky Atkins 2000 2010 G 5-11 160.0 August 14, 1974 University of South Florida 10 0 286 Jonathan Bender 2000 2010 F 6-11 202.0 January 30, 1981 NaN 10 0 386 Calvin Booth 2000 2009 C 6-11 230.0 May 7, 1976 Pennsylvania State University 9 0 403 Ryan Bowen 2000 2010 F 6-7 215.0 November 20, 1975 University of Iowa 10 0 In\u00a0[44]: Copied! <pre># Filtrar por patr\u00f3n de texto \ndf_new = df.loc[df['name'].str.contains('Michael')]\ndf_new.head()\n</pre> # Filtrar por patr\u00f3n de texto  df_new = df.loc[df['name'].str.contains('Michael')] df_new.head() Out[44]: name year_start year_end position height weight birth_date college duration greater_than_10 21 Michael Adams 1986 1996 G 5-10 162.0 January 19, 1963 Boston College 10 0 93 Michael Anderson 1989 1989 G 5-11 184.0 March 23, 1966 Drexel University 0 0 104 Michael Ansley 1990 1992 F 6-7 225.0 February 8, 1967 University of Alabama 2 0 261 Michael Beasley 2009 2018 F 6-9 235.0 January 9, 1989 Kansas State University 9 0 430 Michael Bradley 2002 2006 F-C 6-10 245.0 April 18, 1979 Villanova University 4 0 In\u00a0[45]: Copied! <pre># valores nulos\ndf.isnull().sum()\n</pre> # valores nulos df.isnull().sum() Out[45]: <pre>name                 0\nyear_start           0\nyear_end             0\nposition             1\nheight               1\nweight               8\nbirth_date          31\ncollege            304\nduration             0\ngreater_than_10      0\ndtype: int64</pre> In\u00a0[46]: Copied! <pre># Eliminar las filas que contienen valores nulos\ndf_new = df.dropna()\ndf_new.head()\n</pre> # Eliminar las filas que contienen valores nulos df_new = df.dropna() df_new.head() Out[46]: name year_start year_end position height weight birth_date college duration greater_than_10 2 Zaid Abdul-Aziz 1969 1978 C-F 6-9 235.0 April 7, 1946 Iowa State University 9 0 3 Kareem Abdul-Jabbar 1970 1989 C 7-2 225.0 April 16, 1947 University of California, Los Angeles 19 1 4 Mahmoud Abdul-Rauf 1991 2001 G 6-1 162.0 March 9, 1969 Louisiana State University 10 0 5 Tariq Abdul-Wahad 1998 2003 F 6-6 223.0 November 3, 1974 San Jose State University 5 0 6 Shareef Abdur-Rahim 1997 2008 F 6-9 225.0 December 11, 1976 University of California 11 1 In\u00a0[47]: Copied! <pre># Rellenar los valores nulos con un valor espec\u00edfico, por ejemplo cero\ndf_new = df.fillna(0)\ndf_new.head()\n</pre> # Rellenar los valores nulos con un valor espec\u00edfico, por ejemplo cero df_new = df.fillna(0) df_new.head() Out[47]: name year_start year_end position height weight birth_date college duration greater_than_10 0 Alaa Abdelnaby 1991 1995 F-C 6-10 0.0 June 24, 1968 0 4 0 1 Alaa Abdelnaby 1991 1995 F-C 6-10 0.0 June 24, 1968 0 4 0 2 Zaid Abdul-Aziz 1969 1978 C-F 6-9 235.0 April 7, 1946 Iowa State University 9 0 3 Kareem Abdul-Jabbar 1970 1989 C 7-2 225.0 April 16, 1947 University of California, Los Angeles 19 1 4 Mahmoud Abdul-Rauf 1991 2001 G 6-1 162.0 March 9, 1969 Louisiana State University 10 0 In\u00a0[48]: Copied! <pre># Comprobar filas duplicadas\ndf.duplicated().sum()\n</pre> # Comprobar filas duplicadas df.duplicated().sum() Out[48]: <pre>1</pre> In\u00a0[49]: Copied! <pre># Eliminar filas duplicadas\ndf_new = df.drop_duplicates()\ndf_new.head()\n</pre> # Eliminar filas duplicadas df_new = df.drop_duplicates() df_new.head() Out[49]: name year_start year_end position height weight birth_date college duration greater_than_10 0 Alaa Abdelnaby 1991 1995 F-C 6-10 NaN June 24, 1968 NaN 4 0 2 Zaid Abdul-Aziz 1969 1978 C-F 6-9 235.0 April 7, 1946 Iowa State University 9 0 3 Kareem Abdul-Jabbar 1970 1989 C 7-2 225.0 April 16, 1947 University of California, Los Angeles 19 1 4 Mahmoud Abdul-Rauf 1991 2001 G 6-1 162.0 March 9, 1969 Louisiana State University 10 0 5 Tariq Abdul-Wahad 1998 2003 F 6-6 223.0 November 3, 1974 San Jose State University 5 0 In\u00a0[50]: Copied! <pre># Comprobar filas duplicadas\ndf_new.duplicated().sum()\n</pre> # Comprobar filas duplicadas df_new.duplicated().sum() Out[50]: <pre>0</pre> In\u00a0[51]: Copied! <pre>import datetime\nnow = datetime.datetime.now()\nprint(now)\n</pre> import datetime now = datetime.datetime.now() print(now) <pre>2024-05-02 10:28:54.404800\n</pre> <p>Puedes acceder a partes espec\u00edficas de un objeto <code>datetime.datetime</code>, como el a\u00f1o, el mes, el d\u00eda, la hora, el minuto y el segundo. Algunos de los atributos m\u00e1s comunes son:</p> <ul> <li><code>year</code>: representa el a\u00f1o de la fecha y la hora.</li> <li><code>month</code>: representa el mes de la fecha y la hora, como un n\u00famero entre 1 y 12.</li> <li><code>day</code>: representa el d\u00eda del mes de la fecha y la hora, como un n\u00famero entre 1 y 31.</li> <li><code>hour</code>: representa la hora del d\u00eda de la fecha y la hora, como un n\u00famero entre 0 y 23.</li> <li><code>minute</code>: representa los minutos de la hora de la fecha y la hora, como un n\u00famero entre 0 y 59.</li> <li><code>second</code>: representa los segundos de la hora de la fecha y la hora, como un n\u00famero entre 0 y 59.</li> <li><code>microsecond</code>: representa los microsegundos de la hora de la fecha y la hora, como un n\u00famero entre 0 y 999999.</li> </ul> <p>Veamos todo lo anterior, aplicado al dataframe que estamos trabajando:</p> In\u00a0[52]: Copied! <pre># cambiar formato de la fecha \ndf['new_birth_date'] = pd.to_datetime(df['birth_date'])\ndf.head()\n</pre> # cambiar formato de la fecha  df['new_birth_date'] = pd.to_datetime(df['birth_date']) df.head() Out[52]: name year_start year_end position height weight birth_date college duration greater_than_10 new_birth_date 0 Alaa Abdelnaby 1991 1995 F-C 6-10 NaN June 24, 1968 NaN 4 0 1968-06-24 1 Alaa Abdelnaby 1991 1995 F-C 6-10 NaN June 24, 1968 NaN 4 0 1968-06-24 2 Zaid Abdul-Aziz 1969 1978 C-F 6-9 235.0 April 7, 1946 Iowa State University 9 0 1946-04-07 3 Kareem Abdul-Jabbar 1970 1989 C 7-2 225.0 April 16, 1947 University of California, Los Angeles 19 1 1947-04-16 4 Mahmoud Abdul-Rauf 1991 2001 G 6-1 162.0 March 9, 1969 Louisiana State University 10 0 1969-03-09 In\u00a0[53]: Copied! <pre># ver atributos\ndf.dtypes\n</pre> # ver atributos df.dtypes Out[53]: <pre>name                       object\nyear_start                  int64\nyear_end                    int64\nposition                   object\nheight                     object\nweight                    float64\nbirth_date                 object\ncollege                    object\nduration                    int64\ngreater_than_10             int64\nnew_birth_date     datetime64[ns]\ndtype: object</pre> In\u00a0[54]: Copied! <pre># calcular atributos como a\u00f1o, mes, dia, ...\ndf['year'] = df['new_birth_date'].dt.year\ndf['month'] = df['new_birth_date'].dt.month\ndf['day'] = df['new_birth_date'].dt.day\ndf['hour'] = df['new_birth_date'].dt.hour\ndf['minute'] = df['new_birth_date'].dt.minute\ndf['second'] = df['new_birth_date'].dt.second\ndf['microsecond'] = df['new_birth_date'].dt.microsecond\n\ndf.head()\n</pre> # calcular atributos como a\u00f1o, mes, dia, ... df['year'] = df['new_birth_date'].dt.year df['month'] = df['new_birth_date'].dt.month df['day'] = df['new_birth_date'].dt.day df['hour'] = df['new_birth_date'].dt.hour df['minute'] = df['new_birth_date'].dt.minute df['second'] = df['new_birth_date'].dt.second df['microsecond'] = df['new_birth_date'].dt.microsecond  df.head() Out[54]: name year_start year_end position height weight birth_date college duration greater_than_10 new_birth_date year month day hour minute second microsecond 0 Alaa Abdelnaby 1991 1995 F-C 6-10 NaN June 24, 1968 NaN 4 0 1968-06-24 1968.0 6.0 24.0 0.0 0.0 0.0 0.0 1 Alaa Abdelnaby 1991 1995 F-C 6-10 NaN June 24, 1968 NaN 4 0 1968-06-24 1968.0 6.0 24.0 0.0 0.0 0.0 0.0 2 Zaid Abdul-Aziz 1969 1978 C-F 6-9 235.0 April 7, 1946 Iowa State University 9 0 1946-04-07 1946.0 4.0 7.0 0.0 0.0 0.0 0.0 3 Kareem Abdul-Jabbar 1970 1989 C 7-2 225.0 April 16, 1947 University of California, Los Angeles 19 1 1947-04-16 1947.0 4.0 16.0 0.0 0.0 0.0 0.0 4 Mahmoud Abdul-Rauf 1991 2001 G 6-1 162.0 March 9, 1969 Louisiana State University 10 0 1969-03-09 1969.0 3.0 9.0 0.0 0.0 0.0 0.0 In\u00a0[55]: Copied! <pre># lista de columnas a eliminar\ncolumnas_a_eliminar = ['year', 'month', 'day', 'hour', 'minute', 'second', 'microsecond']\n\n# eliminar las columnas\ndf = df.drop(columnas_a_eliminar, axis=1)\n\n# mostrar dataframe\ndf.head()\n</pre> # lista de columnas a eliminar columnas_a_eliminar = ['year', 'month', 'day', 'hour', 'minute', 'second', 'microsecond']  # eliminar las columnas df = df.drop(columnas_a_eliminar, axis=1)  # mostrar dataframe df.head() Out[55]: name year_start year_end position height weight birth_date college duration greater_than_10 new_birth_date 0 Alaa Abdelnaby 1991 1995 F-C 6-10 NaN June 24, 1968 NaN 4 0 1968-06-24 1 Alaa Abdelnaby 1991 1995 F-C 6-10 NaN June 24, 1968 NaN 4 0 1968-06-24 2 Zaid Abdul-Aziz 1969 1978 C-F 6-9 235.0 April 7, 1946 Iowa State University 9 0 1946-04-07 3 Kareem Abdul-Jabbar 1970 1989 C 7-2 225.0 April 16, 1947 University of California, Los Angeles 19 1 1947-04-16 4 Mahmoud Abdul-Rauf 1991 2001 G 6-1 162.0 March 9, 1969 Louisiana State University 10 0 1969-03-09"},{"location":"lectures/data_manipulation/pd_01a/#pandas-i","title":"Pandas I\u00b6","text":""},{"location":"lectures/data_manipulation/pd_01a/#introduccion","title":"Introducci\u00f3n\u00b6","text":"<p>Pandas es un paquete de Python que proporciona estructuras de datos r\u00e1pidas, flexibles y expresivas dise\u00f1adas para que trabajar con datos \"relacionales\" o \"etiquetados\" sea f\u00e1cil e intuitivo.</p> <p>Su objetivo es ser el bloque de construcci\u00f3n fundamental de alto nivel para hacer an\u00e1lisis de datos pr\u00e1cticos del mundo real en Python. Adem\u00e1s, tiene el objetivo m\u00e1s amplio de convertirse en la herramienta de an\u00e1lisis/manipulaci\u00f3n de datos de c\u00f3digo abierto m\u00e1s potente y flexible disponible en cualquier idioma. Ya est\u00e1 en camino hacia este objetivo.</p>"},{"location":"lectures/data_manipulation/pd_01a/#pandas-dataframes","title":"Pandas Dataframes\u00b6","text":"<p>Como se mencina anteriormente, los dataframes son arreglos de series, los cuales pueden ser de distintos tipos (num\u00e9ricos, string, etc.). En esta parte mostraremos un ejemplo aplicado de las distintas funcionalidades de los dataframes.</p>"},{"location":"lectures/data_manipulation/pd_01a/#creacion-de-dataframes","title":"Creaci\u00f3n de Dataframes\u00b6","text":"<p>La creaci\u00f3n se puede hacer de variadas formas con listas, dictionarios , numpy array , entre otros.</p>"},{"location":"lectures/data_manipulation/pd_01a/#lectura-de-datos","title":"Lectura de Datos\u00b6","text":"<p>En general, cuando se trabajan con datos, estos se almacenan en alg\u00fan lugar y en alg\u00fan tipo de formato, por ejemplo:</p> <ul> <li><code>.txt</code></li> <li><code>.csv</code></li> <li><code>.xlsx</code></li> <li><code>.db</code></li> <li>etc.</li> </ul>"},{"location":"lectures/data_manipulation/pd_01a/#objetos-de-pandas","title":"Objetos de Pandas\u00b6","text":"<p>En un nivel muy b\u00e1sico, los objetos de Pandas se pueden considerar como versiones mejoradas de matrices de <code>numpy</code> en las que las filas y columnas se identifican con etiquetas en lugar de simples \u00edndices enteros.</p>"},{"location":"lectures/data_manipulation/pd_01a/#operaciones-en-pandas","title":"Operaciones en Pandas\u00b6","text":"<p>Las operaciones en Pandas se refieren a las acciones que se pueden realizar sobre los objetos de Pandas, como Series y DataFrames, para manipular, transformar y analizar datos.</p>"},{"location":"lectures/data_manipulation/pd_01a/#operaciones-basicas","title":"Operaciones B\u00e1sicas\u00b6","text":""},{"location":"lectures/data_manipulation/pd_01a/#operaciones-matematicas","title":"Operaciones Matem\u00e1ticas\u00b6","text":""},{"location":"lectures/data_manipulation/pd_01a/#operaciones-avanzadas","title":"Operaciones Avanzadas\u00b6","text":""},{"location":"lectures/data_manipulation/pd_01a/#filtrar-datos","title":"Filtrar Datos\u00b6","text":"<p>Para filtrar datos en Pandas, se utiliza el m\u00e9todo <code>loc()</code> o <code>iloc()</code>, dependiendo de si queremos filtrar por etiquetas de \u00edndice o por posici\u00f3n. Para efectos pr\u00e1cticos, utilizaremos solo <code>loc()</code>.</p>"},{"location":"lectures/data_manipulation/pd_01a/#valores-nulos-y-duplicados","title":"Valores Nulos y Duplicados\u00b6","text":""},{"location":"lectures/data_manipulation/pd_01a/#valores-nulos","title":"Valores Nulos\u00b6","text":"<p>Un valor nulo (o faltante) representa la ausencia de un valor en una celda espec\u00edfica de un DataFrame o una Serie. Los valores nulos pueden ocurrir por varias razones, como datos perdidos o no disponibles, errores de medici\u00f3n o problemas de entrada de datos.</p> <p>Los valores nulos se representan en Pandas mediante el objeto <code>NaN</code> (acr\u00f3nimo de \"Not a Number\"). <code>NaN</code> es un valor especial de punto flotante definido en el est\u00e1ndar IEEE para representar valores no definidos o indefinidos. En Pandas, los valores nulos se representan como <code>NaN</code> para las Series y DataFrames que utilizan datos num\u00e9ricos, y como <code>None</code> para las Series y DataFrames que utilizan datos no num\u00e9ricos.</p>"},{"location":"lectures/data_manipulation/pd_01a/#datos-duplicados","title":"Datos Duplicados\u00b6","text":"<p>En Pandas, se pueden manejar los datos duplicados utilizando el m\u00e9todo <code>duplicated()</code> y <code>drop_duplicates()</code>.</p> <ul> <li>El m\u00e9todo <code>duplicated()</code> devuelve un booleano que indica si una fila es duplicada o no, es decir, si existe otra fila con los mismos valores.</li> <li>El m\u00e9todo <code>drop_duplicates()</code> elimina las filas duplicadas de un DataFrame.</li> </ul>"},{"location":"lectures/data_manipulation/pd_01a/#manipulacion-de-fechas","title":"Manipulaci\u00f3n de Fechas\u00b6","text":"<p>Pandas se desarroll\u00f3 en el contexto del modelado financiero, por lo que, contiene varias herramientas para trabajar con fechas, horas y datos indexados por tiempo.</p> <p>Comenzaremos por entendender las herramientas para manejar fechas y horas en Python, antes de pasar m\u00e1s espec\u00edficamente a las herramientas proporcionadas por Pandas.</p>"},{"location":"lectures/data_manipulation/pd_01a/#referencias","title":"Referencias\u00b6","text":"<ol> <li>Python Pandas Tutorial: A Complete Introduction for Beginners</li> <li>General functions</li> </ol>"},{"location":"lectures/data_manipulation/pd_01b/","title":"Pandas II","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\n</pre> import pandas as pd import numpy as np In\u00a0[2]: Copied! <pre># cargar datos\npath = 'https://raw.githubusercontent.com/fralfaro/MAT281_2024/main/docs/lectures/data_manipulation/data/player_info.csv'\ndf = pd.read_csv(path, sep=\",\" ).dropna()\ndf['Decade'] = df['year_start'].apply(lambda x: '2000' if x&gt;=2000 else '1900')\ndf.head()\n</pre> # cargar datos path = 'https://raw.githubusercontent.com/fralfaro/MAT281_2024/main/docs/lectures/data_manipulation/data/player_info.csv' df = pd.read_csv(path, sep=\",\" ).dropna() df['Decade'] = df['year_start'].apply(lambda x: '2000' if x&gt;=2000 else '1900') df.head() Out[2]: name year_start year_end position height weight birth_date college Decade 2 Zaid Abdul-Aziz 1969 1978 C-F 6-9 235.0 April 7, 1946 Iowa State University 1900 3 Kareem Abdul-Jabbar 1970 1989 C 7-2 225.0 April 16, 1947 University of California, Los Angeles 1900 4 Mahmoud Abdul-Rauf 1991 2001 G 6-1 162.0 March 9, 1969 Louisiana State University 1900 5 Tariq Abdul-Wahad 1998 2003 F 6-6 223.0 November 3, 1974 San Jose State University 1900 6 Shareef Abdur-Rahim 1997 2008 F 6-9 225.0 December 11, 1976 University of California 1900 In\u00a0[3]: Copied! <pre># Agrupar por 'Decade' y calcular la suma de la columna 'Open' en cada grupo\nagrupado = df.groupby('position')['weight'].mean()\nagrupado\n</pre> # Agrupar por 'Decade' y calcular la suma de la columna 'Open' en cada grupo agrupado = df.groupby('position')['weight'].mean() agrupado Out[3]: <pre>position\nC      242.219212\nC-F    228.256158\nF      217.985857\nF-C    222.871866\nF-G    202.604878\nG      186.828115\nG-F    197.017857\nName: weight, dtype: float64</pre> In\u00a0[4]: Copied! <pre># Agrupar por 'Year','Month' y calcular la suma de la columna 'Open' en cada grupo\nagrupado = df.groupby(['Decade','position'])['weight'].mean()\nagrupado\n</pre> # Agrupar por 'Year','Month' y calcular la suma de la columna 'Open' en cada grupo agrupado = df.groupby(['Decade','position'])['weight'].mean() agrupado Out[4]: <pre>Decade  position\n1900    C           237.453642\n        C-F         224.871345\n        F           212.582933\n        F-C         217.908475\n        F-G         200.594444\n        G           182.689150\n        G-F         192.789062\n2000    C           256.057692\n        C-F         246.343750\n        F           230.135135\n        F-C         245.750000\n        F-G         217.080000\n        G           195.686192\n        G-F         210.550000\nName: weight, dtype: float64</pre> In\u00a0[5]: Copied! <pre># Agrupar por 'Year','Month' y calcular la suma,promedio de la columna 'Open' en cada grupo\nagrupado = df.groupby(['Decade','position']).agg({'weight': ['sum', 'mean']})\nagrupado\n</pre> # Agrupar por 'Year','Month' y calcular la suma,promedio de la columna 'Open' en cada grupo agrupado = df.groupby(['Decade','position']).agg({'weight': ['sum', 'mean']}) agrupado Out[5]: weight sum mean Decade position 1900 C 71711.0 237.453642 C-F 38453.0 224.871345 F 176869.0 212.582933 F-C 64283.0 217.908475 F-G 36107.0 200.594444 G 186891.0 182.689150 G-F 49354.0 192.789062 2000 C 26630.0 256.057692 C-F 7883.0 246.343750 F 85150.0 230.135135 F-C 15728.0 245.750000 F-G 5427.0 217.080000 G 93538.0 195.686192 G-F 16844.0 210.550000 In\u00a0[6]: Copied! <pre># Definimos una funci\u00f3n que calcula el promedio arm\u00f3nico\ndef promedio_arm\u00f3nico(datos):\n    n = len(datos)\n    suma_rec\u00edprocos = sum(1 / x for x in datos)\n    promedio_arm\u00f3nico = n / suma_rec\u00edprocos\n    return promedio_arm\u00f3nico\n\n# Aplicamos la funci\u00f3n\ndf.groupby(['Decade', 'position'])['weight'].apply(promedio_arm\u00f3nico)\n</pre> # Definimos una funci\u00f3n que calcula el promedio arm\u00f3nico def promedio_arm\u00f3nico(datos):     n = len(datos)     suma_rec\u00edprocos = sum(1 / x for x in datos)     promedio_arm\u00f3nico = n / suma_rec\u00edprocos     return promedio_arm\u00f3nico  # Aplicamos la funci\u00f3n df.groupby(['Decade', 'position'])['weight'].apply(promedio_arm\u00f3nico) Out[6]: <pre>Decade  position\n1900    C           235.811208\n        C-F         223.766067\n        F           211.205181\n        F-C         216.785001\n        F-G         199.669744\n        G           181.746894\n        G-F         191.847770\n2000    C           254.688414\n        C-F         245.646696\n        F           228.989635\n        F-C         244.951294\n        F-G         216.310153\n        G           194.579990\n        G-F         209.851256\nName: weight, dtype: float64</pre> In\u00a0[7]: Copied! <pre>df['mean_weight'] = df.groupby(['Decade','position'])['weight'].transform('mean')\ndf.head()\n</pre> df['mean_weight'] = df.groupby(['Decade','position'])['weight'].transform('mean') df.head() Out[7]: name year_start year_end position height weight birth_date college Decade mean_weight 2 Zaid Abdul-Aziz 1969 1978 C-F 6-9 235.0 April 7, 1946 Iowa State University 1900 224.871345 3 Kareem Abdul-Jabbar 1970 1989 C 7-2 225.0 April 16, 1947 University of California, Los Angeles 1900 237.453642 4 Mahmoud Abdul-Rauf 1991 2001 G 6-1 162.0 March 9, 1969 Louisiana State University 1900 182.689150 5 Tariq Abdul-Wahad 1998 2003 F 6-6 223.0 November 3, 1974 San Jose State University 1900 212.582933 6 Shareef Abdur-Rahim 1997 2008 F 6-9 225.0 December 11, 1976 University of California 1900 212.582933 In\u00a0[8]: Copied! <pre># cargar datos\npath = 'data/player_info.csv'\ndf = pd.read_csv(path, sep=\",\" ).dropna()\ndf.head()\n</pre> # cargar datos path = 'data/player_info.csv' df = pd.read_csv(path, sep=\",\" ).dropna() df.head() Out[8]: name year_start year_end position height weight birth_date college 2 Zaid Abdul-Aziz 1969 1978 C-F 6-9 235.0 April 7, 1946 Iowa State University 3 Kareem Abdul-Jabbar 1970 1989 C 7-2 225.0 April 16, 1947 University of California, Los Angeles 4 Mahmoud Abdul-Rauf 1991 2001 G 6-1 162.0 March 9, 1969 Louisiana State University 5 Tariq Abdul-Wahad 1998 2003 F 6-6 223.0 November 3, 1974 San Jose State University 6 Shareef Abdur-Rahim 1997 2008 F 6-9 225.0 December 11, 1976 University of California In\u00a0[9]: Copied! <pre># crear datos\ndf_concat1 = df.loc[lambda x: x['year_start']&lt;2000]\ndf_concat1.head()\n</pre> # crear datos df_concat1 = df.loc[lambda x: x['year_start']&lt;2000] df_concat1.head() Out[9]: name year_start year_end position height weight birth_date college 2 Zaid Abdul-Aziz 1969 1978 C-F 6-9 235.0 April 7, 1946 Iowa State University 3 Kareem Abdul-Jabbar 1970 1989 C 7-2 225.0 April 16, 1947 University of California, Los Angeles 4 Mahmoud Abdul-Rauf 1991 2001 G 6-1 162.0 March 9, 1969 Louisiana State University 5 Tariq Abdul-Wahad 1998 2003 F 6-6 223.0 November 3, 1974 San Jose State University 6 Shareef Abdur-Rahim 1997 2008 F 6-9 225.0 December 11, 1976 University of California In\u00a0[10]: Copied! <pre># crear datos\ndf_concat2 = df.loc[lambda x: x['year_start']&gt;=2000]\ndf_concat2.head()\n</pre> # crear datos df_concat2 = df.loc[lambda x: x['year_start']&gt;=2000] df_concat2.head() Out[10]: name year_start year_end position height weight birth_date college 11 Alex Acker 2006 2009 G 6-5 185.0 January 21, 1983 Pepperdine University 15 Quincy Acy 2013 2018 F 6-7 240.0 October 6, 1990 Baylor University 19 Hassan Adams 2007 2009 G 6-4 220.0 June 20, 1984 University of Arizona 20 Jordan Adams 2015 2016 G 6-5 209.0 July 8, 1994 University of California, Los Angeles 22 Steven Adams 2014 2018 C 7-0 255.0 July 20, 1993 University of Pittsburgh In\u00a0[11]: Copied! <pre># concatenar mismas columnas\nresult = pd.concat([df_concat1,df_concat2])\n\n# mostrar resultados\nresult\n</pre> # concatenar mismas columnas result = pd.concat([df_concat1,df_concat2])  # mostrar resultados result Out[11]: name year_start year_end position height weight birth_date college 2 Zaid Abdul-Aziz 1969 1978 C-F 6-9 235.0 April 7, 1946 Iowa State University 3 Kareem Abdul-Jabbar 1970 1989 C 7-2 225.0 April 16, 1947 University of California, Los Angeles 4 Mahmoud Abdul-Rauf 1991 2001 G 6-1 162.0 March 9, 1969 Louisiana State University 5 Tariq Abdul-Wahad 1998 2003 F 6-6 223.0 November 3, 1974 San Jose State University 6 Shareef Abdur-Rahim 1997 2008 F 6-9 225.0 December 11, 1976 University of California ... ... ... ... ... ... ... ... ... 4533 Cody Zeller 2014 2018 C-F 7-0 240.0 October 5, 1992 Indiana University 4537 Luke Zeller 2013 2013 C 6-11 245.0 April 7, 1987 University of Notre Dame 4538 Tyler Zeller 2013 2018 F-C 7-0 253.0 January 17, 1990 University of North Carolina 4543 Derrick Zimmerman 2006 2006 G 6-3 195.0 December 2, 1981 Mississippi State University 4544 Stephen Zimmerman 2017 2017 C 7-0 240.0 September 9, 1996 University of Nevada, Las Vegas <p>4212 rows \u00d7 8 columns</p> In\u00a0[12]: Copied! <pre># cambiar nombre \ndf_concat2 = df_concat2.rename(columns = {'birth_date':'birth'})\n\n# concatenar mismas columnas\nresult = pd.concat([df_concat2,df_concat1])\n\n# mostrar resultados\nresult\n</pre> # cambiar nombre  df_concat2 = df_concat2.rename(columns = {'birth_date':'birth'})  # concatenar mismas columnas result = pd.concat([df_concat2,df_concat1])  # mostrar resultados result Out[12]: name year_start year_end position height weight birth college birth_date 11 Alex Acker 2006 2009 G 6-5 185.0 January 21, 1983 Pepperdine University NaN 15 Quincy Acy 2013 2018 F 6-7 240.0 October 6, 1990 Baylor University NaN 19 Hassan Adams 2007 2009 G 6-4 220.0 June 20, 1984 University of Arizona NaN 20 Jordan Adams 2015 2016 G 6-5 209.0 July 8, 1994 University of California, Los Angeles NaN 22 Steven Adams 2014 2018 C 7-0 255.0 July 20, 1993 University of Pittsburgh NaN ... ... ... ... ... ... ... ... ... ... 4540 Phil Zevenbergen 1988 1988 C 6-10 230.0 NaN University of Washington April 13, 1964 4542 George Zidek 1996 1998 C 7-0 250.0 NaN University of California, Los Angeles August 2, 1973 4547 Jim Zoet 1983 1983 C 7-1 240.0 NaN Kent State University December 20, 1953 4548 Bill Zopf 1971 1971 G 6-1 170.0 NaN Duquesne University June 7, 1948 4550 Matt Zunic 1949 1949 G-F 6-3 195.0 NaN George Washington University December 19, 1919 <p>4212 rows \u00d7 9 columns</p> In\u00a0[13]: Copied! <pre># cargar datos\npath = 'data/player_info.csv'\ndf = pd.read_csv(path, sep=\",\" ).dropna()\ndf.head()\n</pre> # cargar datos path = 'data/player_info.csv' df = pd.read_csv(path, sep=\",\" ).dropna() df.head() Out[13]: name year_start year_end position height weight birth_date college 2 Zaid Abdul-Aziz 1969 1978 C-F 6-9 235.0 April 7, 1946 Iowa State University 3 Kareem Abdul-Jabbar 1970 1989 C 7-2 225.0 April 16, 1947 University of California, Los Angeles 4 Mahmoud Abdul-Rauf 1991 2001 G 6-1 162.0 March 9, 1969 Louisiana State University 5 Tariq Abdul-Wahad 1998 2003 F 6-6 223.0 November 3, 1974 San Jose State University 6 Shareef Abdur-Rahim 1997 2008 F 6-9 225.0 December 11, 1976 University of California <p>Por un columna</p> In\u00a0[14]: Copied! <pre># crear datos\ncols_merge1 = ['name', 'year_start', 'year_end', 'position']\ndf_merge1 = df[cols_merge1]\ndf_merge1.head()\n</pre> # crear datos cols_merge1 = ['name', 'year_start', 'year_end', 'position'] df_merge1 = df[cols_merge1] df_merge1.head() Out[14]: name year_start year_end position 2 Zaid Abdul-Aziz 1969 1978 C-F 3 Kareem Abdul-Jabbar 1970 1989 C 4 Mahmoud Abdul-Rauf 1991 2001 G 5 Tariq Abdul-Wahad 1998 2003 F 6 Shareef Abdur-Rahim 1997 2008 F In\u00a0[15]: Copied! <pre># crear datos\ncols_merge2 = ['name', 'height', 'weight','birth_date', 'college']\ndf_merge2 = df[cols_merge2]\ndf_merge2.head()\n</pre> # crear datos cols_merge2 = ['name', 'height', 'weight','birth_date', 'college'] df_merge2 = df[cols_merge2] df_merge2.head() Out[15]: name height weight birth_date college 2 Zaid Abdul-Aziz 6-9 235.0 April 7, 1946 Iowa State University 3 Kareem Abdul-Jabbar 7-2 225.0 April 16, 1947 University of California, Los Angeles 4 Mahmoud Abdul-Rauf 6-1 162.0 March 9, 1969 Louisiana State University 5 Tariq Abdul-Wahad 6-6 223.0 November 3, 1974 San Jose State University 6 Shareef Abdur-Rahim 6-9 225.0 December 11, 1976 University of California In\u00a0[16]: Copied! <pre># merge por una columna\nresult = pd.merge(df_merge1, df_merge2, on='name')\nresult.head()\n</pre> # merge por una columna result = pd.merge(df_merge1, df_merge2, on='name') result.head() Out[16]: name year_start year_end position height weight birth_date college 0 Zaid Abdul-Aziz 1969 1978 C-F 6-9 235.0 April 7, 1946 Iowa State University 1 Kareem Abdul-Jabbar 1970 1989 C 7-2 225.0 April 16, 1947 University of California, Los Angeles 2 Mahmoud Abdul-Rauf 1991 2001 G 6-1 162.0 March 9, 1969 Louisiana State University 3 Tariq Abdul-Wahad 1998 2003 F 6-6 223.0 November 3, 1974 San Jose State University 4 Shareef Abdur-Rahim 1997 2008 F 6-9 225.0 December 11, 1976 University of California <p>Por Varias columnas</p> In\u00a0[17]: Copied! <pre># crear datos\ncols_merge1 = ['name', 'year_start', 'year_end', 'position']\ndf_merge1 = df[cols_merge1]\ndf_merge1.head()\n</pre> # crear datos cols_merge1 = ['name', 'year_start', 'year_end', 'position'] df_merge1 = df[cols_merge1] df_merge1.head() Out[17]: name year_start year_end position 2 Zaid Abdul-Aziz 1969 1978 C-F 3 Kareem Abdul-Jabbar 1970 1989 C 4 Mahmoud Abdul-Rauf 1991 2001 G 5 Tariq Abdul-Wahad 1998 2003 F 6 Shareef Abdur-Rahim 1997 2008 F In\u00a0[18]: Copied! <pre># crear datos\ncols_merge2 = ['name', 'year_start', 'year_end', 'height', 'weight','birth_date', 'college']\ndf_merge2 = df[cols_merge2]\ndf_merge2.head()\n</pre> # crear datos cols_merge2 = ['name', 'year_start', 'year_end', 'height', 'weight','birth_date', 'college'] df_merge2 = df[cols_merge2] df_merge2.head() Out[18]: name year_start year_end height weight birth_date college 2 Zaid Abdul-Aziz 1969 1978 6-9 235.0 April 7, 1946 Iowa State University 3 Kareem Abdul-Jabbar 1970 1989 7-2 225.0 April 16, 1947 University of California, Los Angeles 4 Mahmoud Abdul-Rauf 1991 2001 6-1 162.0 March 9, 1969 Louisiana State University 5 Tariq Abdul-Wahad 1998 2003 6-6 223.0 November 3, 1974 San Jose State University 6 Shareef Abdur-Rahim 1997 2008 6-9 225.0 December 11, 1976 University of California In\u00a0[19]: Copied! <pre># merge varias columnas\nresult = pd.merge(df_merge1, df_merge2, on=['name', 'year_start', 'year_end'])\nresult.head()\n</pre> # merge varias columnas result = pd.merge(df_merge1, df_merge2, on=['name', 'year_start', 'year_end']) result.head() Out[19]: name year_start year_end position height weight birth_date college 0 Zaid Abdul-Aziz 1969 1978 C-F 6-9 235.0 April 7, 1946 Iowa State University 1 Kareem Abdul-Jabbar 1970 1989 C 7-2 225.0 April 16, 1947 University of California, Los Angeles 2 Mahmoud Abdul-Rauf 1991 2001 G 6-1 162.0 March 9, 1969 Louisiana State University 3 Tariq Abdul-Wahad 1998 2003 F 6-6 223.0 November 3, 1974 San Jose State University 4 Shareef Abdur-Rahim 1997 2008 F 6-9 225.0 December 11, 1976 University of California <p></p> <p></p> <p></p> <p></p> In\u00a0[20]: Copied! <pre># tipos de merge\ncols = ['name', 'year_start', 'year_end']\nmerge_left = pd.merge(df_merge1, df_merge2, on=cols, how= 'left')\nmerge_rigth  = pd.merge(df_merge1, df_merge2, on=cols, how= 'right')\nmerge_inner  = pd.merge(df_merge1, df_merge2, on=cols, how= 'inner')\nmerge_outer   = pd.merge(df_merge1, df_merge2, on=cols, how= 'outer')\n</pre> # tipos de merge cols = ['name', 'year_start', 'year_end'] merge_left = pd.merge(df_merge1, df_merge2, on=cols, how= 'left') merge_rigth  = pd.merge(df_merge1, df_merge2, on=cols, how= 'right') merge_inner  = pd.merge(df_merge1, df_merge2, on=cols, how= 'inner') merge_outer   = pd.merge(df_merge1, df_merge2, on=cols, how= 'outer') In\u00a0[21]: Copied! <pre>df.columns\n</pre> df.columns Out[21]: <pre>Index(['name', 'year_start', 'year_end', 'position', 'height', 'weight',\n       'birth_date', 'college'],\n      dtype='object')</pre> In\u00a0[22]: Copied! <pre># crear datos\ncols_merge1 = ['name', 'year_start', 'year_end', 'position' ]\ndf_merge1 = df[cols_merge1]\ndf_merge1.head()\n</pre> # crear datos cols_merge1 = ['name', 'year_start', 'year_end', 'position' ] df_merge1 = df[cols_merge1] df_merge1.head() Out[22]: name year_start year_end position 2 Zaid Abdul-Aziz 1969 1978 C-F 3 Kareem Abdul-Jabbar 1970 1989 C 4 Mahmoud Abdul-Rauf 1991 2001 G 5 Tariq Abdul-Wahad 1998 2003 F 6 Shareef Abdur-Rahim 1997 2008 F In\u00a0[23]: Copied! <pre># crear datos\ncols_merge2 = ['name', 'year_start', 'year_end', 'height' ]\ndf_merge2 = df[cols_merge2]\ndf_merge2.head()\n</pre> # crear datos cols_merge2 = ['name', 'year_start', 'year_end', 'height' ] df_merge2 = df[cols_merge2] df_merge2.head() Out[23]: name year_start year_end height 2 Zaid Abdul-Aziz 1969 1978 6-9 3 Kareem Abdul-Jabbar 1970 1989 7-2 4 Mahmoud Abdul-Rauf 1991 2001 6-1 5 Tariq Abdul-Wahad 1998 2003 6-6 6 Shareef Abdur-Rahim 1997 2008 6-9 In\u00a0[24]: Copied! <pre># merge llaves duplicadas\nresult = pd.merge(df_merge1, df_merge2, on=['name', 'year_start'])\nresult.head()\n</pre> # merge llaves duplicadas result = pd.merge(df_merge1, df_merge2, on=['name', 'year_start']) result.head() Out[24]: name year_start year_end_x position year_end_y height 0 Zaid Abdul-Aziz 1969 1978 C-F 1978 6-9 1 Kareem Abdul-Jabbar 1970 1989 C 1989 7-2 2 Mahmoud Abdul-Rauf 1991 2001 G 2001 6-1 3 Tariq Abdul-Wahad 1998 2003 F 2003 6-6 4 Shareef Abdur-Rahim 1997 2008 F 2008 6-9 In\u00a0[25]: Copied! <pre># cargar datos\npath = 'data/player_info.csv'\ndf = pd.read_csv(path, sep=\",\" ).dropna().drop_duplicates()\ndf['Decade'] = df['year_start'].apply(lambda x: '2000' if x&gt;=2000 else '1900')\ndf.head()\n</pre> # cargar datos path = 'data/player_info.csv' df = pd.read_csv(path, sep=\",\" ).dropna().drop_duplicates() df['Decade'] = df['year_start'].apply(lambda x: '2000' if x&gt;=2000 else '1900') df.head() Out[25]: name year_start year_end position height weight birth_date college Decade 2 Zaid Abdul-Aziz 1969 1978 C-F 6-9 235.0 April 7, 1946 Iowa State University 1900 3 Kareem Abdul-Jabbar 1970 1989 C 7-2 225.0 April 16, 1947 University of California, Los Angeles 1900 4 Mahmoud Abdul-Rauf 1991 2001 G 6-1 162.0 March 9, 1969 Louisiana State University 1900 5 Tariq Abdul-Wahad 1998 2003 F 6-6 223.0 November 3, 1974 San Jose State University 1900 6 Shareef Abdur-Rahim 1997 2008 F 6-9 225.0 December 11, 1976 University of California 1900 In\u00a0[26]: Copied! <pre># pivot: simple\nagrupado = df.groupby(['Decade','position'])['weight'].mean().reset_index()\npivot_df = agrupado.pivot(index='Decade', columns='position', values='weight')\npivot_df.head(10)\n</pre> # pivot: simple agrupado = df.groupby(['Decade','position'])['weight'].mean().reset_index() pivot_df = agrupado.pivot(index='Decade', columns='position', values='weight') pivot_df.head(10) Out[26]: position C C-F F F-C F-G G G-F Decade 1900 237.453642 224.871345 212.582933 217.908475 200.594444 182.689150 192.789062 2000 256.057692 246.343750 230.135135 245.750000 217.080000 195.686192 210.550000 In\u00a0[27]: Copied! <pre># pivot: multiple\nagrupado = df.groupby(['Decade','position','height'])['weight'].mean().fillna(0).astype(int).reset_index()\npivot_df = agrupado.pivot(index=['Decade','height'], columns='position', values='weight').fillna(0)\npivot_df.head(10)\n</pre> # pivot: multiple agrupado = df.groupby(['Decade','position','height'])['weight'].mean().fillna(0).astype(int).reset_index() pivot_df = agrupado.pivot(index=['Decade','height'], columns='position', values='weight').fillna(0) pivot_df.head(10) Out[27]: position C C-F F F-C F-G G G-F Decade height 1900 5-10 0.0 0.0 0.0 0.0 0.0 167.0 163.0 5-11 0.0 0.0 0.0 0.0 175.0 171.0 171.0 5-3 0.0 0.0 0.0 0.0 0.0 136.0 0.0 5-5 0.0 0.0 0.0 0.0 0.0 135.0 0.0 5-6 0.0 0.0 0.0 0.0 0.0 149.0 0.0 5-7 0.0 0.0 0.0 0.0 0.0 150.0 0.0 5-8 0.0 0.0 0.0 0.0 0.0 165.0 0.0 5-9 0.0 0.0 0.0 0.0 0.0 159.0 0.0 6-0 0.0 0.0 191.0 0.0 180.0 173.0 177.0 6-1 0.0 0.0 180.0 0.0 190.0 177.0 182.0 In\u00a0[28]: Copied! <pre># pivot_table: simple\npivot_df = df.pivot_table(index='Decade', columns='position', values='weight', aggfunc='mean')\npivot_df.head(10)\n</pre> # pivot_table: simple pivot_df = df.pivot_table(index='Decade', columns='position', values='weight', aggfunc='mean') pivot_df.head(10) Out[28]: position C C-F F F-C F-G G G-F Decade 1900 237.453642 224.871345 212.582933 217.908475 200.594444 182.689150 192.789062 2000 256.057692 246.343750 230.135135 245.750000 217.080000 195.686192 210.550000 In\u00a0[29]: Copied! <pre># pivot_table: multiple\npivot_df = df.pivot_table(index=['Decade','height'], columns='position', values='weight', aggfunc='mean').fillna(0)\npivot_df.head(10)\n</pre> # pivot_table: multiple pivot_df = df.pivot_table(index=['Decade','height'], columns='position', values='weight', aggfunc='mean').fillna(0) pivot_df.head(10) Out[29]: position C C-F F F-C F-G G G-F Decade height 1900 5-10 0.0 0.0 0.0 0.0 0.0 167.914286 163.333333 5-11 0.0 0.0 0.0 0.0 175.0 171.666667 171.666667 5-3 0.0 0.0 0.0 0.0 0.0 136.000000 0.000000 5-5 0.0 0.0 0.0 0.0 0.0 135.000000 0.000000 5-6 0.0 0.0 0.0 0.0 0.0 149.000000 0.000000 5-7 0.0 0.0 0.0 0.0 0.0 150.833333 0.000000 5-8 0.0 0.0 0.0 0.0 0.0 165.000000 0.000000 5-9 0.0 0.0 0.0 0.0 0.0 159.363636 0.000000 6-0 0.0 0.0 191.5 0.0 180.0 173.722772 177.500000 6-1 0.0 0.0 180.0 0.0 190.0 177.728916 182.333333 In\u00a0[30]: Copied! <pre>cols_index = ['name','year_start','year_end']\npivot_df = df.pivot_table(index=cols_index, columns='position', values='weight', aggfunc='mean').fillna(0).reset_index()\npivot_df.head()\n</pre> cols_index = ['name','year_start','year_end'] pivot_df = df.pivot_table(index=cols_index, columns='position', values='weight', aggfunc='mean').fillna(0).reset_index() pivot_df.head() Out[30]: position name year_start year_end C C-F F F-C F-G G G-F 0 A.C. Green 1986 2001 0.0 0.0 0.0 220.0 0.0 0.0 0.0 1 A.J. Bramlett 2000 2000 227.0 0.0 0.0 0.0 0.0 0.0 0.0 2 A.J. English 1991 1992 0.0 0.0 0.0 0.0 0.0 175.0 0.0 3 A.J. Guyton 2001 2003 0.0 0.0 0.0 0.0 0.0 180.0 0.0 4 A.J. Hammons 2017 2017 260.0 0.0 0.0 0.0 0.0 0.0 0.0 In\u00a0[31]: Copied! <pre># aplicar comando melt\ndf_melt = pd.melt(\n    df, \n    id_vars=cols_index, \n    var_name='Type',\n    value_name='Value'\n)\n\n\ndf_melt = df_melt.drop('Type',axis=1).rename(columns={'Value':'position'})\ndf_melt.head()\n</pre> # aplicar comando melt df_melt = pd.melt(     df,      id_vars=cols_index,      var_name='Type',     value_name='Value' )   df_melt = df_melt.drop('Type',axis=1).rename(columns={'Value':'position'}) df_melt.head() Out[31]: name year_start year_end position 0 Zaid Abdul-Aziz 1969 1978 C-F 1 Kareem Abdul-Jabbar 1970 1989 C 2 Mahmoud Abdul-Rauf 1991 2001 G 3 Tariq Abdul-Wahad 1998 2003 F 4 Shareef Abdur-Rahim 1997 2008 F"},{"location":"lectures/data_manipulation/pd_01b/#pandas-ii","title":"Pandas II\u00b6","text":""},{"location":"lectures/data_manipulation/pd_01b/#groupby","title":"Groupby\u00b6","text":"<p>Groupby es un concepto bastante simple. Podemos crear una agrupaci\u00f3n de categor\u00edas y aplicar una funci\u00f3n a las categor\u00edas.</p> <p>El proceso de groupby se puede resumiren los siguientes pasos:</p> <ul> <li>Divisi\u00f3n: es un proceso en el que dividimos los datos en grupos aplicando algunas condiciones en los conjuntos de datos.</li> <li>Aplicaci\u00f3n: es un proceso en el que aplicamos una funci\u00f3n a cada grupo de forma independiente</li> <li>Combinaci\u00f3n: es un proceso en el que combinamos diferentes conjuntos de datos despu\u00e9s de aplicar groupby y resultados en una estructura de datos</li> </ul> <p></p> <p>Despu\u00e9s de dividir los datos en un grupo, aplicamos una funci\u00f3n a cada grupo para realizar algunas operaciones que son:</p> <ul> <li>Agregaci\u00f3n: es un proceso en el que calculamos una estad\u00edstica resumida (o estad\u00edstica) sobre cada grupo. Por ejemplo, Calcular sumas de grupo o medios</li> <li>Transformaci\u00f3n: es un proceso en el que realizamos algunos c\u00e1lculos espec\u00edficos del grupo y devolvemos un \u00edndice similar. Por ejemplo, llenar NA dentro de grupos con un valor derivado de cada grupo</li> <li>Filtraci\u00f3n: es un proceso en el cual descartamos algunos grupos, de acuerdo con un c\u00e1lculo grupal que eval\u00faa Verdadero o Falso. Por ejemplo, Filtrar datos en funci\u00f3n de la suma o media grupal</li> </ul>"},{"location":"lectures/data_manipulation/pd_01b/#agrupar-por-una-columna","title":"Agrupar por una columna\u00b6","text":""},{"location":"lectures/data_manipulation/pd_01b/#agrupar-por-varias-columnas","title":"Agrupar por varias columnas\u00b6","text":""},{"location":"lectures/data_manipulation/pd_01b/#aplicar-multiples-funciones","title":"Aplicar m\u00faltiples funciones\u00b6","text":""},{"location":"lectures/data_manipulation/pd_01b/#groupby-apply","title":"Groupby Apply\u00b6","text":""},{"location":"lectures/data_manipulation/pd_01b/#groupby-transform","title":"Groupby Transform\u00b6","text":"<p>En pandas, el m\u00e9todo <code>transform()</code> permite aplicar una funci\u00f3n de transformaci\u00f3n a cada grupo de un objeto groupby. La funci\u00f3n de transformaci\u00f3n se aplica a cada grupo y el resultado se asigna de vuelta a las filas correspondientes en el DataFrame original.</p>"},{"location":"lectures/data_manipulation/pd_01b/#concat","title":"Concat\u00b6","text":"<p>La funci\u00f3n <code>concat()</code> realiza todo el trabajo pesado de realizar operaciones de concatenaci\u00f3n a lo largo de un eje mientras realiza la l\u00f3gica de conjunto opcional (uni\u00f3n o intersecci\u00f3n) de los \u00edndices (si los hay) en los otros ejes. Tenga en cuenta que digo \"si hay alguno\" porque solo hay un \u00fanico eje posible de concatenaci\u00f3n para Series.</p>"},{"location":"lectures/data_manipulation/pd_01b/#concatenar-varias-tablas-con-las-mismas-columnas","title":"Concatenar varias tablas con las mismas columnas\u00b6","text":""},{"location":"lectures/data_manipulation/pd_01b/#concatenar-varias-tablas-distintas-columnas","title":"Concatenar varias tablas distintas columnas\u00b6","text":""},{"location":"lectures/data_manipulation/pd_01b/#merge","title":"Merge\u00b6","text":"<p>La funci\u00f3n <code>merge()</code> se usa para combinar dos (o m\u00e1s) tablas sobre valores de columnas comunes (keys).</p> <p></p>"},{"location":"lectures/data_manipulation/pd_01b/#tipos-de-merge","title":"Tipos de merge\u00b6","text":"<p>La opci\u00f3n how especificica el tipo de cruce que se realizar\u00e1.</p> <ul> <li>left: usa las llaves solo de la tabla izquierda</li> <li>right: usa las llaves solo de la tabla derecha</li> <li>outer: usa las llaves de la uni\u00f3n de  ambas tablas.</li> <li>inner: usa las llaves de la intersecci\u00f3n de  ambas tablas.</li> </ul> <p></p>"},{"location":"lectures/data_manipulation/pd_01b/#problemas-de-llaves-duplicadas","title":"Problemas de llaves duplicadas\u00b6","text":"<p>Cuando se quiere realizar el cruce de dos tablas, pero an ambas tablas existe una columna (key) con el mismo nombre, para diferenciar la informaci\u00f3n entre la columna de una tabla y otra, pandas devulve el nombre de la columna con un gui\u00f3n bajo x (key_x) y otra con un gui\u00f3n bajo y (key_y)</p>"},{"location":"lectures/data_manipulation/pd_01b/#tipos-de-formatos","title":"Tipos de Formatos\u00b6","text":"<p>Dentro del mundo de los dataframe (o datos tabulares) existen dos formas de presentar la naturaleza de los datos: formato wide y formato long.</p> <p>Ejemplo, el siguiente conjunto de datos representa estad\u00edsticas de rendimiento para cuatro equipos (A, B, C y D) en un cierto contexto deportivo. Cada fila corresponde a un equipo y muestra tres medidas diferentes de rendimiento.</p> Team Points Assists Rebounds A 88 12 22 B 91 17 28 C 99 24 30 D 94 28 31 <p>La tabla as\u00ed presentada se encuentra en wide format, es decir, donde los valores se extienden a trav\u00e9s de las columnas.</p> <p>Ser\u00eda posible representar el mismo contenido anterior en long format, es decir, donde los mismos valores se indicaran a trav\u00e9s de las filas:</p> Team Variable Value A Points 88 A Assists 12 A Rebounds 22 B Points 91 B Assists 17 B Rebounds 28 C Points 99 C Assists 24 C Rebounds 30 D Points 94 D Assists 28 D Rebounds 31"},{"location":"lectures/data_manipulation/pd_01b/#formato-long-a-wide","title":"Formato long a wide\u00b6","text":"<p>El pivoteo de una tabla corresponde al paso de una tabla desde el formato long al formato wide. T\u00edpicamente esto se realiza para poder comparar los valores que se obtienen para alg\u00fan registro en particular, o para utilizar algunas herramientas de visualizaci\u00f3n b\u00e1sica que requieren dicho formato.</p> <p>En Pandas se utiliza los comandos <code>pivot</code> y <code>pivot_table</code>. Formato long a wide</p> <p>El pivoteo de una tabla corresponde al paso de una tabla desde el formato long al formato wide. T\u00edpicamente esto se realiza para poder comparar los valores que se obtienen para alg\u00fan registro en particular, o para utilizar algunas herramientas de visualizaci\u00f3n b\u00e1sica que requieren dicho formato.</p> <p>En Pandas se utiliza los comandos <code>pivot</code> y <code>pivot_table</code>.</p>"},{"location":"lectures/data_manipulation/pd_01b/#formato-wide-a-long","title":"Formato wide a long\u00b6","text":"<p>El despivotear una tabla corresponde al paso de una tabla desde el formato wide al formato long.</p> <p>Se reconocen dos situaciones:</p> <ol> <li>El valor indicado para la columna es \u00fanico, y s\u00f3lo se requiere definir correctamente las columnas.</li> <li>El valor indicado por la columna no es \u00fanico, y se requiere una iteraci\u00f3n m\u00e1s profunda.</li> </ol> <p>Para despivotear un dataframe en Pandas, utilizaremos el comando <code>melt</code>.</p>"},{"location":"lectures/data_manipulation/pd_01b/#referencias","title":"Referencias\u00b6","text":"<ul> <li>Groupby</li> <li>Merge, join, and concatenate</li> <li>Reshaping and pivot tables</li> </ul>"},{"location":"lectures/data_manipulation/pd_intro/","title":"Introducci\u00f3n","text":"<p>\u00bfEste formato de datos te parece familar?</p>"},{"location":"lectures/data_manipulation/pd_intro/#introduccion","title":"Introducci\u00f3n\u00b6","text":"<p>\u00bfTe imaginas como las grandes compa\u00f1\u00edas o gobiernos almacenan sus datos?. No, no es en un excel gigante en un pendrive.</p> <p>Es importante saber como modificar o crear valor a trav\u00e9s de distintas tablas de datos, por lo que esta clase se centrar\u00e1 en hacer esto, motivando a partir del uso de bases de datos relacionales.</p> <p>Una Base de Datos es un conjunto de datos almacenados en una computadora (generalmente un servidor, m\u00e1quina virtual, etc.) y que poseen una estructura tal que sean de f\u00e1cil acceso.</p>"},{"location":"lectures/data_manipulation/pd_intro/#base-de-datos-relacional","title":"Base de Datos Relacional\u00b6","text":"<p>Es el tipo de base de datos m\u00e1s ampliamente utilizado, aunque existen otros tipos de bases de datos para fines espec\u00edficos. Utiliza una estructura tal que es posible identificar y acceder a datos relacionados entre si. Generalmente una base de datos relacional est\u00e1 organizada en tablas.</p> <p>Las tablas est\u00e1n conformadas de filas y columnas. Cada columna posee un nombre y tiene un tipo de dato espec\u00edfico, mientras que las filas son registros almacenados.</p> <p>Por ejemplo, la siguiente tabla tiene tres columnas y cuatro registros. En particular, la columna <code>age</code> tiene tipo <code>INTEGER</code> y las otras dos tipo <code>STRING</code>.</p> <p></p>"},{"location":"lectures/data_manipulation/pd_intro/#que-es-sql","title":"\u00bfQu\u00e9 es SQL?\u00b6","text":"<p>Sus siglas significan Structured Query Language (Lenguaje de Consulta Estructurada) es un lenguaje de programaci\u00f3n utilizado para comunicarse con datos almacenados en un Sistema de Gesti\u00f3n de Bases de Datos Relacionales (Relational Database Management System o RDBMS). Posee una sintaxis muy similar al idioma ingl\u00e9s, con lo cual se hace relativamente f\u00e1cil de escribir, leer e interpretar.</p> <p>Hay distintos RDBMS entre los cuales la sintaxis de SQL difiere ligeramente. Los m\u00e1s populares son:</p> <ul> <li>SQLite</li> <li>MySQL / MariaDB</li> <li>PostgreSQL</li> <li>Oracle DB</li> <li>SQL Server</li> </ul> <p>En una empresa de tecnolog\u00eda hay cargos especialmente destinados a todo lo que tenga que ver con bases de datos, por ejemplo: creaci\u00f3n, mantenci\u00f3n, actualizaci\u00f3n, obtenci\u00f3n de datos, transformaci\u00f3n, seguridad y un largo etc.</p> <p>Los matem\u00e1ticos en la industria suelen tener cargos como Data Scientist, Data Analyst, Data Statistician, Data X (reemplace X con algo fancy tal de formar un cargo que quede bien en Linkedin), en donde lo importante es otorgar valor a estos datos. Por ende, lo m\u00ednimo que deben satisfacer es:</p> <ul> <li>Entendimiento casi total del modelo de datos (tablas, relaciones, tipos, etc.)</li> <li>Seleccionar datos a medida (queries).</li> </ul>"},{"location":"lectures/data_manipulation/pd_intro/#modelo-de-datos","title":"Modelo de datos\u00b6","text":"<p>Es la forma en que se organizan los datos. En las bases de datos incluso es posible conocer las relaciones entre tablas. A menudo se presentan gr\u00e1ficamente como en la imagen de abajo (esta ser\u00e1 la base de datos que utilizaremos en los ejericios del d\u00eda de</p> <p></p> <p>Esta base de datos se conoce con el nombre de chinook database. La descripci\u00f3n y las im\u00e1genes se pueden encontrar aqu\u00ed.</p> <p>En la figura anterior, existen algunas columnas especiales con una llave al lado de su nombre. \u00bfQu\u00e9 crees que significan?</p> <p>Las 11 tablas se definen de la siguiente forma (en ingl\u00e9s):</p> <ul> <li><code>employees</code> table stores employees data such as employee id, last name, first name, etc. It also has a field named ReportsTo to specify who reports to whom.</li> <li><code>customers</code> table stores customers data.</li> <li><code>invoices</code> &amp; <code>invoice_items</code> tables: these two tables store invoice data. The <code>invoices</code> table stores invoice header data and the <code>invoice_items</code> table stores the invoice line items data.</li> <li><code>artists</code> table stores artists data. It is a simple table that contains only artist id and name.</li> <li><code>albums</code> table stores data about a list of tracks. Each album belongs to one artist. However, one artist may have multiple albums.</li> <li><code>media_types</code> table stores media types such as MPEG audio file, ACC audio file, etc.</li> <li><code>genres</code> table stores music types such as rock, jazz, metal, etc.</li> <li><code>tracks</code> table store the data of songs. Each track belongs to one album.</li> <li><code>playlists</code> &amp; <code>playlist_track tables</code>: <code>playlists</code> table store data about playlists. Each playlist contains a list of tracks. Each track may belong to multiple playlists. The relationship between the <code>playlists</code> table and <code>tracks</code> table is many-to-many. The <code>playlist_track</code> table is used to reflect this relationship.</li> </ul>"},{"location":"lectures/data_manipulation/sc_01/","title":"Numpy","text":"<p>Diferencias Tiempo - Memoria</p> <p>Los principales beneficios del uso de matrices NumPy deber\u00edan ser un menor consumo de memoria y un mejor comportamiento en tiempo de ejecuci\u00f3n.</p> <p>Para las listas de Python podemos concluir de esto que para cada elemento nuevo, necesitamos otros ocho bytes para la referencia al nuevo objeto. El nuevo objeto entero en s\u00ed consume 28 bytes. El tama\u00f1o de una lista lst sin el tama\u00f1o de los elementos se puede calcular con:</p> <pre>64 + 8 * len (lst) + + len (lst) * 28\n</pre> <p></p> <p>Los array NumPy ocupan menos espacio. Esto significa que una matriz entera arbitraria de longitud n en necesidades numpy se calcula por:</p> <pre>96 + n * 8 bytes\n</pre> <p></p> <p>Veamos un ejemplo:</p> In\u00a0[1]: Copied! <pre>import numpy as np \nimport time\nimport sys\n\ndef arreglo_python(n):\n    t1 = time.time()\n    X = range(n)\n    Y = range(n)\n    Z = [X[i] + Y[i] for i in range(len(X)) ]\n    return (time.time() - t1,sys.getsizeof(Z) )\n\ndef arreglo_numpy(n):\n    t1 = time.time()\n    X = np.arange(n)\n    Y = np.arange(n)\n    Z = X + Y\n    return (time.time() - t1,sys.getsizeof(Z) )\n</pre> import numpy as np  import time import sys  def arreglo_python(n):     t1 = time.time()     X = range(n)     Y = range(n)     Z = [X[i] + Y[i] for i in range(len(X)) ]     return (time.time() - t1,sys.getsizeof(Z) )  def arreglo_numpy(n):     t1 = time.time()     X = np.arange(n)     Y = np.arange(n)     Z = X + Y     return (time.time() - t1,sys.getsizeof(Z) ) In\u00a0[2]: Copied! <pre># generar varios casos\nfor size_of_vec in [10,100,1000,10000,100000,1000000]:\n    print(f\"size of vector: {size_of_vec}\")\n    \n    #list vs numpy\n    t1, size1 = arreglo_python(size_of_vec)\n    t2, size2 = arreglo_numpy(size_of_vec)\n    \n    # resultados\n    print(f\"python list -- time: {round(t1,8)} seg, size: {size1} bytes\")\n    print(f\"numpy array -- time: {round(t2,8)} seg, size: {size2} bytes\\n\")\n</pre> # generar varios casos for size_of_vec in [10,100,1000,10000,100000,1000000]:     print(f\"size of vector: {size_of_vec}\")          #list vs numpy     t1, size1 = arreglo_python(size_of_vec)     t2, size2 = arreglo_numpy(size_of_vec)          # resultados     print(f\"python list -- time: {round(t1,8)} seg, size: {size1} bytes\")     print(f\"numpy array -- time: {round(t2,8)} seg, size: {size2} bytes\\n\") <pre>size of vector: 10\npython list -- time: 0.0 seg, size: 184 bytes\nnumpy array -- time: 0.0 seg, size: 152 bytes\n\nsize of vector: 100\npython list -- time: 0.0 seg, size: 904 bytes\nnumpy array -- time: 0.0 seg, size: 512 bytes\n\nsize of vector: 1000\npython list -- time: 0.0 seg, size: 9016 bytes\nnumpy array -- time: 0.0 seg, size: 4112 bytes\n\nsize of vector: 10000\npython list -- time: 0.002177 seg, size: 87616 bytes\nnumpy array -- time: 0.00052881 seg, size: 40112 bytes\n\nsize of vector: 100000\npython list -- time: 0.01605535 seg, size: 824456 bytes\nnumpy array -- time: 0.0 seg, size: 400112 bytes\n\nsize of vector: 1000000\npython list -- time: 0.14644384 seg, size: 8697456 bytes\nnumpy array -- time: 0.00261927 seg, size: 4000112 bytes\n\n</pre> In\u00a0[3]: Copied! <pre># Crear un array 1D de longitud 3\narr_1d = np.array([1, 2, 3])\nprint(arr_1d)\n</pre> # Crear un array 1D de longitud 3 arr_1d = np.array([1, 2, 3]) print(arr_1d) <pre>[1 2 3]\n</pre> In\u00a0[4]: Copied! <pre># Crear un array 2D de tama\u00f1o 2x3\narr_2d = np.array([\n    [1, 2, 3], \n    [4, 5, 6]\n])\nprint(arr_2d)\n</pre> # Crear un array 2D de tama\u00f1o 2x3 arr_2d = np.array([     [1, 2, 3],      [4, 5, 6] ]) print(arr_2d) <pre>[[1 2 3]\n [4 5 6]]\n</pre> In\u00a0[5]: Copied! <pre># Crear un array 3D de tama\u00f1o 2x2x3\narr_3d = np.array([\n    [\n        [1, 2, 3], \n        [4, 5, 6]\n    ], \n    [\n        [7, 8, 9],\n        [10, 11, 12]\n    ]\n])\nprint(arr_3d)\n</pre> # Crear un array 3D de tama\u00f1o 2x2x3 arr_3d = np.array([     [         [1, 2, 3],          [4, 5, 6]     ],      [         [7, 8, 9],         [10, 11, 12]     ] ]) print(arr_3d) <pre>[[[ 1  2  3]\n  [ 4  5  6]]\n\n [[ 7  8  9]\n  [10 11 12]]]\n</pre> <p>Veamos algunos atributos de los arreglos en Numpy:</p> <ul> <li><code>ndarray.shape</code>: devuelve la forma (dimensiones) del array.</li> <li><code>ndarray.ndim</code>: devuelve el n\u00famero de dimensiones del array.</li> <li><code>ndarray.size</code>: devuelve el n\u00famero total de elementos en el array.</li> <li><code>ndarray.dtype</code>: devuelve el tipo de datos de los elementos del array.</li> </ul> In\u00a0[6]: Copied! <pre># crear objeto\nobj_numpy =np.array(\n    [1, 2, 3, 4, 5, 6, 7, 8, 9])\n</pre> # crear objeto obj_numpy =np.array(     [1, 2, 3, 4, 5, 6, 7, 8, 9]) In\u00a0[7]: Copied! <pre># shape\nobj_numpy.shape\n</pre> # shape obj_numpy.shape Out[7]: <pre>(9,)</pre> In\u00a0[8]: Copied! <pre># ndim\nobj_numpy.ndim\n</pre> # ndim obj_numpy.ndim Out[8]: <pre>1</pre> In\u00a0[9]: Copied! <pre># size\nobj_numpy.size\n</pre> # size obj_numpy.size Out[9]: <pre>9</pre> In\u00a0[10]: Copied! <pre># dtype\nobj_numpy.dtype\n</pre> # dtype obj_numpy.dtype Out[10]: <pre>dtype('int32')</pre> In\u00a0[11]: Copied! <pre>import numpy as np\n\n# Crear dos arrays\na = np.array([1, 2, 3])\nb = np.array([4, 5, 6])\n</pre> import numpy as np  # Crear dos arrays a = np.array([1, 2, 3]) b = np.array([4, 5, 6]) In\u00a0[12]: Copied! <pre># Suma de arrays\nc = a + b\nprint(c)\n</pre> # Suma de arrays c = a + b print(c) <pre>[5 7 9]\n</pre> In\u00a0[13]: Copied! <pre># Resta de arrays\nc = a - b\nprint(c)\n</pre> # Resta de arrays c = a - b print(c) <pre>[-3 -3 -3]\n</pre> In\u00a0[14]: Copied! <pre># Multiplicaci\u00f3n de arrays\nc = a * b\nprint(c)\n</pre> # Multiplicaci\u00f3n de arrays c = a * b print(c) <pre>[ 4 10 18]\n</pre> In\u00a0[15]: Copied! <pre># Divisi\u00f3n de arrays\nc = a / b\nprint(c)\n</pre> # Divisi\u00f3n de arrays c = a / b print(c) <pre>[0.25 0.4  0.5 ]\n</pre> In\u00a0[16]: Copied! <pre># Crear dos arrays\na = np.array([1, 2, 3])\nb = np.array([2, 2, 4])\n</pre> # Crear dos arrays a = np.array([1, 2, 3]) b = np.array([2, 2, 4]) In\u00a0[17]: Copied! <pre># Comparaci\u00f3n de arrays\nc = a == b\nprint(c)\n</pre> # Comparaci\u00f3n de arrays c = a == b print(c) <pre>[False  True False]\n</pre> In\u00a0[18]: Copied! <pre># Comparaci\u00f3n de arrays\nc = a &gt; b\nprint(c)\n</pre> # Comparaci\u00f3n de arrays c = a &gt; b print(c) <pre>[False False False]\n</pre> In\u00a0[19]: Copied! <pre># Comparaci\u00f3n de arrays\nc = a &lt;= b\nprint(c)\n</pre> # Comparaci\u00f3n de arrays c = a &lt;= b print(c) <pre>[ True  True  True]\n</pre> In\u00a0[20]: Copied! <pre># Crear un array\na = np.array([1, 2, 3, 4, 5])\n\n# Suma de los elementos en un array\nsum_a = np.sum(a)\nprint(f\"Suma de elementos: {sum_a}\")\n</pre> # Crear un array a = np.array([1, 2, 3, 4, 5])  # Suma de los elementos en un array sum_a = np.sum(a) print(f\"Suma de elementos: {sum_a}\") <pre>Suma de elementos: 15\n</pre> In\u00a0[21]: Copied! <pre># M\u00ednimo y m\u00e1ximo de los elementos en un array\nmin_a = np.min(a)\nmax_a = np.max(a)\n\nprint(f\"minimo: {min_a}\")\nprint(f\"maximo: {max_a}\")\n</pre> # M\u00ednimo y m\u00e1ximo de los elementos en un array min_a = np.min(a) max_a = np.max(a)  print(f\"minimo: {min_a}\") print(f\"maximo: {max_a}\") <pre>minimo: 1\nmaximo: 5\n</pre> In\u00a0[22]: Copied! <pre># Promedio y desviaci\u00f3n est\u00e1ndar de los elementos en un array\nmean_a = np.mean(a)\nstd_a = np.std(a)\n\nprint(f\"promedio: {mean_a}\")\nprint(f\"desviacion estandar: {std_a}\")\n</pre> # Promedio y desviaci\u00f3n est\u00e1ndar de los elementos en un array mean_a = np.mean(a) std_a = np.std(a)  print(f\"promedio: {mean_a}\") print(f\"desviacion estandar: {std_a}\") <pre>promedio: 3.0\ndesviacion estandar: 1.4142135623730951\n</pre> <p>Acceder a elementos individuales de un array unidimensional</p> In\u00a0[23]: Copied! <pre># Crear un array unidimensional\na = np.array([1, 2, 3, 4, 5])\n</pre> # Crear un array unidimensional a = np.array([1, 2, 3, 4, 5]) In\u00a0[24]: Copied! <pre># Acceder al primer elemento\nprint(a[0])\n</pre> # Acceder al primer elemento print(a[0]) <pre>1\n</pre> In\u00a0[25]: Copied! <pre># Acceder al \u00faltimo elemento\nprint(a[-1])\n</pre> # Acceder al \u00faltimo elemento print(a[-1]) <pre>5\n</pre> In\u00a0[26]: Copied! <pre># Acceder al tercer elemento\nprint(a[2])\n</pre> # Acceder al tercer elemento print(a[2]) <pre>3\n</pre> <p>Acceder a elementos individuales de un array multidimensional</p> In\u00a0[27]: Copied! <pre># Crear un array bidimensional\nb = np.array([\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]]\n)\n</pre> # Crear un array bidimensional b = np.array([     [1, 2, 3],     [4, 5, 6],     [7, 8, 9]] ) In\u00a0[28]: Copied! <pre># Acceder al primer elemento\nprint(b[0, 0])\n</pre> # Acceder al primer elemento print(b[0, 0]) <pre>1\n</pre> In\u00a0[29]: Copied! <pre># Acceder al \u00faltimo elemento\nprint(b[-1, -1])\n</pre> # Acceder al \u00faltimo elemento print(b[-1, -1]) <pre>9\n</pre> In\u00a0[30]: Copied! <pre># Acceder al elemento en la segunda fila y tercer columna\nprint(b[1, 2])\n</pre> # Acceder al elemento en la segunda fila y tercer columna print(b[1, 2]) <pre>6\n</pre> <p>Rebanar un array unidimensional</p> In\u00a0[31]: Copied! <pre># Rebanar un array unidimensional\na = np.array([1, 2, 3, 4, 5])\n</pre> # Rebanar un array unidimensional a = np.array([1, 2, 3, 4, 5]) In\u00a0[32]: Copied! <pre># Rebanar los primeros tres elementos\nprint(a[:3])\n</pre> # Rebanar los primeros tres elementos print(a[:3]) <pre>[1 2 3]\n</pre> In\u00a0[33]: Copied! <pre># Rebanar los \u00faltimos dos elementos\nprint(a[-2:])\n</pre> # Rebanar los \u00faltimos dos elementos print(a[-2:]) <pre>[4 5]\n</pre> In\u00a0[34]: Copied! <pre># Rebanar todos los elementos saltando de dos en dos\nprint(a[::2])\n</pre> # Rebanar todos los elementos saltando de dos en dos print(a[::2]) <pre>[1 3 5]\n</pre> <p>Rebanar un array multidimensional</p> In\u00a0[35]: Copied! <pre># Rebanar un array bidimensional\nb = np.array([\n    [1, 2, 3], \n    [4, 5, 6], \n    [7, 8, 9]]\n)\n</pre> # Rebanar un array bidimensional b = np.array([     [1, 2, 3],      [4, 5, 6],      [7, 8, 9]] ) In\u00a0[36]: Copied! <pre># Rebanar la primera y segunda fila\nprint(b[:2, :])\n</pre> # Rebanar la primera y segunda fila print(b[:2, :]) <pre>[[1 2 3]\n [4 5 6]]\n</pre> In\u00a0[37]: Copied! <pre># Rebanar la segunda y tercera columna\nprint(b[:, 1:])\n</pre> # Rebanar la segunda y tercera columna print(b[:, 1:]) <pre>[[2 3]\n [5 6]\n [8 9]]\n</pre> In\u00a0[38]: Copied! <pre># Rebanar la diagonal principal\nprint(b.diagonal())\n</pre> # Rebanar la diagonal principal print(b.diagonal()) <pre>[1 5 9]\n</pre> <p>Vectores/Matrices especializadas</p> In\u00a0[39]: Copied! <pre># Arreglo de ceros: np.zeros(shape)\nprint(\"Zeros:\")\nprint( np.zeros((3,3)) )\n</pre> # Arreglo de ceros: np.zeros(shape) print(\"Zeros:\") print( np.zeros((3,3)) ) <pre>Zeros:\n[[0. 0. 0.]\n [0. 0. 0.]\n [0. 0. 0.]]\n</pre> In\u00a0[40]: Copied! <pre># Arreglos de uno: np.ones(shape)\nprint(\"\\nOnes:\")\nprint( np.ones((3,3)) )\n</pre> # Arreglos de uno: np.ones(shape) print(\"\\nOnes:\") print( np.ones((3,3)) ) <pre>\nOnes:\n[[1. 1. 1.]\n [1. 1. 1.]\n [1. 1. 1.]]\n</pre> In\u00a0[41]: Copied! <pre># Arreglo vacio: np.empty(shape)\nprint(\"\\nEmpty:\")\nprint( np.empty([2, 2]) )\n</pre> # Arreglo vacio: np.empty(shape) print(\"\\nEmpty:\") print( np.empty([2, 2]) ) <pre>\nEmpty:\n[[2.12199579e-314 4.67296746e-307]\n [7.09478267e-321 3.79442416e-321]]\n</pre> In\u00a0[42]: Copied! <pre># Rango de valores: np.range(start, stop, step)\nprint(\"\\nRange:\")\nnp.arange(0., 10., 1.)\n</pre> # Rango de valores: np.range(start, stop, step) print(\"\\nRange:\") np.arange(0., 10., 1.)  <pre>\nRange:\n</pre> Out[42]: <pre>array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])</pre> In\u00a0[43]: Copied! <pre># Grilla de valores: np.linspace(start, end, n_values)\nprint(\"\\nRegular grid:\")\nprint( np.linspace(0., 1., 9) )\n</pre> # Grilla de valores: np.linspace(start, end, n_values) print(\"\\nRegular grid:\") print( np.linspace(0., 1., 9) ) <pre>\nRegular grid:\n[0.    0.125 0.25  0.375 0.5   0.625 0.75  0.875 1.   ]\n</pre> In\u00a0[44]: Copied! <pre># fijar semilla\nnp.random.seed(42)\n\n# Sequencia de valores aleatorios: np.random\nprint(\"\\nRandom sequences:\")\nprint( np.random.uniform(10, size=6) )\n</pre> # fijar semilla np.random.seed(42)  # Sequencia de valores aleatorios: np.random print(\"\\nRandom sequences:\") print( np.random.uniform(10, size=6) ) <pre>\nRandom sequences:\n[6.62913893 1.44357124 3.41205452 4.61207364 8.59583224 8.59604932]\n</pre> <p>Operaciones con Matrices</p> In\u00a0[45]: Copied! <pre># crear matrices\n\nA = np.array([\n    [1,2,3],\n    [4,5,6],\n    [7,8,9]\n])\n\nB = np.array([\n    [9,8,7],\n    [6,5,4],\n    [3,2,1]]\n)\n\nprint(f\"Matrix A: \\n {A}\\n\")\nprint(f\"Matrix B: \\n {B}\")\n</pre> # crear matrices  A = np.array([     [1,2,3],     [4,5,6],     [7,8,9] ])  B = np.array([     [9,8,7],     [6,5,4],     [3,2,1]] )  print(f\"Matrix A: \\n {A}\\n\") print(f\"Matrix B: \\n {B}\") <pre>Matrix A: \n [[1 2 3]\n [4 5 6]\n [7 8 9]]\n\nMatrix B: \n [[9 8 7]\n [6 5 4]\n [3 2 1]]\n</pre> In\u00a0[46]: Copied! <pre># sumar dos matrices\nprint(\"Sum:\")\nprint( A+B )\n</pre> # sumar dos matrices print(\"Sum:\") print( A+B ) <pre>Sum:\n[[10 10 10]\n [10 10 10]\n [10 10 10]]\n</pre> In\u00a0[47]: Copied! <pre># restar dos matrices\nprint(\"\\nSubtraction\")\nprint( A-B )\n</pre> # restar dos matrices print(\"\\nSubtraction\") print( A-B ) <pre>\nSubtraction\n[[-8 -6 -4]\n [-2  0  2]\n [ 4  6  8]]\n</pre> In\u00a0[48]: Copied! <pre># producto uno a uno\nprint(\"\\nProduct\")\nprint( A*B )\n</pre> # producto uno a uno print(\"\\nProduct\") print( A*B ) <pre>\nProduct\n[[ 9 16 21]\n [24 25 24]\n [21 16  9]]\n</pre> In\u00a0[49]: Copied! <pre># producto matricial\nprint(\"\\nMatricial Product\")\nprint( np.dot(A,B) )\n</pre> # producto matricial print(\"\\nMatricial Product\") print( np.dot(A,B) ) <pre>\nMatricial Product\n[[ 30  24  18]\n [ 84  69  54]\n [138 114  90]]\n</pre> In\u00a0[50]: Copied! <pre># potencia\nprint(\"\\n Power\")\nprint( A**2 )\n</pre> # potencia print(\"\\n Power\") print( A**2 ) <pre>\n Power\n[[ 1  4  9]\n [16 25 36]\n [49 64 81]]\n</pre> <p>Algunas funciones especiales</p> In\u00a0[51]: Copied! <pre># crear matriz\nA = np.array([\n    [1,2,3],\n    [4,5,6],\n    [7,8,9]\n])\n</pre> # crear matriz A = np.array([     [1,2,3],     [4,5,6],     [7,8,9] ]) In\u00a0[52]: Copied! <pre>print(\"funcion exponencial\")\nprint( np.exp(A) )\n</pre> print(\"funcion exponencial\") print( np.exp(A) ) <pre>funcion exponencial\n[[2.71828183e+00 7.38905610e+00 2.00855369e+01]\n [5.45981500e+01 1.48413159e+02 4.03428793e+02]\n [1.09663316e+03 2.98095799e+03 8.10308393e+03]]\n</pre> In\u00a0[53]: Copied! <pre>print(\"funcion seno\")\nprint( np.sin(A) )\n</pre> print(\"funcion seno\") print( np.sin(A) ) <pre>funcion seno\n[[ 0.84147098  0.90929743  0.14112001]\n [-0.7568025  -0.95892427 -0.2794155 ]\n [ 0.6569866   0.98935825  0.41211849]]\n</pre> In\u00a0[54]: Copied! <pre>print(\"funcion coseno\")\nprint( np.cos(A))\n</pre> print(\"funcion coseno\") print( np.cos(A)) <pre>funcion coseno\n[[ 0.54030231 -0.41614684 -0.9899925 ]\n [-0.65364362  0.28366219  0.96017029]\n [ 0.75390225 -0.14550003 -0.91113026]]\n</pre> In\u00a0[55]: Copied! <pre>print(\"funcion tangente\")\nprint( np.tan(A) )\n</pre> print(\"funcion tangente\") print( np.tan(A) ) <pre>funcion tangente\n[[ 1.55740772 -2.18503986 -0.14254654]\n [ 1.15782128 -3.38051501 -0.29100619]\n [ 0.87144798 -6.79971146 -0.45231566]]\n</pre> <p>Operaciones \u00e1lgebra l\u00edneal</p> In\u00a0[56]: Copied! <pre># crear matriz\nA = np.array([[1,2],\n              [3,4]])\n</pre> # crear matriz A = np.array([[1,2],               [3,4]]) In\u00a0[57]: Copied! <pre># matriz transpuesta\nprint(\"Transpose: \")\nprint( A.T )\n</pre> # matriz transpuesta print(\"Transpose: \") print( A.T ) <pre>Transpose: \n[[1 3]\n [2 4]]\n</pre> In\u00a0[58]: Copied! <pre># determinante\nprint(\"determinant\")\nprint( round(np.linalg.det(A) ,2))\n</pre> # determinante print(\"determinant\") print( round(np.linalg.det(A) ,2)) <pre>determinant\n-2.0\n</pre> In\u00a0[59]: Copied! <pre># Matriz Inversa\nprint(\"Inverse\")\nprint( np.linalg.inv(A) )\n</pre> # Matriz Inversa print(\"Inverse\") print( np.linalg.inv(A) ) <pre>Inverse\n[[-2.   1. ]\n [ 1.5 -0.5]]\n</pre> In\u00a0[60]: Copied! <pre># traza\nprint(\"Trace\")\nprint( np.trace(A))\n</pre> # traza print(\"Trace\") print( np.trace(A)) <pre>Trace\n5\n</pre> In\u00a0[61]: Copied! <pre># Valores y vectores propios\neigenvalues, eigenvectors = np.linalg.eig(A) \n\nprint(\"eigenvalues\")\nprint( eigenvalues )\nprint(\"\\neigenvectors\")\nprint( eigenvectors )\n</pre> # Valores y vectores propios eigenvalues, eigenvectors = np.linalg.eig(A)   print(\"eigenvalues\") print( eigenvalues ) print(\"\\neigenvectors\") print( eigenvectors ) <pre>eigenvalues\n[-0.37228132  5.37228132]\n\neigenvectors\n[[-0.82456484 -0.41597356]\n [ 0.56576746 -0.90937671]]\n</pre> In\u00a0[62]: Copied! <pre># Valores y vectores propios\neigenvalues, eigenvectors = np.linalg.eig(A) \n\nprint(\"eigenvalues\")\nprint( eigenvalues )\nprint(\"\\neigenvectors\")\nprint( eigenvectors )\n</pre> # Valores y vectores propios eigenvalues, eigenvectors = np.linalg.eig(A)   print(\"eigenvalues\") print( eigenvalues ) print(\"\\neigenvectors\") print( eigenvectors ) <pre>eigenvalues\n[-0.37228132  5.37228132]\n\neigenvectors\n[[-0.82456484 -0.41597356]\n [ 0.56576746 -0.90937671]]\n</pre> <p>Resolver sitema de ecuaciones</p> In\u00a0[63]: Copied! <pre># crear matriz\nA = np.array([[1,2],\n              [3,4]])\n\n# sistemas lineales: Ax = b\nb = np.array([[5.], [7.]])\n</pre> # crear matriz A = np.array([[1,2],               [3,4]])  # sistemas lineales: Ax = b b = np.array([[5.], [7.]]) In\u00a0[64]: Copied! <pre>print(\"linear system: Ax=b\")\nprint(\"\\nx = \")\nprint( np.linalg.solve(A, b) )\n</pre> print(\"linear system: Ax=b\") print(\"\\nx = \") print( np.linalg.solve(A, b) ) <pre>linear system: Ax=b\n\nx = \n[[-3.]\n [ 4.]]\n</pre> <p></p> <p>Broadcasting es un t\u00e9rmino utilizado en NumPy para describir la forma en que las operaciones entre arrays con diferentes formas se manejan autom\u00e1ticamente por NumPy. En otras palabras, cuando se realizan operaciones aritm\u00e9ticas entre dos arrays de diferentes formas, NumPy ajusta autom\u00e1ticamente la forma del array m\u00e1s peque\u00f1o para que coincida con la forma del array m\u00e1s grande antes de realizar la operaci\u00f3n. Esto permite que las operaciones se realicen de manera eficiente sin la necesidad de crear copias adicionales de los datos.</p> <p>Las reglas de broadcasting en NumPy son las siguientes:</p> <ul> <li><p>Si los dos arrays tienen el mismo n\u00famero de dimensiones, pero las formas no son iguales, NumPy agrega 1 a la forma del array m\u00e1s peque\u00f1o para que coincida con la forma del array m\u00e1s grande.</p> </li> <li><p>Si la forma de los dos arrays no es la misma y no tienen el mismo n\u00famero de dimensiones, NumPy agrega 1 a las dimensiones menos y expande las formas de los arrays para que sean iguales en la dimensi\u00f3n m\u00e1s alta.</p> </li> <li><p>Si la forma de los dos arrays no es la misma y alguna dimensi\u00f3n no coincide, NumPy genera un error.</p> </li> </ul> In\u00a0[65]: Copied! <pre># example 01\na = np.arange(3)+ 5\nprint(f\"np.arange(3)+ 5:\\n{a}\" )\n</pre> # example 01 a = np.arange(3)+ 5 print(f\"np.arange(3)+ 5:\\n{a}\" ) <pre>np.arange(3)+ 5:\n[5 6 7]\n</pre> In\u00a0[66]: Copied! <pre># example 02\nb = np.ones((3,3))+np.arange(3)\nprint(f\"np.ones((3,3))+np.arange(3):\\n{b}\" )\n</pre> # example 02 b = np.ones((3,3))+np.arange(3) print(f\"np.ones((3,3))+np.arange(3):\\n{b}\" ) <pre>np.ones((3,3))+np.arange(3):\n[[1. 2. 3.]\n [1. 2. 3.]\n [1. 2. 3.]]\n</pre> In\u00a0[67]: Copied! <pre># example 03\nc = np.arange(3).reshape((3,1)) +  np.arange(3)\nprint(f\"np.arange(3).reshape((3,1)) +  np.arange(3):\\n{c }\" )\n</pre> # example 03 c = np.arange(3).reshape((3,1)) +  np.arange(3) print(f\"np.arange(3).reshape((3,1)) +  np.arange(3):\\n{c }\" ) <pre>np.arange(3).reshape((3,1)) +  np.arange(3):\n[[0 1 2]\n [1 2 3]\n [2 3 4]]\n</pre>"},{"location":"lectures/data_manipulation/sc_01/#numpy","title":"Numpy\u00b6","text":""},{"location":"lectures/data_manipulation/sc_01/#introduccion","title":"Introducci\u00f3n\u00b6","text":"<p>Numpy s una biblioteca de Python que se utiliza para trabajar con matrices y vectores de datos num\u00e9ricos. Proporciona un conjunto de funciones y m\u00e9todos eficientes y optimizados para el procesamiento de datos num\u00e9ricos, incluyendo operaciones matriciales, estad\u00edsticas, \u00e1lgebra lineal, entre otras.</p> <p>NumPy es ampliamente utilizado en \u00e1reas como la ciencia de datos, el aprendizaje autom\u00e1tico, la ingenier\u00eda y la f\u00edsica, entre otras. Es una herramienta fundamental en el ecosistema de Python para el procesamiento y an\u00e1lisis de datos num\u00e9ricos.</p> <p>Algunas de las caracter\u00edsticas principales de NumPy son:</p> <ul> <li>Arrays multidimensionales eficientes y optimizados para operaciones num\u00e9ricas.</li> <li>Funciones y m\u00e9todos para operaciones matem\u00e1ticas y estad\u00edsticas.</li> <li>Integraci\u00f3n con otras bibliotecas de Python para el procesamiento de datos, como Pandas y Matplotlib.</li> <li>Capacidad de procesar grandes conjuntos de datos num\u00e9ricos de manera eficiente y escalable.</li> </ul>"},{"location":"lectures/data_manipulation/sc_01/#python-lists-vs-numpy-arrays","title":"Python Lists vs Numpy Arrays\u00b6","text":"<p>Las listas de Python y los arrays de NumPy son dos estructuras de datos diferentes que se utilizan para almacenar y manipular conjuntos de datos en Python.</p> <ul> <li><p>Las listas de Python son una colecci\u00f3n de elementos que pueden ser de diferentes tipos de datos, como enteros, flotantes, cadenas, etc. Pueden ser de longitud variable y se pueden modificar en tiempo de ejecuci\u00f3n. Las listas de Python son m\u00e1s flexibles y vers\u00e1tiles que los arrays de NumPy, pero pueden ser m\u00e1s lentas para operaciones matem\u00e1ticas y num\u00e9ricas.</p> </li> <li><p>Los arrays de NumPy son una estructura de datos m\u00e1s especializada que se utiliza para trabajar con datos num\u00e9ricos, como matrices y vectores. Son m\u00e1s r\u00e1pidos y eficientes que las listas de Python para operaciones num\u00e9ricas y matem\u00e1ticas, y proporcionan muchas funciones y m\u00e9todos \u00fatiles para el procesamiento de datos, como operaciones matriciales y de \u00e1lgebra lineal. Los arrays de NumPy tienen una longitud fija y no se pueden modificar una vez creados.</p> </li> </ul> <p>Una pregunta com\u00fan para principiantes es cu\u00e1l es la verdadera diferencia aqu\u00ed. La respuesta es el rendimiento. Las estructuras de datos de Numpy funcionan mejor en:</p> <ul> <li>Tama\u00f1o: las estructuras de datos de Numpy ocupan menos espacio</li> <li>Rendimiento: necesitan velocidad y son m\u00e1s r\u00e1pidos que las listas</li> <li>Funcionalidad: SciPy y NumPy tienen funciones optimizadas, como las operaciones de \u00e1lgebra lineal integradas.</li> </ul>"},{"location":"lectures/data_manipulation/sc_01/#objetos-en-numpy","title":"Objetos en Numpy\u00b6","text":"<p>En NumPy, el objeto principal es el array multidimensional (llamado <code>ndarray</code>), que es una estructura de datos eficiente para el procesamiento de datos num\u00e9ricos. Los arrays en NumPy son objetos homog\u00e9neos y de tama\u00f1o fijo que contienen elementos del mismo tipo de datos.</p> <p>Aqu\u00ed hay algunos ejemplos de objetos en NumPy:</p>"},{"location":"lectures/data_manipulation/sc_01/#operaciones","title":"Operaciones\u00b6","text":"<p>NumPy proporciona una variedad de operaciones b\u00e1sicas que se pueden realizar en los arrays, como operaciones aritm\u00e9ticas, operaciones de comparaci\u00f3n y operaciones de agregaci\u00f3n. Aqu\u00ed hay algunos ejemplos de operaciones b\u00e1sicas en NumPy:</p>"},{"location":"lectures/data_manipulation/sc_01/#operaciones-aritmeticas","title":"Operaciones aritm\u00e9ticas\u00b6","text":""},{"location":"lectures/data_manipulation/sc_01/#operaciones-de-comparacion","title":"Operaciones de comparaci\u00f3n\u00b6","text":""},{"location":"lectures/data_manipulation/sc_01/#operaciones-de-estadisticas","title":"Operaciones de estad\u00edsticas\u00b6","text":""},{"location":"lectures/data_manipulation/sc_01/#indexacion","title":"Indexaci\u00f3n\u00b6","text":"<p>La indexaci\u00f3n en NumPy es similar a la indexaci\u00f3n en listas de Python, pero se extiende a m\u00faltiples dimensiones. En NumPy, los arrays se pueden indexar y rebanar para acceder a elementos y subarrays espec\u00edficos. Aqu\u00ed hay algunos ejemplos de indexaci\u00f3n en NumPy:</p>"},{"location":"lectures/data_manipulation/sc_01/#algebra-lineal","title":"Algebra Lineal\u00b6","text":"<p>NumPy es una biblioteca popular en Python para el \u00e1lgebra lineal, que es una rama de las matem\u00e1ticas que se enfoca en el estudio de vectores, matrices y sistemas de ecuaciones lineales. NumPy proporciona una gran cantidad de funciones y m\u00e9todos para realizar operaciones de \u00e1lgebra lineal de manera eficiente en Python.</p> <p>Aqu\u00ed hay algunos ejemplos de operaciones de \u00e1lgebra lineal que se pueden realizar con NumPy:</p>"},{"location":"lectures/data_manipulation/sc_01/#broadcasting","title":"Broadcasting\u00b6","text":""},{"location":"lectures/data_manipulation/sc_01/#referencias","title":"Referencias\u00b6","text":"<ol> <li>Quickstart tutorial-numpy</li> <li>Mathematical functions</li> </ol>"},{"location":"lectures/machine_learning/011_concepts/","title":"Conceptos B\u00e1sicos ML","text":"In\u00a0[1]: Copied! <pre>import seaborn as sns\niris = sns.load_dataset('iris')\niris.head()\n</pre> import seaborn as sns iris = sns.load_dataset('iris') iris.head() Out[1]: sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa <p>Aqu\u00ed, cada fila representa una flor observada, y el n\u00famero total de filas corresponde a la cantidad de flores en el conjunto de datos, denominado <code>n_samples</code>. Las filas se conocen como muestras (samples).</p> <p>De manera similar, cada columna proporciona informaci\u00f3n espec\u00edfica sobre cada muestra, llamada caracter\u00edstica (feature). El n\u00famero total de columnas se denomina <code>n_features</code>.</p> In\u00a0[2]: Copied! <pre># features matrix\nX = iris.drop('species',axis=1)\nX\n</pre> # features matrix X = iris.drop('species',axis=1) X Out[2]: sepal_length sepal_width petal_length petal_width 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2 ... ... ... ... ... 145 6.7 3.0 5.2 2.3 146 6.3 2.5 5.0 1.9 147 6.5 3.0 5.2 2.0 148 6.2 3.4 5.4 2.3 149 5.9 3.0 5.1 1.8 <p>150 rows \u00d7 4 columns</p> In\u00a0[3]: Copied! <pre># target\ny = iris['species']\ny\n</pre> # target y = iris['species'] y Out[3]: <pre>0         setosa\n1         setosa\n2         setosa\n3         setosa\n4         setosa\n         ...    \n145    virginica\n146    virginica\n147    virginica\n148    virginica\n149    virginica\nName: species, Length: 150, dtype: object</pre> <ul> <li><p>El conjunto de entrenamiento es un subconjunto de datos utilizado para ajustar los par\u00e1metros del modelo y aprender la relaci\u00f3n entre las variables de entrada y la salida. En esencia, sirve para entrenar al modelo.</p> </li> <li><p>El conjunto de prueba es otro subconjunto, separado del proceso de entrenamiento, empleado para evaluar el rendimiento del modelo. Permite medir su capacidad de generalizaci\u00f3n al enfrentarlo a datos nuevos y no vistos previamente.</p> </li> </ul> <p>La funci\u00f3n <code>train_test_split</code> de Scikit-learn es una herramienta esencial para dividir un conjunto de datos en subconjuntos de entrenamiento y prueba. Permite especificar la proporci\u00f3n de datos para cada conjunto, siendo 75% para entrenamiento y 25% para prueba de forma predeterminada. Adem\u00e1s, toma como entrada tanto los datos como las etiquetas correspondientes.</p> <p>Aqu\u00ed un ejemplo de c\u00f3mo utilizarla:</p> In\u00a0[7]: Copied! <pre>from sklearn.model_selection import train_test_split\n\n# separar informacion\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.25, random_state=42)\n</pre> from sklearn.model_selection import train_test_split  # separar informacion X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.25, random_state=42) <p>La funci\u00f3n <code>train_test_split(X, y, test_size=0.25, random_state=42)</code> divide los datos en un conjunto de entrenamiento y otro de prueba.</p> <ul> <li><code>X</code> son los datos de entrada y <code>y</code> las etiquetas.</li> <li><code>test_size=0.25</code> asigna el 25% de los datos al conjunto de prueba y el 75% al de entrenamiento.</li> <li><code>random_state=42</code> asegura que la divisi\u00f3n sea reproducible, generando los mismos subconjuntos cada vez que se ejecuta el c\u00f3digo.</li> </ul> In\u00a0[8]: Copied! <pre>print(f\"dimensiones de X_train: {X_train.shape}\")\nprint(f\"dimensiones de y_train: {y_train.shape}\")\nprint(\"\")\nprint(f\"dimensiones de X_test:   {X_test.shape}\")\nprint(f\"dimensiones de y_test:   {y_test.shape}\")\n</pre> print(f\"dimensiones de X_train: {X_train.shape}\") print(f\"dimensiones de y_train: {y_train.shape}\") print(\"\") print(f\"dimensiones de X_test:   {X_test.shape}\") print(f\"dimensiones de y_test:   {y_test.shape}\") <pre>dimensiones de X_train: (112, 4)\ndimensiones de y_train: (112,)\n\ndimensiones de X_test:   (38, 4)\ndimensiones de y_test:   (38,)\n</pre> <p>Reglas de separaci\u00f3n</p> <p>En general, los datos se dividen en conjuntos de entrenamiento y prueba. El tama\u00f1o de cada conjunto puede variar seg\u00fan el n\u00famero de filas y la complejidad del modelo. Por lo general, el conjunto de entrenamiento ocupa entre el 70% y el 80%, mientras que el de prueba ocupa entre el 20% y el 30%.</p> <p>Aqu\u00ed hay una referencia sugerida:</p> N\u00famero de filas Conjunto de entrenamiento Conjunto de prueba 100 - 1,000 70% 30% 1,000 - 100,000 80% 20% M\u00e1s de 100,000 90% 10% <p>Vamos a ilustrarlo con un ejemplo pr\u00e1ctico:</p> <p>El c\u00f3digo crea un conjunto de datos de ejemplo utilizando n\u00fameros consecutivos (<code>x</code>) y valores calculados (<code>y_real</code> y <code>y_pred</code>). <code>y_real</code> es una funci\u00f3n lineal de <code>x</code>, mientras que <code>y_pred</code> introduce variaci\u00f3n aleatoria para simular diferencias de predicci\u00f3n. Los datos se organizan en un DataFrame de pandas y se muestran las primeras filas para su revisi\u00f3n.</p> In\u00a0[9]: Copied! <pre># Importar las bibliotecas necesarias\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Configurar semilla para reproducibilidad\nnp.random.seed(42)\n\n# Crear un conjunto de datos de ejemplo\nn = 50\nx = np.arange(1, n + 1)\ny_real = 2 * x  \ny_pred = 2 * x + 2 * np.random.randn(n)  # Introducir variaci\u00f3n aleatoria\n\n# Crear DataFrame\ndf = pd.DataFrame({\n    'x': x,\n    'y_real': y_real,\n    'y_pred': y_pred\n})\n\n# Mostrar las primeras filas del DataFrame\ndf.head()\n</pre> # Importar las bibliotecas necesarias import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns  # Configurar semilla para reproducibilidad np.random.seed(42)  # Crear un conjunto de datos de ejemplo n = 50 x = np.arange(1, n + 1) y_real = 2 * x   y_pred = 2 * x + 2 * np.random.randn(n)  # Introducir variaci\u00f3n aleatoria  # Crear DataFrame df = pd.DataFrame({     'x': x,     'y_real': y_real,     'y_pred': y_pred })  # Mostrar las primeras filas del DataFrame df.head() Out[9]: x y_real y_pred 0 1 2 2.993428 1 2 4 3.723471 2 3 6 7.295377 3 4 8 11.046060 4 5 10 9.531693 In\u00a0[10]: Copied! <pre># Visualizar los resultados\nplt.figure(figsize=(12, 4))\nplt.scatter(df['x'], df['y_pred'], color='red', label='Predicciones')\nplt.plot(df['x'], df['y_real'], color='blue', label='Valores reales')\nplt.xlabel('x')\nplt.ylabel('Valores')\nplt.title('Comparaci\u00f3n entre valores reales y predicciones')\nplt.legend()\nplt.show()\n</pre> # Visualizar los resultados plt.figure(figsize=(12, 4)) plt.scatter(df['x'], df['y_pred'], color='red', label='Predicciones') plt.plot(df['x'], df['y_real'], color='blue', label='Valores reales') plt.xlabel('x') plt.ylabel('Valores') plt.title('Comparaci\u00f3n entre valores reales y predicciones') plt.legend() plt.show() <p>La l\u00ednea azul muestra los valores originales, mientras que los puntos rojos representan las predicciones del modelo.</p> <p>A continuaci\u00f3n, analizaremos el error de estimaci\u00f3n entre los valores originales ($y$) y los valores estimados ($\\hat{y}$):</p> In\u00a0[11]: Copied! <pre># Calcular el error de estimaci\u00f3n\ny_original = df['y_real'].values\ny_estimado = df['y_pred'].values\n\nerror_estimacion = y_original - y_estimado\nerror_estimacion\n</pre> # Calcular el error de estimaci\u00f3n y_original = df['y_real'].values y_estimado = df['y_pred'].values  error_estimacion = y_original - y_estimado error_estimacion Out[11]: <pre>array([-0.99342831,  0.2765286 , -1.29537708, -3.04605971,  0.46830675,\n        0.46827391, -3.15842563, -1.53486946,  0.93894877, -1.08512009,\n        0.92683539,  0.93145951, -0.48392454,  3.82656049,  3.44983567,\n        1.12457506,  2.02566224, -0.62849467,  1.81604815,  2.8246074 ,\n       -2.93129754,  0.4515526 , -0.13505641,  2.84949637,  1.08876545,\n       -0.22184518,  2.30198715, -0.75139604,  1.20127738,  0.5833875 ,\n        1.20341322, -3.70455637,  0.02699445,  2.11542186, -1.64508982,\n        2.4416873 , -0.41772719,  3.91934025,  2.6563721 , -0.39372247,\n       -1.47693316, -0.34273656,  0.23129656,  0.60220739,  2.95704398,\n        1.43968842,  0.92127754, -2.11424445, -0.68723658,  3.52608031])</pre> <p>Este resultado nos muestra el error de estimaci\u00f3n para cada muestra (o fila). Para resumir este error, utilizamos m\u00e9tricas de evaluaci\u00f3n, que se dividen en dos categor\u00edas principales:</p> <ol> <li><p>M\u00e9tricas absolutas: Las m\u00e9tricas absolutas o no escalada miden el error sin escalar los valores. Las m\u00e9trica absolutas m\u00e1s ocupadas son:</p> <ul> <li>Mean Absolute Error (MAE)</li> </ul> <p>$$\\textrm{MAE}(y,\\hat{y}) = \\dfrac{1}{n}\\sum_{t=1}^{n}\\left | y_{t}-\\hat{y}_{t}\\right |$$</p> <ul> <li>Root Mean Squared Error (RMSE):</li> </ul> <p>$$\\textrm{RMSE}(y,\\hat{y}) =(\\dfrac{1}{n}\\sum_{t=1}^{n}\\left ( y_{t}-\\hat{y}_{t}\\right )^2)^{1/2}$$</p> </li> <li><p>M\u00e9tricas Porcentuales: Las m\u00e9tricas porcentuales o escaladas miden el error de manera escalada, es decir, se busca acotar el error entre valores de 0 a 1, donde 0 significa que el ajuste es perfecto, mientras que 1 ser\u00eda un mal ajuste. Cabe destacar que muchas veces las m\u00e9tricas porcentuales puden tener valores mayores a 1.Las m\u00e9trica Porcentuales m\u00e1s ocupadas son:</p> <ul> <li>Mean absolute percentage error (MAPE):</li> </ul> <p>$$\\textrm{MAPE}(y,\\hat{y}) = \\dfrac{1}{n}\\sum_{t=1}^{n}\\left | \\frac{y_{t}-\\hat{y}_{t}}{y_{t}} \\right |$$</p> </li> </ol> <p>Veamos un ejemplo pr\u00e1ctico, las m\u00e9tricas de estimaci\u00f3n  las podemos obtener desde <code>sklearn.metrics</code>:</p> In\u00a0[14]: Copied! <pre>from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n\n# Calcular las m\u00e9tricas de error\nmae = mean_absolute_error(y_original, y_estimado)\nrmse = mean_squared_error(y_original, y_estimado)  \nmape = mean_absolute_percentage_error(y_original, y_estimado)\n\n# Mostrar los resultados\nprint(f\"MAE:  {mae:.4f}\")\nprint(f\"RMSE: {rmse:.4f}\")\nprint(f\"MAPE: {mape:.4f}\")\n</pre> from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error  # Calcular las m\u00e9tricas de error mae = mean_absolute_error(y_original, y_estimado) rmse = mean_squared_error(y_original, y_estimado)   mape = mean_absolute_percentage_error(y_original, y_estimado)  # Mostrar los resultados print(f\"MAE:  {mae:.4f}\") print(f\"RMSE: {rmse:.4f}\") print(f\"MAPE: {mape:.4f}\") <pre>MAE:  1.5328\nRMSE: 3.6206\nMAPE: 0.0586\n</pre> <p>El c\u00f3digo calcula tres m\u00e9tricas de error para evaluar la precisi\u00f3n de un modelo de predicci\u00f3n:</p> <ul> <li><code>MAE</code> (Error Absoluto Medio): promedio de los errores absolutos.</li> <li><code>RMSE</code> (Ra\u00edz del Error Cuadr\u00e1tico Medio): mide el error con sensibilidad a valores extremos.</li> <li><code>MAPE</code> (Error Absoluto Porcentual Medio): calcula el error relativo en porcentaje.</li> </ul> <p>Luego, imprime estos valores con cuatro decimales para una mejor interpretaci\u00f3n de los resultados.</p> <p>\u00bfC\u00f3mo identificar la mejor estimaci\u00f3n entre dos modelos?</p> <p>Si tenemos dos estimaciones, $\\hat{y}_{1}$ y $\\hat{y}_{2}$, la mejor ser\u00e1 aquella que tenga el menor error seg\u00fan las m\u00e9tricas de evaluaci\u00f3n (como MAE, RMSE o MAPE).</p> <p>Ahora, definamos una segunda estimaci\u00f3n ($\\hat{y}_{2}$) y comparemos ambas utilizando estas m\u00e9tricas para determinar cu\u00e1l ofrece un mejor ajuste.</p> In\u00a0[15]: Copied! <pre># Definir una nueva estimaci\u00f3n\ny_estimado_2 = y_estimado + np.random.randn(n)\n\n# Calcular el error de la segunda estimaci\u00f3n\nmae_2 = mean_absolute_error(y_original, y_estimado_2)\nrmse_2 = mean_squared_error(y_original, y_estimado_2)  \nmape_2 = mean_absolute_percentage_error(y_original, y_estimado_2)\n\n# Mostrar los resultados\nprint(f\"MAE (segunda estimaci\u00f3n):  {mae_2:.4f}\")\nprint(f\"RMSE (segunda estimaci\u00f3n): {rmse_2:.4f}\")\nprint(f\"MAPE (segunda estimaci\u00f3n): {mape_2:.4f}\")\n</pre> # Definir una nueva estimaci\u00f3n y_estimado_2 = y_estimado + np.random.randn(n)  # Calcular el error de la segunda estimaci\u00f3n mae_2 = mean_absolute_error(y_original, y_estimado_2) rmse_2 = mean_squared_error(y_original, y_estimado_2)   mape_2 = mean_absolute_percentage_error(y_original, y_estimado_2)  # Mostrar los resultados print(f\"MAE (segunda estimaci\u00f3n):  {mae_2:.4f}\") print(f\"RMSE (segunda estimaci\u00f3n): {rmse_2:.4f}\") print(f\"MAPE (segunda estimaci\u00f3n): {mape_2:.4f}\")  <pre>MAE (segunda estimaci\u00f3n):  1.8174\nRMSE (segunda estimaci\u00f3n): 4.7062\nMAPE (segunda estimaci\u00f3n): 0.0671\n</pre> <p>Al analizar las m\u00e9tricas de ambas estimaciones, encontramos los siguientes valores:</p> <ul> <li><p>Primera estimaci\u00f3n ($\\hat{y}_{1}$):</p> <ul> <li>MAE: 1.5328</li> <li>RMSE: 3.6206</li> <li>MAPE: 0.0586</li> </ul> </li> <li><p>Segunda estimaci\u00f3n ($\\hat{y}_{2}$):</p> <ul> <li>MAE: 1.8174</li> <li>RMSE: 4.7062</li> <li>MAPE: 0.0671</li> </ul> </li> </ul> <p>En este caso, todas las m\u00e9tricas (MAE, RMSE, MAPE) para la primera estimaci\u00f3n son menores que las de la segunda. Por lo tanto, la primera estimaci\u00f3n ($\\hat{y}_{1}$) se considera m\u00e1s precisa y efectiva que la segunda ($\\hat{y}_{2}$).</p> In\u00a0[18]: Copied! <pre>import pandas as pd\nimport numpy as np\n\n# Datos reales (50 valores) y predicciones cercanas a 80% de precisi\u00f3n\ny_real = [1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n          1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,\n          0, 1, 1, 0, 0, 1, 0, 1, 1, 0]\n\n# Generar predicciones con aproximadamente un 80% de precisi\u00f3n\ny_pred = [1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n          1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n          0, 1, 1, 0, 0, 1, 0, 1, 1, 0]\n\n# Crear un DataFrame con los valores reales y estimados\ndf = pd.DataFrame({\n    'y_real': y_real,\n    'y_pred': y_pred\n})\n\n# Agregar una columna para indicar coincidencias entre las predicciones y los valores reales\ndf['coincidencia'] = df['y_real'] == df['y_pred']\n\n\n\n# Mostrar los resultados\ndf.head()\n</pre> import pandas as pd import numpy as np  # Datos reales (50 valores) y predicciones cercanas a 80% de precisi\u00f3n y_real = [1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,           1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,           0, 1, 1, 0, 0, 1, 0, 1, 1, 0]  # Generar predicciones con aproximadamente un 80% de precisi\u00f3n y_pred = [1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,           1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,           0, 1, 1, 0, 0, 1, 0, 1, 1, 0]  # Crear un DataFrame con los valores reales y estimados df = pd.DataFrame({     'y_real': y_real,     'y_pred': y_pred })  # Agregar una columna para indicar coincidencias entre las predicciones y los valores reales df['coincidencia'] = df['y_real'] == df['y_pred']    # Mostrar los resultados df.head()  Out[18]: y_real y_pred coincidencia 0 1 1 True 1 0 0 True 2 1 1 True 3 1 1 True 4 0 0 True <p>En este caso, la columna coincidencia indica los casos en los que $y_{i}$ y $\\hat{y}_{i}$ son iguales.</p> In\u00a0[19]: Copied! <pre># total de coincidencias\ndf['coincidencia'].value_counts()\n</pre> # total de coincidencias df['coincidencia'].value_counts() Out[19]: <pre>coincidencia\nTrue     46\nFalse     4\nName: count, dtype: int64</pre> <p>El resultado muestra que de las 50 predicciones realizadas:</p> <ul> <li>46 son True, lo que significa que en estos casos las predicciones ($\\hat{y}_{i}$) coinciden con los valores reales ($y_{i}$).</li> <li>4 son False, lo que indica que en estos casos las predicciones no coinciden con los valores reales.</li> </ul> <p>Esto refleja una alta precisi\u00f3n del modelo, ya que la mayor\u00eda de las predicciones son correctas (46 de 50).</p> <p>Matriz de Confusi\u00f3n</p> <p>La matriz de confusi\u00f3n es una herramienta fundamental para evaluar el rendimiento de un modelo de clasificaci\u00f3n. Proporciona una representaci\u00f3n visual que resume las predicciones realizadas por el modelo y las compara con los valores reales.</p> <p>Para la clasificaci\u00f3n binaria (por ejemplo, clases 0 y 1), la matriz de confusi\u00f3n tiene la siguiente estructura:</p> <p></p> <p>Aqu\u00ed se definen los componentes:</p> <ul> <li>TP (Verdadero Positivo): El modelo predijo correctamente la clase positiva.</li> <li>FP (Falso Positivo): El modelo predijo incorrectamente la clase positiva cuando era realmente negativa.</li> <li>FN (Falso Negativo): El modelo predijo incorrectamente la clase negativa cuando era realmente positiva.</li> <li>TN (Verdadero Negativo): El modelo predijo correctamente la clase negativa.</li> </ul> <p>En resumen, TP y TN representan las predicciones correctas, mientras que FP y FN indican los errores de clasificaci\u00f3n del modelo.</p> <p>La siguiente imagen ilustra estos conceptos de FN y FP:</p> <p></p> <p>Ahora, vamos a calcular la matriz de confusi\u00f3n utilizando el comando <code>confusion_matrix</code>:</p> In\u00a0[20]: Copied! <pre>from sklearn.metrics import confusion_matrix\n\n# Calcular la matriz de confusi\u00f3n\ncm = confusion_matrix(df['y_real'], df['y_pred'])\n\n# Mostrar la matriz de confusi\u00f3n de forma clara\nprint('\\nMatriz de Confusi\u00f3n:\\n')\nprint(cm)\n</pre> from sklearn.metrics import confusion_matrix  # Calcular la matriz de confusi\u00f3n cm = confusion_matrix(df['y_real'], df['y_pred'])  # Mostrar la matriz de confusi\u00f3n de forma clara print('\\nMatriz de Confusi\u00f3n:\\n') print(cm)  <pre>\nMatriz de Confusi\u00f3n:\n\n[[20  3]\n [ 1 26]]\n</pre> <p>M\u00e9tricas de Evaluaci\u00f3n</p> <p>En el contexto de clasificaci\u00f3n, el objetivo es maximizar la cantidad de TP (Verdaderos Positivos) y TN (Verdaderos Negativos) y minimizar los FP (Falsos Positivos) y FN (Falsos Negativos). Para evaluar el rendimiento del modelo, se utilizan las siguientes m\u00e9tricas:</p> <ol> <li><p>Accuracy (Precisi\u00f3n Global):</p> <ul> <li>Mide la proporci\u00f3n de predicciones correctas (positivas y negativas) sobre el total de casos.</li> </ul> <p>$$accuracy(y,\\hat{y}) = \\dfrac{TP+TN}{TP+TN+FP+FN}$$</p> </li> <li><p>Recall (Sensibilidad o Tasa de Verdaderos Positivos):</p> <ul> <li>Eval\u00faa la capacidad del modelo para identificar correctamente las muestras positivas. $$recall(y,\\hat{y}) = \\dfrac{TP}{TP+FN}$$</li> </ul> </li> <li><p>Precision (Precisi\u00f3n Positiva):</p> <ul> <li>Indica la proporci\u00f3n de verdaderos positivos sobre todas las predicciones positivas. $$precision(y,\\hat{y}) = \\dfrac{TP}{TP+FP} $$</li> </ul> </li> <li><p>F-score (F1-Score):</p> <ul> <li>Es la media arm\u00f3nica de la precisi\u00f3n y el recall, proporcionando un balance entre ambos. $$fscore(y,\\hat{y}) = 2\\times \\dfrac{precision(y,\\hat{y})\\times recall(y,\\hat{y})}{precision(y,\\hat{y})+recall(y,\\hat{y})} $$</li> </ul> </li> </ol> <p>Veamos un ejemplo pr\u00e1ctico donde calculamos estas m\u00e9tricas utilizando <code>sklearn.metrics</code>:</p> In\u00a0[21]: Copied! <pre>from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n\n# Calcular las m\u00e9tricas de evaluaci\u00f3n\naccuracy = accuracy_score(df['y_real'], df['y_pred'])\nrecall = recall_score(df['y_real'], df['y_pred'])\nprecision = precision_score(df['y_real'], df['y_pred'])\nfscore = f1_score(df['y_real'], df['y_pred'])\n\n# Mostrar los resultados\nprint(f\"Accuracy:  {accuracy:.4f}\")\nprint(f\"Recall:    {recall:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"F1-Score:  {fscore:.4f}\")\n</pre> from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score  # Calcular las m\u00e9tricas de evaluaci\u00f3n accuracy = accuracy_score(df['y_real'], df['y_pred']) recall = recall_score(df['y_real'], df['y_pred']) precision = precision_score(df['y_real'], df['y_pred']) fscore = f1_score(df['y_real'], df['y_pred'])  # Mostrar los resultados print(f\"Accuracy:  {accuracy:.4f}\") print(f\"Recall:    {recall:.4f}\") print(f\"Precision: {precision:.4f}\") print(f\"F1-Score:  {fscore:.4f}\") <pre>Accuracy:  0.9200\nRecall:    0.9630\nPrecision: 0.8966\nF1-Score:  0.9286\n</pre> <p>Resultados:</p> <ul> <li>Accuracy: 0.9200: El 92% de las predicciones fueron correctas.</li> <li>Recall: 0.9630: El modelo identific\u00f3 el 96.3% de las muestras positivas correctamente.</li> <li>Precision: 0.8966: El 89.66% de las predicciones positivas fueron realmente positivas.</li> <li>F1-Score: 0.9286: Indica un buen balance entre precisi\u00f3n y recall, lo que refleja un modelo con buen desempe\u00f1o en identificar correctamente las clases.</li> </ul> <p>La curva ROC (Receiver Operating Characteristic) es una herramienta gr\u00e1fica que eval\u00faa el rendimiento de algoritmos de clasificaci\u00f3n binaria, representando visualmente el desempe\u00f1o del modelo. Se construye trazando la tasa de verdaderos positivos (TPR) frente a la tasa de falsos positivos (FPR) a diferentes umbrales de decisi\u00f3n, permitiendo analizar el equilibrio entre sensibilidad y especificidad.</p> <p>La curva se genera calculando la TPR y la FPR para distintos umbrales, comenzando generalmente con un valor de 0.5, aunque se pueden usar valores entre 0 y 1. Esto permite observar c\u00f3mo cambia el rendimiento del clasificador al ajustar los umbrales.</p> <p>Definici\u00f3n de las tasas:</p> <ul> <li><p>Tasa de verdaderos positivos (TPR) o Sensibilidad:</p> <p>$$ TPR = \\dfrac{TP}{TP + FN} $$</p> <p>donde TP es el n\u00famero de Verdaderos Positivos y FN es el n\u00famero de Falsos Negativos. El TPR mide la probabilidad de que una instancia verdaderamente positiva sea correctamente clasificada como positiva.</p> </li> <li><p>Tasa de falsos positivos (FPR) o 1 - Especificidad:</p> <p>$$ FPR = \\dfrac{FP}{FP + TN} $$</p> <p>donde FP es el n\u00famero de Falsos Positivos y TN es el n\u00famero de Verdaderos Negativos. La FPR mide la frecuencia con la que una instancia verdaderamente negativa se clasifica incorrectamente como positiva, es decir, una \"falsa alarma\".</p> </li> </ul> <p>\u00c1rea Bajo la Curva (AUC)</p> <p>El AUC (\u00c1rea Bajo la Curva) resume el rendimiento del clasificador:</p> <ul> <li>AUC = 0.5: Sin poder predictivo (predicci\u00f3n aleatoria).</li> <li>AUC = 1.0: Clasificador perfecto.</li> </ul> <p>Un mayor AUC indica mejor rendimiento; AUC &lt; 0.5 sugiere predicciones inversas.</p> <p>Continuando con el ejemplo anterior:</p> In\u00a0[22]: Copied! <pre>from sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\n# Calcular la curva ROC y el AUC\nfpr, tpr, thresholds = roc_curve(df['y_real'], df['y_pred'])\nroc_auc = auc(fpr, tpr)\n\n# Graficar la curva ROC\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='blue', lw=2, label='Curva ROC (AUC = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=2, label='Clasificador Aleatorio (AUC = 0.50)')\n\n# Configurar l\u00edmites y etiquetas\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('Tasa de Falsos Positivos', fontsize=12)\nplt.ylabel('Tasa de Verdaderos Positivos', fontsize=12)\nplt.title('Curva ROC', fontsize=15)\nplt.legend(loc=\"lower right\", fontsize=12)\nplt.grid(alpha=0.3)\n\n# Mostrar la gr\u00e1fica\nplt.show()\n</pre> from sklearn.metrics import roc_curve, auc import matplotlib.pyplot as plt  # Calcular la curva ROC y el AUC fpr, tpr, thresholds = roc_curve(df['y_real'], df['y_pred']) roc_auc = auc(fpr, tpr)  # Graficar la curva ROC plt.figure(figsize=(8, 6)) plt.plot(fpr, tpr, color='blue', lw=2, label='Curva ROC (AUC = %0.2f)' % roc_auc) plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=2, label='Clasificador Aleatorio (AUC = 0.50)')  # Configurar l\u00edmites y etiquetas plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel('Tasa de Falsos Positivos', fontsize=12) plt.ylabel('Tasa de Verdaderos Positivos', fontsize=12) plt.title('Curva ROC', fontsize=15) plt.legend(loc=\"lower right\", fontsize=12) plt.grid(alpha=0.3)  # Mostrar la gr\u00e1fica plt.show()  <p>Un AUC de 0.92 indica que el modelo tiene un rendimiento excelente para distinguir entre clases. Esto significa que el modelo tiene un 92% de probabilidad de clasificar correctamente una instancia positiva.</p> <p>Cuanto m\u00e1s cercano est\u00e9 el AUC a 1.0, mejor es el modelo. Un AUC de 0.5, en cambio, indica que el modelo no tiene capacidad predictiva y est\u00e1 adivinando al azar.</p>"},{"location":"lectures/machine_learning/011_concepts/#conceptos-basicos-ml","title":"Conceptos B\u00e1sicos ML\u00b6","text":""},{"location":"lectures/machine_learning/011_concepts/#representacion-de-datos","title":"Representaci\u00f3n de Datos\u00b6","text":"<p>Una tabla es una cuadr\u00edcula de datos organizada en filas y columnas. Las filas corresponden a elementos individuales del conjunto, mientras que las columnas representan atributos o caracter\u00edsticas de estos elementos.</p> <p>Por ejemplo, el conjunto de datos Iris, estudiado por Ronald Fisher en 1936, utiliza esta estructura para mostrar las caracter\u00edsticas de diferentes especies de flores.</p> <p>Descripci\u00f3n de los columnas</p> <ul> <li>sepal_length: Longitud del s\u00e9palo en cent\u00edmetros.</li> <li>sepal_width: Ancho del s\u00e9palo en cent\u00edmetros.</li> <li>petal_length: Longitud del p\u00e9talo en cent\u00edmetros.</li> <li>petal_width: Ancho del p\u00e9talo en cent\u00edmetros.</li> <li>species: Especie de la flor (setosa, versicolor, virginica).</li> </ul>"},{"location":"lectures/machine_learning/011_concepts/#matriz-de-caracteristicas-y-arreglo-de-etiquetas","title":"Matriz de caracter\u00edsticas y Arreglo de Etiquetas\u00b6","text":""},{"location":"lectures/machine_learning/011_concepts/#matriz-de-caracteristicas","title":"Matriz de Caracter\u00edsticas\u00b6","text":"<p>La matriz de caracter\u00edsticas es la representaci\u00f3n de los datos de entrada en un problema de aprendizaje autom\u00e1tico. Se trata de una matriz bidimensional con forma <code>[n_samples, n_features]</code>, donde las filas representan muestras y las columnas las caracter\u00edsticas o atributos que describen cada muestra.</p> <p>Las caracter\u00edsticas suelen ser valores cuantitativos, aunque pueden ser booleanos o discretos. Por convenci\u00f3n, esta matriz se almacena en una variable llamada <code>X</code>.</p>"},{"location":"lectures/machine_learning/011_concepts/#arreglo-de-etiquetas","title":"Arreglo de Etiquetas\u00b6","text":"<p>Adem\u00e1s de la matriz de caracter\u00edsticas <code>X</code>, trabajamos con un arreglo de etiquetas (<code>target array</code>), com\u00fanmente llamado <code>y</code>. Este arreglo es unidimensional y tiene una longitud igual a <code>n_samples</code>, almacenando los valores objetivo para cada muestra. Los valores pueden ser num\u00e9ricos continuos o clases discretas.</p> <p>Aunque algunos estimadores de Scikit-Learn permiten una matriz bidimensional <code>[n_samples, n_targets]</code>, generalmente nos enfocamos en el caso m\u00e1s com\u00fan de un arreglo unidimensional.</p>"},{"location":"lectures/machine_learning/011_concepts/#conjunto-de-entrenamiento-y-prueba","title":"Conjunto de Entrenamiento y Prueba\u00b6","text":"<p>El conjunto de entrenamiento (train set) y el conjunto de prueba (test set) son dos subconjuntos de datos utilizados en aprendizaje autom\u00e1tico.</p> <p>El primero se usa para entrenar el modelo, mientras que el segundo se reserva para evaluar su rendimiento y capacidad de generalizaci\u00f3n.</p> <p></p>"},{"location":"lectures/machine_learning/011_concepts/#error-de-estimacion","title":"Error de Estimaci\u00f3n\u00b6","text":"<p>El error de estimaci\u00f3n es la diferencia entre los valores reales y los valores predichos por un modelo. En aprendizaje autom\u00e1tico, se refiere a la discrepancia entre la variable objetivo (dependiente) y la predicci\u00f3n del modelo. El objetivo principal es minimizar este error para mejorar la precisi\u00f3n en las predicciones.</p> <p>El c\u00e1lculo del error de estimaci\u00f3n depende del tipo de datos en la variable objetivo:</p> <ul> <li>Num\u00e9ricos: Se utilizan m\u00e9tricas como el Error Cuadr\u00e1tico Medio (MSE) o el Error Absoluto Medio (MAE).</li> <li>Categ\u00f3ricos: Se aplican m\u00e9tricas como la precisi\u00f3n o el F1-score.</li> </ul> <p></p>"},{"location":"lectures/machine_learning/011_concepts/#error-de-estimacion-datos-numericos","title":"Error de Estimaci\u00f3n: Datos Num\u00e9ricos\u00b6","text":"<p>Para datos num\u00e9ricos, el error de estimaci\u00f3n se calcula como la diferencia entre el valor real y el valor predicho por el modelo. Este error se expresa matem\u00e1ticamente como:</p> <p>$$\\text{Error} = \\text{Valor real} - \\text{Valor predicho}$$</p> <p>El objetivo es minimizar esta diferencia. Las m\u00e9tricas comunes para evaluar este error incluyen:</p> <ul> <li>Error Absoluto Medio (MAE): Promedio de los valores absolutos de las diferencias.</li> <li>Error Cuadr\u00e1tico Medio (MSE): Promedio de los cuadrados de las diferencias, sensible a valores at\u00edpicos.</li> <li>Ra\u00edz del Error Cuadr\u00e1tico Medio (RMSE): Ra\u00edz cuadrada del MSE, proporcionando una m\u00e9trica en las mismas unidades que los datos.</li> </ul> <p>Estas m\u00e9tricas permiten evaluar la precisi\u00f3n del modelo y ajustar sus par\u00e1metros para mejorar las predicciones.</p> <p></p>"},{"location":"lectures/machine_learning/011_concepts/#error-de-estimacion-datos-categoricos","title":"Error de Estimaci\u00f3n: Datos Categ\u00f3ricos\u00b6","text":"<p>Para calcular el error de estimaci\u00f3n en datos categ\u00f3ricos, primero es importante comprender conceptos como la matriz de confusi\u00f3n. Por ejemplo, si los valores de $y$ representan categor\u00edas como \"alto\" y \"bajo\", a menudo se convierten en n\u00fameros enteros (por ejemplo, \"alto\" = 1, \"bajo\" = 0). Luego, se cuenta cu\u00e1ntas veces la predicci\u00f3n ($\\hat{y}$) coincide con el valor real ($y$).</p> <p></p> <p>A continuaci\u00f3n, veamos un ejemplo pr\u00e1ctico para un problema de clasificaci\u00f3n binaria.</p>"},{"location":"lectures/machine_learning/011_concepts/#curva-roc-y-auc","title":"Curva ROC y AUC\u00b6","text":""},{"location":"lectures/machine_learning/011_concepts/#referencias","title":"Referencias\u00b6","text":"<ul> <li><p>Scikit-learn: Error Metrics - La documentaci\u00f3n oficial de Scikit-learn proporciona ejemplos de c\u00f3mo calcular m\u00e9tricas de error como MAE, MSE y RMSE.</p> </li> <li><p>Wikipedia: Matriz de Confusi\u00f3n - Un art\u00edculo introductorio que explica los conceptos de TP, FP, TN y FN.</p> </li> <li><p>Scikit-learn: ROC Curve - Ejemplos pr\u00e1cticos sobre c\u00f3mo calcular y graficar la curva ROC y AUC para modelos de clasificaci\u00f3n.</p> </li> <li><p>M\u00e9tricas de Clasificaci\u00f3n en Python - Un art\u00edculo simple que cubre las m\u00e9tricas de evaluaci\u00f3n como Accuracy, Recall, Precision y F1-Score.</p> </li> <li><p>Documentaci\u00f3n de Scikit-learn: train_test_split - Explicaci\u00f3n detallada y ejemplos de c\u00f3mo dividir los datos en conjuntos de entrenamiento y prueba.</p> </li> <li><p>Machine Learning Mastery: ROC Curves and AUC - Explicaci\u00f3n clara y sencilla sobre c\u00f3mo interpretar la curva ROC y el AUC, con ejemplos en Python.</p> </li> </ul>"},{"location":"lectures/machine_learning/012_sklearn/","title":"Scikit-Learn","text":"In\u00a0[57]: Copied! <pre># Importar las bibliotecas necesarias\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Generar datos de ejemplo\nrng = np.random.RandomState(42)  # Semilla para reproducibilidad\nn_size = 500  # N\u00famero de muestras\nx = 10 * rng.rand(n_size)  # Generar valores aleatorios para x\ny = 2 * x - 1 + rng.randn(n_size)  # Generar y con una relaci\u00f3n lineal y ruido\n\n# Crear un DataFrame con los datos generados\ndf = pd.DataFrame({'x': x, 'y': y})\n\n# Graficar los datos\nplt.figure(figsize=(10, 6))  # Ajustar el tama\u00f1o de la figura\nsns.scatterplot(data=df, x=\"x\", y=\"y\", color='blue', alpha=0.6)  # Graficar los puntos\nplt.title('Gr\u00e1fica de Dispersi\u00f3n: Datos Generados')  # T\u00edtulo de la gr\u00e1fica\nplt.xlabel('Variable Independiente (x)')  # Etiqueta del eje x\nplt.ylabel('Variable Dependiente (y)')  # Etiqueta del eje y\nplt.axhline(0, color='gray', linestyle='--', linewidth=0.8)  # L\u00ednea horizontal en y=0\nplt.axvline(0, color='gray', linestyle='--', linewidth=0.8)  # L\u00ednea vertical en x=0\nplt.show()  # Mostrar la gr\u00e1fica\n</pre> # Importar las bibliotecas necesarias import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns   # Generar datos de ejemplo rng = np.random.RandomState(42)  # Semilla para reproducibilidad n_size = 500  # N\u00famero de muestras x = 10 * rng.rand(n_size)  # Generar valores aleatorios para x y = 2 * x - 1 + rng.randn(n_size)  # Generar y con una relaci\u00f3n lineal y ruido  # Crear un DataFrame con los datos generados df = pd.DataFrame({'x': x, 'y': y})  # Graficar los datos plt.figure(figsize=(10, 6))  # Ajustar el tama\u00f1o de la figura sns.scatterplot(data=df, x=\"x\", y=\"y\", color='blue', alpha=0.6)  # Graficar los puntos plt.title('Gr\u00e1fica de Dispersi\u00f3n: Datos Generados')  # T\u00edtulo de la gr\u00e1fica plt.xlabel('Variable Independiente (x)')  # Etiqueta del eje x plt.ylabel('Variable Dependiente (y)')  # Etiqueta del eje y plt.axhline(0, color='gray', linestyle='--', linewidth=0.8)  # L\u00ednea horizontal en y=0 plt.axvline(0, color='gray', linestyle='--', linewidth=0.8)  # L\u00ednea vertical en x=0 plt.show()  # Mostrar la gr\u00e1fica <p>Definici\u00f3n de la Matriz de Caracter\u00edsticas y el Arreglo de Etiquetas</p> <p>En esta secci\u00f3n, definiremos una matriz de caracter\u00edsticas bidimensional y un vector unidimensional de etiquetas (objetivos).</p> In\u00a0[58]: Copied! <pre># definir X e y\nX = df[['x']] # como dataframe\ny = df['y'] # como series\n</pre> # definir X e y X = df[['x']] # como dataframe y = df['y'] # como series <p>Divisi\u00f3n de los Datos en Conjuntos de Entrenamiento y Prueba</p> <p>Para evaluar el rendimiento del modelo, es crucial dividir los datos en conjuntos de entrenamiento y prueba. Utilizamos la funci\u00f3n <code>train_test_split</code>  de Scikit-Learn para realizar esta tarea, especificando un 20% de los datos para el conjunto de prueba. Esto nos permite entrenar el modelo con el conjunto de entrenamiento y validarlo posteriormente con el conjunto de prueba.</p> In\u00a0[59]: Copied! <pre># Importar la funci\u00f3n necesaria para dividir los datos\nfrom sklearn.model_selection import train_test_split\n\n# Dividir los datos en conjuntos de entrenamiento y prueba\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Mostrar el tama\u00f1o de cada conjunto\nprint(\"Tama\u00f1o del conjunto de entrenamiento (x_train):\", x_train.shape)\nprint(\"Tama\u00f1o del arreglo de etiquetas de entrenamiento (y_train):\", y_train.shape)\nprint(\"Tama\u00f1o del conjunto de prueba (x_test):\", x_test.shape)\nprint(\"Tama\u00f1o del arreglo de etiquetas de prueba (y_test):\", y_test.shape)\n</pre> # Importar la funci\u00f3n necesaria para dividir los datos from sklearn.model_selection import train_test_split  # Dividir los datos en conjuntos de entrenamiento y prueba x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Mostrar el tama\u00f1o de cada conjunto print(\"Tama\u00f1o del conjunto de entrenamiento (x_train):\", x_train.shape) print(\"Tama\u00f1o del arreglo de etiquetas de entrenamiento (y_train):\", y_train.shape) print(\"Tama\u00f1o del conjunto de prueba (x_test):\", x_test.shape) print(\"Tama\u00f1o del arreglo de etiquetas de prueba (y_test):\", y_test.shape)  <pre>Tama\u00f1o del conjunto de entrenamiento (x_train): (400, 1)\nTama\u00f1o del arreglo de etiquetas de entrenamiento (y_train): (400,)\nTama\u00f1o del conjunto de prueba (x_test): (100, 1)\nTama\u00f1o del arreglo de etiquetas de prueba (y_test): (100,)\n</pre> <p>Selecci\u00f3n de una Clase de Modelo</p> <p>En Scikit-Learn, cada tipo de modelo se representa mediante una clase de Python. Por ejemplo, si deseamos crear un modelo de regresi\u00f3n lineal simple, podemos importar la clase correspondiente para la regresi\u00f3n lineal. Esto nos permite utilizar todas las funcionalidades y m\u00e9todos asociados a dicho modelo de manera intuitiva.</p> In\u00a0[60]: Copied! <pre>from sklearn.linear_model import LinearRegression\n</pre> from sklearn.linear_model import LinearRegression <p>Selecci\u00f3n de Hiperpar\u00e1metros del Modelo</p> <p>Es fundamental diferenciar entre una clase de modelo y su instancia. Al trabajar con un modelo, es necesario considerar preguntas clave sobre su configuraci\u00f3n, como:</p> <ul> <li>\u00bfAjustamos el offset (intersecci\u00f3n en <code>y</code>)?</li> <li>\u00bfNecesitamos que el modelo est\u00e9 normalizado?</li> <li>\u00bfQu\u00e9 grado de regularizaci\u00f3n deseamos?</li> </ul> <p>Estas decisiones se reflejan en los hiperpar\u00e1metros, que se establecen al instanciar el modelo en Scikit-Learn. Por ejemplo, al crear un modelo de regresi\u00f3n lineal con <code>LinearRegression</code>, podemos ajustar el offset usando el hiperpar\u00e1metro <code>fit_intercept</code>.</p> In\u00a0[61]: Copied! <pre># Entrenar el modelo con los datos de entrenamiento\nmodel = LinearRegression(fit_intercept=True)\nmodel\n</pre> # Entrenar el modelo con los datos de entrenamiento model = LinearRegression(fit_intercept=True) model Out[61]: <pre>LinearRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0LinearRegression?Documentation for LinearRegressioniNot fitted<pre>LinearRegression()</pre> <p>Es importante destacar que al crear una instancia del modelo, la \u00fanica acci\u00f3n realizada es almacenar los valores de los hiperpar\u00e1metros. En este punto, a\u00fan no hemos aplicado el modelo a ning\u00fan dato. La API de Scikit-Learn hace una clara distinci\u00f3n entre la selecci\u00f3n del modelo y su aplicaci\u00f3n a los datos.</p> <p>Ajuste del Modelo a los Datos</p> <p>Ahora es el momento de aplicar nuestro modelo a los datos. Esto se realiza utilizando el m\u00e9todo <code>fit()</code> del modelo:</p> In\u00a0[62]: Copied! <pre># Aplicar modelo a los datos\nmodel.fit(x_train, y_train)\n</pre> # Aplicar modelo a los datos model.fit(x_train, y_train) Out[62]: <pre>LinearRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0LinearRegression?Documentation for LinearRegressioniFitted<pre>LinearRegression()</pre> <p>El m\u00e9todo <code>fit()</code> realiza una serie de c\u00e1lculos internos espec\u00edficos del modelo, y los resultados se almacenan en atributos del modelo que el usuario puede explorar.</p> <p>En Scikit-Learn, por convenci\u00f3n, todos los par\u00e1metros aprendidos durante el proceso <code>fit()</code> terminan con guiones bajos. Por ejemplo, en el caso de un modelo lineal, encontramos lo siguiente:</p> In\u00a0[63]: Copied! <pre># Imprimir coeficientes del modelo\nprint(\"Coeficientes del modelo:\", model.coef_)\n\n# Imprimir intercepto del modelo\nprint(\"Intercepto del modelo:\", model.intercept_)\n</pre> # Imprimir coeficientes del modelo print(\"Coeficientes del modelo:\", model.coef_)  # Imprimir intercepto del modelo print(\"Intercepto del modelo:\", model.intercept_) <pre>Coeficientes del modelo: [2.02327729]\nIntercepto del modelo: -1.1227537053702488\n</pre> <p>Los resultados que has obtenido indican lo siguiente:</p> <ul> <li><p>Coeficientes del modelo: [2.02327729]: Este valor representa la pendiente de la l\u00ednea de regresi\u00f3n. En este caso, un coeficiente de aproximadamente 2.02 sugiere que, por cada unidad de incremento en la variable independiente $ x $, se espera que la variable dependiente $ y $ aumente en aproximadamente 2.02 unidades.</p> </li> <li><p>Intercepto del modelo: -1.1227537053702488: Este valor representa el punto donde la l\u00ednea de regresi\u00f3n cruza el eje $ y $. Un intercepto de aproximadamente -1.12 significa que cuando $ x $ es 0, el valor predicho de $ y $ es aproximadamente -1.12.</p> </li> </ul> <p>En resumen, la ecuaci\u00f3n de la regresi\u00f3n lineal que modela tus datos ser\u00eda:</p> <p>$$ y = 2.02x - 1.12 $$</p> <p>Esto implica que existe una relaci\u00f3n lineal positiva entre $ x $ e $ y $ y que la l\u00ednea de regresi\u00f3n se sit\u00faa por debajo del eje $ y $ cuando $ x = 0 $.</p> <p>Predicci\u00f3n de Datos Desconocidos</p> <p>Una vez que el modelo ha sido entrenado, la principal tarea del aprendizaje autom\u00e1tico supervisado es evaluarlo con nuevos datos que no formaron parte del conjunto de entrenamiento. En Scikit-Learn, esto se logra utilizando el m\u00e9todo <code>predict()</code>. Para ilustrar este proceso, generaremos una cuadr\u00edcula de valores de ( x ) y utilizaremos el modelo para predecir los correspondientes valores de ( y ).</p> In\u00a0[64]: Copied! <pre># Crear un conjunto de nuevos valores de x con etiquetas (y) desconocidas\nxfit = x_test.copy()\n\n# Predecir los valores de y utilizando el modelo\nyfit = model.predict(xfit)\n\n# Visualizar los resultados\nplt.scatter(x_test, y_test, label='Datos Reales', color='blue')  # Puntos de datos reales\nplt.plot(xfit, yfit, label='Predicci\u00f3n del Modelo', color='red')  # L\u00ednea de predicci\u00f3n\nplt.xlabel('Valores de x')  # Etiqueta del eje x\nplt.ylabel('Valores de y')  # Etiqueta del eje y\nplt.title('Predicci\u00f3n de Valores de y')  # T\u00edtulo de la gr\u00e1fica\nplt.legend()  # Mostrar leyenda\nplt.show()  # Mostrar la gr\u00e1fica\n</pre> # Crear un conjunto de nuevos valores de x con etiquetas (y) desconocidas xfit = x_test.copy()  # Predecir los valores de y utilizando el modelo yfit = model.predict(xfit)  # Visualizar los resultados plt.scatter(x_test, y_test, label='Datos Reales', color='blue')  # Puntos de datos reales plt.plot(xfit, yfit, label='Predicci\u00f3n del Modelo', color='red')  # L\u00ednea de predicci\u00f3n plt.xlabel('Valores de x')  # Etiqueta del eje x plt.ylabel('Valores de y')  # Etiqueta del eje y plt.title('Predicci\u00f3n de Valores de y')  # T\u00edtulo de la gr\u00e1fica plt.legend()  # Mostrar leyenda plt.show()  # Mostrar la gr\u00e1fica <p>El c\u00f3digo copia <code>x_test</code> a <code>xfit</code>, predice los valores de ( y ) usando <code>xfit</code>, y luego grafica tanto los datos reales como las predicciones, a\u00f1adiendo etiquetas y un t\u00edtulo a la gr\u00e1fica. En resumen, predice y visualiza los valores de ( y ) para los datos de prueba.</p> <p>Explicaci\u00f3n del c\u00f3digo:</p> <ol> <li>Importar Bibliotecas: Se cargan las herramientas necesarias.</li> <li>Cargar Datos: Se carga el conjunto de datos Iris.</li> <li>Preparar y Dividir Datos: Se separan caracter\u00edsticas y etiquetas, y se dividen en conjuntos de entrenamiento y prueba.</li> <li>Entrenar Modelo: Se crea y entrena un modelo de regresi\u00f3n log\u00edstica.</li> <li>Hacer Predicciones: Se predicen las etiquetas del conjunto de prueba.</li> <li>Evaluar Modelo: Se calculan m\u00e9tricas como precisi\u00f3n, F1 Score, y se muestra la matriz de confusi\u00f3n.</li> </ol> <p>M\u00e9tricas de Evaluaci\u00f3n</p> <p>Al evaluar el rendimiento de un modelo de regresi\u00f3n, es fundamental utilizar m\u00e9tricas que cuantifiquen la precisi\u00f3n de las predicciones.</p> In\u00a0[65]: Copied! <pre>from sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Hacer predicciones sobre el conjunto de prueba\ny_pred = model.predict(x_test)\n\n# Calcular MAE, RMSE y MAPE\nmae = mean_absolute_error(y_test, y_pred)\nrmse = mean_squared_error(y_test, y_pred)  # RMSE\nmape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100  # MAPE\n\n# Mostrar resultados\nprint(f'MAE: {mae:.2f}')\nprint(f'RMSE: {rmse:.2f}')\nprint(f'MAPE: {mape:.2f}%')\n</pre> from sklearn.metrics import mean_absolute_error, mean_squared_error  # Hacer predicciones sobre el conjunto de prueba y_pred = model.predict(x_test)  # Calcular MAE, RMSE y MAPE mae = mean_absolute_error(y_test, y_pred) rmse = mean_squared_error(y_test, y_pred)  # RMSE mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100  # MAPE  # Mostrar resultados print(f'MAE: {mae:.2f}') print(f'RMSE: {rmse:.2f}') print(f'MAPE: {mape:.2f}%') <pre>MAE: 0.81\nRMSE: 1.04\nMAPE: 34.64%\n</pre> <p>Explicaci\u00f3n de los resultados:</p> <ul> <li>MAE: 0.81: El error promedio absoluto de las predicciones es 0.81 unidades.</li> <li>RMSE: 1.04: La ra\u00edz del error cuadr\u00e1tico medio es 1.04, indicando que los errores m\u00e1s grandes tienen un impacto significativo.</li> <li>MAPE: 34.64%: El error promedio porcentual absoluto es 34.64%, lo que sugiere que las predicciones son inexactas en un 34.64% en promedio.</li> </ul> In\u00a0[66]: Copied! <pre># Importar las bibliotecas necesarias\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, precision_score, recall_score\n\n# Cargar el conjunto de datos Iris\niris = sns.load_dataset('iris')\n\n# Mostrar las primeras filas del conjunto de datos\niris.head()\n</pre> # Importar las bibliotecas necesarias import seaborn as sns import pandas as pd from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, precision_score, recall_score  # Cargar el conjunto de datos Iris iris = sns.load_dataset('iris')  # Mostrar las primeras filas del conjunto de datos iris.head() Out[66]: sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa In\u00a0[67]: Copied! <pre># Preparar los datos para la regresi\u00f3n log\u00edstica\nX = iris.drop('species', axis=1)  # Caracter\u00edsticas\ny = iris['species']  # Etiquetas\n\n# Dividir los datos en conjuntos de entrenamiento y prueba\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Crear y entrenar el modelo de regresi\u00f3n log\u00edstica\nmodel = LogisticRegression(max_iter=200)\nmodel.fit(X_train, y_train)\n\n# Predecir las etiquetas del conjunto de prueba\ny_pred = model.predict(X_test)\n\n# Calcular y mostrar varias m\u00e9tricas de evaluaci\u00f3n\naccuracy = accuracy_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred, average='weighted')\nprecision = precision_score(y_test, y_pred, average='weighted')\nrecall = recall_score(y_test, y_pred, average='weighted')\nconf_matrix = confusion_matrix(y_test, y_pred)\n\nprint(f'Accuracy: {accuracy:.2f}')\nprint(f'F1 Score: {f1:.2f}')\nprint(f'Precisi\u00f3n: {precision:.2f}')\nprint(f'Recall: {recall:.2f}')\n\nprint()\nprint('Matriz de confusi\u00f3n:')\nprint(conf_matrix)\n</pre> # Preparar los datos para la regresi\u00f3n log\u00edstica X = iris.drop('species', axis=1)  # Caracter\u00edsticas y = iris['species']  # Etiquetas  # Dividir los datos en conjuntos de entrenamiento y prueba X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Crear y entrenar el modelo de regresi\u00f3n log\u00edstica model = LogisticRegression(max_iter=200) model.fit(X_train, y_train)  # Predecir las etiquetas del conjunto de prueba y_pred = model.predict(X_test)  # Calcular y mostrar varias m\u00e9tricas de evaluaci\u00f3n accuracy = accuracy_score(y_test, y_pred) f1 = f1_score(y_test, y_pred, average='weighted') precision = precision_score(y_test, y_pred, average='weighted') recall = recall_score(y_test, y_pred, average='weighted') conf_matrix = confusion_matrix(y_test, y_pred)  print(f'Accuracy: {accuracy:.2f}') print(f'F1 Score: {f1:.2f}') print(f'Precisi\u00f3n: {precision:.2f}') print(f'Recall: {recall:.2f}')  print() print('Matriz de confusi\u00f3n:') print(conf_matrix) <pre>Accuracy: 1.00\nF1 Score: 1.00\nPrecisi\u00f3n: 1.00\nRecall: 1.00\n\nMatriz de confusi\u00f3n:\n[[10  0  0]\n [ 0  9  0]\n [ 0  0 11]]\n</pre> <p>Explicaci\u00f3n del c\u00f3digo:</p> <ol> <li>Importar Bibliotecas: Se cargan las herramientas necesarias.</li> <li>Cargar Datos: Se carga el conjunto de datos Iris.</li> <li>Preparar y Dividir Datos: Se separan caracter\u00edsticas y etiquetas, y se dividen en conjuntos de entrenamiento y prueba.</li> <li>Entrenar Modelo: Se crea y entrena un modelo de regresi\u00f3n log\u00edstica.</li> <li>Hacer Predicciones: Se predicen las etiquetas del conjunto de prueba.</li> <li>Evaluar Modelo: Se calculan m\u00e9tricas como precisi\u00f3n, F1 Score, y se muestra la matriz de confusi\u00f3n.</li> </ol> <p>Resultados del Modelo</p> <ul> <li>Precisi\u00f3n, F1 Score, Precisi\u00f3n y Recall: 1.00 (perfecto).</li> <li>Matriz de Confusi\u00f3n:  Todas las muestras clasificadas correctamente.</li> </ul> <p>El modelo logr\u00f3 un rendimiento perfecto en el conjunto de datos Iris.</p> In\u00a0[68]: Copied! <pre># Importar las bibliotecas necesarias\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\n\n# Cargar el conjunto de datos Iris\niris = sns.load_dataset('iris')\n\n# Separar las caracter\u00edsticas (X) y las etiquetas (y)\nX_iris = iris.drop('species', axis=1)  # Caracter\u00edsticas\ny_iris = iris['species']  # Etiquetas\n\n# Dividir los datos en conjuntos de entrenamiento y prueba (opcional, solo para referencia)\nX_train, X_test, y_train, y_test = train_test_split(X_iris, y_iris, test_size=0.2, random_state=42)\n\n# 1. Elegir la clase de modelo\nfrom sklearn.decomposition import PCA  \n\n# 2. Instanciar el modelo con hiperpar\u00e1metros\nmodel = PCA(n_components=2)    \n\n# 3. Ajustar el modelo a los datos\nmodel.fit(X_iris)\n\n# 4. Transformar los datos\nX_pca = model.transform(X_iris)\n\n# 5. Crear un DataFrame con los resultados\ndf_pca = pd.DataFrame(data=X_pca, columns=['Componente 1', 'Componente 2'])\ndf_pca['Especie'] = y_iris.values  # Agregar la especie al DataFrame\n\n# 6. Visualizar los resultados\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=df_pca, x='Componente 1', y='Componente 2', hue='Especie', palette='Set2', alpha=0.7)\nplt.title('Reducci\u00f3n de Dimensionalidad de Iris usando PCA')\nplt.xlabel('Componente 1')\nplt.ylabel('Componente 2')\nplt.legend(title='Especie')\nplt.grid(True)\nplt.show()\n</pre> # Importar las bibliotecas necesarias import seaborn as sns import pandas as pd import matplotlib.pyplot as plt from sklearn.decomposition import PCA from sklearn.model_selection import train_test_split  # Cargar el conjunto de datos Iris iris = sns.load_dataset('iris')  # Separar las caracter\u00edsticas (X) y las etiquetas (y) X_iris = iris.drop('species', axis=1)  # Caracter\u00edsticas y_iris = iris['species']  # Etiquetas  # Dividir los datos en conjuntos de entrenamiento y prueba (opcional, solo para referencia) X_train, X_test, y_train, y_test = train_test_split(X_iris, y_iris, test_size=0.2, random_state=42)  # 1. Elegir la clase de modelo from sklearn.decomposition import PCA    # 2. Instanciar el modelo con hiperpar\u00e1metros model = PCA(n_components=2)      # 3. Ajustar el modelo a los datos model.fit(X_iris)  # 4. Transformar los datos X_pca = model.transform(X_iris)  # 5. Crear un DataFrame con los resultados df_pca = pd.DataFrame(data=X_pca, columns=['Componente 1', 'Componente 2']) df_pca['Especie'] = y_iris.values  # Agregar la especie al DataFrame  # 6. Visualizar los resultados plt.figure(figsize=(10, 6)) sns.scatterplot(data=df_pca, x='Componente 1', y='Componente 2', hue='Especie', palette='Set2', alpha=0.7) plt.title('Reducci\u00f3n de Dimensionalidad de Iris usando PCA') plt.xlabel('Componente 1') plt.ylabel('Componente 2') plt.legend(title='Especie') plt.grid(True) plt.show() <p>Explicaci\u00f3n del C\u00f3digo</p> <ol> <li>Carga de Bibliotecas: Se importan las bibliotecas necesarias para el an\u00e1lisis y visualizaci\u00f3n.</li> <li>Carga del Conjunto de Datos: Se carga el conjunto de datos Iris y se separan las caracter\u00edsticas de las etiquetas.</li> <li>Divisi\u00f3n de Datos: (Opcional) Se dividen los datos en conjuntos de entrenamiento y prueba, sin uso en PCA.</li> <li>Instanciaci\u00f3n del Modelo PCA: Se crea un modelo PCA para reducir a dos componentes.</li> <li>Ajuste y Transformaci\u00f3n: Se ajusta el modelo a los datos y se obtienen las dos dimensiones.</li> <li>Creaci\u00f3n de un DataFrame: Se genera un DataFrame con las componentes y las especies.</li> <li>Visualizaci\u00f3n: Se grafican las componentes en un gr\u00e1fico de dispersi\u00f3n, diferenciando por especie.</li> </ol> <p>En la representaci\u00f3n bidimensional, las especies se encuentran bien separadas, \u00a1a pesar de que el algoritmo PCA no utiliz\u00f3 las etiquetas de las especies!</p> <p>Esto sugiere que es probable que una clasificaci\u00f3n simple sea efectiva en este conjunto de datos, como se observ\u00f3 anteriormente.</p> In\u00a0[70]: Copied! <pre># Importar las bibliotecas necesarias\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\n# Cargar el conjunto de datos Iris\niris = sns.load_dataset('iris')\n\n# Separar las caracter\u00edsticas (X)\nX_iris = iris.drop('species', axis=1)  # Caracter\u00edsticas\n\n# 1. Elegir la clase de modelo\n# 2. Instanciar el modelo con hiperpar\u00e1metros\nmodel = KMeans(n_clusters=3, n_init=100, random_state=42)\n\n# 3. Ajustar el modelo a los datos\nmodel.fit(X_iris)\n\n# 4. Predecir los grupos\nclusters = model.predict(X_iris)\n\n# 5. A\u00f1adir los grupos al DataFrame\niris['Cluster'] = clusters\n\n# 6. Reducir dimensionalidad para visualizaci\u00f3n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_iris)\n\n# 7. Crear un DataFrame con las componentes PCA y los cl\u00fasteres\ndf_pca = pd.DataFrame(data=X_pca, columns=['PCA1', 'PCA2'])\ndf_pca['Cluster'] = clusters\ndf_pca['Species'] = iris['species']  # Agregar la especie al DataFrame\n\n# 8. Visualizar los resultados con sns.lmplot\nsns.lmplot(x='PCA1', y='PCA2', data=df_pca,\n           hue='Species', col='Cluster', fit_reg=False, palette='Set2')\n\nplt.suptitle('Agrupamiento de Iris usando KMeans y PCA', y=1.02)\nplt.show()\n</pre> # Importar las bibliotecas necesarias import seaborn as sns import pandas as pd import matplotlib.pyplot as plt from sklearn.cluster import KMeans from sklearn.decomposition import PCA  # Cargar el conjunto de datos Iris iris = sns.load_dataset('iris')  # Separar las caracter\u00edsticas (X) X_iris = iris.drop('species', axis=1)  # Caracter\u00edsticas  # 1. Elegir la clase de modelo # 2. Instanciar el modelo con hiperpar\u00e1metros model = KMeans(n_clusters=3, n_init=100, random_state=42)  # 3. Ajustar el modelo a los datos model.fit(X_iris)  # 4. Predecir los grupos clusters = model.predict(X_iris)  # 5. A\u00f1adir los grupos al DataFrame iris['Cluster'] = clusters  # 6. Reducir dimensionalidad para visualizaci\u00f3n pca = PCA(n_components=2) X_pca = pca.fit_transform(X_iris)  # 7. Crear un DataFrame con las componentes PCA y los cl\u00fasteres df_pca = pd.DataFrame(data=X_pca, columns=['PCA1', 'PCA2']) df_pca['Cluster'] = clusters df_pca['Species'] = iris['species']  # Agregar la especie al DataFrame  # 8. Visualizar los resultados con sns.lmplot sns.lmplot(x='PCA1', y='PCA2', data=df_pca,            hue='Species', col='Cluster', fit_reg=False, palette='Set2')  plt.suptitle('Agrupamiento de Iris usando KMeans y PCA', y=1.02) plt.show() <p>Explicaci\u00f3n del C\u00f3digo</p> <ol> <li>Carga de Bibliotecas: Se importan las bibliotecas necesarias para an\u00e1lisis, agrupamiento y visualizaci\u00f3n.</li> <li>Carga del Conjunto de Datos: Se carga el conjunto de datos Iris y se separan las caracter\u00edsticas.</li> <li>Instanciaci\u00f3n del Modelo KMeans: Se crea una instancia de KMeans especificando el n\u00famero de cl\u00fasteres.</li> <li>Ajuste del Modelo: Se ajusta el modelo a los datos sin etiquetas.</li> <li>Predicci\u00f3n de Grupos: Se predicen los grupos para cada muestra en el conjunto de datos.</li> <li>A\u00f1adir Grupos al DataFrame: Se a\u00f1aden los resultados del agrupamiento al DataFrame.</li> <li>Reducci\u00f3n de Dimensionalidad: Se utiliza PCA para reducir a dos dimensiones.</li> <li>Visualizaci\u00f3n con <code>sns.lmplot</code>: Se visualizan los resultados, mostrando las componentes PCA y coloreando por especie y cl\u00faster.</li> </ol> <p>Al dividir los datos por grupo, podemos observar con claridad cu\u00e1n efectivamente el algoritmo KMeans ha recuperado las etiquetas subyacentes.</p> <p>Esto indica que, incluso sin la intervenci\u00f3n de un experto para identificar las especies de las flores, las caracter\u00edsticas de las flores son lo suficientemente distintas como para que un algoritmo de agrupamiento simple pueda identificar autom\u00e1ticamente la presencia de diferentes grupos de especies.</p>"},{"location":"lectures/machine_learning/012_sklearn/#scikit-learn","title":"Scikit-Learn\u00b6","text":"<p>Scikit-Learn (sklearn) es una de las bibliotecas m\u00e1s populares para aprendizaje autom\u00e1tico en Python, ofreciendo herramientas y algoritmos para tareas como clasificaci\u00f3n, regresi\u00f3n, agrupamiento y reducci\u00f3n de dimensionalidad. Construida sobre NumPy, SciPy, y matplotlib, se integra f\u00e1cilmente con pandas y otros paquetes de an\u00e1lisis de datos.</p>"},{"location":"lectures/machine_learning/012_sklearn/#caracteristicas","title":"Caracter\u00edsticas\u00b6","text":"<ul> <li>F\u00e1cil de usar, eficiente y con excelente documentaci\u00f3n.</li> <li>Herramientas integradas para selecci\u00f3n de caracter\u00edsticas, evaluaci\u00f3n de modelos y validaci\u00f3n cruzada.</li> <li>Ideal para todo tipo de usuarios, desde principiantes hasta expertos.</li> </ul>"},{"location":"lectures/machine_learning/012_sklearn/#api-de-scikit-learn","title":"API de Scikit-Learn\u00b6","text":"<p>La API de Scikit-Learn se rige por principios clave:</p> <ul> <li>Consistencia: Interfaz com\u00fan (<code>fit</code>, <code>predict</code>, <code>transform</code>) para todos los objetos.</li> <li>Inspecci\u00f3n: Exposici\u00f3n del estado interno de los objetos para facilitar su comprensi\u00f3n.</li> <li>Simplicidad: Jerarqu\u00eda de objetos limitada para una curva de aprendizaje m\u00e1s corta.</li> <li>Composici\u00f3n: Facilidad para crear tuber\u00edas y modelos complejos con bloques simples.</li> <li>Valores predeterminados: Configuraciones por defecto adecuadas para obtener resultados sin ajustar muchos par\u00e1metros.</li> </ul> <p>Para m\u00e1s detalles, consulta el Scikit-Learn API paper.</p>"},{"location":"lectures/machine_learning/012_sklearn/#como-usar-scikit-learn","title":"C\u00f3mo Usar Scikit-Learn\u00b6","text":"<p>Para utilizar la API de Scikit-Learn, sigue estos pasos:</p> <ol> <li>Organizar los datos: Estructura tus datos en una matriz de caracter\u00edsticas ($X$) y un vector de objetivos ($y$).</li> <li>Seleccionar un modelo: Importa la clase de estimador adecuada desde Scikit-Learn.</li> <li>Definir hiperpar\u00e1metros: Instancia la clase del modelo con los hiperpar\u00e1metros deseados.</li> <li>Ajustar el modelo: Usa el m\u00e9todo <code>fit()</code> con los datos para entrenar el modelo.</li> <li>Aplicar el modelo a nuevos datos:<ul> <li>Para aprendizaje supervisado, utiliza <code>predict()</code> para generar predicciones.</li> <li>Para aprendizaje no supervisado, emplea <code>transform()</code> o <code>predict()</code> para inferir propiedades o transformar los datos.</li> </ul> </li> </ol> <p>A continuaci\u00f3n, veremos algunos ejemplos pr\u00e1cticos de aprendizaje supervisado y no supervisado.</p>"},{"location":"lectures/machine_learning/012_sklearn/#aprendizaje-supervisado-regresion-lineal-simple","title":"Aprendizaje Supervisado: Regresi\u00f3n Lineal Simple\u00b6","text":"<p>Para ilustrar el proceso, consideremos un ejemplo b\u00e1sico de regresi\u00f3n lineal, donde ajustamos una l\u00ednea a un conjunto de datos $(x, y)$. Usaremos un conjunto de datos sencillo para este caso de regresi\u00f3n:</p>"},{"location":"lectures/machine_learning/012_sklearn/#aprendizaje-supervisado-clasificacion-iris","title":"Aprendizaje Supervisado: Clasificaci\u00f3n Iris\u00b6","text":"<p>En este ejemplo, utilizaremos el conjunto de datos Iris. La pregunta que abordaremos es: dado un modelo entrenado con una parte del conjunto de datos Iris, \u00bfqu\u00e9 tan bien podemos predecir las etiquetas de los datos restantes?</p>"},{"location":"lectures/machine_learning/012_sklearn/#aprendizaje-no-supervisado-reduccion-de-dimensionalidad-en-iris","title":"Aprendizaje No Supervisado: Reducci\u00f3n de Dimensionalidad en Iris\u00b6","text":"<p>En este ejemplo, reduciremos la dimensionalidad del conjunto de datos Iris, que tiene cuatro caracter\u00edsticas por muestra. La reducci\u00f3n de dimensionalidad facilita la visualizaci\u00f3n de datos, ya que es m\u00e1s sencillo graficar en dos dimensiones.</p> <p>Usaremos el an\u00e1lisis de componentes principales (PCA) para obtener dos componentes que representen los datos en dos dimensiones.</p> <p>A continuaci\u00f3n, se presentan los pasos a seguir:</p>"},{"location":"lectures/machine_learning/012_sklearn/#aprendizaje-no-supervisado-agrupamiento-de-iris","title":"Aprendizaje No Supervisado: Agrupamiento de Iris\u00b6","text":"<p>A continuaci\u00f3n, exploraremos c\u00f3mo aplicar el agrupamiento a los datos de Iris. Un algoritmo de agrupamiento busca identificar diferentes grupos en los datos sin hacer referencia a etiquetas predefinidas. En este caso, utilizaremos un m\u00e9todo eficaz de agrupamiento conocido como KMeans.</p>"},{"location":"lectures/machine_learning/012_sklearn/#referencias","title":"Referencias\u00b6","text":"<ul> <li>Scikit-learn Documentation: Scikit-learn</li> <li>Introduction to Statistical Learning: ISLR</li> </ul>"},{"location":"lectures/machine_learning/01_intro/","title":"Introducci\u00f3n","text":""},{"location":"lectures/machine_learning/01_intro/#introduccion","title":"Introducci\u00f3n\u00b6","text":"<p>El Machine Learning (aprendizaje autom\u00e1tico) es una rama de la inteligencia artificial que desarrolla algoritmos y modelos matem\u00e1ticos para que las computadoras aprendan a realizar tareas a partir de datos, sin programaci\u00f3n espec\u00edfica para cada tarea.</p> <p>Este proceso implica entrenar un modelo con datos que le ense\u00f1an a reconocer patrones y relaciones. Una vez entrenado, el modelo puede aplicarse a nuevos datos para hacer predicciones o tomar decisiones.</p> <p>El machine learning se utiliza en diversas aplicaciones, como an\u00e1lisis de datos, reconocimiento de voz, clasificaci\u00f3n de im\u00e1genes y veh\u00edculos aut\u00f3nomos. En esencia, permite a las computadoras aprender y mejorar con la experiencia.</p>"},{"location":"lectures/machine_learning/01_intro/#modelos-matematicos","title":"Modelos Matem\u00e1ticos\u00b6","text":"<p>Un modelo matem\u00e1tico es una representaci\u00f3n de un sistema real mediante ecuaciones que relacionan sus variables, usado para describir y predecir su comportamiento.</p> <p>Caracter\u00edsticas:</p> <ul> <li>Se aplican en disciplinas como f\u00edsica, econom\u00eda y biolog\u00eda.</li> <li>Su validez depende de pruebas y observaciones experimentales.</li> </ul> <p>No es:</p> <ul> <li>Una r\u00e9plica exacta de la realidad.</li> <li>Un sustituto de mediciones o experimentos.</li> </ul>"},{"location":"lectures/machine_learning/01_intro/#tipos-de-problemas","title":"Tipos de Problemas\u00b6","text":"<p>El Machine Learning es una t\u00e9cnica que aborda diversos problemas en distintos campos y aplicaciones. Entre los principales tipos de problemas que puede resolver se incluyen:</p> <ul> <li>Aprendizaje supervisado</li> <li>Aprendizaje no supervisado</li> <li>Aprendizaje por refuerzo</li> </ul>"},{"location":"lectures/machine_learning/01_intro/#aprendizaje-supervisado","title":"Aprendizaje Supervisado\u00b6","text":"<p>El aprendizaje supervisado es un enfoque en el que el sistema aprende a partir de datos etiquetados, es decir, datos en los que se conoce la respuesta correcta. El algoritmo analiza estos datos y genera una funci\u00f3n que relaciona las entradas (caracter\u00edsticas) con las salidas esperadas (etiquetas o valores).</p> <ul> <li>Ejemplo: La clasificaci\u00f3n de correos electr\u00f3nicos como spam o no spam. El algoritmo se entrena con un conjunto de correos previamente clasificados para identificar patrones y aplicarlos a nuevos correos.</li> <li>Aplicaciones: Reconocimiento de voz, predicci\u00f3n de precios, diagn\u00f3sticos m\u00e9dicos, etc.</li> </ul> <p></p>"},{"location":"lectures/machine_learning/01_intro/#aprendizaje-no-supervisado","title":"Aprendizaje No Supervisado\u00b6","text":"<p>El aprendizaje no supervisado trabaja con datos que no han sido etiquetados previamente. El sistema debe identificar patrones, agrupaciones o estructuras ocultas dentro de los datos sin una respuesta espec\u00edfica proporcionada durante el entrenamiento.</p> <ul> <li>Ejemplo: Agrupar clientes en un mercado seg\u00fan sus h\u00e1bitos de compra. El algoritmo analiza los datos y detecta grupos de comportamiento similares sin conocer de antemano a qu\u00e9 categor\u00eda pertenece cada cliente.</li> <li>Aplicaciones: An\u00e1lisis de segmentaci\u00f3n de clientes, reducci\u00f3n de dimensionalidad, detecci\u00f3n de anomal\u00edas.</li> </ul> <p></p>"},{"location":"lectures/machine_learning/01_intro/#aprendizaje-por-refuerzo","title":"Aprendizaje por Refuerzo\u00b6","text":"<p>El aprendizaje por refuerzo es un enfoque inspirado en la psicolog\u00eda conductista. Un agente aprende a tomar decisiones en un entorno para maximizar una \"recompensa\" acumulada. Se basa en la prueba y error, donde el agente recibe recompensas o castigos seg\u00fan las acciones que tome, lo que le permite mejorar su comportamiento a lo largo del tiempo.</p> <ul> <li>Ejemplo: Un agente de inteligencia artificial aprendiendo a jugar un videojuego. El agente recibe puntos (recompensas) por tomar acciones correctas y pierde puntos (castigos) por acciones incorrectas, refinando su estrategia con la experiencia.</li> <li>Aplicaciones: Conducci\u00f3n aut\u00f3noma, rob\u00f3tica, sistemas de recomendaci\u00f3n, optimizaci\u00f3n de procesos.</li> </ul> <p></p>"},{"location":"lectures/machine_learning/01_intro/#algoritmos-mas-utilizados","title":"Algoritmos m\u00e1s Utilizados\u00b6","text":"<p>Los algoritmos m\u00e1s comunes en Machine Learning incluyen:</p> <ol> <li><p>Regresi\u00f3n Lineal: M\u00e9todo estad\u00edstico que modela la relaci\u00f3n entre una variable dependiente y una o m\u00e1s variables independientes utilizando una l\u00ednea recta. Es \u00fatil para predecir valores continuos, como precios o temperaturas.</p> </li> <li><p>Regresi\u00f3n Log\u00edstica: Modelo utilizado para problemas de clasificaci\u00f3n binaria (s\u00ed/no, verdadero/falso). Utiliza una funci\u00f3n log\u00edstica para predecir la probabilidad de que una entrada pertenezca a una de las dos clases.</p> </li> <li><p>\u00c1rboles de Decisi\u00f3n: Estructuras de datos que dividen iterativamente el conjunto de datos en subconjuntos basados en caracter\u00edsticas espec\u00edficas. Son \u00fatiles para problemas de clasificaci\u00f3n y regresi\u00f3n, proporcionando interpretaciones claras de las decisiones.</p> </li> <li><p>Random Forest: Conjunto de \u00e1rboles de decisi\u00f3n entrenados con diferentes subconjuntos del conjunto de datos. Combina las predicciones de m\u00faltiples \u00e1rboles para mejorar la precisi\u00f3n y reducir el riesgo de sobreajuste.</p> </li> <li><p>SVM (M\u00e1quinas de Vectores de Soporte): Algoritmo de clasificaci\u00f3n que busca encontrar el hiperplano que mejor separa las clases de datos. Es efectivo en espacios de alta dimensionalidad y cuando la separaci\u00f3n entre clases no es lineal.</p> </li> <li><p>KNN (K-Vecinos m\u00e1s Cercanos): Algoritmo de clasificaci\u00f3n que asigna una nueva instancia a la clase m\u00e1s com\u00fan entre sus \"K\" vecinos m\u00e1s cercanos en el espacio de caracter\u00edsticas. Es sencillo y \u00fatil para tareas de clasificaci\u00f3n y regresi\u00f3n.</p> </li> <li><p>K-means: Algoritmo de agrupamiento (clustering) que divide un conjunto de datos en \"K\" grupos bas\u00e1ndose en sus caracter\u00edsticas. Asigna cada punto de datos al grupo con el centroide m\u00e1s cercano, minimizando la variaci\u00f3n interna de los grupos.</p> </li> </ol>"},{"location":"lectures/machine_learning/01_intro/#que-se-necesita-para-aprender-machine-learning","title":"\u00bfQu\u00e9 se Necesita para Aprender Machine Learning?\u00b6","text":"<ul> <li><p>\u00c1lgebra Lineal: Base matem\u00e1tica que involucra vectores, matrices y operaciones lineales. Es fundamental para entender modelos, optimizar funciones y manejar datos.</p> </li> <li><p>Probabilidad y Estad\u00edstica: Permite analizar y entender los datos, medir la incertidumbre y construir modelos predictivos.</p> </li> <li><p>Optimizaci\u00f3n: Proceso de encontrar los mejores par\u00e1metros para un modelo. Implica minimizar o maximizar funciones objetivo para mejorar el rendimiento de los algoritmos de Machine Learning.</p> </li> </ul>"},{"location":"lectures/machine_learning/01_intro/#librerias-de-machine-learning-en-python","title":"Librer\u00edas de Machine Learning en Python\u00b6","text":"<p>Python destaca por su comunidad activa y numerosas librer\u00edas para Machine Learning. Entre las m\u00e1s utilizadas se encuentran:</p>"},{"location":"lectures/machine_learning/01_intro/#scikit-learn","title":"Scikit-Learn\u00b6","text":"<p>Scikit-learn es la librer\u00eda principal para Machine Learning en Python. Incluye implementaciones de algoritmos para clasificaci\u00f3n, extracci\u00f3n de caracter\u00edsticas, regresi\u00f3n, agrupamiento, reducci\u00f3n de dimensiones, selecci\u00f3n de modelos y preprocesamiento.</p>"},{"location":"lectures/machine_learning/01_intro/#statsmodels","title":"Statsmodels\u00b6","text":"<p>Statsmodels se enfoca en modelos estad\u00edsticos y se utiliza principalmente para an\u00e1lisis predictivos y exploratorios, ofreciendo amplias pruebas estad\u00edsticas para validar modelos.</p>"},{"location":"lectures/machine_learning/01_intro/#metodologias-para-proyectos-de-ciencia-de-datos","title":"Metodolog\u00edas para Proyectos de Ciencia de Datos\u00b6","text":""},{"location":"lectures/machine_learning/01_intro/#principales-metodologias","title":"Principales Metodolog\u00edas\u00b6","text":"<p>Existen diversas metodolog\u00edas para proyectos de ciencia de datos, cada una con un enfoque particular. Las m\u00e1s destacadas incluyen:</p> <ul> <li><p>CRISP-DM: Un proceso est\u00e1ndar para la miner\u00eda de datos, estructurado en seis fases que gu\u00edan el proyecto de principio a fin.</p> </li> <li><p>KDD: Se enfoca en la extracci\u00f3n de conocimiento a partir de grandes conjuntos de datos, siguiendo un proceso sistem\u00e1tico.</p> </li> </ul> <p>La elecci\u00f3n de la metodolog\u00eda depende de los objetivos y requisitos espec\u00edficos del proyecto. No obstante, un enfoque estructurado y sistem\u00e1tico siempre es fundamental para abordar todas las etapas del proceso de ciencia de datos.</p>"},{"location":"lectures/machine_learning/01_intro/#pasos-generales-a-seguir","title":"Pasos Generales a Seguir\u00b6","text":"<p>Las metodolog\u00edas se pueden resumir en 6 pasos clave:</p> <ol> <li><p>Recolectar los datos: Los datos se pueden obtener de diversas fuentes, como sitios web, APIs o bases de datos.</p> </li> <li><p>Preprocesar los datos: Requiere realizar tareas como limpieza, transformaci\u00f3n y normalizaci\u00f3n de datos, esenciales para asegurar su calidad antes de su uso.</p> </li> <li><p>Explorar los datos: Consiste en un an\u00e1lisis preliminar para identificar patrones, valores faltantes o errores que puedan influir en la etapa de modelado.</p> </li> <li><p>Entrenar el modelo: En esta etapa, se construyen y ajustan los modelos utilizando los datos preprocesados, extrayendo informaci\u00f3n \u00fatil para futuras predicciones.</p> </li> <li><p>Evaluar el modelo: Se verifica la precisi\u00f3n y el rendimiento del modelo. Si los resultados no son satisfactorios, se regresa a la etapa de entrenamiento para ajustar los par\u00e1metros.</p> </li> <li><p>Utilizar el modelo: Una vez validado, el modelo se implementa en un entorno real. Su desempe\u00f1o se monitorea continuamente, lo que puede requerir revisar y mejorar las etapas anteriores.</p> </li> </ol> <p>Los pasos 1, 2 y 3 ya se han detallado en este curso, mientras que las etapas de modelado (entrenamiento, evaluaci\u00f3n y predicci\u00f3n) introducen nuevos conceptos que exploraremos a continuaci\u00f3n.</p>"},{"location":"lectures/machine_learning/01_intro/#referencias","title":"Referencias\u00b6","text":"<ol> <li>Basic Concepts in Machine Learning</li> <li>An Introduction to Machine Learning Theory and Its Applications: A Visual Tutorial with Examples</li> </ol>"},{"location":"lectures/machine_learning/basic_01/","title":"Conceptos B\u00e1sicos ML","text":"In\u00a0[4]: Copied! <pre># Carga el conjunto de dato\n\nimport seaborn as sns\niris = sns.load_dataset('iris')\niris.head()\n</pre> # Carga el conjunto de dato  import seaborn as sns iris = sns.load_dataset('iris') iris.head() Out[4]: sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa <p>En este conjunto de datos, cada fila representa una flor observada, y el total de filas indica cu\u00e1ntas flores hay en el conjunto, conocido como <code>n_samples</code>. A estas filas se les denomina muestras (samples).</p> <p>Por otro lado, cada columna contiene informaci\u00f3n espec\u00edfica sobre cada muestra, denominada caracter\u00edstica (feature). El total de columnas se refiere como <code>n_features</code>.</p> <p>La matriz de caracter\u00edsticas es la representaci\u00f3n de los datos de entrada en un problema de aprendizaje autom\u00e1tico. Se trata de una matriz bidimensional con forma <code>[n_samples, n_features]</code>, donde las filas representan muestras y las columnas las caracter\u00edsticas o atributos que describen cada muestra.</p> <p>Las caracter\u00edsticas suelen ser valores cuantitativos, aunque pueden ser booleanos o discretos. Por convenci\u00f3n, esta matriz se almacena en una variable llamada <code>X</code>.</p> In\u00a0[5]: Copied! <pre># Matriz de caracter\u00edsticas\nX = iris.drop('species',axis=1)\nX\n</pre> # Matriz de caracter\u00edsticas X = iris.drop('species',axis=1) X Out[5]: sepal_length sepal_width petal_length petal_width 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2 ... ... ... ... ... 145 6.7 3.0 5.2 2.3 146 6.3 2.5 5.0 1.9 147 6.5 3.0 5.2 2.0 148 6.2 3.4 5.4 2.3 149 5.9 3.0 5.1 1.8 <p>150 rows \u00d7 4 columns</p> In\u00a0[6]: Copied! <pre># target\ny = iris['species']\ny\n</pre> # target y = iris['species'] y Out[6]: <pre>0         setosa\n1         setosa\n2         setosa\n3         setosa\n4         setosa\n         ...    \n145    virginica\n146    virginica\n147    virginica\n148    virginica\n149    virginica\nName: species, Length: 150, dtype: object</pre> <p>La funci\u00f3n <code>train_test_split</code> de Scikit-learn es una herramienta clave para dividir un conjunto de datos en subconjuntos de entrenamiento y prueba. Esta funci\u00f3n permite especificar la proporci\u00f3n de datos que se asignar\u00e1 a cada conjunto, siendo el valor predeterminado 75% para entrenamiento y 25% para prueba. Adem\u00e1s, acepta tanto los datos como sus etiquetas correspondientes como entradas.</p> <p>A continuaci\u00f3n, se muestra un ejemplo de c\u00f3mo utilizarla:</p> In\u00a0[8]: Copied! <pre>from sklearn.model_selection import train_test_split\n\n# separar informacion\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.25, random_state=42)\n\nprint(f\"dimensiones de X_train: {X_train.shape}\")\nprint(f\"dimensiones de y_train: {y_train.shape}\")\nprint(\"\")\nprint(f\"dimensiones de X_test:   {X_test.shape}\")\nprint(f\"dimensiones de y_test:   {y_test.shape}\")\n</pre> from sklearn.model_selection import train_test_split  # separar informacion X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.25, random_state=42)  print(f\"dimensiones de X_train: {X_train.shape}\") print(f\"dimensiones de y_train: {y_train.shape}\") print(\"\") print(f\"dimensiones de X_test:   {X_test.shape}\") print(f\"dimensiones de y_test:   {y_test.shape}\") <pre>dimensiones de X_train: (112, 4)\ndimensiones de y_train: (112,)\n\ndimensiones de X_test:   (38, 4)\ndimensiones de y_test:   (38,)\n</pre> <p>La funci\u00f3n <code>train_test_split(X, y, test_size=0.25, random_state=42)</code> divide los datos en un conjunto de entrenamiento y otro de prueba.</p> <ul> <li><code>X</code> son los datos de entrada y <code>y</code> las etiquetas.</li> <li><code>test_size=0.25</code> asigna el 25% de los datos al conjunto de prueba y el 75% al de entrenamiento.</li> <li><code>random_state=42</code> asegura que la divisi\u00f3n sea reproducible, generando los mismos subconjuntos cada vez que se ejecuta el c\u00f3digo.</li> </ul> <p>Reglas de Separaci\u00f3n</p> <p>Es esencial dividir los datos en conjuntos de entrenamiento y prueba para entrenar el modelo y evaluar su rendimiento en datos no vistos. Generalmente, el conjunto de entrenamiento ocupa entre el 70% y el 80%, mientras que el de prueba entre el 20% y el 30%. En conjuntos grandes, se puede asignar m\u00e1s al entrenamiento.</p> <p>Referencia de divisi\u00f3n:</p> N\u00famero de filas Entrenamiento Prueba 100 - 1,000 70% 30% 1,000 - 100,000 80% 20% M\u00e1s de 100,000 90% 10% In\u00a0[11]: Copied! <pre>from sklearn.linear_model import LogisticRegression\n\n# Crear el modelo de regresi\u00f3n log\u00edstica\nmodel = LogisticRegression()\n\n# Entrenar el modelo con el conjunto de entrenamiento\nmodel.fit(X_train, y_train)\n</pre> from sklearn.linear_model import LogisticRegression  # Crear el modelo de regresi\u00f3n log\u00edstica model = LogisticRegression()  # Entrenar el modelo con el conjunto de entrenamiento model.fit(X_train, y_train) Out[11]: <pre>LogisticRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0LogisticRegression?Documentation for LogisticRegressioniFitted<pre>LogisticRegression()</pre> <p>Es fundamental destacar que al crear una instancia de un modelo en Scikit-Learn, la \u00fanica acci\u00f3n realizada es almacenar los valores de los hiperpar\u00e1metros. En este momento, a\u00fan no hemos aplicado el modelo a ning\u00fan dato. La API de Scikit-Learn establece una clara distinci\u00f3n entre la selecci\u00f3n del modelo y su aplicaci\u00f3n a los datos.</p> <p>Los hiperpar\u00e1metros son configuraciones que se establecen antes de entrenar el modelo y pueden influir significativamente en su rendimiento. Estos par\u00e1metros controlan aspectos como la complejidad del modelo, el tama\u00f1o del \u00e1rbol en un \u00e1rbol de decisi\u00f3n, la tasa de aprendizaje en un modelo de gradiente, o el n\u00famero de vecinos en un clasificador KNN. La elecci\u00f3n adecuada de los hiperpar\u00e1metros puede mejorar la precisi\u00f3n del modelo y ayudar a evitar el sobreajuste.</p> <p>Ejemplo con el modelo de regresi\u00f3n log\u00edstica</p> <pre>model = LogisticRegression(solver='liblinear', C=0.5, max_iter=200)\nmodel.fit(X_train, y_train)\n</pre> In\u00a0[12]: Copied! <pre># Realizar predicciones sobre el conjunto de prueba\ny_pred = model.predict(X_test)\n\n# Mostrar las predicciones\nprint(\"Predicciones:\", y_pred)\n</pre> # Realizar predicciones sobre el conjunto de prueba y_pred = model.predict(X_test)  # Mostrar las predicciones print(\"Predicciones:\", y_pred) <pre>Predicciones: ['versicolor' 'setosa' 'virginica' 'versicolor' 'versicolor' 'setosa'\n 'versicolor' 'virginica' 'versicolor' 'versicolor' 'virginica' 'setosa'\n 'setosa' 'setosa' 'setosa' 'versicolor' 'virginica' 'versicolor'\n 'versicolor' 'virginica' 'setosa' 'virginica' 'setosa' 'virginica'\n 'virginica' 'virginica' 'virginica' 'virginica' 'setosa' 'setosa'\n 'setosa' 'setosa' 'versicolor' 'setosa' 'setosa' 'virginica' 'versicolor'\n 'setosa']\n</pre> In\u00a0[15]: Copied! <pre>from sklearn.metrics import confusion_matrix\n\n# Realizar predicciones sobre el conjunto de prueba\ny_pred = model.predict(X_test)\n\n# Calcular la matriz de confusi\u00f3n\ncm = confusion_matrix(y_test, y_pred)\n\n# Mostrar la matriz de confusi\u00f3n\nprint(\"Matriz de Confusi\u00f3n:\")\nprint(cm)\n</pre> from sklearn.metrics import confusion_matrix  # Realizar predicciones sobre el conjunto de prueba y_pred = model.predict(X_test)  # Calcular la matriz de confusi\u00f3n cm = confusion_matrix(y_test, y_pred)  # Mostrar la matriz de confusi\u00f3n print(\"Matriz de Confusi\u00f3n:\") print(cm) <pre>Matriz de Confusi\u00f3n:\n[[15  0  0]\n [ 0 11  0]\n [ 0  0 12]]\n</pre> In\u00a0[16]: Copied! <pre>from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n\n# Calcular las m\u00e9tricas\naccuracy = accuracy_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred, average='weighted')\nprecision = precision_score(y_test, y_pred, average='weighted')\nf1 = f1_score(y_test, y_pred, average='weighted')\n\n# Mostrar los resultados\nprint(f'Accuracy (Precisi\u00f3n Global): {accuracy:.2f}')\nprint(f'Recall (Sensibilidad): {recall:.2f}')\nprint(f'Precision (Precisi\u00f3n Positiva): {precision:.2f}')\nprint(f'F1-Score: {f1:.2f}')\n</pre> from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix  # Calcular las m\u00e9tricas accuracy = accuracy_score(y_test, y_pred) recall = recall_score(y_test, y_pred, average='weighted') precision = precision_score(y_test, y_pred, average='weighted') f1 = f1_score(y_test, y_pred, average='weighted')  # Mostrar los resultados print(f'Accuracy (Precisi\u00f3n Global): {accuracy:.2f}') print(f'Recall (Sensibilidad): {recall:.2f}') print(f'Precision (Precisi\u00f3n Positiva): {precision:.2f}') print(f'F1-Score: {f1:.2f}') <pre>Accuracy (Precisi\u00f3n Global): 1.00\nRecall (Sensibilidad): 1.00\nPrecision (Precisi\u00f3n Positiva): 1.00\nF1-Score: 1.00\n</pre> <p>Resultados:</p> <ul> <li>Accuracy: 1.00: El 100% de las predicciones fueron correctas, lo que indica que el modelo no cometi\u00f3 errores.</li> <li>Recall: 1.00: El modelo identific\u00f3 el 100% de las muestras positivas correctamente, sin pasar por alto ninguna.</li> <li>Precision: 1.00: El 100% de las predicciones positivas fueron realmente positivas, lo que significa que no hubo falsos positivos.</li> <li>F1-Score: 1.00: Este valor perfecto indica un balance ideal entre precisi\u00f3n y recall, reflejando un modelo excepcional en la identificaci\u00f3n correcta de las clases.</li> </ul> <p>Nota: Se utiliza <code>average='weighted'</code> cuando se tienen tres o m\u00e1s clases en un conjunto de datos. Esto permite calcular las m\u00e9tricas como precisi\u00f3n, recall y F1-Score tomando en cuenta la proporci\u00f3n de cada clase. Por ejemplo, si una clase es m\u00e1s frecuente que otra, su contribuci\u00f3n al resultado final ser\u00e1 mayor. Si solo hay dos clases, este argumento se ignora, ya que las m\u00e9tricas se calculan directamente sin necesidad de ponderar.</p> <p>Teniendo en cuenta lo anterior, el proceso hasta el momento de entrenar el modelo es similar al de un problema de clasificaci\u00f3n. Sin embargo, es importante reconocer que, en el caso de la regresi\u00f3n, debemos enfocarnos en entender las m\u00e9tricas espec\u00edficas que la eval\u00faan. A continuaci\u00f3n, explicaremos estas m\u00e9tricas asociadas a la regresi\u00f3n.</p> In\u00a0[22]: Copied! <pre>import pandas as pd\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\n# Cargar el conjunto de datos de California Housing\ndata = fetch_california_housing()\n\ncalifornia = pd.DataFrame(data.data, columns=data.feature_names)    \ncalifornia['price'] = data.target\ncalifornia.head()\n</pre> import pandas as pd from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression  # Cargar el conjunto de datos de California Housing data = fetch_california_housing()  california = pd.DataFrame(data.data, columns=data.feature_names)     california['price'] = data.target california.head()  Out[22]: MedInc HouseAge AveRooms AveBedrms Population AveOccup Latitude Longitude price 0 8.3252 41.0 6.984127 1.023810 322.0 2.555556 37.88 -122.23 4.526 1 8.3014 21.0 6.238137 0.971880 2401.0 2.109842 37.86 -122.22 3.585 2 7.2574 52.0 8.288136 1.073446 496.0 2.802260 37.85 -122.24 3.521 3 5.6431 52.0 5.817352 1.073059 558.0 2.547945 37.85 -122.25 3.413 4 3.8462 52.0 6.281853 1.081081 565.0 2.181467 37.85 -122.25 3.422 In\u00a0[23]: Copied! <pre># Caracter\u00edsticas dy Target\nX = california.drop(columns=['price'])\ny = california['price'] \n\n# Dividir el conjunto de datos en entrenamiento y prueba\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Crear el modelo de regresi\u00f3n lineal\nmodel = LinearRegression()\n\n# Entrenar el modelo con el conjunto de entrenamiento\nmodel.fit(X_train, y_train)\n\n# Realizar predicciones sobre el conjunto de prueba\ny_pred = model.predict(X_test)\n\n# Imprimir las predicciones\nprint(y_pred)\n</pre>  # Caracter\u00edsticas dy Target X = california.drop(columns=['price']) y = california['price']   # Dividir el conjunto de datos en entrenamiento y prueba X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Crear el modelo de regresi\u00f3n lineal model = LinearRegression()  # Entrenar el modelo con el conjunto de entrenamiento model.fit(X_train, y_train)  # Realizar predicciones sobre el conjunto de prueba y_pred = model.predict(X_test)  # Imprimir las predicciones print(y_pred) <pre>[0.71912284 1.76401657 2.70965883 ... 4.46877017 1.18751119 2.00940251]\n</pre> In\u00a0[29]: Copied! <pre>from sklearn.metrics import mean_absolute_error, root_mean_squared_error, mean_absolute_percentage_error\n\n# Realizar predicciones sobre el conjunto de prueba\ny_pred = model.predict(X_test)\n\n# Calcular las m\u00e9tricas de evaluaci\u00f3n\nmae = mean_absolute_error(y_test, y_pred)\nrmse = root_mean_squared_error(y_test, y_pred)  # RMSE\nmape = mean_absolute_percentage_error(y_test, y_pred)\n\n# Mostrar los resultados\nprint(f'Error Absoluto Medio (MAE): {mae:.2f}')\nprint(f'Ra\u00edz del Error Cuadr\u00e1tico Medio (RMSE): {rmse:.2f}')\nprint(f'Error Porcentual Absoluto Medio (MAPE): {mape:.2f}')\n</pre> from sklearn.metrics import mean_absolute_error, root_mean_squared_error, mean_absolute_percentage_error  # Realizar predicciones sobre el conjunto de prueba y_pred = model.predict(X_test)  # Calcular las m\u00e9tricas de evaluaci\u00f3n mae = mean_absolute_error(y_test, y_pred) rmse = root_mean_squared_error(y_test, y_pred)  # RMSE mape = mean_absolute_percentage_error(y_test, y_pred)  # Mostrar los resultados print(f'Error Absoluto Medio (MAE): {mae:.2f}') print(f'Ra\u00edz del Error Cuadr\u00e1tico Medio (RMSE): {rmse:.2f}') print(f'Error Porcentual Absoluto Medio (MAPE): {mape:.2f}') <pre>Error Absoluto Medio (MAE): 0.53\nRa\u00edz del Error Cuadr\u00e1tico Medio (RMSE): 0.75\nError Porcentual Absoluto Medio (MAPE): 0.32\n</pre> <p>Resultados:</p> <ul> <li>Error Absoluto Medio (MAE): 0.53: Indica que, en promedio, las predicciones se desv\u00edan en 0.53 unidades de los valores reales.</li> <li>Ra\u00edz del Error Cuadr\u00e1tico Medio (RMSE): 0.75: Mide que la desviaci\u00f3n t\u00edpica de las predicciones es de aproximadamente 0.75 unidades.</li> <li>Error Porcentual Absoluto Medio (MAPE): 0.32: Significa que, en promedio, las predicciones se desv\u00edan un 32% respecto a los valores reales.</li> </ul> <p>La normalizaci\u00f3n y la estandarizaci\u00f3n son t\u00e9cnicas clave en el preprocesamiento de datos que ayudan a mejorar el rendimiento de los modelos de aprendizaje autom\u00e1tico.</p> <ul> <li><p>Normalizaci\u00f3n:</p> <ul> <li>La normalizaci\u00f3n es el proceso de escalar los datos para que se encuentren dentro de un rango espec\u00edfico, generalmente entre 0 y 1. Esto se logra utilizando la siguiente f\u00f3rmula: $$ X' = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} $$</li> <li>La normalizaci\u00f3n es \u00fatil cuando las caracter\u00edsticas tienen diferentes escalas y se desea que todas tengan el mismo peso en el modelo.</li> </ul> </li> <li><p>Estandarizaci\u00f3n:</p> <ul> <li>La estandarizaci\u00f3n transforma los datos para que tengan una media de 0 y una desviaci\u00f3n est\u00e1ndar de 1. Se utiliza la siguiente f\u00f3rmula: $$ Z = \\frac{X - \\mu}{\\sigma} $$ donde $ \\mu $ es la media y $ \\sigma $ es la desviaci\u00f3n est\u00e1ndar.</li> <li>La estandarizaci\u00f3n es especialmente \u00fatil para algoritmos que asumen que los datos est\u00e1n distribuidos normalmente, como la regresi\u00f3n log\u00edstica y las m\u00e1quinas de soporte vectorial.</li> </ul> </li> </ul> <p>Ambas t\u00e9cnicas son importantes para asegurar que los modelos de aprendizaje autom\u00e1tico funcionen correctamente y mejoren su capacidad de generalizaci\u00f3n.</p> In\u00a0[33]: Copied! <pre>import pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\n# Cargar el conjunto de datos Iris\niris = load_iris()\nX = pd.DataFrame(iris.data, columns=iris.feature_names)\n\n# Mostrar los datos originales\nprint(\"Datos originales:\")\nX.head()\n</pre> import pandas as pd from sklearn.datasets import load_iris from sklearn.preprocessing import MinMaxScaler, StandardScaler  # Cargar el conjunto de datos Iris iris = load_iris() X = pd.DataFrame(iris.data, columns=iris.feature_names)  # Mostrar los datos originales print(\"Datos originales:\") X.head() <pre>Datos originales:\n</pre> Out[33]: sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2 In\u00a0[34]: Copied! <pre># Normalizaci\u00f3n\nnormalizer = MinMaxScaler()\nX_normalized = normalizer.fit_transform(X)\n\n# Convertir el resultado de la normalizaci\u00f3n a un DataFrame\nX_normalized = pd.DataFrame(X_normalized, columns=X.columns)\n\n# Mostrar los datos normalizados\nprint(\"\\nDatos normalizados:\")\nX_normalized.head()\n</pre>  # Normalizaci\u00f3n normalizer = MinMaxScaler() X_normalized = normalizer.fit_transform(X)  # Convertir el resultado de la normalizaci\u00f3n a un DataFrame X_normalized = pd.DataFrame(X_normalized, columns=X.columns)  # Mostrar los datos normalizados print(\"\\nDatos normalizados:\") X_normalized.head() <pre>\nDatos normalizados:\n</pre> Out[34]: sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 0 0.222222 0.625000 0.067797 0.041667 1 0.166667 0.416667 0.067797 0.041667 2 0.111111 0.500000 0.050847 0.041667 3 0.083333 0.458333 0.084746 0.041667 4 0.194444 0.666667 0.067797 0.041667 In\u00a0[35]: Copied! <pre># Estandarizaci\u00f3n\nscaler = StandardScaler()\nX_standardized = scaler.fit_transform(X)\n\n# Convertir el resultado de la estandarizaci\u00f3n a un DataFrame\nX_standardized = pd.DataFrame(X_standardized, columns=X.columns)\n\n# Mostrar los datos estandarizados\nprint(\"\\nDatos estandarizados:\")\nX_standardized.head()\n</pre> # Estandarizaci\u00f3n scaler = StandardScaler() X_standardized = scaler.fit_transform(X)  # Convertir el resultado de la estandarizaci\u00f3n a un DataFrame X_standardized = pd.DataFrame(X_standardized, columns=X.columns)  # Mostrar los datos estandarizados print(\"\\nDatos estandarizados:\") X_standardized.head() <pre>\nDatos estandarizados:\n</pre> Out[35]: sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 0 -0.900681 1.019004 -1.340227 -1.315444 1 -1.143017 -0.131979 -1.340227 -1.315444 2 -1.385353 0.328414 -1.397064 -1.315444 3 -1.506521 0.098217 -1.283389 -1.315444 4 -1.021849 1.249201 -1.340227 -1.315444 In\u00a0[36]: Copied! <pre>import pandas as pd\n\n# Crear un DataFrame de ejemplo con una variable categ\u00f3rica\ndata = {\n    'Color': ['Rojo', 'Verde', 'Azul', 'Rojo', 'Verde'],\n    'Cantidad': [5, 3, 2, 8, 6]\n}\ndf = pd.DataFrame(data)\n\n# Mostrar el DataFrame original\nprint(\"DataFrame Original:\")\ndf\n</pre> import pandas as pd  # Crear un DataFrame de ejemplo con una variable categ\u00f3rica data = {     'Color': ['Rojo', 'Verde', 'Azul', 'Rojo', 'Verde'],     'Cantidad': [5, 3, 2, 8, 6] } df = pd.DataFrame(data)  # Mostrar el DataFrame original print(\"DataFrame Original:\") df <pre>DataFrame Original:\n</pre> Out[36]: Color Cantidad 0 Rojo 5 1 Verde 3 2 Azul 2 3 Rojo 8 4 Verde 6 In\u00a0[38]: Copied! <pre># Aplicar One Hot Encoding\ndf_encoded = pd.get_dummies(df, columns=['Color'], prefix='Color')\n\n# Mostrar el DataFrame despu\u00e9s de One Hot Encoding\nprint(\"\\nDataFrame con One Hot Encoding:\")\ndf_encoded\n</pre> # Aplicar One Hot Encoding df_encoded = pd.get_dummies(df, columns=['Color'], prefix='Color')  # Mostrar el DataFrame despu\u00e9s de One Hot Encoding print(\"\\nDataFrame con One Hot Encoding:\") df_encoded <pre>\nDataFrame con One Hot Encoding:\n</pre> Out[38]: Cantidad Color_Azul Color_Rojo Color_Verde 0 5 False True False 1 3 False False True 2 2 True False False 3 8 False True False 4 6 False False True"},{"location":"lectures/machine_learning/basic_01/#conceptos-basicos-ml","title":"Conceptos B\u00e1sicos ML\u00b6","text":"<p>Machine Learning (ML) es una rama de la inteligencia artificial que permite a las computadoras aprender de datos y hacer predicciones sin programaci\u00f3n expl\u00edcita. A trav\u00e9s de algoritmos, las m\u00e1quinas identifican patrones y toman decisiones, transformando industrias y mejorando la eficiencia. En esta secci\u00f3n, exploraremos los conceptos fundamentales de ML.</p> <p>\ud83c\udfaf Objetivos:</p> <ul> <li>Entender conceptos b\u00e1sicos de ML</li> <li>Entender el proceso de aprendizaje supervisado (regresi\u00f3n y clasificaci\u00f3n)</li> <li>Heramientas \u00fatiles de Scikit-Learn</li> </ul>"},{"location":"lectures/machine_learning/basic_01/#como-usar-scikit-learn","title":"C\u00f3mo Usar Scikit-Learn\u00b6","text":"<p>Para utilizar la API de Scikit-Learn, sigue estos pasos:</p> <ol> <li>Organizar los datos: Estructura tus datos en una matriz de caracter\u00edsticas ($X$) y un vector de objetivos ($y$).</li> <li>Seleccionar un modelo: Importa la clase de estimador adecuada desde Scikit-Learn.</li> <li>Definir hiperpar\u00e1metros: Instancia la clase del modelo con los hiperpar\u00e1metros deseados.</li> <li>Ajustar el modelo: Usa el m\u00e9todo <code>fit()</code> con los datos para entrenar el modelo.</li> <li>Aplicar el modelo a nuevos datos:<ul> <li>Para aprendizaje supervisado, utiliza <code>predict()</code> para generar predicciones.</li> <li>Para aprendizaje no supervisado, emplea <code>transform()</code> o <code>predict()</code> para inferir propiedades o transformar los datos.</li> </ul> </li> </ol>"},{"location":"lectures/machine_learning/basic_01/#analisis-supervisado-clasificacion","title":"Analisis Supervisado: Clasificaci\u00f3n\u00b6","text":"<p>Un ejemplo cl\u00e1sico es el conjunto de datos Iris, analizado por Ronald Fisher en 1936, que utiliza esta estructura para presentar las caracter\u00edsticas de diferentes especies de flores.</p> <p>Descripci\u00f3n de las Columnas</p> <ul> <li>sepal_length: Longitud del s\u00e9palo en cent\u00edmetros.</li> <li>sepal_width: Ancho del s\u00e9palo en cent\u00edmetros.</li> <li>petal_length: Longitud del p\u00e9talo en cent\u00edmetros.</li> <li>petal_width: Ancho del p\u00e9talo en cent\u00edmetros.</li> <li>species: Especie de la flor (setosa, versicolor, virginica).</li> </ul>"},{"location":"lectures/machine_learning/basic_01/#matriz-de-caracteristicas-y-arreglo-de-etiquetas","title":"Matriz de caracter\u00edsticas y Arreglo de Etiquetas\u00b6","text":""},{"location":"lectures/machine_learning/basic_01/#arreglo-de-etiquetas","title":"Arreglo de Etiquetas\u00b6","text":"<p>Adem\u00e1s de la matriz de caracter\u00edsticas <code>X</code>, trabajamos con un arreglo de etiquetas (<code>target array</code>), com\u00fanmente llamado <code>y</code>. Este arreglo es unidimensional y tiene una longitud igual a <code>n_samples</code>, almacenando los valores objetivo para cada muestra. Los valores pueden ser num\u00e9ricos continuos o clases discretas.</p> <p>Aunque algunos estimadores de Scikit-Learn permiten una matriz bidimensional <code>[n_samples, n_targets]</code>, generalmente nos enfocamos en el caso m\u00e1s com\u00fan de un arreglo unidimensional.</p>"},{"location":"lectures/machine_learning/basic_01/#conjunto-de-entrenamiento-y-prueba","title":"Conjunto de Entrenamiento y Prueba\u00b6","text":"<p>En el aprendizaje autom\u00e1tico, el conjunto de entrenamiento (train set) y el conjunto de prueba (test set) son dos subconjuntos de datos fundamentales.</p> <p>El conjunto de entrenamiento se utiliza para entrenar el modelo, mientras que el conjunto de prueba se reserva para evaluar su rendimiento y capacidad de generalizaci\u00f3n a datos no vistos.</p> <p></p> <ul> <li><p>El conjunto de entrenamiento es un subconjunto de datos utilizado para ajustar los par\u00e1metros del modelo y aprender la relaci\u00f3n entre las variables de entrada y la salida. En esencia, sirve para entrenar al modelo.</p> </li> <li><p>El conjunto de prueba es otro subconjunto, separado del proceso de entrenamiento, empleado para evaluar el rendimiento del modelo. Permite medir su capacidad de generalizaci\u00f3n al enfrentarlo a datos nuevos y no vistos previamente.</p> </li> </ul>"},{"location":"lectures/machine_learning/basic_01/#seleccion-de-una-clase-de-modelo","title":"Selecci\u00f3n de una Clase de Modelo\u00b6","text":"<p>En Scikit-Learn, cada modelo de aprendizaje autom\u00e1tico se representa como una clase de Python. Por ejemplo, para predecir la especie de flores en el conjunto de datos Iris, podemos utilizar una clase de clasificaci\u00f3n, como la regresi\u00f3n log\u00edstica. Este enfoque simplifica la construcci\u00f3n y evaluaci\u00f3n del modelo, permitiendo clasificar las flores en funci\u00f3n de sus caracter\u00edsticas, como la longitud y el ancho de los s\u00e9palos y p\u00e9talos.</p> <p>Nota: Existen diversos modelos para tareas de clasificaci\u00f3n. Puedes consultar la lista completa en el siguiente enlace.</p>"},{"location":"lectures/machine_learning/basic_01/#prediccion","title":"Predicci\u00f3n\u00b6","text":"<p>En el \u00e1mbito del aprendizaje autom\u00e1tico, la predicci\u00f3n se refiere al proceso mediante el cual un modelo utiliza datos de entrada para estimar resultados o clasificaciones. A continuaci\u00f3n, se muestra c\u00f3mo se puede realizar una predicci\u00f3n utilizando un modelo de regresi\u00f3n log\u00edstica en el conjunto de datos Iris.</p>"},{"location":"lectures/machine_learning/basic_01/#metricas","title":"M\u00e9tricas\u00b6","text":"<p>La efectividad de estas predicciones es crucial y se eval\u00faa a trav\u00e9s de diversas m\u00e9tricas que cuantifican el rendimiento del modelo. Estas m\u00e9tricas permiten analizar c\u00f3mo de bien el modelo ha aprendido a partir de los datos de entrenamiento y su capacidad para generalizar a nuevos datos.</p> <p>En esta secci\u00f3n, exploraremos la importancia de la matriz de confusi\u00f3n y las m\u00e9tricas de evaluaci\u00f3n, que nos ayudar\u00e1n a entender y optimizar el rendimiento de nuestros modelos de clasificaci\u00f3n.</p>"},{"location":"lectures/machine_learning/basic_01/#matriz-de-confusion","title":"Matriz de Confusi\u00f3n\u00b6","text":"<p>La matriz de confusi\u00f3n es una herramienta esencial para evaluar el rendimiento de un modelo de clasificaci\u00f3n. Proporciona una representaci\u00f3n visual que resume las predicciones realizadas por el modelo y las compara con los valores reales.</p> <p>Para la clasificaci\u00f3n binaria (por ejemplo, clases 0 y 1), la matriz de confusi\u00f3n tiene la siguiente estructura:</p> <p></p> <p>En esta matriz, se definen los siguientes componentes:</p> <ul> <li>TP (Verdadero Positivo): El modelo predijo correctamente la clase positiva.</li> <li>FP (Falso Positivo): El modelo predijo incorrectamente la clase positiva cuando realmente era negativa.</li> <li>FN (Falso Negativo): El modelo predijo incorrectamente la clase negativa cuando realmente era positiva.</li> <li>TN (Verdadero Negativo): El modelo predijo correctamente la clase negativa.</li> </ul> <p>En resumen, TP y TN representan las predicciones correctas del modelo, mientras que FP y FN indican los errores de clasificaci\u00f3n.</p> <p>La siguiente imagen ilustra los conceptos de FN y FP:</p> <p></p> <p>Ahora, vamos a calcular la matriz de confusi\u00f3n utilizando el comando <code>confusion_matrix</code> de Scikit-Learn:</p>"},{"location":"lectures/machine_learning/basic_01/#metricas-de-evaluacion","title":"M\u00e9tricas de Evaluaci\u00f3n\u00b6","text":"<p>En el contexto de clasificaci\u00f3n, el objetivo es maximizar la cantidad de TP (Verdaderos Positivos) y TN (Verdaderos Negativos), mientras se minimizan los FP (Falsos Positivos) y FN (Falsos Negativos). Para evaluar el rendimiento del modelo, se utilizan las siguientes m\u00e9tricas:</p> <ol> <li><p>Accuracy (Precisi\u00f3n Global):</p> <ul> <li>Mide la proporci\u00f3n de predicciones correctas (positivas y negativas) sobre el total de casos.</li> </ul> <p>$$ accuracy(y, \\hat{y}) = \\frac{TP + TN}{TP + TN + FP + FN} $$</p> </li> <li><p>Recall (Sensibilidad o Tasa de Verdaderos Positivos):</p> <ul> <li>Eval\u00faa la capacidad del modelo para identificar correctamente las muestras positivas.</li> </ul> <p>$$ recall(y, \\hat{y}) = \\frac{TP}{TP + FN} $$</p> </li> <li><p>Precision (Precisi\u00f3n Positiva):</p> <ul> <li>Indica la proporci\u00f3n de verdaderos positivos sobre todas las predicciones positivas.</li> </ul> <p>$$ precision(y, \\hat{y}) = \\frac{TP}{TP + FP} $$</p> </li> <li><p>F-score (F1-Score):</p> <ul> <li>Es la media arm\u00f3nica de la precisi\u00f3n y el recall, proporcionando un balance entre ambos.</li> </ul> <p>$$ fscore(y, \\hat{y}) = 2 \\times \\frac{precision(y, \\hat{y}) \\times recall(y, \\hat{y})}{precision(y, \\hat{y}) + recall(y, \\hat{y})} $$</p> </li> </ol> <p>Veamos un ejemplo pr\u00e1ctico donde calculamos estas m\u00e9tricas utilizando <code>sklearn.metrics</code>:</p>"},{"location":"lectures/machine_learning/basic_01/#analisis-supervisado-regresion","title":"Analisis Supervisado: Regresi\u00f3n\u00b6","text":"<p>Un ejemplo cl\u00e1sico es el conjunto de datos California Housing, que se utiliza para predecir el valor mediano de las viviendas en California en base a diversas caracter\u00edsticas. Este conjunto de datos es ampliamente utilizado en la pr\u00e1ctica de modelos de regresi\u00f3n.</p> <p>Descripci\u00f3n de las Columnas</p> <ul> <li>MedInc: Ingreso mediano de los hogares en la unidad de censos (en miles de d\u00f3lares).</li> <li>HouseAge: Edad mediana de las casas en la unidad de censos (en a\u00f1os).</li> <li>AveRooms: N\u00famero promedio de habitaciones por vivienda.</li> <li>AveOccup: Promedio de ocupantes por vivienda.</li> <li>Latitude: Latitud de la ubicaci\u00f3n de la vivienda.</li> <li>Longitude: Longitud de la ubicaci\u00f3n de la vivienda.</li> <li>Population: N\u00famero de personas que viven en la unidad de censos.</li> <li>Households: N\u00famero de hogares en la unidad de censos.</li> </ul>"},{"location":"lectures/machine_learning/basic_01/#metricas","title":"M\u00e9tricas\u00b6","text":"<p>La efectividad de las predicciones en problemas de regresi\u00f3n es fundamental y se eval\u00faa mediante diversas m\u00e9tricas que cuantifican el rendimiento del modelo. Estas m\u00e9tricas permiten analizar qu\u00e9 tan bien el modelo ha aprendido de los datos de entrenamiento y su capacidad para generalizar a nuevos datos.</p>"},{"location":"lectures/machine_learning/basic_01/#metricas-de-evaluacion","title":"M\u00e9tricas de Evaluaci\u00f3n\u00b6","text":"<p>El resultado de las predicciones nos proporciona el error de estimaci\u00f3n para cada muestra (o fila). Para resumir este error, empleamos m\u00e9tricas de evaluaci\u00f3n, que se dividen en dos categor\u00edas principales:</p> <ol> <li><p>M\u00e9tricas Absolutas: Estas m\u00e9tricas miden el error sin escalar los valores. Las m\u00e9tricas absolutas m\u00e1s comunes son:</p> <ul> <li>Mean Absolute Error (MAE): Esta m\u00e9trica calcula el promedio de los errores absolutos entre las predicciones y los valores reales, proporcionando una medida clara del error promedio.</li> </ul> <p>$$ \\textrm{MAE}(y,\\hat{y}) = \\dfrac{1}{n}\\sum_{t=1}^{n}\\left | y_{t}-\\hat{y}_{t}\\right | $$</p> <ul> <li>Root Mean Squared Error (RMSE): RMSE mide la ra\u00edz cuadrada del promedio de los errores al cuadrado. Esta m\u00e9trica penaliza m\u00e1s fuertemente los errores grandes, lo cual es \u00fatil en contextos donde los errores significativos son especialmente indeseables.</li> </ul> <p>$$ \\textrm{RMSE}(y,\\hat{y}) = \\left( \\dfrac{1}{n}\\sum_{t=1}^{n}\\left ( y_{t}-\\hat{y}_{t}\\right )^2 \\right)^{1/2} $$</p> </li> <li><p>M\u00e9tricas Porcentuales: Estas m\u00e9tricas eval\u00faan el error de manera escalada, acotando los resultados entre 0 y 1, donde 0 indica un ajuste perfecto y 1 refleja un mal ajuste. Es importante destacar que, en algunos casos, estas m\u00e9tricas pueden superar el valor de 1.</p> <ul> <li>Mean Absolute Percentage Error (MAPE): MAPE calcula el error absoluto como un porcentaje del valor real, ofreciendo una forma de entender el error en t\u00e9rminos relativos. $$ \\textrm{MAPE}(y,\\hat{y}) = \\dfrac{1}{n}\\sum_{t=1}^{n}\\left | \\frac{y_{t}-\\hat{y}_{t}}{y_{t}} \\right | $$</li> </ul> </li> </ol> <p>Veamos un ejemplo pr\u00e1ctico donde obtendremos estas m\u00e9tricas utilizando <code>sklearn.metrics</code>:</p>"},{"location":"lectures/machine_learning/basic_01/#otros-conceptos-importantes","title":"Otros Conceptos Importantes\u00b6","text":""},{"location":"lectures/machine_learning/basic_01/#normalizar-y-estandarizar-los-datos","title":"Normalizar y estandarizar los datos\u00b6","text":""},{"location":"lectures/machine_learning/basic_01/#one-hot-encoding","title":"One Hot Encoding\u00b6","text":"<p>One Hot Encoding es una t\u00e9cnica utilizada en el preprocesamiento de datos para convertir variables categ\u00f3ricas en un formato que puede ser f\u00e1cilmente utilizado por algoritmos de aprendizaje autom\u00e1tico. Dado que muchos modelos de machine learning requieren entradas num\u00e9ricas, esta t\u00e9cnica transforma cada categor\u00eda en una columna binaria (0 o 1), lo que permite representar la presencia o ausencia de una categor\u00eda espec\u00edfica.</p> <p>Por ejemplo, si tenemos una variable categ\u00f3rica llamada \"Color\" con las categor\u00edas \"Rojo\", \"Verde\" y \"Azul\", One Hot Encoding convertir\u00e1 esta variable en tres nuevas columnas: \"Color_Rojo\", \"Color_Verde\" y \"Color_Azul\". Si un registro tiene el color \"Rojo\", la columna \"Color_Rojo\" ser\u00e1 1 y las otras dos ser\u00e1n 0.</p> <p>Esta transformaci\u00f3n es esencial para evitar que los modelos interpreten incorrectamente la relaci\u00f3n ordinal entre las categor\u00edas y ayuda a mejorar la precisi\u00f3n del modelo en la clasificaci\u00f3n y regresi\u00f3n.</p>"},{"location":"lectures/machine_learning/basic_01/#referencias","title":"Referencias\u00b6","text":"<ul> <li>Modelos de Regresi\u00f3n en Scikit-Learn: Documentaci\u00f3n sobre los modelos de regresi\u00f3n disponibles en Scikit-Learn.</li> <li>Modelos de Clasificaci\u00f3n en Scikit-Learn: Documentaci\u00f3n sobre los modelos de clasificaci\u00f3n disponibles en Scikit-Learn.</li> </ul>"},{"location":"lectures/machine_learning/cla_01/","title":"Clasificaci\u00f3n I","text":"<p>El modelo es entonces obtenido a base de lo que cada ensayo (valor de $i$) y el conjunto de variables explicativas/independientes puedan informar acerca de la probabilidad final. Estas variables explicativas pueden pensarse como un vector $X_i$ k-dimensional y el modelo toma entonces la forma:</p> <p>$$p_i=\\mathbb{E}(\\dfrac{Y_i}{n_i}|X_i)$$</p> <p>Los logits de las probabilidades binomiales desconocidas (i.e., los logaritmos de la raz\u00f3n de momios) son modeladas como una funci\u00f3n lineal de los $X_i$:</p> <p>$$logit(p_i) = ln(\\dfrac{p_i}{1-p_i}) = \\beta_0+\\beta_1x_{1,i} +....+\\beta_kx_{k,i}$$</p> <p>Note que un elemento particular de $X_i$ puede ser ajustado a 1 para todo $i$ obteni\u00e9ndose una constante independiente en el modelo. Los par\u00e1metros desconocidos $\\beta _{j}$ son usualmente estimados a trav\u00e9s de m\u00e1xima verosimilitud.</p> <p>La interpretaci\u00f3n de los estimados del par\u00e1metro $\\beta _{j}$ es como los efectos aditivos en el logaritmo de la raz\u00f3n de momios para una unidad de cambio en la j\u00e9sima variable explicativa. En el caso de una variable explicativa dicot\u00f3mica, por ejemplo g\u00e9nero, $e^{\\beta}$ es la estimaci\u00f3n de la raz\u00f3n de momios (odds ratio) de tener el resultado para, por decir algo, hombres comparados con mujeres. El modelo tiene una formulaci\u00f3n equivalente dada por:</p> <p>$$p_i=\\dfrac{1}{e^{-(\\beta_0+\\beta_1x_{1,i} +....+\\beta_kx_{k,i})}+1}$$</p> Valores categor\u00eda Probabilidad Logit $X_1$ $\\epsilon_1$ $\\pi(X_1)$ $g(X_1)$ $X_2$ $\\epsilon_2$ $\\pi(X_2)$ $g(X_2)$ $X_n$ $\\epsilon_n$ $\\pi(X_n)$ $g(X_n)$ <p>Donde $\u03b5_i$ es \"0\" o \"1\" seg\u00fan el caso y adem\u00e1s:</p> <p>$$0 \\leq \u03c0(X_i) = \\dfrac{1}{i}\\sum_{k=1}^i \u03b5_k\\leq 1 \\, \\ g(X_i) =  ln(\\dfrac{\\pi(X_i)}{1- \\pi(X_i)})=\\beta_0+\\beta_1 X_i $$</p> <p></p> <p>Veamos un peque\u00f1o ejemplo de como se implementa en python. En este ejemplo voy a utilizar el dataset Iris que ya viene junto con Scikit-learn y es ideal para practicar con regresiones log\u00edstica ; el mismo contiene los tipos de flores basado en en largo y ancho de su s\u00e9palo y p\u00e9talo.</p> In\u00a0[1]: Copied! <pre># librerias\n \nimport os\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \npd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes\n\n# Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab\n%matplotlib inline\n</pre> # librerias   import os import numpy as np import pandas as pd  from sklearn import datasets from sklearn.model_selection import train_test_split  import matplotlib.pyplot as plt import seaborn as sns  pd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes  # Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab %matplotlib inline In\u00a0[2]: Copied! <pre># cargar datos\niris = datasets.load_iris()\nprint(iris.DESCR)\n</pre> # cargar datos iris = datasets.load_iris() print(iris.DESCR) <pre>.. _iris_dataset:\n\nIris plants dataset\n--------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 150 (50 in each of three classes)\n    :Number of Attributes: 4 numeric, predictive attributes and the class\n    :Attribute Information:\n        - sepal length in cm\n        - sepal width in cm\n        - petal length in cm\n        - petal width in cm\n        - class:\n                - Iris-Setosa\n                - Iris-Versicolour\n                - Iris-Virginica\n                \n    :Summary Statistics:\n\n    ============== ==== ==== ======= ===== ====================\n                    Min  Max   Mean    SD   Class Correlation\n    ============== ==== ==== ======= ===== ====================\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n    ============== ==== ==== ======= ===== ====================\n\n    :Missing Attribute Values: None\n    :Class Distribution: 33.3% for each of 3 classes.\n    :Creator: R.A. Fisher\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n    :Date: July, 1988\n\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\nfrom Fisher's paper. Note that it's the same as in R, but not as in the UCI\nMachine Learning Repository, which has two wrong data points.\n\nThis is perhaps the best known database to be found in the\npattern recognition literature.  Fisher's paper is a classic in the field and\nis referenced frequently to this day.  (See Duda &amp; Hart, for example.)  The\ndata set contains 3 classes of 50 instances each, where each class refers to a\ntype of iris plant.  One class is linearly separable from the other 2; the\nlatter are NOT linearly separable from each other.\n\n.. topic:: References\n\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n     Mathematical Statistics\" (John Wiley, NY, 1950).\n   - Duda, R.O., &amp; Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n     (Q327.D83) John Wiley &amp; Sons.  ISBN 0-471-22361-1.  See page 218.\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n     Structure and Classification Rule for Recognition in Partially Exposed\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n     on Information Theory, May 1972, 431-433.\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n     conceptual clustering system finds 3 classes in the data.\n   - Many, many more ...\n</pre> In\u00a0[3]: Copied! <pre># dejar en formato dataframe\n\niris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\niris_df['TARGET'] = iris.target\niris_df.head() # estructura de nuestro dataset.\n</pre> # dejar en formato dataframe  iris_df = pd.DataFrame(iris.data, columns=iris.feature_names) iris_df['TARGET'] = iris.target iris_df.head() # estructura de nuestro dataset. Out[3]: sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) TARGET 0 5.1 3.5 1.4 0.2 0 1 4.9 3.0 1.4 0.2 0 2 4.7 3.2 1.3 0.2 0 3 4.6 3.1 1.5 0.2 0 4 5.0 3.6 1.4 0.2 0 <p>Para ver gr\u00e1ficamente el modelo de regresi\u00f3n log\u00edstica, ajustemos el modelo solo a dos variables: petal length (cm), petal width (cm).</p> In\u00a0[4]: Copied! <pre># datos \nfrom sklearn.linear_model import LogisticRegression\n\nX = iris_df[['sepal length (cm)', 'sepal width (cm)']]\nY = iris_df['TARGET']\n\n# split dataset\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state = 2)\n</pre> # datos  from sklearn.linear_model import LogisticRegression  X = iris_df[['sepal length (cm)', 'sepal width (cm)']] Y = iris_df['TARGET']  # split dataset X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state = 2)  In\u00a0[5]: Copied! <pre># print rows train and test sets\nprint('Separando informacion:\\n')\nprint('numero de filas data original : ',len(X))\nprint('numero de filas train set     : ',len(X_train))\nprint('numero de filas test set      : ',len(X_test))\n</pre> # print rows train and test sets print('Separando informacion:\\n') print('numero de filas data original : ',len(X)) print('numero de filas train set     : ',len(X_train)) print('numero de filas test set      : ',len(X_test)) <pre>Separando informacion:\n\nnumero de filas data original :  150\nnumero de filas train set     :  120\nnumero de filas test set      :  30\n</pre> In\u00a0[6]: Copied! <pre># Creando el modelo\nrlog = LogisticRegression()\nrlog.fit(X_train, Y_train) # ajustando el modelo\n</pre> # Creando el modelo rlog = LogisticRegression() rlog.fit(X_train, Y_train) # ajustando el modelo Out[6]: <pre>LogisticRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression<pre>LogisticRegression()</pre> In\u00a0[7]: Copied! <pre>rlog.score(X_train,Y_train)\n</pre> rlog.score(X_train,Y_train) Out[7]: <pre>0.825</pre> In\u00a0[8]: Copied! <pre>rlog.predict_log_proba(X_train)\n</pre> rlog.predict_log_proba(X_train) Out[8]: <pre>array([[ -3.99669749,  -0.66190745,  -0.76409046],\n       [ -0.21107893,  -1.90464333,  -3.18413364],\n       [ -1.8065849 ,  -0.52153993,  -1.41807287],\n       [ -6.47991623,  -3.03584962,  -0.05083842],\n       [ -1.05110824,  -0.71678289,  -1.81936211],\n       [ -0.17004605,  -2.74876413,  -2.3819842 ],\n       [ -4.68728813,  -0.80298536,  -0.61101654],\n       [ -2.96770066,  -0.71008075,  -0.78312858],\n       [ -3.63294174,  -0.2154118 ,  -1.78765395],\n       [ -5.51537947,  -1.46091555,  -0.26925051],\n       [ -2.98670271,  -0.60263228,  -0.91086157],\n       [ -3.64615407,  -0.71257092,  -0.72664838],\n       [ -4.79873937,  -1.4202387 ,  -0.28754402],\n       [ -2.24998139,  -0.20623326,  -2.51385499],\n       [ -0.06253829,  -2.98157867,  -4.61419118],\n       [ -4.37794957,  -0.51550211,  -0.94097214],\n       [ -0.19806777,  -2.0180827 ,  -3.06239155],\n       [ -4.75300091,  -1.24804972,  -0.35053651],\n       [ -0.10414361,  -2.60012179,  -3.703401  ],\n       [ -0.1399092 ,  -2.36113102,  -3.31733432],\n       [ -0.33299812,  -1.72995085,  -2.24492641],\n       [ -1.36052475,  -0.39394677,  -2.67243331],\n       [ -2.00164029,  -1.07409158,  -0.64764129],\n       [ -7.3103791 ,  -3.31679977,  -0.03763674],\n       [ -0.34589815,  -1.61640035,  -2.36655737],\n       [ -4.75300091,  -1.24804972,  -0.35053651],\n       [ -2.32561455,  -1.12330948,  -0.54978327],\n       [ -8.62419813,  -2.24523278,  -0.11214189],\n       [ -5.53049122,  -0.33797257,  -1.26294071],\n       [ -0.06998861,  -2.75798325,  -5.47839753],\n       [ -0.18803569,  -1.9034554 ,  -3.80038456],\n       [ -3.06265058,  -0.4256791 ,  -1.20427132],\n       [ -2.4142983 ,  -0.45329007,  -1.29085265],\n       [ -1.43446384,  -0.44603054,  -2.10707824],\n       [ -9.03137722,  -2.37765551,  -0.09748871],\n       [ -4.69174057,  -0.68098728,  -0.72419992],\n       [ -3.66230617,  -0.60227249,  -0.85153142],\n       [ -0.05027585,  -3.34866781,  -4.27573592],\n       [ -2.4142983 ,  -0.45329007,  -1.29085265],\n       [ -0.06340959,  -3.33994623,  -3.6495755 ],\n       [ -2.71090648,  -0.4751419 ,  -1.16562856],\n       [ -3.422256  ,  -0.38407763,  -1.2507754 ],\n       [ -1.82139702,  -0.30530631,  -2.28964102],\n       [ -0.23342127,  -2.28448194,  -2.24098902],\n       [ -3.25980162,  -0.24347857,  -1.72761517],\n       [ -2.33611822,  -0.62801104,  -0.99521069],\n       [ -4.75300091,  -1.24804972,  -0.35053651],\n       [ -9.67524275,  -2.21910725,  -0.11515155],\n       [ -0.34353786,  -1.38299432,  -3.22095311],\n       [ -2.6423909 ,  -1.03887894,  -0.55345829],\n       [ -0.03480928,  -3.73440813,  -4.57337069],\n       [ -3.06265058,  -0.4256791 ,  -1.20427132],\n       [ -5.18402425,  -1.53076722,  -0.25099663],\n       [ -0.24523474,  -1.68589809,  -3.43575133],\n       [ -3.35645967,  -0.10909088,  -2.68102926],\n       [ -0.24523474,  -1.68589809,  -3.43575133],\n       [ -0.07746962,  -2.84820416,  -4.09855929],\n       [ -0.12756408,  -2.60168695,  -3.08752732],\n       [ -5.46678423,  -1.28586978,  -0.32938621],\n       [ -2.6423909 ,  -1.03887894,  -0.55345829],\n       [ -0.11158762,  -2.48111528,  -3.81957595],\n       [ -0.01601167,  -4.64447482,  -5.07204482],\n       [ -5.40710339,  -0.59393632,  -0.81336006],\n       [-10.32214698,  -2.06359771,  -0.13585311],\n       [ -5.53049122,  -0.33797257,  -1.26294071],\n       [ -5.07086967,  -2.1981711 ,  -0.12475057],\n       [ -1.66675571,  -0.29897084,  -2.66556294],\n       [ -3.66230617,  -0.60227249,  -0.85153142],\n       [ -4.35319491,  -0.99654955,  -0.48129372],\n       [ -0.10414361,  -2.60012179,  -3.703401  ],\n       [ -8.46843914,  -1.83657274,  -0.17384478],\n       [ -2.33611822,  -0.62801104,  -0.99521069],\n       [ -5.03844127,  -0.75293161,  -0.64906835],\n       [ -0.13370583,  -2.14573723,  -4.80718114],\n       [ -2.13015961,  -0.44390775,  -1.42854624],\n       [ -0.14884726,  -2.24361855,  -3.43500331],\n       [ -0.15976538,  -2.12808615,  -3.55465238],\n       [ -3.1182411 ,  -0.3548191 ,  -1.36859278],\n       [ -5.51537947,  -1.46091555,  -0.26925051],\n       [ -2.5357413 ,  -0.32183203,  -1.62975754],\n       [ -4.33853357,  -0.72898717,  -0.68409426],\n       [ -5.57382666,  -1.64581327,  -0.21896676],\n       [ -6.14300989,  -1.15968166,  -0.37940919],\n       [ -3.67733948,  -1.1231079 ,  -0.43164097],\n       [ -5.51537947,  -1.46091555,  -0.26925051],\n       [ -3.25980162,  -0.24347857,  -1.72761517],\n       [ -1.50374348,  -0.6199054 ,  -1.42833279],\n       [ -2.64559993,  -0.66273639,  -0.88286013],\n       [ -3.69170941,  -0.5052252 ,  -0.98966559],\n       [ -3.06265058,  -0.4256791 ,  -1.20427132],\n       [ -1.70787592,  -0.80218252,  -0.99317107],\n       [ -0.03480928,  -3.73440813,  -4.57337069],\n       [ -6.17079981,  -0.42876844,  -1.05958475],\n       [ -1.50374348,  -0.6199054 ,  -1.42833279],\n       [ -3.18432235,  -0.29444982,  -1.54340497],\n       [ -3.06265058,  -0.4256791 ,  -1.20427132],\n       [ -3.65400219,  -0.97332008,  -0.51703462],\n       [ -5.05989815,  -1.02728954,  -0.45306334],\n       [ -0.03094358,  -4.23634452,  -4.13458123],\n       [ -0.09199006,  -2.50522834,  -5.0785667 ],\n       [ -3.98888929,  -0.78054977,  -0.64755132],\n       [ -3.31830082,  -1.0388256 ,  -0.49443459],\n       [ -7.74336497,  -1.78746182,  -0.18370422],\n       [ -0.05357357,  -3.225515  ,  -4.38776458],\n       [ -6.62509295,  -1.61990942,  -0.2221981 ],\n       [ -0.03965368,  -3.48635147,  -4.79567696],\n       [ -4.35319491,  -0.99654955,  -0.48129372],\n       [ -4.08715355,  -1.38461613,  -0.31089182],\n       [ -0.34589815,  -1.61640035,  -2.36655737],\n       [ -3.31177254,  -0.65294575,  -0.81409912],\n       [ -0.07511489,  -2.74125422,  -4.84422965],\n       [ -5.0899367 ,  -1.18377862,  -0.37437096],\n       [ -1.76295435,  -0.6043599 ,  -1.26571138],\n       [ -0.11158762,  -2.48111528,  -3.81957595],\n       [ -4.35319491,  -0.99654955,  -0.48129372],\n       [ -4.71854451,  -1.08714279,  -0.42481105],\n       [ -0.07746962,  -2.84820416,  -4.09855929],\n       [ -0.01626132,  -4.2872282 ,  -6.03778143],\n       [ -5.40710339,  -0.59393632,  -0.81336006],\n       [ -0.03160108,  -4.48990308,  -3.91777686]])</pre> <p>Grafiquemos nuestro resultados:</p> In\u00a0[9]: Copied! <pre># dataframe a matriz\nX = X.values\nY = Y.values\n</pre> # dataframe a matriz X = X.values Y = Y.values In\u00a0[10]: Copied! <pre># grafica de la regresion logistica \nplt.figure(figsize=(12,4))\n\nx_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\ny_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n\nh = .02  # step size in the mesh\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = rlog.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure(1, figsize=(4, 3))\nplt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired, shading='auto')\n\n# Plot also the training points\nplt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\nplt.show()\n</pre> # grafica de la regresion logistica  plt.figure(figsize=(12,4))  x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5 y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5  h = .02  # step size in the mesh xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) Z = rlog.predict(np.c_[xx.ravel(), yy.ravel()])  # Put the result into a color plot Z = Z.reshape(xx.shape) plt.figure(1, figsize=(4, 3)) plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired, shading='auto')  # Plot also the training points plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired) plt.xlabel('Sepal length') plt.ylabel('Sepal width') plt.show() <pre>C:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\db-connectors-RsEieBu8-py3.8\\lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n  warnings.warn(\n</pre> <p>Gr\u00e1ficamente podemos decir que el modelo se ajusta bastante bien, puesto que las clasificaciones son adecuadas y el modelo no se confunde entre una clase y otra. Por otro lado, existe valores num\u00e9ricos que tambi\u00e9n nos pueden ayudar a convensernos de estos, que son las m\u00e9tricas que se habian definidos con anterioridad.</p> <p>Para ello, instanciaremos las distintas metricas del archivo metrics_classification.py y calcularemos sus distintos valores.</p> In\u00a0[11]: Copied! <pre>from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n</pre> from sklearn.metrics import confusion_matrix from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score In\u00a0[12]: Copied! <pre># Evaluar las m\u00e9tricas\ndef classification_metrics(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Aplicar las distintas m\u00e9tricas definidas\n    :param df: dataframe con las columnas: ['y', 'yhat']\n    :return: dataframe con las m\u00e9tricas especificadas\n    \"\"\"\n    y_true = df['y']\n    y_pred = df['yhat']\n\n    accuracy = round(accuracy_score(y_true, y_pred), 4)\n    recall = round(recall_score(y_true, y_pred, average='macro'), 4)\n    precision = round(precision_score(y_true, y_pred, average='macro'), 4)\n    fscore = round(f1_score(y_true, y_pred, average='macro'), 4)\n\n    df_result = pd.DataFrame({'accuracy': [accuracy],\n                              'recall': [recall],\n                              'precision': [precision],\n                              'fscore': [fscore]})\n\n    return df_result\n</pre> # Evaluar las m\u00e9tricas def classification_metrics(df: pd.DataFrame) -&gt; pd.DataFrame:     \"\"\"     Aplicar las distintas m\u00e9tricas definidas     :param df: dataframe con las columnas: ['y', 'yhat']     :return: dataframe con las m\u00e9tricas especificadas     \"\"\"     y_true = df['y']     y_pred = df['yhat']      accuracy = round(accuracy_score(y_true, y_pred), 4)     recall = round(recall_score(y_true, y_pred, average='macro'), 4)     precision = round(precision_score(y_true, y_pred, average='macro'), 4)     fscore = round(f1_score(y_true, y_pred, average='macro'), 4)      df_result = pd.DataFrame({'accuracy': [accuracy],                               'recall': [recall],                               'precision': [precision],                               'fscore': [fscore]})      return df_result In\u00a0[13]: Copied! <pre># metrics\n\ny_true =  list(Y_test)\ny_pred = list(rlog.predict(X_test))\n\nprint('Valores:\\n')\nprint('originales: ', y_true)\nprint('predicho:   ', y_pred)\n</pre> # metrics  y_true =  list(Y_test) y_pred = list(rlog.predict(X_test))  print('Valores:\\n') print('originales: ', y_true) print('predicho:   ', y_pred) <pre>Valores:\n\noriginales:  [0, 0, 2, 0, 0, 2, 0, 2, 2, 0, 0, 0, 0, 0, 1, 1, 0, 1, 2, 1, 1, 1, 2, 1, 1, 0, 0, 2, 0, 2]\npredicho:    [0, 0, 1, 0, 0, 2, 0, 2, 2, 0, 0, 0, 0, 0, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 0, 0, 1, 0, 2]\n</pre> In\u00a0[14]: Copied! <pre>print('\\nMatriz de confusion:\\n ')\nprint(confusion_matrix(y_true,y_pred))\n</pre> print('\\nMatriz de confusion:\\n ') print(confusion_matrix(y_true,y_pred)) <pre>\nMatriz de confusion:\n \n[[13  1  0]\n [ 0  4  4]\n [ 0  2  6]]\n</pre> In\u00a0[15]: Copied! <pre># ejemplo \ndf_temp = pd.DataFrame(\n    {\n        'y':y_true,\n        'yhat':y_pred\n        }\n)\n\ndf_metrics = classification_metrics(df_temp)\nprint(\"\\nMetricas para los regresores : 'sepal length (cm)' y  'sepal width (cm)'\")\nprint(\"\")\ndf_metrics\n</pre> # ejemplo  df_temp = pd.DataFrame(     {         'y':y_true,         'yhat':y_pred         } )  df_metrics = classification_metrics(df_temp) print(\"\\nMetricas para los regresores : 'sepal length (cm)' y  'sepal width (cm)'\") print(\"\") df_metrics <pre>\nMetricas para los regresores : 'sepal length (cm)' y  'sepal width (cm)'\n\n</pre> Out[15]: accuracy recall precision fscore 0 0.7667 0.7262 0.7238 0.721 <p>Basado en las m\u00e9tricas y en la gr\u00e1fica, podemos concluir que el ajuste realizado es bastante asertado.</p> <p>Ahora, calculamos la curva AUC-ROC para nuestro ejemplo. Cabe destacar que esta curva es efectiva solo para clasificaci\u00f3n binaria, por lo que para efectos pr\u00e1cticos convertiremos nuestro TARGET en binarios (0 \u00f3 1).</p> <p>Para efectos pr\u00e1cticos tranformaremos la clase objetivo (en este caso, la clase 0) a 1, y el resto de las clases (clase 1 y 2) las dejaremos en la clase 0.</p> In\u00a0[16]: Copied! <pre>from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n</pre> from sklearn.metrics import roc_curve from sklearn.metrics import roc_auc_score In\u00a0[17]: Copied! <pre># graficar curva roc\ndef plot_roc_curve(fpr, tpr):\n    plt.figure(figsize=(8,8))\n    plt.plot(fpr, tpr, color='orange', label='ROC')\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()\n</pre> # graficar curva roc def plot_roc_curve(fpr, tpr):     plt.figure(figsize=(8,8))     plt.plot(fpr, tpr, color='orange', label='ROC')     plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')     plt.xlabel('False Positive Rate')     plt.ylabel('True Positive Rate')     plt.title('Receiver Operating Characteristic (ROC) Curve')     plt.legend()     plt.show() In\u00a0[18]: Copied! <pre># separar clase 0 del resto\nX = iris_df[['sepal length (cm)', 'sepal width (cm)']]\nY = iris_df['TARGET'].apply(lambda x: 1 if x ==2 else 0)\nmodel =  LogisticRegression()\n\n# split dataset\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state = 2)\n\n# ajustar modelo \nmodel.fit(X_train,Y_train)\n</pre> # separar clase 0 del resto X = iris_df[['sepal length (cm)', 'sepal width (cm)']] Y = iris_df['TARGET'].apply(lambda x: 1 if x ==2 else 0) model =  LogisticRegression()  # split dataset X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state = 2)  # ajustar modelo  model.fit(X_train,Y_train) Out[18]: <pre>LogisticRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression<pre>LogisticRegression()</pre> In\u00a0[19]: Copied! <pre># calcular score AUC\nprobs = model.predict_proba(X_test) # predecir probabilidades para X_test\nprobs_tp = probs[:, 1] # mantener solo las probabilidades de la clase positiva \n       \nauc = roc_auc_score(Y_test, probs_tp)  # calcular score AUC \n\nprint('AUC: %.2f' % auc)\n</pre> # calcular score AUC probs = model.predict_proba(X_test) # predecir probabilidades para X_test probs_tp = probs[:, 1] # mantener solo las probabilidades de la clase positiva          auc = roc_auc_score(Y_test, probs_tp)  # calcular score AUC   print('AUC: %.2f' % auc) <pre>AUC: 0.93\n</pre> In\u00a0[20]: Copied! <pre># calcular curva ROC\nfpr, tpr, thresholds = roc_curve(Y_test, probs_tp) # obtener curva ROC\nplot_roc_curve(fpr, tpr)\n</pre> # calcular curva ROC fpr, tpr, thresholds = roc_curve(Y_test, probs_tp) # obtener curva ROC plot_roc_curve(fpr, tpr) In\u00a0[21]: Copied! <pre>from sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.preprocessing import StandardScaler\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom matplotlib.colors import ListedColormap\n\n\nh = .02  # step size in the mesh\nplt.figure(figsize=(12,12))\nnames = [\"Logistic\",\n         \"RBF SVM\", \n         \"Decision Tree\", \n         \"Random Forest\"\n]\n\nclassifiers = [\n    LogisticRegression(),\n    SVC(gamma=2, C=1),\n    DecisionTreeClassifier(max_depth=5),\n    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n]\n\n\n\nX, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n                           random_state=1, n_clusters_per_class=1)\nrng = np.random.RandomState(2)\nX += 2 * rng.uniform(size=X.shape)\nlinearly_separable = (X, y)\n\ndatasets = [make_moons(noise=0.3, random_state=0),\n            make_circles(noise=0.2, factor=0.5, random_state=1),\n            linearly_separable\n            ]\n\n\n\nfigure = plt.figure(figsize=(27, 9))\ni = 1\n\n\n\n# iterate over datasets\nfor ds_cnt, ds in enumerate(datasets):\n    # preprocess dataset, split into training and test part\n    X, y = ds\n    X = StandardScaler().fit_transform(X)\n    X_train, X_test, y_train, y_test = \\\n        train_test_split(X, y, test_size=.4, random_state=42)\n\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    # just plot the dataset first\n    cm = plt.cm.RdBu\n    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n    if ds_cnt == 0:\n        ax.set_title(\"Input data\")\n    # Plot the training points\n    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n               edgecolors='k')\n    # Plot the testing points\n    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n               edgecolors='k')\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    ax.set_xticks(())\n    ax.set_yticks(())\n    i += 1\n\n    # iterate over classifiers\n    for name, clf in zip(names, classifiers):\n        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n        clf.fit(X_train, y_train)\n        score = clf.score(X_test, y_test)\n\n        # Plot the decision boundary. For that, we will assign a color to each\n        # point in the mesh [x_min, x_max]x[y_min, y_max].\n        if hasattr(clf, \"decision_function\"):\n            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n        else:\n            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n\n        # Put the result into a color plot\n        Z = Z.reshape(xx.shape)\n        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n\n        # Plot the training points\n        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n                   edgecolors='k')\n        # Plot the testing points\n        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n                   edgecolors='k', alpha=0.6)\n\n        ax.set_xlim(xx.min(), xx.max())\n        ax.set_ylim(yy.min(), yy.max())\n        ax.set_xticks(())\n        ax.set_yticks(())\n        if ds_cnt == 0:\n            ax.set_title(name)\n        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n                size=15, horizontalalignment='right')\n        i += 1\n\nplt.tight_layout()\nplt.show()\n</pre> from sklearn.datasets import make_moons, make_circles, make_classification from sklearn.preprocessing import StandardScaler   from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier  from matplotlib.colors import ListedColormap   h = .02  # step size in the mesh plt.figure(figsize=(12,12)) names = [\"Logistic\",          \"RBF SVM\",           \"Decision Tree\",           \"Random Forest\" ]  classifiers = [     LogisticRegression(),     SVC(gamma=2, C=1),     DecisionTreeClassifier(max_depth=5),     RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1), ]    X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,                            random_state=1, n_clusters_per_class=1) rng = np.random.RandomState(2) X += 2 * rng.uniform(size=X.shape) linearly_separable = (X, y)  datasets = [make_moons(noise=0.3, random_state=0),             make_circles(noise=0.2, factor=0.5, random_state=1),             linearly_separable             ]    figure = plt.figure(figsize=(27, 9)) i = 1    # iterate over datasets for ds_cnt, ds in enumerate(datasets):     # preprocess dataset, split into training and test part     X, y = ds     X = StandardScaler().fit_transform(X)     X_train, X_test, y_train, y_test = \\         train_test_split(X, y, test_size=.4, random_state=42)      x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5     y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5     xx, yy = np.meshgrid(np.arange(x_min, x_max, h),                          np.arange(y_min, y_max, h))      # just plot the dataset first     cm = plt.cm.RdBu     cm_bright = ListedColormap(['#FF0000', '#0000FF'])     ax = plt.subplot(len(datasets), len(classifiers) + 1, i)     if ds_cnt == 0:         ax.set_title(\"Input data\")     # Plot the training points     ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,                edgecolors='k')     # Plot the testing points     ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,                edgecolors='k')     ax.set_xlim(xx.min(), xx.max())     ax.set_ylim(yy.min(), yy.max())     ax.set_xticks(())     ax.set_yticks(())     i += 1      # iterate over classifiers     for name, clf in zip(names, classifiers):         ax = plt.subplot(len(datasets), len(classifiers) + 1, i)         clf.fit(X_train, y_train)         score = clf.score(X_test, y_test)          # Plot the decision boundary. For that, we will assign a color to each         # point in the mesh [x_min, x_max]x[y_min, y_max].         if hasattr(clf, \"decision_function\"):             Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])         else:             Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]          # Put the result into a color plot         Z = Z.reshape(xx.shape)         ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)          # Plot the training points         ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,                    edgecolors='k')         # Plot the testing points         ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,                    edgecolors='k', alpha=0.6)          ax.set_xlim(xx.min(), xx.max())         ax.set_ylim(yy.min(), yy.max())         ax.set_xticks(())         ax.set_yticks(())         if ds_cnt == 0:             ax.set_title(name)         ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),                 size=15, horizontalalignment='right')         i += 1  plt.tight_layout() plt.show() <pre>&lt;Figure size 1200x1200 with 0 Axes&gt;</pre> In\u00a0[22]: Copied! <pre>class SklearnClassificationModels:\n    def __init__(self,model,name_model):\n\n        self.model = model\n        self.name_model = name_model\n        \n    @staticmethod\n    def test_train_model(X,y,n_size):\n        X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=n_size , random_state=42)\n        return X_train, X_test, y_train, y_test\n    \n    def fit_model(self,X,y,test_size):\n        X_train, X_test, y_train, y_test = self.test_train_model(X,y,test_size )\n        return self.model.fit(X_train, y_train) \n    \n    def df_testig(self,X,y,test_size):\n        X_train, X_test, y_train, y_test = self.test_train_model(X,y,test_size )\n        model_fit = self.model.fit(X_train, y_train)\n        preds = model_fit.predict(X_test)\n        df_temp = pd.DataFrame(\n            {\n                'y':y_test,\n                'yhat': model_fit.predict(X_test)\n            }\n        )\n        \n        return df_temp\n    \n    def metrics(self,X,y,test_size):\n        df_temp = self.df_testig(X,y,test_size)\n        df_metrics = classification_metrics(df_temp)\n        df_metrics['model'] = self.name_model\n        \n        return df_metrics\n</pre> class SklearnClassificationModels:     def __init__(self,model,name_model):          self.model = model         self.name_model = name_model              @staticmethod     def test_train_model(X,y,n_size):         X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=n_size , random_state=42)         return X_train, X_test, y_train, y_test          def fit_model(self,X,y,test_size):         X_train, X_test, y_train, y_test = self.test_train_model(X,y,test_size )         return self.model.fit(X_train, y_train)           def df_testig(self,X,y,test_size):         X_train, X_test, y_train, y_test = self.test_train_model(X,y,test_size )         model_fit = self.model.fit(X_train, y_train)         preds = model_fit.predict(X_test)         df_temp = pd.DataFrame(             {                 'y':y_test,                 'yhat': model_fit.predict(X_test)             }         )                  return df_temp          def metrics(self,X,y,test_size):         df_temp = self.df_testig(X,y,test_size)         df_metrics = classification_metrics(df_temp)         df_metrics['model'] = self.name_model                  return df_metrics  In\u00a0[23]: Copied! <pre># metrics \n\nimport itertools\n\n# nombre modelos\nnames_models = [\"Logistic\",\n         \"RBF SVM\", \n         \"Decision Tree\", \n         \"Random Forest\"\n]\n\n# modelos\nclassifiers = [\n    LogisticRegression(),\n    SVC(gamma=2, C=1),\n    DecisionTreeClassifier(max_depth=5),\n    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n]\n\n# datasets\nnames_dataset = ['make_moons',\n                 'make_circles',\n                 'linearly_separable'\n                ]\n\nX, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n                           random_state=1, n_clusters_per_class=1)\nrng = np.random.RandomState(2)\nX += 2 * rng.uniform(size=X.shape)\nlinearly_separable = (X, y)\n\ndatasets = [make_moons(noise=0.3, random_state=0),\n            make_circles(noise=0.2, factor=0.5, random_state=1),\n            linearly_separable\n            ]\n\n\n# juntar informacion\nlist_models = list(zip(names_models,classifiers))\nlist_dataset = list(zip(names_dataset,datasets))\n\nframes = []\nfor x in itertools.product(list_models, list_dataset):\n    \n    name_model = x[0][0]\n    classifier = x[0][1]\n    \n    name_dataset = x[1][0]\n    dataset = x[1][1]\n    \n    X = dataset[0]\n    Y =  dataset[1]\n    \n    fit_model =  SklearnClassificationModels( classifier,name_model)\n    df = fit_model.metrics(X,Y,0.2)\n    df['dataset'] = name_dataset\n    \n    frames.append(df)\n</pre> # metrics   import itertools  # nombre modelos names_models = [\"Logistic\",          \"RBF SVM\",           \"Decision Tree\",           \"Random Forest\" ]  # modelos classifiers = [     LogisticRegression(),     SVC(gamma=2, C=1),     DecisionTreeClassifier(max_depth=5),     RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1), ]  # datasets names_dataset = ['make_moons',                  'make_circles',                  'linearly_separable'                 ]  X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,                            random_state=1, n_clusters_per_class=1) rng = np.random.RandomState(2) X += 2 * rng.uniform(size=X.shape) linearly_separable = (X, y)  datasets = [make_moons(noise=0.3, random_state=0),             make_circles(noise=0.2, factor=0.5, random_state=1),             linearly_separable             ]   # juntar informacion list_models = list(zip(names_models,classifiers)) list_dataset = list(zip(names_dataset,datasets))  frames = [] for x in itertools.product(list_models, list_dataset):          name_model = x[0][0]     classifier = x[0][1]          name_dataset = x[1][0]     dataset = x[1][1]          X = dataset[0]     Y =  dataset[1]          fit_model =  SklearnClassificationModels( classifier,name_model)     df = fit_model.metrics(X,Y,0.2)     df['dataset'] = name_dataset          frames.append(df) <pre>C:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\db-connectors-RsEieBu8-py3.8\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n</pre> In\u00a0[24]: Copied! <pre># juntar resultados\npd.concat(frames)\n</pre> # juntar resultados pd.concat(frames) Out[24]: accuracy recall precision fscore model dataset 0 0.90 0.9000 0.9000 0.9000 Logistic make_moons 0 0.35 0.5000 0.1750 0.2593 Logistic make_circles 0 0.95 0.9545 0.9500 0.9499 Logistic linearly_separable 0 0.95 0.9500 0.9545 0.9499 RBF SVM make_moons 0 0.80 0.8462 0.8182 0.7980 RBF SVM make_circles 0 0.95 0.9545 0.9500 0.9499 RBF SVM linearly_separable 0 0.95 0.9500 0.9545 0.9499 Decision Tree make_moons 0 0.75 0.8077 0.7917 0.7494 Decision Tree make_circles 0 0.85 0.8535 0.8500 0.8496 Decision Tree linearly_separable 0 0.95 0.9500 0.9545 0.9499 Random Forest make_moons 0 0.75 0.8077 0.7917 0.7494 Random Forest make_circles 0 0.80 0.8081 0.8081 0.8000 Random Forest linearly_separable"},{"location":"lectures/machine_learning/cla_01/#clasificacion-i","title":"Clasificaci\u00f3n I\u00b6","text":""},{"location":"lectures/machine_learning/cla_01/#regresion-logistica-y-otros-modelos","title":"Regresi\u00f3n Log\u00edstica y otros modelos\u00b6","text":"<p>A modo de recuerdo, el modelo de regresio\u0301n lineal general  supone que, $\\boldsymbol{Y} =  \\boldsymbol{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon},$ donde:</p> <ul> <li>$\\boldsymbol{X} = (x_1,...,x_n)^{T}$: variable explicativa</li> <li>$\\boldsymbol{Y} = (y_1,...,y_n)^{T}$: variable respuesta</li> <li>$\\boldsymbol{\\epsilon} = (\\epsilon_1,...,\\epsilon_n)^{T}$: error se asume un ruido blanco, es decir, $\\epsilon \\sim \\mathcal{N}( \\boldsymbol{0},\\sigma^2I)$</li> <li>$\\boldsymbol{\\beta} = (\\beta_1,...,\\beta_n)^{T}$: coeficientes de regresio\u0301n.</li> </ul> <p>Por otro lado, el modelo de regresi\u00f3n log\u00edstica  analiza datos distribuidos binomialmente de la forma: $Y_i \\sim B(p_i,n_i)$, para $i=1,...,m$ donde los n\u00fameros de ensayos Bernoulli $n_{i}$ son conocidos y las probabilidades de \u00e9xito $p_{i}$ son desconocidas. Un ejemplo de esta distribuci\u00f3n es el porcentaje de semillas $p_{i} $ que germinan despu\u00e9s de que $n_{i}$ son plantadas.</p>"},{"location":"lectures/machine_learning/cla_01/#interpretacion-para-el-caso-binario","title":"Interpretaci\u00f3n para el caso binario\u00b6","text":"<p>La idea es que la regresi\u00f3n log\u00edstica aproxime la probabilidad de obtener  0 (no ocurre cierto suceso) o  1 (ocurre el suceso) con el valor de la variable explicativa $x$.</p> <p>En esas condiciones, la probabilidad aproximada del suceso se aproximar\u00e1 mediante una funci\u00f3n log\u00edstica del tipo:</p> <p>$$\\pi(x) =\\dfrac{e^{\\beta_0+\\beta_1x}}{e^{\\beta_0+\\beta_1x}+1}=\\dfrac{1}{e^{-(\\beta_0+\\beta_1x)}+1}$$</p> <p>que puede reducirse al c\u00e1lculo de una regresi\u00f3n lineal para la funci\u00f3n logit de la probabilidad:</p> <p>$$g(x) = ln(\\dfrac{\\pi(x)}{1- \\pi(x)})=\\beta_0+\\beta_1 x$$</p> <p>El gr\u00e1fico de la funci\u00f3n log\u00edstica se muestra en la figura que encabeza esta secci\u00f3n, la variable independiente es la combinaci\u00f3n lineal $=\\beta_0+\\beta_1$ y la variable dependiente es la probabilidad estimada $ \\pi (x)$. Si se realiza la regresi\u00f3n lineal, la forma de la probabilidad estimada puede ser f\u00e1cilmente recuperada a partir de los coeficientes calculados.</p> <p>Para hacer la regresi\u00f3n deben tomarse los valores $X_i$ de las observaciones ordenados de mayor a menor y formar la siguiente tabla:</p>"},{"location":"lectures/machine_learning/cla_01/#error-de-prediccion","title":"Error de Predicci\u00f3n\u00b6","text":""},{"location":"lectures/machine_learning/cla_01/#matriz-de-confusion","title":"Matriz de confusi\u00f3n\u00b6","text":"<p>Los modelos de clasificacion son ocupadas para predecir valores categ\u00f3ricos, por ejemplo, determinar la especie de una flor basado en el largo (y ancho) de su p\u00e9talo (y s\u00e9palo).Para este caso, es necesario introducir el concepto de matriz de confusi\u00f3n.</p> <p>La matriz de confusi\u00f3n es una herramienta que permite la visualizaci\u00f3n del desempe\u00f1o de un algoritmo Para la clasificaci\u00f3n de dos clases (por ejemplo, 0 y 1), se tiene la siguiente matriz de confusi\u00f3n:</p> <p></p> <p>Ac\u00e1 se define:</p> <ul> <li>TP = Verdadero positivo: el modelo predijo la clase positiva correctamente, para ser una clase positiva.</li> <li>FP = Falso positivo: el modelo predijo la clase negativa incorrectamente, para ser una clase positiva.</li> <li>FN = Falso negativo: el modelo predijo incorrectamente que la clase positiva ser\u00eda la clase negativa.</li> <li>TN = Verdadero negativo: el modelo predijo la clase negativa correctamente, para ser la clase negativa.</li> </ul> <p>En este contexto, los valores TP Y TN muestran los valores correctos que tuve al momento de realizar la predicci\u00f3n, mientras que los valores de de FN Y FP denotan los valores que me equivoque de clase.</p> <p>Los conceptos de FN y FP se pueden interpretar con la siguiente imagen:</p> <p></p>"},{"location":"lectures/machine_learning/cla_01/#metricas-de-error","title":"M\u00e9tricas de Error\u00b6","text":"<p>En este contexto, se busca maximizar el n\u00famero al m\u00e1ximo la suma de los elementos TP Y TN, mientras que se busca disminuir la suma de los elementos de FN y FP. Para esto se definen las siguientes m\u00e9tricas:</p> <ol> <li>Accuracy</li> </ol> <p>$$accuracy(y,\\hat{y}) = \\dfrac{TP+TN}{TP+TN+FP+FN}$$</p> <ol> <li>Recall:</li> </ol> <p>$$recall(y,\\hat{y}) = \\dfrac{TP}{TP+FN}$$</p> <ol> <li>Precision:</li> </ol> <p>$$precision(y,\\hat{y}) = \\dfrac{TP}{TP+FP} $$</p> <ol> <li>F-score:</li> </ol> <p>$$fscore(y,\\hat{y}) = 2\\times \\dfrac{precision(y,\\hat{y})\\times recall(y,\\hat{y})}{precision(y,\\hat{y})+recall(y,\\hat{y})} $$</p>"},{"location":"lectures/machine_learning/cla_01/#curva-aucroc","title":"Curva  AUC\u2013ROC\u00b6","text":"<p>La curva AUC\u2013ROC es una representaci\u00f3n gr\u00e1fica de la sensibilidad frente a la especificidad para un sistema clasificador binario seg\u00fan se var\u00eda el umbral de discriminaci\u00f3n. Otra interpretaci\u00f3n de este gr\u00e1fico es la representaci\u00f3n de la raz\u00f3n o proporci\u00f3n de verdaderos positivos (VPR = Raz\u00f3n de Verdaderos Positivos) frente a la raz\u00f3n o proporci\u00f3n de falsos positivos (FPR = Raz\u00f3n de Falsos Positivos) tambi\u00e9n seg\u00fan se var\u00eda el umbral de discriminaci\u00f3n (valor a partir del cual decidimos que un caso es un positivo). ROC tambi\u00e9n puede significar Relative Operating Characteristic (Caracter\u00edstica Operativa Relativa) porque es una comparaci\u00f3n de dos caracter\u00edsticas operativas (VPR y FPR) seg\u00fan cambiamos el umbral para la decisi\u00f3n.</p> <p>En espa\u00f1ol es preferible mantener el acr\u00f3nimo ingl\u00e9s, aunque es posible encontrar el equivalente espa\u00f1ol COR. No se suele utilizar ROC aislado, debemos decir \u201ccurva ROC\u201d o \u201can\u00e1lisis ROC\u201d.</p> <p></p> <p>El \u00e1rea cubierta por la curva es el \u00e1rea entre la l\u00ednea naranja (ROC) y el eje. Esta \u00e1rea cubierta es AUC. Cuanto m\u00e1s grande sea el \u00e1rea cubierta, mejores ser\u00e1n los modelos de aprendizaje autom\u00e1tico para distinguir las clases dadas. El valor ideal para AUC es 1.</p>"},{"location":"lectures/machine_learning/cla_01/#ejemplo-dataset-iris","title":"Ejemplo: Dataset Iris\u00b6","text":""},{"location":"lectures/machine_learning/cla_01/#otros-modelos-de-clasificacion","title":"Otros modelos de clasificaci\u00f3n\u00b6","text":"<p>Existen varios modelos de clasificaci\u00f3n que podemos ir comparando unos con otros, dentro de los cuales estacamos los siguientes:</p> <ul> <li>Regresi\u00f3n Log\u00edstica</li> <li>Arboles de Decision</li> <li>Random Forest</li> <li>SVM</li> </ul> <p>Nos basaremos en un ejemplo de sklearn que muestra los resultados de aplicar estos cuatro modelos sobre tres conjunto de datos distintos ( make_moons, make_circles, make_classification). Adem\u00e1s, se crea un rutina para comparar los resultados de las distintas m\u00e9tricas.</p>"},{"location":"lectures/machine_learning/cla_01/#graficos","title":"Gr\u00e1ficos\u00b6","text":"<p>Similar al gr\u00e1fico aplicado al conjunto de datos Iris, aca se realiza el mismo ejercicio pero para tres conjunto de datos sobre los distintos modelos.</p>"},{"location":"lectures/machine_learning/cla_01/#metricas","title":"M\u00e9tricas\u00b6","text":"<p>Dado que el sistema de calcular m\u00e9tricas sigue el mismo formato, solo cambiando el conjunto de datos y el modelo, se decide realizar una clase que automatice este proceso.</p>"},{"location":"lectures/machine_learning/cla_01/#referencias","title":"Referencias\u00b6","text":"<ol> <li>Supervised learning</li> </ol>"},{"location":"lectures/machine_learning/ns_01/","title":"No supervisado I","text":"<p>Consid\u00e9rense  $\ud835\udc36_1 ,...,  \ud835\udc36_k$  como los sets formados por los \u00edndices de las observaciones de cada uno de los clusters. Por ejemplo, el set  $\ud835\udc36_1$  contiene los \u00edndices de las observaciones agrupadas en el cluster 1. La nomenclatura empleada para indicar que la observaci\u00f3n  $i$ pertenece al cluster  $k$  es:  $i \\in C_k$ . Todos los sets satisfacen dos propiedades:</p> <ul> <li><p>$C_1 \\cup C_2 \\cup ... \\cup C_k = {1,...,n} $ . Significa que toda observaci\u00f3n pertenece a uno de los $k$ clusters.</p> </li> <li><p>$C_i \\cap C_{j} = \\emptyset $   para todo $i \\neq j$   . Implica que los clusters no solapan, ninguna observaci\u00f3n pertenece a m\u00e1s de un cluster a la vez.</p> </li> </ul> <p>El algoritmo consiste en reducir al m\u00ednimo la suma de las distancias cuadradas desde la media dentro del agrupamiento. Matem\u00e1ticamente: \\begin{align*} (P) \\ \\textrm{Minimizar } f(C_l,\\mu_l) = \\sum_{l=1}^k \\sum_{x_n \\in C_l} ||x_n - \\mu_l ||^2 \\textrm{, respecto a } C_l, \\mu_l, \\end{align*} donde $C_l$ es el cluster l-\u00e9simo y $\\mu_l$ es el centroide l-\u00e9simo.</p> <p></p> In\u00a0[1]: Copied! <pre># librerias\n \nimport os\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nfrom sklearn.datasets import make_blobs\n\npd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes\n\n# Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab\n%matplotlib inline\n</pre> # librerias   import os import numpy as np import pandas as pd  import matplotlib.pyplot as plt import seaborn as sns   from sklearn.datasets import make_blobs  pd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes  # Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab %matplotlib inline In\u00a0[2]: Copied! <pre>def init_blobs(N, k, seed=42):\n    X, y = make_blobs(n_samples=N, centers=k,\n                      random_state=seed, cluster_std=0.60)\n    return X\n\n# generar datos\ndata = init_blobs(10000, 6, seed=43)\ndf = pd.DataFrame(data, columns=[\"x\", \"y\"])\n\n\n\ndf.head()\n</pre> def init_blobs(N, k, seed=42):     X, y = make_blobs(n_samples=N, centers=k,                       random_state=seed, cluster_std=0.60)     return X  # generar datos data = init_blobs(10000, 6, seed=43) df = pd.DataFrame(data, columns=[\"x\", \"y\"])    df.head() Out[2]: x y 0 -6.953617 -4.989933 1 -2.681117 7.583914 2 -1.510161 4.933676 3 -9.748491 5.479457 4 -7.438017 -4.597754 <p>Debido a que trabajamos con el concepto de distancia, muchas veces las columnas del dataframe pueden estar en distintas escalas, lo cual puede complicar a los algoritmos ocupados (al menos con sklearn).</p> <p>En estos casos, se suele normalizar los atributos, es decir, dejar los valores en una escala acotada y/o con estimadores fijos. Por ejemplo, en ***sklearn** podemos encontrar las siguientes formas de normalizar:</p> <ul> <li>StandardScaler: se normaliza  restando la media y escalando por su desviaci\u00f3n estanda. $$x_{prep} = \\dfrac{x-u}{s}$$</li> </ul> <p>La ventaja es que la media del nuevo conjunto de datos cumple con la propiedad que su media $\\mu$ es igual a cero y su desviaci\u00f3n estandar $s$ es igual a 1.</p> <ul> <li>MinMaxScaler:  se normaliza ocupando los valores de los m\u00ednimos y m\u00e1ximo del conjunto de datos. $$x_{prep} = \\dfrac{x-x_{min}}{x_{min}-x_{max}}$$</li> </ul> <p>Esta forma de normalizar resulta \u00fatil cuando la desviaci\u00f3n estandar $s$ es muy peque\u00f1a (cercana) a cero, por lo que lo convierte en un estimador m\u00e1s roubusto que el StandardScaler.</p> In\u00a0[3]: Copied! <pre>from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ncolumns = ['x', 'y']\ndf[columns] = scaler.fit_transform(df[columns])\n\ndf.head()\n</pre> from sklearn.preprocessing import StandardScaler  scaler = StandardScaler() columns = ['x', 'y'] df[columns] = scaler.fit_transform(df[columns])  df.head() Out[3]: x y 0 -0.579033 -1.831435 1 0.408821 1.194578 2 0.679560 0.556774 3 -1.225241 0.688121 4 -0.691032 -1.737053 In\u00a0[4]: Copied! <pre># comprobar resultados del estimador\ndf.describe()\n</pre> # comprobar resultados del estimador df.describe() Out[4]: x y count 1.000000e+04 1.000000e+04 mean 2.060574e-16 -2.285105e-15 std 1.000050e+00 1.000050e+00 min -1.638247e+00 -2.410317e+00 25% -8.015576e-01 -4.418042e-01 50% -2.089351e-01 1.863259e-01 75% 5.480066e-01 8.159808e-01 max 2.243358e+00 1.639547e+00 <p>Con esta parametrizaci\u00f3n procedemos a graficar nuestros resultados:</p> In\u00a0[5]: Copied! <pre># graficar \nsns.set(rc={'figure.figsize':(11.7,8.27)})\nax = sns.scatterplot( data=df,x=\"x\", y=\"y\")\n</pre> # graficar  sns.set(rc={'figure.figsize':(11.7,8.27)}) ax = sns.scatterplot( data=df,x=\"x\", y=\"y\") <p>Ahora ajustamos el algoritmo KMeans de sklearn. Primero, comprendamos los hiperpar\u00e1metros m\u00e1s importantes:</p> <ul> <li>n_clusters: El n\u00famero de clusters a crear, o sea K. Por defecto es 8</li> <li>init: M\u00e9todo de inicializaci\u00f3n. Un problema que tiene el algoritmo K-Medias es que la solucci\u00f3n alcanzada varia seg\u00fan la inicializaci\u00f3n de los centroides. <code>sklearn</code> empieza usando el m\u00e9todo <code>kmeans++</code> que es una versi\u00f3n m\u00e1s moderna y que proporciona mejores resultados que la inicializaci\u00f3n aleatoria (random)</li> <li>n_init: El n\u00famero de inicializaciones a probar. B\u00e1sicamente <code>KMeans</code> aplica el algoritmo <code>n_init</code> veces y elige los clusters que minimizan la inercia.</li> <li>max_iter: M\u00e1ximo n\u00famero de iteraciones para llegar al criterio de parada.</li> <li>random_state: semilla para garantizar la reproducibilidad de los resultados.</li> <li>tol: Tolerancia para declarar criterio de parada (cuanto m\u00e1s grande, antes parar\u00e1 el algoritmo).</li> </ul> In\u00a0[6]: Copied! <pre># ajustar modelo: k-means\n\nfrom sklearn.cluster import KMeans\n\nX = np.array(df)\nkmeans = KMeans(n_clusters=6,n_init=25, random_state=123)\nkmeans.fit(X)\n\n\ncentroids = kmeans.cluster_centers_ # centros \nclusters = kmeans.labels_ # clusters\n</pre> # ajustar modelo: k-means  from sklearn.cluster import KMeans  X = np.array(df) kmeans = KMeans(n_clusters=6,n_init=25, random_state=123) kmeans.fit(X)   centroids = kmeans.cluster_centers_ # centros  clusters = kmeans.labels_ # clusters In\u00a0[7]: Copied! <pre># etiquetar los datos con los clusters encontrados\ndf[\"cluster\"] = clusters\ndf[\"cluster\"] = df[\"cluster\"].astype('category')\ncentroids_df = pd.DataFrame(centroids, columns=[\"x\", \"y\"])\ncentroids_df[\"cluster\"] = [1,2,3,4,5,6]\n</pre> # etiquetar los datos con los clusters encontrados df[\"cluster\"] = clusters df[\"cluster\"] = df[\"cluster\"].astype('category') centroids_df = pd.DataFrame(centroids, columns=[\"x\", \"y\"]) centroids_df[\"cluster\"] = [1,2,3,4,5,6] In\u00a0[8]: Copied! <pre># graficar los datos etiquetados con k-means\nfig, ax = plt.subplots(figsize=(11, 8.5))\n\nsns.scatterplot( data=df,\n                     x=\"x\",\n                     y=\"y\",\n                     hue=\"cluster\",\n                     legend='full',\n                     palette=\"Set2\")\n\nsns.scatterplot(x=\"x\", y=\"y\",\n                     s=100, color=\"black\", marker=\"x\",\n                     data=centroids_df)\nplt.show()\n</pre> # graficar los datos etiquetados con k-means fig, ax = plt.subplots(figsize=(11, 8.5))  sns.scatterplot( data=df,                      x=\"x\",                      y=\"y\",                      hue=\"cluster\",                      legend='full',                      palette=\"Set2\")  sns.scatterplot(x=\"x\", y=\"y\",                      s=100, color=\"black\", marker=\"x\",                      data=centroids_df) plt.show() <p>Ahora la pregunta que surge de manera natural es ... \u00bf c\u00f3mo escoger el mejor n\u00famero de clusters?.</p> <p>No existe un criterio objetivo ni ampliamente v\u00e1lido para la  elecci\u00f3n de un n\u00famero \u00f3ptimo de clusters. Aunque no exista un criterio objetivo para la selecci\u00f3n del n\u00famero de clusters, si que se han implementado diferentes m\u00e9todos que nos ayudan a elegir un n\u00famero apropiado de clusters para agrupar los datos; como son,</p> <ul> <li>m\u00e9todo del codo (elbow method)</li> <li>criterio de Calinsky</li> <li>Affinity Propagation (AP)</li> <li>Gap (tambi\u00e9n con su versi\u00f3n estad\u00edstica)</li> <li>Dendrogramas</li> <li>etc.</li> </ul> In\u00a0[9]: Copied! <pre># implementaci\u00f3n de la regla del codo\nNc = range(1, 15)\nkmeans = [KMeans(n_clusters=i, n_init=10) for i in Nc]  # Suppressing the warning here\nscore = [kmeans[i].fit(df).inertia_ for i in range(len(kmeans))]\n\ndf_Elbow = pd.DataFrame({'Number of Clusters': Nc, 'Score': score})\n\ndf_Elbow.head()\n</pre> # implementaci\u00f3n de la regla del codo Nc = range(1, 15) kmeans = [KMeans(n_clusters=i, n_init=10) for i in Nc]  # Suppressing the warning here score = [kmeans[i].fit(df).inertia_ for i in range(len(kmeans))]  df_Elbow = pd.DataFrame({'Number of Clusters': Nc, 'Score': score})  df_Elbow.head() Out[9]: Number of Clusters Score 0 1 49054.876400 1 2 23034.738275 2 3 11952.113532 3 4 6562.349711 4 5 1665.378832 In\u00a0[10]: Copied! <pre># graficar los datos etiquetados con k-means\nfig, ax = plt.subplots(figsize=(11, 8.5))\nplt.title('Elbow Curve')\nsns.lineplot(x=\"Number of Clusters\",\n             y=\"Score\",\n            data=df_Elbow)\nsns.scatterplot(x=\"Number of Clusters\",\n             y=\"Score\",\n             data=df_Elbow)\nplt.show()\n</pre> # graficar los datos etiquetados con k-means fig, ax = plt.subplots(figsize=(11, 8.5)) plt.title('Elbow Curve') sns.lineplot(x=\"Number of Clusters\",              y=\"Score\",             data=df_Elbow) sns.scatterplot(x=\"Number of Clusters\",              y=\"Score\",              data=df_Elbow) plt.show() <p>A partir de 4 clusters la reducci\u00f3n en la suma total de cuadrados internos parece estabilizarse, indicando que $k$ = 4 es una buena opci\u00f3n.</p> <p>En la base del dendrograma, cada observaci\u00f3n forma una terminaci\u00f3n individual conocida como hoja o leaf del \u00e1rbol. A medida que se asciende por la estructura, pares de hojas se fusionan formando las primeras ramas. Estas uniones se corresponden con los pares de observaciones m\u00e1s similares. Tambi\u00e9n ocurre que las ramas se fusionan con otras ramas o con hojas. Cuanto m\u00e1s temprana (m\u00e1s pr\u00f3xima a la base del dendrograma) ocurre una fusi\u00f3n, mayor es la similitud.</p> <p>Para cualquier par de observaciones, se puede identificar el punto del \u00e1rbol en el que las ramas que contienen dichas observaciones se fusionan. La altura a la que esto ocurre (eje vertical) indica c\u00f3mo de similares/diferentes son las dos observaciones. Los dendrogramas, por lo tanto, se deben interpretar \u00fanicamente en base al eje vertical y no por las posiciones que ocupan las observaciones en el eje horizontal, esto \u00faltimo es simplemente por est\u00e9tica y puede variar de un programa a otro.</p> <p>Por ejemplo, la observaci\u00f3n 8 es la m\u00e1s similar a la 10 ya que es la primera fusi\u00f3n que recibe la observaci\u00f3n 10 (y viceversa). Podr\u00eda resultar tentador decir que la observaci\u00f3n 14, situada inmediatamente a la derecha de la 10, es la siguiente m\u00e1s similar, sin embargo, las observaciones 28 y 44 son m\u00e1s similares a la 10 a pesar de que se encuentran m\u00e1s alejadas en el eje horizontal. Del mismo modo, no es correcto decir que la observaci\u00f3n 14 es m\u00e1s similar a la observaci\u00f3n 10 de lo que lo es la 36 por el hecho de que est\u00e1 m\u00e1s pr\u00f3xima en el eje horizontal. Prestando atenci\u00f3n a la altura en que las respectivas ramas se unen, la \u00fanica conclusi\u00f3n v\u00e1lida es que la similitud entre los pares 10-14 y 10-36 es la misma.</p> <p>Cortar el dendograma para generar los clusters</p> <p>Adem\u00e1s de representar en un dendrograma la similitud entre observaciones, se tiene que identificar el n\u00famero de clusters creados y qu\u00e9 observaciones forman parte de cada uno. Si se realiza un corte horizontal a una determinada altura del dendrograma, el n\u00famero de ramas que sobrepasan (en sentido ascendente) dicho corte se corresponde con el n\u00famero de clusters. La siguiente imagen muestra dos veces el mismo dendrograma. Si se realiza el corte a la altura de 5, se obtienen dos clusters, mientras que si se hace a la de 3.5 se obtienen 4. La altura de corte tiene por lo tanto la misma funci\u00f3n que el valor K en K-means-clustering: controla el n\u00famero de clusters obtenidos.</p> <p></p> <p></p> <p>Dos propiedades adicionales se derivan de la forma en que se generan los clusters en el m\u00e9todo de hierarchical clustering:</p> <ul> <li><p>Dada la longitud variable de las ramas, siempre existe un intervalo de altura para el que cualquier corte da lugar al mismo n\u00famero de clusters. En el ejemplo anterior, todos los cortes entre las alturas 5 y 6 tienen como resultado los mismos 2 clusters.</p> </li> <li><p>Con un solo dendrograma se dispone de la flexibilidad para generar cualquier n\u00famero de clusters desde 1 a n. La selecci\u00f3n del n\u00famero \u00f3ptimo puede valorarse de forma visual, tratando de identificar las ramas principales en base a la altura a la que ocurren las uniones. En el ejemplo expuesto es razonable elegir entre 2 o 4 clusters.</p> </li> </ul> In\u00a0[11]: Copied! <pre># Tratamiento de datos\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_blobs\n\n# Gr\u00e1ficos\n# ==============================================================================\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('ggplot') or plt.style.use('ggplot')\n\n# Preprocesado y modelado\n# ==============================================================================\nfrom sklearn.cluster import AgglomerativeClustering\nfrom scipy.cluster.hierarchy import dendrogram\nfrom sklearn.preprocessing import scale\nfrom sklearn.metrics import silhouette_score\n\n# Configuraci\u00f3n warnings\n# ==============================================================================\nimport warnings\nwarnings.filterwarnings('ignore')\n</pre> # Tratamiento de datos # ============================================================================== import numpy as np import pandas as pd from sklearn.datasets import make_blobs  # Gr\u00e1ficos # ============================================================================== import matplotlib.pyplot as plt from matplotlib import style style.use('ggplot') or plt.style.use('ggplot')  # Preprocesado y modelado # ============================================================================== from sklearn.cluster import AgglomerativeClustering from scipy.cluster.hierarchy import dendrogram from sklearn.preprocessing import scale from sklearn.metrics import silhouette_score  # Configuraci\u00f3n warnings # ============================================================================== import warnings warnings.filterwarnings('ignore') In\u00a0[12]: Copied! <pre># generar datos\nX, y = make_blobs(\n        n_samples    = 200, \n        n_features   = 2, \n        centers      = 4, \n        cluster_std  = 0.60, \n        shuffle      = True, \n        random_state = 0\n       )\n\n\ndf = pd.DataFrame({\n    'x':X[:,0],\n    'y':X[:,1]\n})\n\n\n# Escalado de datos\nscaler = StandardScaler()\ncolumns = ['x', 'y']\ndf[columns] = scaler.fit_transform(df[columns])\n\ndf.head()\n</pre> # generar datos X, y = make_blobs(         n_samples    = 200,          n_features   = 2,          centers      = 4,          cluster_std  = 0.60,          shuffle      = True,          random_state = 0        )   df = pd.DataFrame({     'x':X[:,0],     'y':X[:,1] })   # Escalado de datos scaler = StandardScaler() columns = ['x', 'y'] df[columns] = scaler.fit_transform(df[columns])  df.head() Out[12]: x y 0 1.348818 -0.908114 1 -0.638621 -0.534950 2 0.653079 0.027910 3 -1.573023 1.276049 4 0.970706 -1.418431 In\u00a0[13]: Copied! <pre># graficar \nsns.set(rc={'figure.figsize':(11.7,8.27)})\nax = sns.scatterplot( data=df,x=\"x\", y=\"y\")\n</pre> # graficar  sns.set(rc={'figure.figsize':(11.7,8.27)}) ax = sns.scatterplot( data=df,x=\"x\", y=\"y\") In\u00a0[14]: Copied! <pre># Modelos\n\nX = np.array(df[['x','y']])\n</pre> # Modelos  X = np.array(df[['x','y']]) In\u00a0[15]: Copied! <pre># primer modelo\nmodelo_hclust_complete = AgglomerativeClustering(\n                            affinity = 'euclidean',\n                            linkage  = 'complete',\n                            distance_threshold = 0,\n                            n_clusters         = None\n                        )\nmodelo_hclust_complete.fit(X)\n</pre> # primer modelo modelo_hclust_complete = AgglomerativeClustering(                             affinity = 'euclidean',                             linkage  = 'complete',                             distance_threshold = 0,                             n_clusters         = None                         ) modelo_hclust_complete.fit(X) Out[15]: <pre>AgglomerativeClustering(affinity='euclidean', distance_threshold=0,\n                        linkage='complete', n_clusters=None)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.AgglomerativeClustering<pre>AgglomerativeClustering(affinity='euclidean', distance_threshold=0,\n                        linkage='complete', n_clusters=None)</pre> In\u00a0[16]: Copied! <pre># segundo modelo\nmodelo_hclust_average = AgglomerativeClustering(\n                            affinity = 'euclidean',\n                            linkage  = 'average',\n                            distance_threshold = 0,\n                            n_clusters         = None\n                        )\nmodelo_hclust_average.fit(X)\n</pre> # segundo modelo modelo_hclust_average = AgglomerativeClustering(                             affinity = 'euclidean',                             linkage  = 'average',                             distance_threshold = 0,                             n_clusters         = None                         ) modelo_hclust_average.fit(X) Out[16]: <pre>AgglomerativeClustering(affinity='euclidean', distance_threshold=0,\n                        linkage='average', n_clusters=None)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.AgglomerativeClustering<pre>AgglomerativeClustering(affinity='euclidean', distance_threshold=0,\n                        linkage='average', n_clusters=None)</pre> In\u00a0[17]: Copied! <pre># tercer modelo\nmodelo_hclust_ward = AgglomerativeClustering(\n                            affinity = 'euclidean',\n                            linkage  = 'ward',\n                            distance_threshold = 0,\n                            n_clusters         = None\n                     )\nmodelo_hclust_ward.fit(X)\n</pre> # tercer modelo modelo_hclust_ward = AgglomerativeClustering(                             affinity = 'euclidean',                             linkage  = 'ward',                             distance_threshold = 0,                             n_clusters         = None                      ) modelo_hclust_ward.fit(X) Out[17]: <pre>AgglomerativeClustering(affinity='euclidean', distance_threshold=0,\n                        n_clusters=None)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.AgglomerativeClustering<pre>AgglomerativeClustering(affinity='euclidean', distance_threshold=0,\n                        n_clusters=None)</pre> In\u00a0[18]: Copied! <pre>def plot_dendrogram(model, **kwargs):\n    '''\n    Esta funci\u00f3n extrae la informaci\u00f3n de un modelo AgglomerativeClustering\n    y representa su dendograma con la funci\u00f3n dendogram de scipy.cluster.hierarchy\n    '''\n    \n    counts = np.zeros(model.children_.shape[0])\n    n_samples = len(model.labels_)\n    for i, merge in enumerate(model.children_):\n        current_count = 0\n        for child_idx in merge:\n            if child_idx &lt; n_samples:\n                current_count += 1  # leaf node\n            else:\n                current_count += counts[child_idx - n_samples]\n        counts[i] = current_count\n\n    linkage_matrix = np.column_stack([model.children_, model.distances_,\n                                      counts]).astype(float)\n\n    # Plot\n    dendrogram(linkage_matrix, **kwargs)\n</pre> def plot_dendrogram(model, **kwargs):     '''     Esta funci\u00f3n extrae la informaci\u00f3n de un modelo AgglomerativeClustering     y representa su dendograma con la funci\u00f3n dendogram de scipy.cluster.hierarchy     '''          counts = np.zeros(model.children_.shape[0])     n_samples = len(model.labels_)     for i, merge in enumerate(model.children_):         current_count = 0         for child_idx in merge:             if child_idx &lt; n_samples:                 current_count += 1  # leaf node             else:                 current_count += counts[child_idx - n_samples]         counts[i] = current_count      linkage_matrix = np.column_stack([model.children_, model.distances_,                                       counts]).astype(float)      # Plot     dendrogram(linkage_matrix, **kwargs) In\u00a0[19]: Copied! <pre># Dendrogramas\nplt.figure(figsize=(20,10)) \nplot_dendrogram(modelo_hclust_average, color_threshold=0)\nplt.title(\"Distancia eucl\u00eddea, Linkage average\")\nplt.show()\n</pre> # Dendrogramas plt.figure(figsize=(20,10))  plot_dendrogram(modelo_hclust_average, color_threshold=0) plt.title(\"Distancia eucl\u00eddea, Linkage average\") plt.show() In\u00a0[20]: Copied! <pre># Dendrogramas\nplt.figure(figsize=(20,10)) \nplot_dendrogram(modelo_hclust_complete, color_threshold=0)\nplt.title(\"Distancia eucl\u00eddea, Linkage complete\")\nplt.show()\n</pre> # Dendrogramas plt.figure(figsize=(20,10))  plot_dendrogram(modelo_hclust_complete, color_threshold=0) plt.title(\"Distancia eucl\u00eddea, Linkage complete\") plt.show() In\u00a0[21]: Copied! <pre># Dendrogramas\nplt.figure(figsize=(20,10)) \nplot_dendrogram(modelo_hclust_ward, color_threshold=0)\nplt.title(\"Distancia eucl\u00eddea, Linkage ward\")\nplt.show()\n</pre> # Dendrogramas plt.figure(figsize=(20,10))  plot_dendrogram(modelo_hclust_ward, color_threshold=0) plt.title(\"Distancia eucl\u00eddea, Linkage ward\") plt.show() <p>En este caso, los tres tipos de linkage identifican claramente 4 clusters, si bien esto no significa que en los 3 dendrogramas los clusters est\u00e9n formados por exactamente las mismas observaciones.</p> <p>N\u00famero de clusters</p> <p>Una forma de identificar el n\u00famero de clusters, es inspeccionar visualmente el dendograma y decidir a qu\u00e9 altura se corta para generar los clusters. Por ejemplo, para los resultados generados mediante distancia eucl\u00eddea y linkage ward, parece sensato cortar el dendograma a una altura de entre 5 y 10, de forma que se creen 4 clusters.</p> In\u00a0[22]: Copied! <pre>plt.figure(figsize=(20,10)) \naltura_corte = 6\nplot_dendrogram(modelo_hclust_ward, color_threshold=altura_corte)\nplt.title(\"Distancia eucl\u00eddea, Linkage ward\")\nplt.axhline(y=altura_corte, c = 'black', linestyle='--', label='altura corte')\nplt.legend()\nplt.show()\n</pre> plt.figure(figsize=(20,10))  altura_corte = 6 plot_dendrogram(modelo_hclust_ward, color_threshold=altura_corte) plt.title(\"Distancia eucl\u00eddea, Linkage ward\") plt.axhline(y=altura_corte, c = 'black', linestyle='--', label='altura corte') plt.legend() plt.show() <p>Una vez identificado el n\u00famero \u00f3ptimo de clusters, se reentrena el modelo indicando este valor.</p> <p>El cerebro humano identifica f\u00e1cilmente 5 agrupaciones y algunas observaciones aisladas (ruido). V\u00e9anse ahora los clusters que se obtienen si se aplica, por ejemplo, K-means clustering.</p> <p></p> <p>Los clusters generados distan mucho de representar las verdaderas agrupaciones. Esto es as\u00ed porque los m\u00e9todos de partitioning clustering como k-means, hierarchical, k-medoids, ... son buenos encontrando agrupaciones con forma esf\u00e9rica o convexa que no contengan un exceso de outliers o ruido, pero fallan al tratar de identificar formas arbitrarias. De ah\u00ed que el \u00fanico cluster que se corresponde con un grupo real sea el amarillo.</p> <p>DBSCAN evita este problema siguiendo la idea de que, para que una observaci\u00f3n forme parte de un cluster, tiene que haber un m\u00ednimo de observaciones vecinas dentro de un radio de proximidad y de que los clusters est\u00e1n separados por regiones vac\u00edas o con pocas observaciones.</p> <p>El algoritmo DBSCAN necesita dos par\u00e1metros:</p> <ul> <li>Epsilon  ($\\epsilon$) : radio que define la regi\u00f3n vecina a una observaci\u00f3n, tambi\u00e9n llamada  \ud835\udf16 -neighborhood.</li> <li>Minimum points ($min_samples$): n\u00famero m\u00ednimo de observaciones dentro de la regi\u00f3n epsilon.</li> </ul> <p>Empleando estos dos par\u00e1metros, cada observaci\u00f3n del set de datos se puede clasificar en una de las siguientes tres categor\u00edas:</p> <ul> <li><p>Core point: observaci\u00f3n que tiene en su  \ud835\udf16 -neighborhood un n\u00famero de observaciones vecinas igual o mayor a min_samples.</p> </li> <li><p>Border point: observaci\u00f3n no satisface el m\u00ednimo de observaciones vecinas para ser core point pero que pertenece al  $\\epsilon$-neighborhood de otra observaci\u00f3n que s\u00ed es core point.</p> </li> <li><p>Noise-outlier: observaci\u00f3n que no es core point ni border point.</p> </li> </ul> <p>Por \u00faltimo, empleando las tres categor\u00edas anteriores se pueden definir tres niveles de conectividad entre observaciones:</p> <ul> <li><p>Directamente alcanzable (direct density reachable): una observaci\u00f3n  $A$  es directamente alcanzable desde otra observaci\u00f3n  $B$  si  $A$  forma parte del  $\\epsilon$ -neighborhood de  $B$  y  $B$  es un core point. Por definici\u00f3n, las observaciones solo pueden ser directamente alcanzables desde un core point.</p> </li> <li><p>Alcanzable (density reachable): una observaci\u00f3n  $A$  es alcanzable desde otra observaci\u00f3n  $\ud835\udc35$  si existe una secuencia de core points que van desde  $B$  a  $A$ .</p> </li> <li><p>Densamente conectadas (density conected): dos observaciones $A$  y  $B$  est\u00e1n densamente conectadas si existe una observaci\u00f3n core point  $C$  tal que  $A$  y  $B$  son alcanzables desde  $C$ .</p> </li> </ul> <p>La siguiente imagen muestra las conexiones existentes entre un conjunto de observaciones si se emplea  $min_samples = 4$ . La observaci\u00f3n  $A$  y el resto de observaciones marcadas en rojo son core points, ya que todas ellas contienen al menos 4 observaciones vecinas (incluy\u00e9ndose a ellas mismas) en su  $\\epsilon$-neighborhood. Como todas son alcanzables entre ellas, forman un cluster. Las observaciones  $B$  y  $C$  no son core points pero son alcanzables desde  $A$  a trav\u00e9s de otros core points, por lo tanto, pertenecen al mismo cluster que  $A$ . La observaci\u00f3n  $N$  no es ni un core point ni es directamente alcanzable, por lo que se considera como ruido.</p> <p></p> In\u00a0[23]: Copied! <pre># Tratamiento de datos\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_blobs\n\n# Gr\u00e1ficos\n# ==============================================================================\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('ggplot') or plt.style.use('ggplot')\n\n# Preprocesado y modelado\n# ==============================================================================\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import scale\nfrom sklearn.metrics import silhouette_score\n\n# Configuraci\u00f3n warnings\n# ==============================================================================\nimport warnings\nwarnings.filterwarnings('ignore')\n</pre> # Tratamiento de datos # ============================================================================== import numpy as np import pandas as pd from sklearn.datasets import make_blobs  # Gr\u00e1ficos # ============================================================================== import matplotlib.pyplot as plt from matplotlib import style style.use('ggplot') or plt.style.use('ggplot')  # Preprocesado y modelado # ============================================================================== from sklearn.cluster import DBSCAN from sklearn.preprocessing import scale from sklearn.metrics import silhouette_score  # Configuraci\u00f3n warnings # ============================================================================== import warnings warnings.filterwarnings('ignore') In\u00a0[24]: Copied! <pre># leer datos\nurl='https://drive.google.com/file/d/1sLxYBCZJawCHyxEjHnn-hnBDInXx4hby/view?usp=drive_link'\nurl='https://drive.google.com/uc?id=' + url.split('/')[-2]\n\ndf = pd.read_csv(url, sep=\",\")\ndf.head()\n</pre> # leer datos url='https://drive.google.com/file/d/1sLxYBCZJawCHyxEjHnn-hnBDInXx4hby/view?usp=drive_link' url='https://drive.google.com/uc?id=' + url.split('/')[-2]  df = pd.read_csv(url, sep=\",\") df.head() Out[24]: x y shape 0 -0.803739 -0.853053 1 1 0.852851 0.367618 1 2 0.927180 -0.274902 1 3 -0.752626 -0.511565 1 4 0.706846 0.810679 1 In\u00a0[25]: Copied! <pre># graficar \nsns.set(rc={'figure.figsize':(11.7,8.27)})\nax = sns.scatterplot( data=df,x=\"x\", y=\"y\")\n</pre> # graficar  sns.set(rc={'figure.figsize':(11.7,8.27)}) ax = sns.scatterplot( data=df,x=\"x\", y=\"y\") In\u00a0[26]: Copied! <pre># Escalado de datos\nscaler = StandardScaler()\ncolumns = ['x', 'y']\ndf[columns] = scaler.fit_transform(df[columns])\ndf.head()\n</pre> # Escalado de datos scaler = StandardScaler() columns = ['x', 'y'] df[columns] = scaler.fit_transform(df[columns]) df.head() Out[26]: x y shape 0 -1.120749 -0.193616 1 1 1.448907 0.844692 1 2 1.564203 0.298161 1 3 -1.041463 0.096855 1 4 1.222429 1.221561 1 In\u00a0[27]: Copied! <pre># Modelo\nX = np.array(df[['x','y']])\n\nmodelo_dbscan = DBSCAN(\n                    eps          = 0.2,\n                    min_samples  = 5,\n                    metric       = 'euclidean',\n                )\n\n\nmodelo_dbscan.fit(X)\n</pre> # Modelo X = np.array(df[['x','y']])  modelo_dbscan = DBSCAN(                     eps          = 0.2,                     min_samples  = 5,                     metric       = 'euclidean',                 )   modelo_dbscan.fit(X) Out[27]: <pre>DBSCAN(eps=0.2)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DBSCAN<pre>DBSCAN(eps=0.2)</pre> In\u00a0[28]: Copied! <pre># agregar labels\ndf['labels'] = modelo_dbscan.labels_\n</pre> # agregar labels df['labels'] = modelo_dbscan.labels_ In\u00a0[29]: Copied! <pre># graficar \nsns.set(rc={'figure.figsize':(11.7,8.27)})\nax = sns.scatterplot( data=df,x=\"x\", y=\"y\",hue = \"labels\")\n</pre> # graficar  sns.set(rc={'figure.figsize':(11.7,8.27)}) ax = sns.scatterplot( data=df,x=\"x\", y=\"y\",hue = \"labels\") In\u00a0[30]: Copied! <pre># N\u00famero de clusters y observaciones \"outliers\"\nn_clusters = len(set(df['labels'])) - (1 if -1 in df['labels'] else 0)\nn_noise    = list(df['labels']).count(-1)\n\nprint(f'N\u00famero de clusters encontrados: {n_clusters}')\nprint(f'N\u00famero de outliers encontrados: {n_noise}')\n</pre> # N\u00famero de clusters y observaciones \"outliers\" n_clusters = len(set(df['labels'])) - (1 if -1 in df['labels'] else 0) n_noise    = list(df['labels']).count(-1)  print(f'N\u00famero de clusters encontrados: {n_clusters}') print(f'N\u00famero de outliers encontrados: {n_noise}') <pre>N\u00famero de clusters encontrados: 6\nN\u00famero de outliers encontrados: 25\n</pre> In\u00a0[31]: Copied! <pre># Tratamiento de datos\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_blobs\n\n# Gr\u00e1ficos\n# ==============================================================================\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom matplotlib.patches import Ellipse\nfrom matplotlib import style\nstyle.use('ggplot') or plt.style.use('ggplot')\n\n# Preprocesado y modelado\n# ==============================================================================\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.preprocessing import scale\nfrom sklearn.metrics import silhouette_score\n\n# Configuraci\u00f3n warnings\n# ==============================================================================\nimport warnings\nwarnings.filterwarnings('ignore')\n</pre> # Tratamiento de datos # ============================================================================== import numpy as np import pandas as pd from sklearn.datasets import make_blobs  # Gr\u00e1ficos # ============================================================================== import matplotlib.pyplot as plt import matplotlib as mpl from matplotlib.patches import Ellipse from matplotlib import style style.use('ggplot') or plt.style.use('ggplot')  # Preprocesado y modelado # ============================================================================== from sklearn.mixture import GaussianMixture from sklearn.preprocessing import scale from sklearn.metrics import silhouette_score  # Configuraci\u00f3n warnings # ============================================================================== import warnings warnings.filterwarnings('ignore') In\u00a0[32]: Copied! <pre># generar datos\nX, y = make_blobs(\n        n_samples    = 300, \n        n_features   = 2, \n        centers      = 4, \n        cluster_std  = 0.60, \n        shuffle      = True, \n        random_state = 0\n       )\n\n\ndf = pd.DataFrame({\n    'x':X[:,0],\n    'y':X[:,1]\n})\n\n\n# Escalado de datos\nscaler = StandardScaler()\ncolumns = ['x', 'y']\ndf[columns] = scaler.fit_transform(df[columns])\n\ndf.head()\n</pre> # generar datos X, y = make_blobs(         n_samples    = 300,          n_features   = 2,          centers      = 4,          cluster_std  = 0.60,          shuffle      = True,          random_state = 0        )   df = pd.DataFrame({     'x':X[:,0],     'y':X[:,1] })   # Escalado de datos scaler = StandardScaler() columns = ['x', 'y'] df[columns] = scaler.fit_transform(df[columns])  df.head() Out[32]: x y 0 0.516255 -0.707227 1 -0.861664 1.329068 2 0.711174 0.437049 3 -0.619792 1.485573 4 0.782282 -0.801378 In\u00a0[33]: Copied! <pre># graficar \nsns.set(rc={'figure.figsize':(11.7,8.27)})\nax = sns.scatterplot( data=df,x=\"x\", y=\"y\")\n</pre> # graficar  sns.set(rc={'figure.figsize':(11.7,8.27)}) ax = sns.scatterplot( data=df,x=\"x\", y=\"y\") In\u00a0[34]: Copied! <pre># Modelo\n\nX = np.array(df[['x','y']])\nmodelo_gmm = GaussianMixture(n_components=4, covariance_type='full', random_state=123)\nmodelo_gmm.fit(X=X)\n</pre> # Modelo  X = np.array(df[['x','y']]) modelo_gmm = GaussianMixture(n_components=4, covariance_type='full', random_state=123) modelo_gmm.fit(X=X) Out[34]: <pre>GaussianMixture(n_components=4, random_state=123)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianMixture<pre>GaussianMixture(n_components=4, random_state=123)</pre> In\u00a0[35]: Copied! <pre># Media de cada componente\nmodelo_gmm.means_\n</pre> # Media de cada componente modelo_gmm.means_ Out[35]: <pre>array([[ 0.57844185,  0.17292982],\n       [ 1.2180002 , -1.19725866],\n       [-0.96910551, -0.44143927],\n       [-0.83710796,  1.46219241]])</pre> In\u00a0[36]: Copied! <pre># Matriz de covarianza de cada componente\nmodelo_gmm.covariances_\n</pre> # Matriz de covarianza de cada componente modelo_gmm.covariances_ Out[36]: <pre>array([[[ 0.14277634, -0.00527707],\n        [-0.00527707,  0.05201453]],\n\n       [[ 0.12745218, -0.00619666],\n        [-0.00619666,  0.05157763]],\n\n       [[ 0.12131004,  0.00243031],\n        [ 0.00243031,  0.04602115]],\n\n       [[ 0.15451151,  0.0068188 ],\n        [ 0.0068188 ,  0.05660383]]])</pre> <p>Predicci\u00f3n y clasificaci\u00f3n</p> <p>Una vez entrenado el modelo GMMs, se puede predecir la probabilidad que tiene cada observaci\u00f3n de pertenecer a cada una de las componentes (clusters). Para obtener la clasificaci\u00f3n final, se asigna a la componente con mayor probabilidad</p> In\u00a0[37]: Copied! <pre># Probabilidades\n# ==============================================================================\n# Cada fila es una observaci\u00f3n y cada columna la probabilidad de pertenecer a\n# cada una de las componentes.\nprobabilidades = modelo_gmm.predict_proba(X)\nprobabilidades\n</pre> # Probabilidades # ============================================================================== # Cada fila es una observaci\u00f3n y cada columna la probabilidad de pertenecer a # cada una de las componentes. probabilidades = modelo_gmm.predict_proba(X) probabilidades Out[37]: <pre>array([[2.59319058e-02, 9.71686641e-01, 2.38145325e-03, 8.05199375e-21],\n       [7.16006205e-09, 7.13831446e-33, 2.34989165e-15, 9.99999993e-01],\n       [9.99999970e-01, 8.78380305e-12, 9.13663813e-09, 2.04805600e-08],\n       ...,\n       [9.99965889e-01, 4.92493619e-10, 3.41016281e-05, 8.75675460e-09],\n       [3.01319652e-06, 6.45628361e-30, 1.52897049e-18, 9.99996987e-01],\n       [4.39337172e-07, 1.99785604e-11, 9.99999561e-01, 4.05381245e-15]])</pre> In\u00a0[38]: Copied! <pre># Clasificaci\u00f3n (asignaci\u00f3n a la componente de mayor probabilidad)\n# ==============================================================================\n# Cada fila es una observaci\u00f3n y cada columna la probabilidad de pertenecer a\n# cada una de las componentes.\nclasificacion = modelo_gmm.predict(X)\nclasificacion\n</pre> # Clasificaci\u00f3n (asignaci\u00f3n a la componente de mayor probabilidad) # ============================================================================== # Cada fila es una observaci\u00f3n y cada columna la probabilidad de pertenecer a # cada una de las componentes. clasificacion = modelo_gmm.predict(X) clasificacion Out[38]: <pre>array([1, 3, 0, 3, 1, 1, 2, 0, 3, 3, 2, 3, 0, 3, 1, 0, 0, 1, 2, 2, 1, 1,\n       0, 2, 2, 0, 1, 0, 2, 0, 3, 3, 0, 3, 3, 3, 3, 3, 2, 1, 0, 2, 0, 0,\n       2, 2, 3, 2, 3, 1, 2, 1, 3, 1, 1, 2, 3, 2, 3, 1, 3, 0, 3, 2, 2, 2,\n       3, 1, 3, 2, 0, 2, 3, 2, 2, 3, 2, 0, 1, 3, 1, 0, 1, 1, 3, 0, 1, 0,\n       3, 3, 0, 1, 3, 2, 2, 0, 1, 1, 0, 2, 3, 1, 3, 1, 0, 1, 1, 0, 3, 0,\n       2, 2, 1, 3, 1, 0, 3, 1, 1, 0, 2, 1, 2, 1, 1, 1, 1, 2, 1, 2, 3, 2,\n       2, 1, 3, 2, 2, 3, 0, 3, 3, 2, 0, 2, 0, 2, 3, 0, 3, 3, 3, 0, 3, 0,\n       1, 2, 3, 2, 1, 0, 3, 0, 0, 1, 0, 2, 2, 0, 1, 0, 0, 3, 1, 0, 2, 3,\n       1, 1, 0, 2, 1, 0, 2, 2, 0, 0, 0, 0, 1, 3, 0, 2, 0, 0, 2, 2, 2, 0,\n       2, 3, 0, 2, 1, 2, 0, 3, 2, 3, 0, 3, 0, 2, 0, 0, 3, 2, 2, 1, 1, 0,\n       3, 1, 1, 2, 1, 2, 0, 3, 3, 0, 0, 3, 0, 1, 2, 0, 1, 2, 3, 2, 1, 0,\n       1, 3, 3, 3, 3, 2, 2, 3, 0, 2, 1, 0, 2, 2, 2, 1, 1, 3, 0, 0, 2, 1,\n       3, 2, 0, 3, 0, 1, 1, 2, 2, 0, 1, 1, 1, 0, 3, 3, 1, 1, 0, 1, 1, 1,\n       3, 2, 3, 0, 1, 1, 3, 3, 3, 1, 1, 0, 3, 2], dtype=int64)</pre> In\u00a0[39]: Copied! <pre># Representaci\u00f3n gr\u00e1fica\n# ==============================================================================\n# Codigo obtenido de:\n# https://github.com/amueller/COMS4995-s20/tree/master/slides/aml-14-clustering-mixture-models \ndef make_ellipses(gmm, ax):\n    for n in range(gmm.n_components):\n        if gmm.covariance_type == 'full':\n            covariances = gmm.covariances_[n]\n        elif gmm.covariance_type == 'tied':\n            covariances = gmm.covariances_\n        elif gmm.covariance_type == 'diag':\n            covariances = np.diag(gmm.covariances_[n])\n        elif gmm.covariance_type == 'spherical':\n            covariances = np.eye(gmm.means_.shape[1]) * gmm.covariances_[n]\n        v, w = np.linalg.eigh(covariances)\n        u = w[0] / np.linalg.norm(w[0])\n        angle = np.arctan2(u[1], u[0])\n        angle = 180 * angle / np.pi  # convert to degrees\n        v = 2. * np.sqrt(2.) * np.sqrt(v)\n        \n        for i in range(1,3):\n            ell = mpl.patches.Ellipse(gmm.means_[n], i*v[0], i*v[1],\n                                      180 + angle, color=\"blue\")\n            ell.set_clip_box(ax.bbox)\n            ell.set_alpha(0.1)\n            ax.add_artist(ell)\n</pre> # Representaci\u00f3n gr\u00e1fica # ============================================================================== # Codigo obtenido de: # https://github.com/amueller/COMS4995-s20/tree/master/slides/aml-14-clustering-mixture-models  def make_ellipses(gmm, ax):     for n in range(gmm.n_components):         if gmm.covariance_type == 'full':             covariances = gmm.covariances_[n]         elif gmm.covariance_type == 'tied':             covariances = gmm.covariances_         elif gmm.covariance_type == 'diag':             covariances = np.diag(gmm.covariances_[n])         elif gmm.covariance_type == 'spherical':             covariances = np.eye(gmm.means_.shape[1]) * gmm.covariances_[n]         v, w = np.linalg.eigh(covariances)         u = w[0] / np.linalg.norm(w[0])         angle = np.arctan2(u[1], u[0])         angle = 180 * angle / np.pi  # convert to degrees         v = 2. * np.sqrt(2.) * np.sqrt(v)                  for i in range(1,3):             ell = mpl.patches.Ellipse(gmm.means_[n], i*v[0], i*v[1],                                       180 + angle, color=\"blue\")             ell.set_clip_box(ax.bbox)             ell.set_alpha(0.1)             ax.add_artist(ell)          In\u00a0[40]: Copied! <pre>fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n\n# Distribuci\u00f3n de probabilidad de cada componente\nfor i in np.unique(clasificacion):\n    axs[0].scatter(\n        x = X[clasificacion == i, 0],\n        y = X[clasificacion == i, 1], \n        c = plt.rcParams['axes.prop_cycle'].by_key()['color'][i],\n        marker    = 'o',\n        edgecolor = 'black', \n        label= f\"Componente {i}\"\n    )\n\nmake_ellipses(modelo_gmm, ax = axs[0])\naxs[0].set_title('Distribuci\u00f3n de prob. de cada componente')\naxs[0].legend()\n\n\n# Distribuci\u00f3n de probabilidad del modelo completo\nxs = np.linspace(min(X[:, 0]), max(X[:, 0]), 1000)\nys = np.linspace(min(X[:, 1]), max(X[:, 1]), 1000)\nxx, yy = np.meshgrid(xs, ys)\nscores = modelo_gmm.score_samples(np.c_[xx.ravel(), yy.ravel()], )\naxs[1].scatter(X[:, 0], X[:, 1], s=5, alpha=.6, c=plt.cm.tab10(clasificacion))\nscores = np.exp(scores) # Las probabilidades est\u00e1n en log\naxs[1].contour(\n    xx, yy, scores.reshape(xx.shape),\n    levels=np.percentile(scores, np.linspace(0, 100, 10))[1:-1]\n)\naxs[1].set_title('Distribuci\u00f3n de prob. del modelo completo');\n</pre> fig, axs = plt.subplots(1, 2, figsize=(15, 5))  # Distribuci\u00f3n de probabilidad de cada componente for i in np.unique(clasificacion):     axs[0].scatter(         x = X[clasificacion == i, 0],         y = X[clasificacion == i, 1],          c = plt.rcParams['axes.prop_cycle'].by_key()['color'][i],         marker    = 'o',         edgecolor = 'black',          label= f\"Componente {i}\"     )  make_ellipses(modelo_gmm, ax = axs[0]) axs[0].set_title('Distribuci\u00f3n de prob. de cada componente') axs[0].legend()   # Distribuci\u00f3n de probabilidad del modelo completo xs = np.linspace(min(X[:, 0]), max(X[:, 0]), 1000) ys = np.linspace(min(X[:, 1]), max(X[:, 1]), 1000) xx, yy = np.meshgrid(xs, ys) scores = modelo_gmm.score_samples(np.c_[xx.ravel(), yy.ravel()], ) axs[1].scatter(X[:, 0], X[:, 1], s=5, alpha=.6, c=plt.cm.tab10(clasificacion)) scores = np.exp(scores) # Las probabilidades est\u00e1n en log axs[1].contour(     xx, yy, scores.reshape(xx.shape),     levels=np.percentile(scores, np.linspace(0, 100, 10))[1:-1] ) axs[1].set_title('Distribuci\u00f3n de prob. del modelo completo'); <p>N\u00famero de clusters</p> <p>Dado que los modelos GMM son modelos probabil\u00edsticos, se puede recurrir a m\u00e9tricas como el Akaike information criterion (AIC) o Bayesian information criterion (BIC) para identificar c\u00f3mo de bien se ajustan los datos observados a modelo creado.</p> In\u00a0[41]: Copied! <pre>n_components = range(1, 21)\nvalores_bic = []\nvalores_aic = []\n\nfor i in n_components:\n    modelo = GaussianMixture(n_components=i, covariance_type=\"full\")\n    modelo = modelo.fit(X)\n    valores_bic.append(modelo.bic(X))\n    valores_aic.append(modelo.aic(X))\n\nfig, ax = plt.subplots(1, 1, figsize=(12,6))\nax.plot(n_components, valores_bic, label='BIC')\nax.plot(n_components, valores_aic, label='AIC')\nax.set_title(\"Valores BIC y AIC\")\nax.set_xlabel(\"N\u00famero componentes\")\nax.legend();\n</pre> n_components = range(1, 21) valores_bic = [] valores_aic = []  for i in n_components:     modelo = GaussianMixture(n_components=i, covariance_type=\"full\")     modelo = modelo.fit(X)     valores_bic.append(modelo.bic(X))     valores_aic.append(modelo.aic(X))  fig, ax = plt.subplots(1, 1, figsize=(12,6)) ax.plot(n_components, valores_bic, label='BIC') ax.plot(n_components, valores_aic, label='AIC') ax.set_title(\"Valores BIC y AIC\") ax.set_xlabel(\"N\u00famero componentes\") ax.legend(); In\u00a0[42]: Copied! <pre>print(f\"N\u00famero \u00f3ptimo acorde al BIC: {range(1, 21)[np.argmin(valores_bic)]}\")\nprint(f\"N\u00famero \u00f3ptimo acorde al AIC: {range(1, 21)[np.argmin(valores_aic)]}\")\n</pre> print(f\"N\u00famero \u00f3ptimo acorde al BIC: {range(1, 21)[np.argmin(valores_bic)]}\") print(f\"N\u00famero \u00f3ptimo acorde al AIC: {range(1, 21)[np.argmin(valores_aic)]}\") <pre>N\u00famero \u00f3ptimo acorde al BIC: 4\nN\u00famero \u00f3ptimo acorde al AIC: 4\n</pre> <p>Ambas m\u00e9tricas identifican el 4 como n\u00famero \u00f3ptimo de clusters (componentes).</p>"},{"location":"lectures/machine_learning/ns_01/#no-supervisado-i","title":"No supervisado I\u00b6","text":""},{"location":"lectures/machine_learning/ns_01/#clustering","title":"Clustering\u00b6","text":"<p>El Clustering es la tarea de agrupar objetos por similitud, en grupos o conjuntos de manera que los miembros del mismo grupo tengan caracter\u00edsticas similares. Es la tarea principal de la miner\u00eda de datos exploratoria y es una t\u00e9cnica com\u00fan en el an\u00e1lisis de datos estad\u00edsticos.</p>"},{"location":"lectures/machine_learning/ns_01/#k-means","title":"K-means\u00b6","text":""},{"location":"lectures/machine_learning/ns_01/#teoria","title":"Teor\u00eda\u00b6","text":"<p>El algoritmo K-means  (MacQueen, 1967) agrupa las observaciones en un n\u00famero predefinido de $k$ clusters de forma que, la suma de las varianzas internas de los clusters, sea lo menor posible.</p> <p>Existen varias implementaciones de este algoritmo, la m\u00e1s com\u00fan de ellas se conoce como Lloyd\u2019s. En la bibliograf\u00eda es com\u00fan encontrar los t\u00e9rminos inertia, within-cluster sum-of-squares o varianza intra-cluster para referirse a la varianza interna de los clusters.</p>"},{"location":"lectures/machine_learning/ns_01/#algoritmo","title":"Algoritmo\u00b6","text":"<ol> <li>Especificar el n\u00famero $k$ de clusters que se quieren crear.</li> <li>Seleccionar de forma aleatoria $k$ observaciones del set de datos como centroides iniciales.</li> <li>Asignar cada una de las observaciones al centroide m\u00e1s cercano.</li> <li>Para cada uno de los $k$ clusters generados en el paso 3, recalcular su centroide.</li> </ol> <p>Repetir los pasos 3 y 4 hasta que las asignaciones no cambien o se alcance el n\u00famero m\u00e1ximo de iteraciones establecido.</p> <p>El problema anterior es NP-hard (imposible de resolver en tiempo polinomial, del tipo m\u00e1s dif\u00edcil de los probleams NP).</p>"},{"location":"lectures/machine_learning/ns_01/#ventajas-y-desventajas","title":"Ventajas y desventajas\u00b6","text":"<p>K-means es uno de los m\u00e9todos de clustering m\u00e1s utilizados. Destaca por la sencillez y velocidad de su algoritmo, sin embargo, presenta una serie de limitaciones que se deben tener en cuenta.</p> <ul> <li><p>Requiere que se indique de antemano el n\u00famero de clusters que se van a crear. Esto puede ser complicado si no se dispone de informaci\u00f3n adicional sobre los datos con los que se trabaja. Se han desarrollado varias estrategias para ayudar a identificar potenciales valores \u00f3ptimos de $k$ (elbow, shilouette), pero todas ellas son orientativas.</p> </li> <li><p>Dificultad para detectar clusters alargados o con formas irregulares.</p> </li> <li><p>Las agrupaciones resultantes pueden variar dependiendo de la asignaci\u00f3n aleatoria inicial de los centroides. Para minimizar este problema, se recomienda repetir el proceso de clustering entre 25-50 veces y seleccionar como resultado definitivo el que tenga menor suma total de varianza interna. Aun as\u00ed, solo se puede garantizar la reproducibilidad de los resultados si se emplean semillas.</p> </li> <li><p>Presenta problemas de robustez frente a outliers.</p> </li> </ul>"},{"location":"lectures/machine_learning/ns_01/#aplicacion","title":"Aplicaci\u00f3n\u00b6","text":"<p>Veamos un ejemplo de an\u00e1lisis no supervisado ocupando el algoritmo k-means.</p>"},{"location":"lectures/machine_learning/ns_01/#regla-del-codo","title":"Regla del codo\u00b6","text":"<p>Este m\u00e9todo utiliza los valores de la funci\u00f3n de perdida, $f(C_l,\\mu_l)$, obtenidos tras aplicar el $K$-means a diferente n\u00famero de Clusters (desde 1 a $N$ clusters).</p> <p>Una vez obtenidos los valores de la funci\u00f3n de p\u00e9rdida  tras aplicar el K-means de 1 a $N$ clusters, representamos en una gr\u00e1fica lineal la funci\u00f3n de p\u00e9rdida  respecto del n\u00famero de clusters.</p> <p>En esta gr\u00e1fica se deber\u00eda de apreciar un cambio brusco en la evoluci\u00f3n de la funci\u00f3n de p\u00e9rdida, teniendo la l\u00ednea representada una forma similar a la de un brazo y su codo.</p> <p>El punto en el que se observa ese cambio brusco en la funci\u00f3n de p\u00e9rdida nos dir\u00e1 el n\u00famero \u00f3ptimo de clusters a seleccionar para ese data set; o dicho de otra manera: el punto que representar\u00eda al codo del brazo ser\u00e1 el n\u00famero \u00f3ptimo de clusters para ese data set .</p>"},{"location":"lectures/machine_learning/ns_01/#hierarchical-clustering","title":"Hierarchical clustering\u00b6","text":""},{"location":"lectures/machine_learning/ns_01/#teoria","title":"Teor\u00eda\u00b6","text":"<p>Hierarchical clustering es una alternativa a los m\u00e9todos de partitioning clustering que no requiere que se pre-especifique el n\u00famero de clusters. Los m\u00e9todos que engloba el hierarchical clustering se subdividen en dos tipos dependiendo de la estrategia seguida para crear los grupos:</p> <ul> <li><p>Aglomerativo (agglomerative clustering o bottom-up): el agrupamiento se inicia con todas las observaciones separadas, cada una formando un cluster individual. Los clusters se van combinado a medida que la estructura crece hasta converger en uno solo.</p> </li> <li><p>Divisivo (divisive clustering o top-down): es la estrategia opuesta al aglomerativo. Se inicia con todas las observaciones contenidas en un mismo cluster y se suceden divisiones hasta que cada observaci\u00f3n forma un cluster* individual.</p> </li> </ul> <p>En ambos casos, los resultados pueden representarse de forma muy intuitiva en una estructura de \u00e1rbol llamada dendrograma.</p>"},{"location":"lectures/machine_learning/ns_01/#algoritmo","title":"Algoritmo\u00b6","text":""},{"location":"lectures/machine_learning/ns_01/#aglomerativo","title":"Aglomerativo\u00b6","text":"<p>El algoritmo seguido para por el clustering aglomerativo es:</p> <ol> <li><p>Considerar cada una de las n observaciones como un cluster individual, formando as\u00ed la base del dendrograma (hojas).</p> </li> <li><p>Proceso iterativo hasta que todas las observaciones pertenecen a un \u00fanico cluster:</p> <ul> <li><p>Calcular la distancia entre cada posible par de los n clusters. El investigador debe determinar el tipo de medida empleada para cuantificar la similitud entre observaciones o grupos (distancia y linkage).</p> </li> <li><p>Los dos clusters m\u00e1s similares se fusionan, de forma que quedan n-1 clusters.</p> </li> </ul> </li> <li><p>Cortar la estructura de \u00e1rbol generada (dendrograma) a una determinada altura para crear los clusters finales.</p> </li> </ol> <p>Para que el proceso de agrupamiento pueda llevarse a cabo tal como indica el algoritmo anterior, es necesario definir c\u00f3mo se cuantifica la similitud entre dos clusters. Es decir, se tiene que extender el concepto de distancia entre pares de observaciones para que sea aplicable a pares de grupos, cada uno formado por varias observaciones. A este proceso se le conoce como linkage. A continuaci\u00f3n, se describen los 5 tipos de linkage m\u00e1s empleados y sus definiciones.</p> <ul> <li><p>Complete or Maximum: se calcula la distancia entre todos los posibles pares formados por una observaci\u00f3n del cluster A y una del cluster B. La mayor de todas ellas se selecciona como la distancia entre los dos clusters. Se trata de la medida m\u00e1s conservadora (maximal intercluster dissimilarity).</p> </li> <li><p>Single or Minimum: se calcula la distancia entre todos los posibles pares formados por una observaci\u00f3n del cluster A y una del cluster B. La menor de todas ellas se selecciona como la distancia entre los dos clusters. Se trata de la medida menos conservadora (minimal intercluster dissimilarity).</p> </li> <li><p>Average: Se calcula la distancia entre todos los posibles pares formados por una observaci\u00f3n del cluster A y una del cluster B. El valor promedio de todas ellas se selecciona como la distancia entre los dos clusters (mean intercluster dissimilarity).</p> </li> <li><p>Centroid: Se calcula el centroide de cada uno de los clusters y se selecciona la distancia entre ellos como la distancia entre los dos clusters.</p> </li> <li><p>Ward: Se trata de un m\u00e9todo general. La selecci\u00f3n del par de clusters que se combinan en cada paso del agglomerative hierarchical clustering se basa en el valor \u00f3ptimo de una funci\u00f3n objetivo, pudiendo ser esta \u00faltima cualquier funci\u00f3n definida por el analista. El m\u00e9todo Ward's minimum variance es un caso particular en el que el objetivo es minimizar la suma total de varianza intra-cluster. En cada paso, se identifican aquellos 2 clusters cuya fusi\u00f3n conlleva menor incremento de la varianza total intra-cluster. Esta es la misma m\u00e9trica que se minimiza en K-means.</p> </li> </ul> <p>Los m\u00e9todos de complete, average y Ward's minimum variance suelen ser los preferidos por los analistas debido a que generan dendrogramas m\u00e1s compensados. Sin embargo, no se puede determinar que uno sea mejor que otro, ya que depende del caso de estudio en cuesti\u00f3n. Por ejemplo, en gen\u00f3mica, se emplea con frecuencia el m\u00e9todo de centroides. Junto con los resultados de un proceso de hierarchical clustering siempre hay que indicar qu\u00e9 distancia se ha empleado, as\u00ed como el tipo de linkage, ya que, dependiendo de estos, los resultados pueden variar en gran medida.</p>"},{"location":"lectures/machine_learning/ns_01/#divisivo","title":"Divisivo\u00b6","text":"<p>El algoritmo m\u00e1s conocido de divisive hierarchical clustering es DIANA (DIvisive ANAlysis Clustering). Este algoritmo se inicia con un \u00fanico cluster que contiene todas las observaciones. A continuaci\u00f3n, se van sucediendo divisiones hasta que cada observaci\u00f3n forma un cluster independiente. En cada iteraci\u00f3n, se selecciona el cluster con mayor di\u00e1metro, entendiendo por di\u00e1metro de un cluster la mayor de las diferencias entre dos de sus observaciones. Una vez seleccionado el cluster, se identifica la observaci\u00f3n m\u00e1s dispar, que es aquella con mayor distancia promedio respecto al resto de observaciones que forman el cluster. Esta observaci\u00f3n inicia el nuevo cluster. Se reasignan las observaciones en funci\u00f3n de si est\u00e1n m\u00e1s pr\u00f3ximas al nuevo cluster o al resto de la partici\u00f3n, dividiendo as\u00ed el cluster seleccionado en dos nuevos clusters.</p> <ol> <li>Todas las $n$ observaciones forman un \u00fanico cluster.</li> <li>Repetir hasta que haya $n$ clusters:<ul> <li>Calcular para cada cluster la mayor de las distancias entre pares de observaciones (di\u00e1metro del cluster).</li> <li>Seleccionar el cluster con mayor di\u00e1metro.</li> <li>Calcular la distancia media de cada observaci\u00f3n respecto a las dem\u00e1s.</li> <li>La observaci\u00f3n m\u00e1s distante inicia un nuevo cluster.</li> <li>Se reasignan las observaciones restantes al nuevo cluster o al viejo dependiendo de cu\u00e1l est\u00e1 m\u00e1s pr\u00f3ximo.</li> </ul> </li> </ol> <p>A diferencia del clustering aglomerativo, en el que hay que elegir un tipo de distancia y un m\u00e9todo de linkage, en el clustering divisivo solo hay que elegir la distancia, no hay linkage.</p>"},{"location":"lectures/machine_learning/ns_01/#dendograma","title":"Dendograma\u00b6","text":"<p>Los resultados del hierarchical clustering pueden representarse como un \u00e1rbol en el que las ramas representan la jerarqu\u00eda con la que se van sucediendo las uniones de clusters.</p> <p>Sup\u00f3ngase que se dispone de 45 observaciones en un espacio de dos dimensiones, a los que se les aplica hierarchical clustering para intentar identificar grupos. El siguiente dendrograma representa los resultados obtenidos.</p> <p></p>"},{"location":"lectures/machine_learning/ns_01/#aplicacion","title":"Aplicaci\u00f3n\u00b6","text":""},{"location":"lectures/machine_learning/ns_01/#density-based-clustering-dbscan","title":"Density based clustering (DBSCAN)\u00b6","text":""},{"location":"lectures/machine_learning/ns_01/#teoria","title":"Teor\u00eda\u00b6","text":"<p>Density-based spatial clustering of applications with noise (DBSCAN) fue presentado en 1996 por Ester et al. como una forma de identificar clusters siguiendo el modo intuitivo en el que lo hace el cerebro humano, identificando regiones con alta densidad de observaciones separadas por regiones de baja densidad.</p> <p></p>"},{"location":"lectures/machine_learning/ns_01/#algoritmo","title":"Algoritmo\u00b6","text":"<ol> <li><p>Para cada observaci\u00f3n $x_i$ calcular la distancia entre ella y el resto de observaciones. Si en su  $epsilon$ -neighborhood hay un n\u00famero de observaciones $\\geq min_samples$  marcar la observaci\u00f3n como core point, de lo contrario marcarla como visitada.</p> </li> <li><p>Para cada observaci\u00f3n  $x_i$   marcada como core point, si todav\u00eda no ha sido asignada a ning\u00fan cluster, crear uno nuevo y asignarla a \u00e9l. Encontrar recursivamente todas las observaciones densamente conectadas a ella y asignarlas al mismo cluster.</p> </li> <li><p>Iterar el mismo proceso para todas las observaciones que no hayan sido visitadas.</p> </li> <li><p>Aquellas observaciones que tras haber sido visitadas no pertenecen a ning\u00fan cluster se marcan como outliers.</p> </li> </ol> <p>Como resultado, todo cluster cumple dos propiedades: todos los puntos que forman parte de un mismo cluster est\u00e1n densamente conectados entre ellos y, si una observaci\u00f3n $A$  es densamente alcanzable desde cualquier otra observaci\u00f3n de un cluster, entonces $A$  tambi\u00e9n pertenece al cluster.</p>"},{"location":"lectures/machine_learning/ns_01/#hiperparametros","title":"Hiperpar\u00e1metros\u00b6","text":"<p>Como ocurre en muchas otras t\u00e9cnicas estad\u00edsticas, en DBSCAN no existe una forma \u00fanica y exacta de encontrar el valor adecuado de epsilon  ($\\epsilon$))  y  $min_samples$) . A modo orientativo se pueden seguir las siguientes premisas:</p> <ul> <li><p>$min_samples$ : cuanto mayor sea el tama\u00f1o del set de datos, mayor debe ser el valor m\u00ednimo de observaciones vecinas. En el libro Practical Guide to Cluster Analysis in R recomiendan no bajar nunca de 3. Si los datos contienen niveles altos de ruido, aumentar  $min_samples$  favorecer\u00e1 la creaci\u00f3n de clusters significativos menos influenciados por outliers.</p> </li> <li><p>epsilon ($\\epsilon$): una buena forma de escoger el valor de $\\epsilon$  es estudiar las distancias promedio entre las  $k = minsamples\ud835\udc60$  observaciones m\u00e1s pr\u00f3ximas. Al representar estas distancias en funci\u00f3n de  $\\epsilon$ , el punto de inflexi\u00f3n de la curva suele ser un valor \u00f3ptimo. Si el valor de  $\\epsilon$ escogido es muy peque\u00f1o, una proporci\u00f3n alta de las observaciones no se asignar\u00e1n a ning\u00fan cluster, por el contrario, si el valor es demasiado grande, la mayor\u00eda de observaciones se agrupar\u00e1n en un \u00fanico cluster.</p> </li> </ul>"},{"location":"lectures/machine_learning/ns_01/#ventajas-y-desventajas","title":"Ventajas y desventajas\u00b6","text":"<ul> <li><p>Ventajas</p> <ul> <li>No requiere que el usuario especifique el n\u00famero de clusters.</li> <li>Es independiente de la forma que tengan los clusters.</li> <li>Puede identificar outliers, por lo que los clusters generados no se ven influenciados por ellos.</li> </ul> </li> <li><p>Desventajas</p> <ul> <li><p>Es un m\u00e9todo determin\u00edstico siempre y cuando el orden de los datos sea el mismo. Los border points que son alcanzables desde m\u00e1s de un cluster pueden asignarse a uno u otro dependiendo del orden en el que se procesen los datos.</p> </li> <li><p>No genera buenos resultados cuando la densidad de los grupos es muy distinta, ya que no es posible encontrar los par\u00e1metros  \ud835\udf16  y min_samples que sirvan para todos a la vez.</p> </li> </ul> </li> </ul>"},{"location":"lectures/machine_learning/ns_01/#aplicacion","title":"Aplicaci\u00f3n\u00b6","text":"<p>Veamos un ejemplo de an\u00e1lisis no supervisado ocupando el algoritmo DBSCAN.</p>"},{"location":"lectures/machine_learning/ns_01/#gaussian-mixture-models-gmms","title":"Gaussian mixture models (GMMs)\u00b6","text":""},{"location":"lectures/machine_learning/ns_01/#teoria","title":"Teor\u00eda\u00b6","text":"<p>Un Gaussian Mixture model es un modelo probabil\u00edstico en el que se considera que las observaciones siguen una distribuci\u00f3n probabil\u00edstica formada por la combinaci\u00f3n de m\u00faltiples distribuciones normales (componentes). En su aplicaci\u00f3n al clustering, puede entenderse como una generalizaci\u00f3n de K-means con la que, en lugar de asignar cada observaci\u00f3n a un \u00fanico cluster, se obtiene una probabilidad de pertenencia a cada uno.</p> <p>Para estimar los par\u00e1metros que definen la funci\u00f3n de distribuci\u00f3n de cada cluster (media y matriz de covarianza) se recurre al algoritmo de Expectation-Maximization (EM). Una vez aprendidos los par\u00e1metros, se puede calcular la probabilidad que tiene cada observaci\u00f3n de pertenecer a cada cluster y asignarla a aquel con mayor probabilidad.</p> <p></p>"},{"location":"lectures/machine_learning/ns_01/#algoritmo","title":"Algoritmo\u00b6","text":"<p>Junto con el n\u00famero de clusters (componentes), hay que determinar el tipo de matriz de covarianza que pueden tener los clusters. Dependiendo del tipo de matriz, la forma de los clusters puede ser:</p> <ul> <li><p>tied: todos los clusters comparten la misma matriz de covarianza.</p> </li> <li><p>diagonal: las dimensiones de cada cluster a lo largo de cada dimensi\u00f3n puede ser distinto, pero las elipses generadas siempre quedan alineadas con los ejes, es decir, su orientaciones son limitadas.</p> </li> <li><p>spherical: las dimensiones de cada cluster son las mismas en todas las dimensiones. Esto permite generar clusters de distinto tama\u00f1o pero todos esf\u00e9ricos.</p> </li> <li><p>full: cada cluster puede puede ser modelado como una elipse cualquier orientaci\u00f3n y dimensiones.</p> </li> </ul>"},{"location":"lectures/machine_learning/ns_01/#aplicacion","title":"Aplicaci\u00f3n\u00b6","text":""},{"location":"lectures/machine_learning/ns_01/#limitaciones-del-clustering","title":"Limitaciones del clustering\u00b6","text":"<p>El clustering puede ser una herramienta muy \u00fatil para encontrar agrupaciones en los datos, sobre todo a medida que el volumen de los mismos aumenta. Sin embargo, es importante recordar sus limitaciones o problemas que pueden surgir al aplicarlo. Algunas de ellas son:</p> <ul> <li><p>Peque\u00f1as decisiones pueden tener grandes consecuencias:a la hora de utilizar los m\u00e9todos de clustering se tienen que tomar decisiones que influyen en gran medida en los resultados obtenidos. No existe una \u00fanica respuesta correcta, por lo que en la pr\u00e1ctica se prueban diferentes opciones.</p> <ul> <li>Escalado y centrado de las variables</li> <li>Qu\u00e9 medida de distancia/similitud emplear</li> <li>N\u00famero de clusters</li> <li>Tipo de linkage empleado en hierarchical clustering</li> <li>A que altura establecer el corte de un dendrograma</li> </ul> </li> <li><p>Validaci\u00f3n de los clusters obtenidos: no es f\u00e1cil comprobar la validez de los resultados ya que en la mayor\u00eda de escenarios se desconoce la verdadera agrupaci\u00f3n.</p> </li> <li><p>Falta de robustez: los m\u00e9todos de K-means-clustering e hierarchical clustering asignan obligatoriamente cada observaci\u00f3n a un grupo. Si existe en la muestra alg\u00fan outlier, a pesar de que realmente no pertenezca a ning\u00fan grupo, el algoritmo lo asignar\u00e1 a uno de ellos provocando una distorsi\u00f3n significativa del cluster en cuesti\u00f3n. Algunas alternativas son k-medoids y DBSCAN.</p> </li> <li><p>La naturaleza del algoritmo de hierarchical clustering conlleva que, si se realiza una mala divisi\u00f3n en los pasos iniciales, no se pueda corregir en los pasos siguientes.</p> </li> </ul>"},{"location":"lectures/machine_learning/ns_01/#referencias","title":"Referencias\u00b6","text":"<ol> <li>Unsupervised learning</li> <li>Clustering con Python (Joaqu\u00edn Amat Rodrigo)</li> </ol>"},{"location":"lectures/machine_learning/ns_02/","title":"No supervisado II","text":"In\u00a0[1]: Copied! <pre># librerias\n \nimport os\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\n\npd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes\n\n# Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab\n%matplotlib inline\n</pre> # librerias   import os import numpy as np import pandas as pd  import matplotlib.pyplot as plt import seaborn as sns    pd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes  # Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab %matplotlib inline In\u00a0[2]: Copied! <pre>from sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import scale\n</pre> from sklearn.model_selection import train_test_split from sklearn.decomposition import PCA from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.pipeline import Pipeline from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler from sklearn.preprocessing import scale In\u00a0[3]: Copied! <pre># Datos\n\nurl='https://drive.google.com/file/d/1ckxtKR1U_ySdtMy1uWLo_KYGbUYrKfmI/view?usp=drive_link'\nurl='https://drive.google.com/uc?id=' + url.split('/')[-2]\n\n\ndatos = pd.read_csv(url, sep=\",\")\ndatos = datos.rename(columns = {datos.columns[0]:'index'}).set_index('index')\ndatos.head()\n</pre> # Datos  url='https://drive.google.com/file/d/1ckxtKR1U_ySdtMy1uWLo_KYGbUYrKfmI/view?usp=drive_link' url='https://drive.google.com/uc?id=' + url.split('/')[-2]   datos = pd.read_csv(url, sep=\",\") datos = datos.rename(columns = {datos.columns[0]:'index'}).set_index('index') datos.head() Out[3]: Murder Assault UrbanPop Rape index Alabama 13.2 236 58 21.2 Alaska 10.0 263 48 44.5 Arizona 8.1 294 80 31.0 Arkansas 8.8 190 50 19.5 California 9.0 276 91 40.6 <p>Veamos una exploraci\u00f3n inicial de los datos:</p> In\u00a0[4]: Copied! <pre>print('----------------------')\nprint('Media de cada variable')\nprint('----------------------')\ndatos.mean(axis=0)\n</pre> print('----------------------') print('Media de cada variable') print('----------------------') datos.mean(axis=0) <pre>----------------------\nMedia de cada variable\n----------------------\n</pre> Out[4]: <pre>Murder        7.788\nAssault     170.760\nUrbanPop     65.540\nRape         21.232\ndtype: float64</pre> <p>La media de las variables muestra que hay tres veces m\u00e1s secuestros que asesinatos y 8 veces m\u00e1s asaltos que secuestros.</p> In\u00a0[5]: Copied! <pre>print('-------------------------')\nprint('Varianza de cada variable')\nprint('-------------------------')\ndatos.var(axis=0)\n</pre> print('-------------------------') print('Varianza de cada variable') print('-------------------------') datos.var(axis=0) <pre>-------------------------\nVarianza de cada variable\n-------------------------\n</pre> Out[5]: <pre>Murder        18.970465\nAssault     6945.165714\nUrbanPop     209.518776\nRape          87.729159\ndtype: float64</pre> <p>La varianza es muy distinta entre las variables, en el caso de Assault, la varianza es varios \u00f3rdenes de magnitud superior al resto.</p> <p>Si no se estandarizan las variables para que tengan media cero y desviaci\u00f3n est\u00e1ndar de uno antes de realizar el estudio PCA, la variable Assault, que tiene una media y dispersi\u00f3n muy superior al resto, dominar\u00e1 la mayor\u00eda de las componentes principales.</p> <p>Modelo PCA</p> <p>La clase <code>sklearn.decomposition.PCA</code> incorpora las principales funcionalidades que se necesitan a la hora de trabajar con modelos PCA. El argumento <code>n_components</code> determina el n\u00famero de componentes calculados. Si se indica None, se calculan todas las posibles (min(filas, columnas) - 1).</p> <p>Por defecto, <code>PCA()</code> centra los valores pero no los escala. Esto es importante ya que, si las variables tienen distinta dispersi\u00f3n, como en este caso, es necesario escalarlas. Una forma de hacerlo es combinar un <code>StandardScaler()</code> y un <code>PCA()</code> dentro de un <code>pipeline</code>.</p> In\u00a0[6]: Copied! <pre># Entrenamiento modelo PCA con escalado de los datos\n# ==============================================================================\npca_pipe = make_pipeline(StandardScaler(), PCA())\npca_pipe.fit(datos)\n\n# Se extrae el modelo entrenado del pipeline\nmodelo_pca = pca_pipe.named_steps['pca']\n</pre> # Entrenamiento modelo PCA con escalado de los datos # ============================================================================== pca_pipe = make_pipeline(StandardScaler(), PCA()) pca_pipe.fit(datos)  # Se extrae el modelo entrenado del pipeline modelo_pca = pca_pipe.named_steps['pca'] <p>Una vez entrenado el objeto <code>PCA</code>, pude accederse a toda la informaci\u00f3n de las componentes creadas.</p> <p><code>components_</code> contiene el valor de los loadings  \ud835\udf19  que definen cada componente (eigenvector). Las filas se corresponden con las componentes principals (ordenadas de mayor a menor varianza explicada). Las filas se corresponden con las variables de entrada.</p> In\u00a0[7]: Copied! <pre># Se combierte el array a dataframe para a\u00f1adir nombres a los ejes.\npd.DataFrame(\n    data    = modelo_pca.components_,\n    columns = datos.columns,\n    index   = ['PC1', 'PC2', 'PC3', 'PC4']\n)\n</pre> # Se combierte el array a dataframe para a\u00f1adir nombres a los ejes. pd.DataFrame(     data    = modelo_pca.components_,     columns = datos.columns,     index   = ['PC1', 'PC2', 'PC3', 'PC4'] ) Out[7]: Murder Assault UrbanPop Rape PC1 0.535899 0.583184 0.278191 0.543432 PC2 0.418181 0.187986 -0.872806 -0.167319 PC3 -0.341233 -0.268148 -0.378016 0.817778 PC4 0.649228 -0.743407 0.133878 0.089024 <p>Analizar con detalle el vector de loadings que forma cada componente puede ayudar a interpretar qu\u00e9 tipo de informaci\u00f3n recoge cada una de ellas. Por ejemplo, la primera componente es el resultado de la siguiente combinaci\u00f3n lineal de las variables originales:</p> <p>$$PC1=0.535899 Murder+0.583184 Assault+0.278191 UrbanPop+0.543432 Rape$$</p> <p>Los pesos asignados en la primera componente a las variables Assault, Murder y Rape son aproximadamente iguales entre ellos y superiores al asignado a UrbanPoP. Esto significa que la primera componente recoge mayoritariamente la informaci\u00f3n correspondiente a los delitos. En la segunda componente, es la variable UrbanPoP es la que tiene con diferencia mayor peso, por lo que se corresponde principalmente con el nivel de urbanizaci\u00f3n del estado. Si bien en este ejemplo la interpretaci\u00f3n de las componentes es bastante clara, no en todos los casos ocurre lo mismo, sobre todo a medida que aumenta el n\u00famero de variables.</p> <p>La influencia de las variables en cada componente analizarse visualmente con un gr\u00e1fico de tipo heatmap.</p> In\u00a0[8]: Copied! <pre># Heatmap componentes\n# ==============================================================================\nplt.figure(figsize=(12,4))\ncomponentes = modelo_pca.components_\nplt.imshow(componentes.T, cmap='viridis', aspect='auto')\nplt.yticks(range(len(datos.columns)), datos.columns)\nplt.xticks(range(len(datos.columns)), np.arange(modelo_pca.n_components_) + 1)\nplt.grid(False)\nplt.colorbar();\n</pre> # Heatmap componentes # ============================================================================== plt.figure(figsize=(12,4)) componentes = modelo_pca.components_ plt.imshow(componentes.T, cmap='viridis', aspect='auto') plt.yticks(range(len(datos.columns)), datos.columns) plt.xticks(range(len(datos.columns)), np.arange(modelo_pca.n_components_) + 1) plt.grid(False) plt.colorbar(); <p>Una vez calculadas las componentes principales, se puede conocer la varianza explicada por cada una de ellas, la proporci\u00f3n respecto al total y la proporci\u00f3n de varianza acumulada. Esta informaci\u00f3n est\u00e1 almacenada en los atributos <code>explained_variance_</code> y <code>explained_variance_ratio_</code> del modelo.</p> In\u00a0[9]: Copied! <pre># graficar varianza por componente\npercent_variance = np.round(modelo_pca.explained_variance_ratio_* 100, decimals =2)\ncolumns = ['PC1', 'PC2', 'PC3', 'PC4']\n\nplt.figure(figsize=(12,4))\nplt.bar(x= range(1,5), height=percent_variance, tick_label=columns)\nplt.xticks(np.arange(modelo_pca.n_components_) + 1)\n\nplt.ylabel('Componente principal')\nplt.xlabel('Por. varianza explicada')\nplt.title('Porcentaje de varianza explicada por cada componente')\nplt.show()\n</pre> # graficar varianza por componente percent_variance = np.round(modelo_pca.explained_variance_ratio_* 100, decimals =2) columns = ['PC1', 'PC2', 'PC3', 'PC4']  plt.figure(figsize=(12,4)) plt.bar(x= range(1,5), height=percent_variance, tick_label=columns) plt.xticks(np.arange(modelo_pca.n_components_) + 1)  plt.ylabel('Componente principal') plt.xlabel('Por. varianza explicada') plt.title('Porcentaje de varianza explicada por cada componente') plt.show() <p>Ahora realizamos el gr\u00e1fico pero respecto a la suma acumulada.</p> In\u00a0[10]: Copied! <pre># graficar varianza por la suma acumulada de los componente\npercent_variance_cum = np.cumsum(percent_variance)\ncolumns = ['PC1', 'PC1+PC2', 'PC1+PC2+PC3', 'PC1+PC2+PC3+PC4']\n\nplt.figure(figsize=(12,4))\nplt.bar(x= range(1,5), height=percent_variance_cum, tick_label=columns)\nplt.ylabel('Percentate of Variance Explained')\nplt.xlabel('Principal Component Cumsum')\nplt.title('PCA Scree Plot')\nplt.show()\n</pre> # graficar varianza por la suma acumulada de los componente percent_variance_cum = np.cumsum(percent_variance) columns = ['PC1', 'PC1+PC2', 'PC1+PC2+PC3', 'PC1+PC2+PC3+PC4']  plt.figure(figsize=(12,4)) plt.bar(x= range(1,5), height=percent_variance_cum, tick_label=columns) plt.ylabel('Percentate of Variance Explained') plt.xlabel('Principal Component Cumsum') plt.title('PCA Scree Plot') plt.show() <p>Si se empleasen \u00fanicamente las dos primeras componentes se conseguir\u00eda explicar el 87% de la varianza observada.</p> <p>Transformaci\u00f3n</p> <p>Una vez entrenado el modelo, con el m\u00e9todo <code>transform()</code> se puede reducir la dimensionalidad de nuevas observaciones proyect\u00e1ndolas en el espacio definido por las componentes.</p> In\u00a0[11]: Copied! <pre># Proyecci\u00f3n de las observaciones de entrenamiento\n# ==============================================================================\nproyecciones = pca_pipe.transform(X=datos)\nproyecciones = pd.DataFrame(\n    proyecciones,\n    columns = ['PC1', 'PC2', 'PC3', 'PC4'],\n    index   = datos.index\n)\nproyecciones.head()\n</pre> # Proyecci\u00f3n de las observaciones de entrenamiento # ============================================================================== proyecciones = pca_pipe.transform(X=datos) proyecciones = pd.DataFrame(     proyecciones,     columns = ['PC1', 'PC2', 'PC3', 'PC4'],     index   = datos.index ) proyecciones.head() Out[11]: PC1 PC2 PC3 PC4 index Alabama 0.985566 1.133392 -0.444269 0.156267 Alaska 1.950138 1.073213 2.040003 -0.438583 Arizona 1.763164 -0.745957 0.054781 -0.834653 Arkansas -0.141420 1.119797 0.114574 -0.182811 California 2.523980 -1.542934 0.598557 -0.341996 <p>La transformaci\u00f3n es el resultado de multiplicar los vectores que definen cada componente con el valor de las variables. Puede calcularse de forma manual:</p> In\u00a0[12]: Copied! <pre>proyecciones = np.dot(modelo_pca.components_, scale(datos).T)\nproyecciones = pd.DataFrame(proyecciones, index = ['PC1', 'PC2', 'PC3', 'PC4'])\nproyecciones = proyecciones.transpose().set_index(datos.index)\nproyecciones.head()\n</pre> proyecciones = np.dot(modelo_pca.components_, scale(datos).T) proyecciones = pd.DataFrame(proyecciones, index = ['PC1', 'PC2', 'PC3', 'PC4']) proyecciones = proyecciones.transpose().set_index(datos.index) proyecciones.head() Out[12]: PC1 PC2 PC3 PC4 index Alabama 0.985566 1.133392 -0.444269 0.156267 Alaska 1.950138 1.073213 2.040003 -0.438583 Arizona 1.763164 -0.745957 0.054781 -0.834653 Arkansas -0.141420 1.119797 0.114574 -0.182811 California 2.523980 -1.542934 0.598557 -0.341996 <p>Reconstrucci\u00f3n</p> <p>Puede revertirse la transformaci\u00f3n y reconstruir el valor inicial con el m\u00e9todo inverse_transform(). Es importante tener en cuenta que, la reconstrucci\u00f3n, solo ser\u00e1 completa si se han incluido todas las componentes.</p> In\u00a0[13]: Copied! <pre># Recostruccion de las proyecciones\n# ==============================================================================\nrecostruccion = pca_pipe.inverse_transform(proyecciones)\nrecostruccion = pd.DataFrame(\n                    recostruccion,\n                    columns = datos.columns,\n                    index   = datos.index\n)\nprint('------------------')\nprint('Valores originales')\nprint('------------------')\ndisplay(recostruccion.head())\n\nprint('---------------------')\nprint('Valores reconstruidos')\nprint('---------------------')\ndisplay(datos.head())\n</pre> # Recostruccion de las proyecciones # ============================================================================== recostruccion = pca_pipe.inverse_transform(proyecciones) recostruccion = pd.DataFrame(                     recostruccion,                     columns = datos.columns,                     index   = datos.index ) print('------------------') print('Valores originales') print('------------------') display(recostruccion.head())  print('---------------------') print('Valores reconstruidos') print('---------------------') display(datos.head()) <pre>------------------\nValores originales\n------------------\n</pre> Murder Assault UrbanPop Rape index Alabama 13.2 236.0 58.0 21.2 Alaska 10.0 263.0 48.0 44.5 Arizona 8.1 294.0 80.0 31.0 Arkansas 8.8 190.0 50.0 19.5 California 9.0 276.0 91.0 40.6 <pre>---------------------\nValores reconstruidos\n---------------------\n</pre> Murder Assault UrbanPop Rape index Alabama 13.2 236 58 21.2 Alaska 10.0 263 48 44.5 Arizona 8.1 294 80 31.0 Arkansas 8.8 190 50 19.5 California 9.0 276 91 40.6 <p>Figura 2 \u2013 Medici\u00f3n de similitudes por pares en el espacio de alta dimensi\u00f3n 2. El paso 2 es similar al paso 1, pero en lugar de usar una distribuci\u00f3n gaussiana se usa una distribuci\u00f3n t de Student con un grado de libertad, que tambi\u00e9n se conoce como la distribuci\u00f3n de Cauchy (Figura 3). Esto nos da un segundo conjunto de probabilidades ($Q_{ij}$) en el espacio de baja dimensi\u00f3n.</p> <p>Como puede ver, la distribuci\u00f3n t de Student tiene colas m\u00e1s pesadas que la distribuci\u00f3n normal. Las colas pesadas permiten un mejor modelado de distancias muy separadas.</p> <p></p> <p>Figura 3 \u2013 Distribuci\u00f3n noraml vs t-student</p> <ol> <li>El \u00faltimo paso es que queremos que este conjunto de probabilidades del espacio de baja dimensi\u00f3n ($Q_{ij}$) refleje las del espacio de alta dimensi\u00f3n ($P_{ij}$) de la mejor manera posible.</li> </ol> <p>Queremos que las dos estructuras de mapa sean similares. Medimos la diferencia entre las distribuciones de probabilidad de los espacios bidimensionales utilizando la divergencia de Kullback-Liebler (KL).</p> <p>No incluir\u00e9 mucho en KL, excepto que es un enfoque asim\u00e9trico que compara de manera eficiente los grandes valores $P_{ij}$ y $Q_{ij}$. Finalmente, utilizamos el descenso de gradiente para minimizar nuestra funci\u00f3n de costo KL.</p> In\u00a0[14]: Copied! <pre># Load Python Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom time import time\n%matplotlib inline\n</pre> # Load Python Libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from time import time %matplotlib inline In\u00a0[15]: Copied! <pre>from sklearn.datasets import load_digits\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\n</pre> from sklearn.datasets import load_digits from sklearn.manifold import TSNE from sklearn.decomposition import PCA In\u00a0[16]: Copied! <pre>digits = load_digits()\n\ndf = pd.DataFrame(digits['data'])\ndf['label'] = digits['target']\ndf.head()\n</pre> digits = load_digits()  df = pd.DataFrame(digits['data']) df['label'] = digits['target'] df.head() Out[16]: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 label 0 0.0 0.0 5.0 13.0 9.0 1.0 0.0 0.0 0.0 0.0 13.0 15.0 10.0 15.0 5.0 0.0 0.0 3.0 15.0 2.0 0.0 11.0 8.0 0.0 0.0 4.0 12.0 0.0 0.0 8.0 8.0 0.0 0.0 5.0 8.0 0.0 0.0 9.0 8.0 0.0 0.0 4.0 11.0 0.0 1.0 12.0 7.0 0.0 0.0 2.0 14.0 5.0 10.0 12.0 0.0 0.0 0.0 0.0 6.0 13.0 10.0 0.0 0.0 0.0 0 1 0.0 0.0 0.0 12.0 13.0 5.0 0.0 0.0 0.0 0.0 0.0 11.0 16.0 9.0 0.0 0.0 0.0 0.0 3.0 15.0 16.0 6.0 0.0 0.0 0.0 7.0 15.0 16.0 16.0 2.0 0.0 0.0 0.0 0.0 1.0 16.0 16.0 3.0 0.0 0.0 0.0 0.0 1.0 16.0 16.0 6.0 0.0 0.0 0.0 0.0 1.0 16.0 16.0 6.0 0.0 0.0 0.0 0.0 0.0 11.0 16.0 10.0 0.0 0.0 1 2 0.0 0.0 0.0 4.0 15.0 12.0 0.0 0.0 0.0 0.0 3.0 16.0 15.0 14.0 0.0 0.0 0.0 0.0 8.0 13.0 8.0 16.0 0.0 0.0 0.0 0.0 1.0 6.0 15.0 11.0 0.0 0.0 0.0 1.0 8.0 13.0 15.0 1.0 0.0 0.0 0.0 9.0 16.0 16.0 5.0 0.0 0.0 0.0 0.0 3.0 13.0 16.0 16.0 11.0 5.0 0.0 0.0 0.0 0.0 3.0 11.0 16.0 9.0 0.0 2 3 0.0 0.0 7.0 15.0 13.0 1.0 0.0 0.0 0.0 8.0 13.0 6.0 15.0 4.0 0.0 0.0 0.0 2.0 1.0 13.0 13.0 0.0 0.0 0.0 0.0 0.0 2.0 15.0 11.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 12.0 12.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 10.0 8.0 0.0 0.0 0.0 8.0 4.0 5.0 14.0 9.0 0.0 0.0 0.0 7.0 13.0 13.0 9.0 0.0 0.0 3 4 0.0 0.0 0.0 1.0 11.0 0.0 0.0 0.0 0.0 0.0 0.0 7.0 8.0 0.0 0.0 0.0 0.0 0.0 1.0 13.0 6.0 2.0 2.0 0.0 0.0 0.0 7.0 15.0 0.0 9.0 8.0 0.0 0.0 5.0 16.0 10.0 0.0 16.0 6.0 0.0 0.0 4.0 15.0 16.0 13.0 16.0 1.0 0.0 0.0 0.0 0.0 3.0 15.0 10.0 0.0 0.0 0.0 0.0 0.0 2.0 16.0 4.0 0.0 0.0 4 In\u00a0[17]: Copied! <pre># PCA\nscaler = StandardScaler()\n\nX = df.drop(columns='label')\ny = df['label']\n    \nembedding = PCA(n_components=2)\nX_transform = embedding.fit_transform(X)\n    \ndf_pca = pd.DataFrame(X_transform,columns = ['Score1','Score2'])\ndf_pca['label'] = y\n</pre> # PCA scaler = StandardScaler()  X = df.drop(columns='label') y = df['label']      embedding = PCA(n_components=2) X_transform = embedding.fit_transform(X)      df_pca = pd.DataFrame(X_transform,columns = ['Score1','Score2']) df_pca['label'] = y In\u00a0[18]: Copied! <pre># Plot Digits PCA\n\n\n# Set style of scatterplot\nsns.set_context(\"notebook\", font_scale=1.1)\nsns.set_style(\"ticks\")\n\n# Create scatterplot of dataframe\nsns.lmplot(x='Score1',\n           y='Score2',\n           data=df_pca,\n           fit_reg=False,\n           legend=True,\n           height=9,\n           hue='label',\n           scatter_kws={\"s\":200, \"alpha\":0.3})\n\nplt.title('PCA Results: Digits', weight='bold').set_fontsize('14')\nplt.xlabel('Prin Comp 1', weight='bold').set_fontsize('10')\nplt.ylabel('Prin Comp 2', weight='bold').set_fontsize('10')\n</pre> # Plot Digits PCA   # Set style of scatterplot sns.set_context(\"notebook\", font_scale=1.1) sns.set_style(\"ticks\")  # Create scatterplot of dataframe sns.lmplot(x='Score1',            y='Score2',            data=df_pca,            fit_reg=False,            legend=True,            height=9,            hue='label',            scatter_kws={\"s\":200, \"alpha\":0.3})  plt.title('PCA Results: Digits', weight='bold').set_fontsize('14') plt.xlabel('Prin Comp 1', weight='bold').set_fontsize('10') plt.ylabel('Prin Comp 2', weight='bold').set_fontsize('10') <p>Al graficar las dos componentes principales del m\u00e9todo PCA, se observa que no existe una clara distinci\u00f3n entre la distintas clases (solo se ve un gran c\u00famulo de puntos mezclados).</p> In\u00a0[19]: Copied! <pre># tsne\nscaler = StandardScaler()\n\nX = df.drop(columns='label')\ny = df['label']\n    \nembedding = TSNE(n_components=2)\nX_transform = embedding.fit_transform(X)\n    \ndf_tsne = pd.DataFrame(X_transform,columns = ['_DIM_1_','_DIM_2_'])\ndf_tsne['label'] = y\n</pre> # tsne scaler = StandardScaler()  X = df.drop(columns='label') y = df['label']      embedding = TSNE(n_components=2) X_transform = embedding.fit_transform(X)      df_tsne = pd.DataFrame(X_transform,columns = ['_DIM_1_','_DIM_2_']) df_tsne['label'] = y In\u00a0[20]: Copied! <pre># Plot Digits t-SNE\nsns.set_context(\"notebook\", font_scale=1.1)\nsns.set_style(\"ticks\")\n\nsns.lmplot(x='_DIM_1_',\n           y='_DIM_2_',\n           data=df_tsne,\n           fit_reg=False,\n           legend=True,\n           height=9,\n           hue='label',\n           scatter_kws={\"s\":200, \"alpha\":0.3})\n\nplt.title('t-SNE Results: Digits', weight='bold').set_fontsize('14')\nplt.xlabel('Dimension 1', weight='bold').set_fontsize('10')\nplt.ylabel('Dimension 2', weight='bold').set_fontsize('10')\n</pre> # Plot Digits t-SNE sns.set_context(\"notebook\", font_scale=1.1) sns.set_style(\"ticks\")  sns.lmplot(x='_DIM_1_',            y='_DIM_2_',            data=df_tsne,            fit_reg=False,            legend=True,            height=9,            hue='label',            scatter_kws={\"s\":200, \"alpha\":0.3})  plt.title('t-SNE Results: Digits', weight='bold').set_fontsize('14') plt.xlabel('Dimension 1', weight='bold').set_fontsize('10') plt.ylabel('Dimension 2', weight='bold').set_fontsize('10') <p>Para el caso del m\u00e9todo TSNE, se observa una diferenciaci\u00f3n entre los grupos de estudios (aspecto que fue muy distinto al momento de analizar el m\u00e9todo del PCA).</p> <p>Observaci\u00f3n: Si bien se muestra donde el m\u00e9todo TSNE logra ser superior en aspecto de reducci\u00f3n de dimensionalidad que el m\u00e9todo PCA, no significa que para distintos experimientos se tengan los mismo resultados.</p> In\u00a0[21]: Copied! <pre># Librerias\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.decomposition import PCA,TruncatedSVD,NMF\nfrom sklearn.manifold import Isomap,LocallyLinearEmbedding,MDS,SpectralEmbedding,TSNE\n</pre> # Librerias from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split  from sklearn.decomposition import PCA,TruncatedSVD,NMF from sklearn.manifold import Isomap,LocallyLinearEmbedding,MDS,SpectralEmbedding,TSNE In\u00a0[22]: Copied! <pre>from sklearn.metrics import mean_absolute_error, mean_squared_error\n\ndef mean_absolute_percentage_error(y_true, y_pred):\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\ndef regression_metrics(df):\n    \"\"\"\n    Aplicar las distintas m\u00e9tricas definidas\n    :param df: DataFrame con las columnas: ['y', 'yhat']\n    :return: DataFrame con las m\u00e9tricas especificadas\n    \"\"\"\n    df_result = pd.DataFrame()\n\n    y_true = df['y']\n    y_pred = df['yhat']\n\n    df_result['mae'] = [round(mean_absolute_error(y_true, y_pred), 4)]\n    df_result['mse'] = [round(mean_squared_error(y_true, y_pred), 4)]\n    df_result['rmse'] = [round(np.sqrt(mean_squared_error(y_true, y_pred)), 4)]\n    df_result['mape'] = [round(mean_absolute_percentage_error(y_true, y_pred), 4)]\n    df_result['smape'] = [round(2 * mean_absolute_percentage_error(y_true, y_pred) / (mean_absolute_percentage_error(y_true, y_pred) + 100), 4)]\n\n    return df_result\n</pre> from sklearn.metrics import mean_absolute_error, mean_squared_error  def mean_absolute_percentage_error(y_true, y_pred):     return np.mean(np.abs((y_true - y_pred) / y_true)) * 100  def regression_metrics(df):     \"\"\"     Aplicar las distintas m\u00e9tricas definidas     :param df: DataFrame con las columnas: ['y', 'yhat']     :return: DataFrame con las m\u00e9tricas especificadas     \"\"\"     df_result = pd.DataFrame()      y_true = df['y']     y_pred = df['yhat']      df_result['mae'] = [round(mean_absolute_error(y_true, y_pred), 4)]     df_result['mse'] = [round(mean_squared_error(y_true, y_pred), 4)]     df_result['rmse'] = [round(np.sqrt(mean_squared_error(y_true, y_pred)), 4)]     df_result['mape'] = [round(mean_absolute_percentage_error(y_true, y_pred), 4)]     df_result['smape'] = [round(2 * mean_absolute_percentage_error(y_true, y_pred) / (mean_absolute_percentage_error(y_true, y_pred) + 100), 4)]      return df_result In\u00a0[23]: Copied! <pre># Datos\nurl='https://drive.google.com/file/d/1zqoLNbuysK5Idrb9--DFtCoxkapruSDc/view?usp=drive_link'\nurl='https://drive.google.com/uc?id=' + url.split('/')[-2]\n\ndatos = pd.read_csv(url, sep=\",\")\ndatos = datos.drop(columns = datos.columns[0])\ndatos['fat'] = datos['fat'].astype(float)\ndatos.head()\n</pre> # Datos url='https://drive.google.com/file/d/1zqoLNbuysK5Idrb9--DFtCoxkapruSDc/view?usp=drive_link' url='https://drive.google.com/uc?id=' + url.split('/')[-2]  datos = pd.read_csv(url, sep=\",\") datos = datos.drop(columns = datos.columns[0]) datos['fat'] = datos['fat'].astype(float) datos.head() Out[23]: V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20 V21 V22 V23 V24 V25 V26 V27 V28 V29 V30 V31 V32 V33 V34 V35 V36 V37 V38 V39 V40 V41 V42 V43 V44 V45 V46 V47 V48 V49 V50 V51 V52 V53 V54 V55 V56 V57 V58 V59 V60 V61 V62 V63 V64 V65 V66 V67 V68 V69 V70 V71 V72 V73 V74 V75 V76 V77 V78 V79 V80 V81 V82 V83 V84 V85 V86 V87 V88 V89 V90 V91 V92 V93 V94 V95 V96 V97 V98 V99 V100 fat 0 2.61776 2.61814 2.61859 2.61912 2.61981 2.62071 2.62186 2.62334 2.62511 2.62722 2.62964 2.63245 2.63565 2.63933 2.64353 2.64825 2.65350 2.65937 2.66585 2.67281 2.68008 2.68733 2.69427 2.70073 2.70684 2.71281 2.71914 2.72628 2.73462 2.74416 2.75466 2.76568 2.77679 2.78790 2.79949 2.81225 2.82706 2.84356 2.86106 2.87857 2.89497 2.90924 2.92085 2.93015 2.93846 2.94771 2.96019 2.97831 3.00306 3.03506 3.07428 3.11963 3.16868 3.21771 3.26254 3.29988 3.32847 3.34899 3.36342 3.37379 3.38152 3.38741 3.39164 3.39418 3.39490 3.39366 3.39045 3.38541 3.37869 3.37041 3.36073 3.34979 3.33769 3.32443 3.31013 3.29487 3.27891 3.26232 3.24542 3.22828 3.21080 3.19287 3.17433 3.15503 3.13475 3.11339 3.09116 3.06850 3.04596 3.02393 3.00247 2.98145 2.96072 2.94013 2.91978 2.89966 2.87964 2.85960 2.83940 2.81920 22.5 1 2.83454 2.83871 2.84283 2.84705 2.85138 2.85587 2.86060 2.86566 2.87093 2.87661 2.88264 2.88898 2.89577 2.90308 2.91097 2.91953 2.92873 2.93863 2.94929 2.96072 2.97272 2.98493 2.99690 3.00833 3.01920 3.02990 3.04101 3.05345 3.06777 3.08416 3.10221 3.12106 3.13983 3.15810 3.17623 3.19519 3.21584 3.23747 3.25889 3.27835 3.29384 3.30362 3.30681 3.30393 3.29700 3.28925 3.28409 3.28505 3.29326 3.30923 3.33267 3.36251 3.39661 3.43188 3.46492 3.49295 3.51458 3.53004 3.54067 3.54797 3.55306 3.55675 3.55921 3.56045 3.56034 3.55876 3.55571 3.55132 3.54585 3.53950 3.53235 3.52442 3.51583 3.50668 3.49700 3.48683 3.47626 3.46552 3.45501 3.44481 3.43477 3.42465 3.41419 3.40303 3.39082 3.37731 3.36265 3.34745 3.33245 3.31818 3.30473 3.29186 3.27921 3.26655 3.25369 3.24045 3.22659 3.21181 3.19600 3.17942 40.1 2 2.58284 2.58458 2.58629 2.58808 2.58996 2.59192 2.59401 2.59627 2.59873 2.60131 2.60414 2.60714 2.61029 2.61361 2.61714 2.62089 2.62486 2.62909 2.63361 2.63835 2.64330 2.64838 2.65354 2.65870 2.66375 2.66880 2.67383 2.67892 2.68411 2.68937 2.69470 2.70012 2.70563 2.71141 2.71775 2.72490 2.73344 2.74327 2.75433 2.76642 2.77931 2.79272 2.80649 2.82064 2.83541 2.85121 2.86872 2.88905 2.91289 2.94088 2.97325 3.00946 3.04780 3.08554 3.11947 3.14696 3.16677 3.17938 3.18631 3.18924 3.18950 3.18801 3.18498 3.18039 3.17411 3.16611 3.15641 3.14512 3.13241 3.11843 3.10329 3.08714 3.07014 3.05237 3.03393 3.01504 2.99569 2.97612 2.95642 2.93660 2.91667 2.89655 2.87622 2.85563 2.83474 2.81361 2.79235 2.77113 2.75015 2.72956 2.70934 2.68951 2.67009 2.65112 2.63262 2.61461 2.59718 2.58034 2.56404 2.54816 8.4 3 2.82286 2.82460 2.82630 2.82814 2.83001 2.83192 2.83392 2.83606 2.83842 2.84097 2.84374 2.84664 2.84975 2.85307 2.85661 2.86038 2.86437 2.86860 2.87308 2.87789 2.88301 2.88832 2.89374 2.89917 2.90457 2.90991 2.91521 2.92043 2.92565 2.93082 2.93604 2.94128 2.94658 2.95202 2.95777 2.96419 2.97159 2.98045 2.99090 3.00284 3.01611 3.03048 3.04579 3.06194 3.07889 3.09686 3.11629 3.13775 3.16217 3.19068 3.22376 3.26172 3.30379 3.34793 3.39093 3.42920 3.45998 3.48227 3.49687 3.50558 3.51026 3.51221 3.51215 3.51036 3.50682 3.50140 3.49398 3.48457 3.47333 3.46041 3.44595 3.43005 3.41285 3.39450 3.37511 3.35482 3.33376 3.31204 3.28986 3.26730 3.24442 3.22117 3.19757 3.17357 3.14915 3.12429 3.09908 3.07366 3.04825 3.02308 2.99820 2.97367 2.94951 2.92576 2.90251 2.87988 2.85794 2.83672 2.81617 2.79622 5.9 4 2.78813 2.78989 2.79167 2.79350 2.79538 2.79746 2.79984 2.80254 2.80553 2.80890 2.81272 2.81704 2.82184 2.82710 2.83294 2.83945 2.84664 2.85458 2.86331 2.87280 2.88291 2.89335 2.90374 2.91371 2.92305 2.93187 2.94060 2.94986 2.96035 2.97241 2.98606 3.00097 3.01652 3.03220 3.04793 3.06413 3.08153 3.10078 3.12185 3.14371 3.16510 3.18470 3.20140 3.21477 3.22544 3.23505 3.24586 3.26027 3.28063 3.30889 3.34543 3.39019 3.44198 3.49800 3.55407 3.60534 3.64789 3.68011 3.70272 3.71815 3.72863 3.73574 3.74059 3.74357 3.74453 3.74336 3.73991 3.73418 3.72638 3.71676 3.70553 3.69289 3.67900 3.66396 3.64785 3.63085 3.61305 3.59463 3.57582 3.55695 3.53796 3.51880 3.49936 3.47938 3.45869 3.43711 3.41458 3.39129 3.36772 3.34450 3.32201 3.30025 3.27907 3.25831 3.23784 3.21765 3.19766 3.17770 3.15770 3.13753 25.5 <p>El set de datos contiene 101 columnas. Las 100 primeras, nombradas como  $V_1,...,V_{100}$  recogen el valor de absorbancia para cada una de las 100 longitudes de onda analizadas (predictores), y la columna fat el contenido en grasa medido por t\u00e9cnicas qu\u00edmicas (variable respuesta).</p> <p>Muchas de las variables est\u00e1n altamente correlacionadas (correlaci\u00f3n absoluta &gt; 0.8), lo que supone un problema a la hora de emplear modelos de regresi\u00f3n lineal.</p> In\u00a0[24]: Copied! <pre># Correlaci\u00f3n entre columnas num\u00e9ricas\ndef tidy_corr_matrix(corr_mat):\n    '''\n    Funci\u00f3n para convertir una matriz de correlaci\u00f3n de pandas en formato tidy\n    '''\n    corr_mat = corr_mat.stack().reset_index()\n    corr_mat.columns = ['variable_1','variable_2','r']\n    corr_mat = corr_mat.loc[corr_mat['variable_1'] != corr_mat['variable_2'], :]\n    corr_mat['abs_r'] = np.abs(corr_mat['r'])\n    corr_mat = corr_mat.sort_values('abs_r', ascending=False)\n    \n    return(corr_mat)\n\ncorr_matrix = datos.select_dtypes(include=['float64', 'int']) \\\n              .corr(method='pearson')\ndisplay(tidy_corr_matrix(corr_matrix).head(5))\n</pre> # Correlaci\u00f3n entre columnas num\u00e9ricas def tidy_corr_matrix(corr_mat):     '''     Funci\u00f3n para convertir una matriz de correlaci\u00f3n de pandas en formato tidy     '''     corr_mat = corr_mat.stack().reset_index()     corr_mat.columns = ['variable_1','variable_2','r']     corr_mat = corr_mat.loc[corr_mat['variable_1'] != corr_mat['variable_2'], :]     corr_mat['abs_r'] = np.abs(corr_mat['r'])     corr_mat = corr_mat.sort_values('abs_r', ascending=False)          return(corr_mat)  corr_matrix = datos.select_dtypes(include=['float64', 'int']) \\               .corr(method='pearson') display(tidy_corr_matrix(corr_matrix).head(5)) variable_1 variable_2 r abs_r 1019 V11 V10 0.999996 0.999996 919 V10 V11 0.999996 0.999996 1021 V11 V12 0.999996 0.999996 1121 V12 V11 0.999996 0.999996 917 V10 V9 0.999996 0.999996 <p>Se procede aplicar el modelo de regresi\u00f3n lineal .</p> In\u00a0[25]: Copied! <pre># Divisi\u00f3n de los datos en train y test\nX = datos.drop(columns='fat')\ny = datos['fat']\n\nX_train, X_test, y_train, y_test = train_test_split(\n                                        X,\n                                        y.values,\n                                        train_size   = 0.7,\n                                        random_state = 1234,\n                                        shuffle      = True\n                                    )\n</pre> # Divisi\u00f3n de los datos en train y test X = datos.drop(columns='fat') y = datos['fat']  X_train, X_test, y_train, y_test = train_test_split(                                         X,                                         y.values,                                         train_size   = 0.7,                                         random_state = 1234,                                         shuffle      = True                                     ) In\u00a0[26]: Copied! <pre># Creaci\u00f3n y entrenamiento del modelo\nmodelo = LinearRegression()\nmodelo.fit(X = X_train, y = y_train)\n</pre> # Creaci\u00f3n y entrenamiento del modelo modelo = LinearRegression() modelo.fit(X = X_train, y = y_train) Out[26]: <pre>LinearRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression<pre>LinearRegression()</pre> In\u00a0[27]: Copied! <pre># Predicciones test\npredicciones = modelo.predict(X=X_test)\npredicciones = predicciones.flatten()\n\n# Error de test del modelo \ndf_pred = pd.DataFrame({\n    'y':y_test,\n    'yhat':predicciones\n})\n\ndf_summary = regression_metrics(df_pred)\ndf_summary\n</pre> # Predicciones test predicciones = modelo.predict(X=X_test) predicciones = predicciones.flatten()  # Error de test del modelo  df_pred = pd.DataFrame({     'y':y_test,     'yhat':predicciones })  df_summary = regression_metrics(df_pred) df_summary Out[27]: mae mse rmse mape smape 0 2.0904 14.743 3.8397 16.1573 0.2782 <p>Ahora se ocuparan los modelos de reducci\u00f3n de dimensionalidad para entrenar el modelo de regresi\u00f3n lineal. Para ello se ocuparan los distintos algoritmos mencionados. Lo primero es crear una funci\u00f3n que pueda realizar esta tarea de manera autom\u00e1tica.</p> In\u00a0[28]: Copied! <pre># funcion de reduccion de dimensionalidad y aplicacion de la regresion lineal\n\ndef dr_pipeline(df, model_dr):\n    \n    # datos\n    X = df.drop(columns='fat')\n    y = df['fat']\n    \n    # reduccion de la dimensionalidad\n    embedding = model_dr\n    X = embedding.fit_transform(X)\n    \n\n    X_train, X_test, y_train, y_test = train_test_split(\n                                        X,\n                                        y,\n                                        train_size   = 0.7,\n                                        random_state = 1234,\n                                        shuffle      = True\n                                    )\n    \n    # Creaci\u00f3n y entrenamiento del modelo\n    modelo = LinearRegression()\n    modelo.fit(X = X_train, y = y_train)\n\n    \n    # Predicciones test\n    predicciones = modelo.predict(X=X_test)\n    predicciones = predicciones.flatten()\n\n    # Error de test del modelo \n    df_pred = pd.DataFrame({\n        'y':y_test,\n        'yhat':predicciones\n    })\n\n    df_summary = regression_metrics(df_pred)\n    \n    return df_summary\n</pre> # funcion de reduccion de dimensionalidad y aplicacion de la regresion lineal  def dr_pipeline(df, model_dr):          # datos     X = df.drop(columns='fat')     y = df['fat']          # reduccion de la dimensionalidad     embedding = model_dr     X = embedding.fit_transform(X)           X_train, X_test, y_train, y_test = train_test_split(                                         X,                                         y,                                         train_size   = 0.7,                                         random_state = 1234,                                         shuffle      = True                                     )          # Creaci\u00f3n y entrenamiento del modelo     modelo = LinearRegression()     modelo.fit(X = X_train, y = y_train)           # Predicciones test     predicciones = modelo.predict(X=X_test)     predicciones = predicciones.flatten()      # Error de test del modelo      df_pred = pd.DataFrame({         'y':y_test,         'yhat':predicciones     })      df_summary = regression_metrics(df_pred)          return df_summary <p>Enfoque: Algebra lineal</p> In\u00a0[29]: Copied! <pre>modelos_algebra_lineal = [\n    ('PCA',PCA(n_components=5)),\n    ('SVD',TruncatedSVD(n_components=5)),\n    ('NMF',NMF(n_components=5))\n]\n\nnames = [x[0] for x in modelos_algebra_lineal]\nresults = [dr_pipeline(datos,x[1]) for x in modelos_algebra_lineal]\n</pre> modelos_algebra_lineal = [     ('PCA',PCA(n_components=5)),     ('SVD',TruncatedSVD(n_components=5)),     ('NMF',NMF(n_components=5)) ]  names = [x[0] for x in modelos_algebra_lineal] results = [dr_pipeline(datos,x[1]) for x in modelos_algebra_lineal] In\u00a0[30]: Copied! <pre>df_algebra_lineal = pd.concat(results).reset_index(drop=True)\ndf_algebra_lineal['metodo'] =names\ndf_algebra_lineal\n</pre> df_algebra_lineal = pd.concat(results).reset_index(drop=True) df_algebra_lineal['metodo'] =names df_algebra_lineal Out[30]: mae mse rmse mape smape metodo 0 2.8050 12.2999 3.5071 28.6888 0.4459 PCA 1 2.6344 11.3195 3.3645 26.0475 0.4133 SVD 2 6.3295 75.1477 8.6688 68.0810 0.8101 NMF <p>Enfoque: Manifold Learning</p> In\u00a0[31]: Copied! <pre>modelos_manifold= [\n    ('Isomap',Isomap(n_components=5)),\n    ('LocallyLinearEmbedding', LocallyLinearEmbedding(n_components=5)),\n    ('MDS',  MDS(n_components=5)),\n    ('SpectralEmbedding', SpectralEmbedding(n_components=5)),\n    ('TSNE', TSNE(n_components=2)),\n]\n\nnames = [x[0] for x in modelos_manifold]\nresults = [dr_pipeline(datos,x[1]) for x in modelos_manifold]\n\n\ndf_manifold = pd.concat(results).reset_index(drop=True)\ndf_manifold['metodo'] =names\ndf_manifold\n</pre> modelos_manifold= [     ('Isomap',Isomap(n_components=5)),     ('LocallyLinearEmbedding', LocallyLinearEmbedding(n_components=5)),     ('MDS',  MDS(n_components=5)),     ('SpectralEmbedding', SpectralEmbedding(n_components=5)),     ('TSNE', TSNE(n_components=2)), ]  names = [x[0] for x in modelos_manifold] results = [dr_pipeline(datos,x[1]) for x in modelos_manifold]   df_manifold = pd.concat(results).reset_index(drop=True) df_manifold['metodo'] =names df_manifold <pre>C:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\db-connectors-RsEieBu8-py3.8\\lib\\site-packages\\sklearn\\manifold\\_mds.py:299: FutureWarning: The default value of `normalized_stress` will change to `'auto'` in version 1.4. To suppress this warning, manually set the value of `normalized_stress`.\n  warnings.warn(\n</pre> Out[31]: mae mse rmse mape smape metodo 0 9.4021 133.7433 11.5647 99.1430 0.9957 Isomap 1 9.1331 130.6859 11.4318 88.6049 0.9396 LocallyLinearEmbedding 2 7.1867 76.1355 8.7256 83.9630 0.9128 MDS 3 8.9529 130.9545 11.4435 83.3004 0.9089 SpectralEmbedding 4 8.8753 128.9746 11.3567 88.1109 0.9368 TSNE In\u00a0[32]: Copied! <pre>df_manifold.sort_values([\"mae\",\"mape\"])\n</pre> df_manifold.sort_values([\"mae\",\"mape\"]) Out[32]: mae mse rmse mape smape metodo 2 7.1867 76.1355 8.7256 83.9630 0.9128 MDS 4 8.8753 128.9746 11.3567 88.1109 0.9368 TSNE 3 8.9529 130.9545 11.4435 83.3004 0.9089 SpectralEmbedding 1 9.1331 130.6859 11.4318 88.6049 0.9396 LocallyLinearEmbedding 0 9.4021 133.7433 11.5647 99.1430 0.9957 Isomap <p>En este caso en particular, funciona de mejor forma aplicar los m\u00e9todos de descomposici\u00f3n del Algebra Lineal en relaci\u00f3n de los m\u00e9todos de Manifold Learning. La ense\u00f1anza que se lleva de esto que, dependiendo del volumen de datos que se trabaje, la capidad de c\u00f3mputo y las habilidades de programaci\u00f3n suficiente, se pueden probar y automatizar varios de estos m\u00e9todos. Por supuesto, quedar\u00e1 como responsabilidad del programador buscar el criterio para poder seleccionar el mejor m\u00e9todo (dependiendo del caso en estudio).</p>"},{"location":"lectures/machine_learning/ns_02/#no-supervisado-ii","title":"No supervisado II\u00b6","text":""},{"location":"lectures/machine_learning/ns_02/#reduccion-de-dimensionalidad","title":"Reducci\u00f3n de dimensionalidad\u00b6","text":"<p>En aprendizaje autom\u00e1tico y estad\u00edsticas reducci\u00f3n de dimensionalidad  es el proceso de reducci\u00f3n del n\u00famero de variables aleatorias que se trate, y se puede dividir en selecci\u00f3n de funci\u00f3n y extracci\u00f3n de funci\u00f3n</p> <p>Sin embargo, se puede utilizar como un paso de preprocesamiento de transformaci\u00f3n de datos para algoritmos de aprendizaje autom\u00e1tico en conjuntos de datos de modelado predictivo de clasificaci\u00f3n y regresi\u00f3n con algoritmos de aprendizaje supervisado.</p> <p>Hay muchos algoritmos de reducci\u00f3n de dimensionalidad entre los que elegir y no existe el mejor algoritmo para todos los casos. En cambio, es una buena idea explorar una variedad de algoritmos de reducci\u00f3n de dimensionalidad y diferentes configuraciones para cada algoritmo.</p>"},{"location":"lectures/machine_learning/ns_02/#algoritmos-de-reduccion-de-la-dimensionalidad","title":"Algoritmos de reducci\u00f3n de la dimensionalidad\u00b6","text":"<p>Hay muchos algoritmos que pueden ser usados para la reducci\u00f3n de la dimensionalidad.</p> <p>Dos clases principales de m\u00e9todos son los que se extraen del \u00e1lgebra lineal y los que se extraen del aprendizaje m\u00faltiple.</p>"},{"location":"lectures/machine_learning/ns_02/#metodos-de-algebra-lineal","title":"M\u00e9todos de \u00e1lgebra lineal\u00b6","text":"<p>Los m\u00e9todos de factorizaci\u00f3n matricial extra\u00eddos del campo del \u00e1lgebra lineal pueden utilizarse para la dimensionalidad. Algunos de los m\u00e9todos m\u00e1s populares incluyen:</p> <ul> <li>An\u00e1lisis de los componentes principales</li> <li>Descomposici\u00f3n del valor singular</li> <li>Factorizaci\u00f3n de matriz no negativa</li> </ul>"},{"location":"lectures/machine_learning/ns_02/#multiples-metodos-de-aprendizaje","title":"M\u00faltiples m\u00e9todos de aprendizaje\u00b6","text":"<p>Los m\u00faltiples m\u00e9todos de aprendizaje buscan una proyecci\u00f3n de dimensiones inferiores de alta entrada dimensional que capte las propiedades salientes de los datos de entrada.</p> <p>Algunos de los m\u00e9todos m\u00e1s populares incluyen:</p> <ul> <li>Isomap Embedding</li> <li>Locally Linear Embedding</li> <li>Multidimensional Scaling</li> <li>Spectral Embedding</li> <li>t-distributed Stochastic Neighbor Embedding (t-sne)</li> </ul> <p>Cada algoritmo ofrece un enfoque diferente para el desaf\u00edo de descubrir las relaciones naturales en los datos de dimensiones inferiores.</p> <p>No hay un mejor algoritmo de reducci\u00f3n de la dimensionalidad, y no hay una manera f\u00e1cil de encontrar el mejor algoritmo para sus datos sin usar experimentos controlados.</p> <p>Debido a la importancia que se tiene en el mundo del machine lerning, se dar\u00e1 un explicaci\u00f3n formal del m\u00e9todo de PCA y luego se dar\u00e1 una breve rese\u00f1a de los dem\u00e1s m\u00e9todos.</p>"},{"location":"lectures/machine_learning/ns_02/#pca","title":"PCA\u00b6","text":"<p>El an\u00e1lisis de componentes principales (Principal Component Analysis PCA) es un m\u00e9todo de reducci\u00f3n de dimensionalidad que permite simplificar la complejidad de espacios con m\u00faltiples dimensiones a la vez que conserva su informaci\u00f3n.</p> <p>Sup\u00f3ngase que existe una muestra con  $n$  individuos cada uno con  $p$  variables ( $X_1$,...,$X_p$), es decir, el espacio muestral tiene  $p$  dimensiones. PCA permite encontrar un n\u00famero de factores subyacentes  ($z&lt;p$)  que explican aproximadamente lo mismo que las  $p$  variables originales. Donde antes se necesitaban  $p$  valores para caracterizar a cada individuo, ahora bastan  $z$  valores. Cada una de estas  $z$  nuevas variables recibe el nombre de componente principal.</p> <p></p> <p>El m\u00e9todo de PCA permite por lo tanto \"condensar\" la informaci\u00f3n aportada por m\u00faltiples variables en solo unas pocas componentes. Aun as\u00ed, no hay que olvidar que sigue siendo necesario disponer del valor de las variables originales para calcular las componentes. Dos de las principales aplicaciones del PCA son la visualizaci\u00f3n y el preprocesado de predictores previo ajuste de modelos supervisados.</p>"},{"location":"lectures/machine_learning/ns_02/#interpretacion-geometrica-de-las-componentes-principales","title":"Interpretaci\u00f3n geom\u00e9trica de las componentes principales\u00b6","text":"<p>Una forma intuitiva de entender el proceso de PCA es interpretar las componentes principales desde un punto de vista geom\u00e9trico. Sup\u00f3ngase un conjunto de observaciones para las que se dispone de dos variables ( $X_1$, $X_2$ ). El vector que define la primera componente principal ($Z_1$ ) sigue la direcci\u00f3n en la que las observaciones tienen m\u00e1s varianza (l\u00ednea roja). La proyecci\u00f3n de cada observaci\u00f3n sobre esa direcci\u00f3n equivale al valor de la primera componente para dicha observaci\u00f3n (principal component score,  $z_{i1}$ ).</p> <p></p> <p>La segunda componente ( $Z_2$ ) sigue la segunda direcci\u00f3n en la que los datos muestran mayor varianza y que no est\u00e1 correlacionada con la primera componente. La condici\u00f3n de no correlaci\u00f3n entre componentes principales equivale a decir que sus direcciones son perpendiculares/ortogonales.</p> <p></p>"},{"location":"lectures/machine_learning/ns_02/#calculo-de-las-componentes-principales","title":"C\u00e1lculo de las componentes principales\u00b6","text":"<p>Cada componente principal ( $Z_i$ ) se obtiene por combinaci\u00f3n lineal de las variables originales. Se pueden entender como nuevas variables obtenidas al combinar de una determinada forma las variables originales. La primera componente principal de un grupo de variables ( $X_1,...,X_p$ ) es la combinaci\u00f3n lineal normalizada de dichas variables que tiene mayor varianza:</p> <p>$$ Z_1 = \\phi_{11}X_1 + ... + \\phi_{p1}X_p$$</p> <p>Que la combinaci\u00f3n lineal sea normalizada implica que:</p> <p>$$\\sum_{j=1}^p \\phi^2_{j1} = 1$$</p> <p>Los t\u00e9rminos  $\\phi_{11},...,\\phi_{p1}$  reciben en el nombre de loadings y son los que definen las componentes. Por ejemplo,  $\\phi_{11}$  es el loading de la variable  $X_1$  de la primera componente principal. Los loadings pueden interpretarse como el peso/importancia que tiene cada variable en cada componente y, por lo tanto, ayudan a conocer que tipo de informaci\u00f3n recoge cada una de las componentes.</p> <p>Dado un set de datos  $X$  con $n$ observaciones y $p$ variables, el proceso a seguir para calcular la primera componente principal es:</p> <ul> <li><p>Centrar las variables: se resta a cada valor la media de la variable a la que pertenece. Con esto se consigue que todas las variables tengan media cero.</p> </li> <li><p>Se resuelve un problema de optimizaci\u00f3n para encontrar el valor de los loadings con los que se maximiza la varianza. Una forma de resolver esta optimizaci\u00f3n es mediante el c\u00e1lculo de eigenvector-eigenvalue de la matriz de covarianzas.</p> </li> </ul> <p>Una vez calculada la primera componente ( $Z_1$ ), se calcula la segunda ( $Z_2$ ) repitiendo el mismo proceso pero a\u00f1adiendo la condici\u00f3n de que la combinaci\u00f3n lineal no pude estar correlacionada con la primera componente. Esto equivale a decir que  $Z_1$  y  $Z_2$  tienen que ser perpendiculares. EL proceso se repite de forma iterativa hasta calcular todas las posibles componentes (min($n-1, p$)) o hasta que se decida detener el proceso. El orden de importancia de las componentes viene dado por la magnitud del eigenvalue asociado a cada eigenvector.</p>"},{"location":"lectures/machine_learning/ns_02/#caracteristicas-del-pca","title":"Caracter\u00edsticas del PCA\u00b6","text":"<ul> <li>Escalado de las variables: El proceso de PCA identifica las direcciones con mayor varianza.</li> <li>Reproducibilidad de las componentes: El proceso de PCA est\u00e1ndar es determinista, genera siempre las mismas componentes principales, es decir, el valor de los loadings resultantes es el mismo.</li> <li>Influencia de outliers: Al trabajar con varianzas, el m\u00e9todo PCA es muy sensible a outliers, por lo que es recomendable estudiar si los hay. La detecci\u00f3n de valores at\u00edpicos con respecto a una determinada dimensi\u00f3n es algo relativamente sencillo de hacer mediante comprobaciones gr\u00e1ficas.</li> </ul>"},{"location":"lectures/machine_learning/ns_02/#proporcion-de-varianza-explicada","title":"Proporci\u00f3n de varianza explicada\u00b6","text":"<p>Una de las preguntas m\u00e1s frecuentes que surge tras realizar un PCA es: \u00bfCu\u00e1nta informaci\u00f3n presente en el set de datos original se pierde al proyectar las observaciones en un espacio de menor dimensi\u00f3n? o lo que es lo mismo \u00bfCuanta informaci\u00f3n es capaz de capturar cada una de las componentes principales obtenidas? Para contestar a estas preguntas se recurre a la proporci\u00f3n de varianza explicada por cada componente principal.</p> <p>Asumiendo que las variables se han normalizado para tener media cero, la varianza total presente en el set de datos se define como</p> <p>$$\\sum_{j=1}^p Var(X_j) = \\dfrac{1}{n}\\sum_{j=1}^p\\sum_{i=1}^nx_{ij}^2$$</p> <p>y la varianza explicada por la componente m es</p> <p>$$\\dfrac{1}{n}\\sum_{i=1}^n z_{im}^2 = \\dfrac{1}{n}\\sum_{i=1}^n (\\sum_{j=1}^p \\phi_{jm}x_{ij})^2$$</p> <p>Por lo tanto, la proporci\u00f3n de varianza explicada por la componente m viene dada por el ratio</p> <p>$$ \\dfrac{\\sum_{i=1}^n (\\sum_{j=1}^p \\phi_{jm}x_{ij})^2}{\\sum_{j=1}^p\\sum_{i=1}^nx_{ij}^2}$$</p> <p>Tanto la proporci\u00f3n de varianza explicada, como la proporci\u00f3n de varianza explicada acumulada, son dos valores de gran utilidad a la hora de decidir el n\u00famero de componentes principales a utilizar en los an\u00e1lisis posteriores. Si se calculan todas las componentes principales de un set de datos, entonces, aunque transformada, se est\u00e1 almacenando toda la informaci\u00f3n presente en los datos originales. El sumatorio de la proporci\u00f3n de varianza explicada acumulada de todas las componentes es siempre 1.</p>"},{"location":"lectures/machine_learning/ns_02/#numero-optimo-de-componentes-principales","title":"N\u00famero \u00f3ptimo de componentes principales\u00b6","text":"<p>Por lo general, dada una matriz de datos de dimensiones $n \\times p$, el n\u00famero de componentes principales que se pueden calcular es como m\u00e1ximo de $n-1$ o $p$ (el menor de los dos valores es el limitante). Sin embargo, siendo el objetivo del PCA reducir la dimensionalidad, suelen ser de inter\u00e9s utilizar el n\u00famero m\u00ednimo de componentes que resultan suficientes para explicar los datos. No existe una respuesta o m\u00e9todo \u00fanico que permita identificar cual es el n\u00famero \u00f3ptimo de componentes principales a utilizar. Una forma de proceder muy extendida consiste en evaluar la proporci\u00f3n de varianza explicada acumulada y seleccionar el n\u00famero de componentes m\u00ednimo a partir del cual el incremento deja de ser sustancial.</p> <p></p>"},{"location":"lectures/machine_learning/ns_02/#aplicacion","title":"Aplicaci\u00f3n\u00b6","text":"<p>El m\u00e9todo Principal Components Regression PCR consiste en ajustar un modelo de regresi\u00f3n lineal por m\u00ednimos cuadrados empleando como predictores las componentes generadas a partir de un Principal Component Analysis (PCA). De esta forma, con un n\u00famero reducido de componentes se puede explicar la mayor parte de la varianza de los datos.</p> <p>En los estudios observacionales, es frecuente disponer de un n\u00famero elevado de variables que se pueden emplear como predictores, sin embargo, esto no implica necesariamente que se disponga de mucha informaci\u00f3n. Si las variables est\u00e1n correlacionadas entre ellas, la informaci\u00f3n que aportan es redundante y adem\u00e1s, se incumple la condici\u00f3n de no colinealidad necesaria en la regresi\u00f3n por m\u00ednimos cuadrados. Dado que el PCA es \u00fatil eliminando informaci\u00f3n redundante, si se emplean como predictores las componentes principales, se puede mejorar el modelo de regresi\u00f3n. Es importante tener en cuenta que, si bien el Principal Components Regression reduce el n\u00famero de predictores del modelo, no se puede considerar como un m\u00e9todo de selecci\u00f3n de variables ya que todas ellas se necesitan para el c\u00e1lculo de las componentes. La identificaci\u00f3n del n\u00famero \u00f3ptimo de componentes principales que se emplean como predictores en PCR puede identificarse por validaci\u00f3n cruzada.</p> <p>Datos: El set de datos <code>USArrests</code> contiene el porcentaje de asaltos (Assault), asesinatos (Murder) y secuestros (Rape) por cada 100,000 habitantes para cada uno de los 50 estados de USA (1973). Adem\u00e1s, tambi\u00e9n incluye el porcentaje de la poblaci\u00f3n de cada estado que vive en zonas rurales (UrbanPoP).</p>"},{"location":"lectures/machine_learning/ns_02/#t-distributed-stochastic-neighbor-embedding-t-sne","title":"t-Distributed Stochastic Neighbor Embedding (t-SNE)\u00b6","text":"<p>t-Distributed Stochastic Neighbor Embedding (t-SNE)  es una t\u00e9cnica no lineal no supervisada utilizada principalmente para la exploraci\u00f3n de datos y la visualizaci\u00f3n de datos de alta dimensi\u00f3n.</p> <p>En t\u00e9rminos m\u00e1s simples, tSNE le da una sensaci\u00f3n o intuici\u00f3n de c\u00f3mo se organizan los datos en un espacio de alta dimensi\u00f3n. Fue desarrollado por Laurens van der Maatens y Geoffrey Hinton en 2008.</p>"},{"location":"lectures/machine_learning/ns_02/#comparando-con-pca","title":"Comparando con PCA\u00b6","text":"<p>Si est\u00e1 familiarizado con An\u00e1lisis de componentes principales (PCA), entonces como yo , probablemente se est\u00e9 preguntando la diferencia entre PCA y tSNE.</p> <p>Lo primero a tener en cuenta es que PCA se desarroll\u00f3 en 1933, mientras que tSNE se desarroll\u00f3 en 2008. Mucho ha cambiado en el mundo de la ciencia de datos desde 1933, principalmente en el \u00e1mbito del c\u00e1lculo y el tama\u00f1o de los datos.</p> <p>En segundo lugar, PCA es una t\u00e9cnica de reducci\u00f3n de dimensi\u00f3n lineal que busca maximizar la varianza y preserva las distancias pares grandes. En otras palabras, las cosas que son diferentes terminan muy separadas. Esto puede conducir a una visualizaci\u00f3n deficiente, especialmente cuando se trata de estructuras distribuidoras no lineales. Piense en una estructura m\u00faltiple como cualquier forma geom\u00e9trica como: cilindro, bola, curva, etc.</p> <p>tSNE difiere de PCA al preservar solo peque\u00f1as distancias por pares o similitudes locales, mientras que PCA se preocupa por preservar distancias pares grandes para maximizar la varianza.</p> <p>Laurens ilustra bastante bien el enfoque PCA y tSNE utilizando el conjunto de datos Swiss Roll en la Figura 1 [1].</p> <p>Puede ver que debido a la no linealidad de este conjunto de datos de juguete (m\u00faltiple) y la preservaci\u00f3n de grandes distancias, PCA conservar\u00eda incorrectamente la estructura de los datos.</p> <p></p> <p>Figura 1 \u2013 Dataset de rollo suizo. Conservar la distancia peque\u00f1a con tSNE (l\u00ednea continua) frente a la maximizaci\u00f3n de la variaci\u00f3n PCA [1]</p>"},{"location":"lectures/machine_learning/ns_02/#explicacion","title":"Explicaci\u00f3n\u00b6","text":"<p>Ahora que sabemos por qu\u00e9 podr\u00edamos usar tSNE sobre PCA, analicemos c\u00f3mo funciona tSNE. El algoritmo tSNE calcula una medida de similitud entre pares de instancias en el espacio de alta dimensi\u00f3n y en el espacio de baja dimensi\u00f3n. Luego trata de optimizar estas dos medidas de similitud usando una funci\u00f3n de costo. Vamos a dividirlo en 3 pasos b\u00e1sicos.</p> <ol> <li>Paso 1, mide similitudes entre puntos en el espacio de alta dimensi\u00f3n. Piense en un conjunto de puntos de datos dispersos en un espacio 2D (Figura 2).</li> </ol> <p>Para cada punto de datos (xi) centraremos una distribuci\u00f3n Gaussiana sobre ese punto. Luego medimos la densidad de todos los puntos (xj) bajo esa distribuci\u00f3n Gaussiana. Luego renormalize para todos los puntos.</p> <p>Esto nos da un conjunto de probabilidades (Pij) para todos los puntos. Esas probabilidades son proporcionales a las similitudes.</p> <p>Todo lo que eso significa es que si los puntos de datos x1 y x2 tienen valores iguales bajo este c\u00edrculo gaussiano, entonces sus proporciones y similitudes son iguales y, por lo tanto, tienes similitudes locales en la estructura de este espacio de alta dimensi\u00f3n.</p> <p>La distribuci\u00f3n gaussiana o el c\u00edrculo se pueden manipular usando lo que se llama perplejidad, que influye en la varianza de la distribuci\u00f3n (tama\u00f1o del c\u00edrculo) y esencialmente en el n\u00famero de vecinos m\u00e1s cercanos. El rango normal para la perplejidad est\u00e1 entre 5 y 50 [2].</p> <p></p> <p>Figura 2 \u2013 Medici\u00f3n de similitudes por pares en el espacio de alta dimensi\u00f3n</p>"},{"location":"lectures/machine_learning/ns_02/#aplicacion","title":"Aplicaci\u00f3n\u00b6","text":"<p>Laurens van der Maaten menciona el uso de tSNE en \u00e1reas como investigaci\u00f3n del clima, seguridad inform\u00e1tica, bioinform\u00e1tica, investigaci\u00f3n del c\u00e1ncer, etc. tSNE podr\u00eda usarse en datos de alta dimensi\u00f3n y luego el resultado de esas dimensiones se convierte en insumos para alg\u00fan otro modelo de clasificaci\u00f3n .</p> <p>Adem\u00e1s, tSNE podr\u00eda usarse para investigar, aprender o evaluar la segmentaci\u00f3n. Muchas veces seleccionamos la cantidad de segmentos antes del modelado o iteramos despu\u00e9s de los resultados. tSNE a menudo puede mostrar una separaci\u00f3n clara en los datos.</p> <p>Esto se puede usar antes de usar su modelo de segmentaci\u00f3n para seleccionar un n\u00famero de cl\u00faster o despu\u00e9s para evaluar si sus segmentos realmente se mantienen. tSNE, sin embargo, no es un enfoque de agrupamiento, ya que no conserva las entradas como PCA y los valores a menudo pueden cambiar entre ejecuciones, por lo que es pura exploraci\u00f3n.</p> <p>A continuaci\u00f3n se procede a comparar de manera  visual los algoritmos de PCA y tSNE en el conjunto de datos <code>Digits</code> .</p> <p>Datos: El conjunto de datos contiene im\u00e1genes de d\u00edgitos escritos a mano: 10 clases donde cada clase se refiere a un d\u00edgito. Los programas de preprocesamiento puestos a disposici\u00f3n por NIST se utilizaron para extraer mapas de bits normalizados de d\u00edgitos escritos a mano de un formulario preimpreso. De un total de 43 personas, 30 contribuyeron al conjunto de entrenamiento y diferentes 13 al conjunto de prueba. Los mapas de bits de 32x32 se dividen en bloques no superpuestos de 4x4 y se cuenta el n\u00famero de p\u00edxeles en cada bloque. Esto genera una matriz de entrada de 8x8 donde cada elemento es un n\u00famero entero en el rango 0..16. Esto reduce la dimensionalidad y da invariancia a peque\u00f1as distorsiones.</p>"},{"location":"lectures/machine_learning/ns_02/#otros-metodos-de-reduccion-de-dimensionalidad","title":"Otros m\u00e9todos de reducci\u00f3n de dimensionalidad\u00b6","text":"<p>Existen otro m\u00e9todos de reducci\u00f3n de dimencionalidad, a continuaci\u00f3n se deja una referencia con la descripci\u00f3n de cada uno de estos algoritmos.</p> <ul> <li>Descomposici\u00f3n del valor singular </li> <li>Non-Negative Matrix Factorization </li> <li>Isomap Embedding </li> <li>Locally Linear Embedding </li> <li>Multidimensional Scaling </li> <li>Spectral Embedding </li> </ul>"},{"location":"lectures/machine_learning/ns_02/#aplicacion","title":"Aplicaci\u00f3n\u00b6","text":"<p>En este ejemplo se quiere aprovechar las bondades de aplicar la reducci\u00f3n de dimensionalidad para ocupar un modelo de clasificaci\u00f3n (en este caso, el modelo de regresi\u00f3n log\u00edstica). Para ello se ocupar\u00e1 el conjunto de datos <code>meatspec.csv</code></p> <p>Datos: El departamento de calidad de una empresa de alimentaci\u00f3n se encarga de medir el contenido en grasa de la carne que comercializa. Este estudio se realiza mediante t\u00e9cnicas de anal\u00edtica qu\u00edmica, un proceso relativamente costoso en tiempo y recursos. Una alternativa que permitir\u00eda reducir costes y optimizar tiempo es emplear un espectrofot\u00f3metro (instrumento capaz de detectar la absorbancia que tiene un material a diferentes tipos de luz en funci\u00f3n de sus caracter\u00edsticas) e inferir el contenido en grasa a partir de sus medidas.</p> <p>Antes de dar por v\u00e1lida esta nueva t\u00e9cnica, la empresa necesita comprobar qu\u00e9 margen de error tiene respecto al an\u00e1lisis qu\u00edmico. Para ello, se mide el espectro de absorbancia a 100 longitudes de onda en 215 muestras de carne, cuyo contenido en grasa se obtiene tambi\u00e9n por an\u00e1lisis qu\u00edmico, y se entrena un modelo con el objetivo de predecir el contenido en grasa a partir de los valores dados por el espectrofot\u00f3metro.</p>"},{"location":"lectures/machine_learning/ns_02/#referencias","title":"Referencias\u00b6","text":"<ol> <li>In Depth: Principal Component Analysis</li> <li>Unsupervised dimensionality reduction</li> </ol>"},{"location":"lectures/machine_learning/over_01/","title":"Overfitting I","text":"<p>El overfitting ocurre cuando el algoritmo de machine learning captura el ruido de los datos. Intuitivamente, el overfitting ocurre cuando el modelo o el algoritmo se ajusta demasiado bien a los datos. Espec\u00edficamente, el sobreajuste ocurre si el modelo o algoritmo muestra un sesgo bajo pero una varianza alta.</p> <p>El overfitting a menudo es el resultado de un modelo excesivamente complicado, y puede evitarse ajustando m\u00faltiples modelos y utilizando validaci\u00f3n o validaci\u00f3n cruzada para comparar sus precisiones predictivas en los datos de prueba.</p> <p></p> <p>El underfitting ocurre cuando un modelo estad\u00edstico o un algoritmo de machine learning no pueden capturar la tendencia subyacente de los datos. Intuitivamente, el underfitting ocurre cuando el modelo o el algoritmo no se ajustan suficientemente a los datos. Espec\u00edficamente, el underfitting ocurre si el modelo o algoritmo muestra una varianza baja pero un sesgo alto.</p> <p>El underfitting suele ser el resultado de un modelo excesivamente simple.</p> <p></p> <p>\u00bfC\u00f3mo escoger el mejor modelo?</p> <p></p> <ul> <li><p>El sobreajuste va a estar relacionado con la complejidad del modelo, mientras m\u00e1s complejidad le agreguemos, mayor va a ser la tendencia a sobreajuste a los datos.</p> </li> <li><p>No existe una regla general para establecer cual es el nivel ideal de complejidad que le podemos otorgar a nuestro modelo sin caer en el sobreajuste; pero podemos valernos de algunas herramientas anal\u00edticas para intentar entender como el modelo se ajusta a los datos y reconocer el sobreajuste.</p> </li> </ul> <p>Para entender esto, veamos un ejemplo con el m\u00e9todo de \u00e1rboles de decisiones. Los \u00e1rboles de decisi\u00f3n (DT) son un m\u00e9todo de aprendizaje supervisado no param\u00e9trico utilizado para la clasificaci\u00f3n y la regresi\u00f3n.</p> In\u00a0[1]: Copied! <pre># librerias \n\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_classification\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\nimport random\n\nrandom.seed(1982) # semilla\n\n# graficos incrustados\n%matplotlib inline\n\n# parametros esteticos de seaborn\nsns.set_palette(\"deep\", desat=.6)\nsns.set_context(rc={\"figure.figsize\": (12, 4)})\n</pre> # librerias   import pandas as pd import numpy as np  import matplotlib.pyplot as plt  import seaborn as sns  from sklearn.model_selection import train_test_split from sklearn.datasets import make_classification from sklearn.svm import SVC from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor import random  random.seed(1982) # semilla  # graficos incrustados %matplotlib inline  # parametros esteticos de seaborn sns.set_palette(\"deep\", desat=.6) sns.set_context(rc={\"figure.figsize\": (12, 4)}) In\u00a0[2]: Copied! <pre># Create a random dataset\nrng = np.random.RandomState(1)\nX = np.sort(5 * rng.rand(80, 1), axis=0)\ny = np.sin(X).ravel()\ny[::5] += 3 * (0.5 - rng.rand(16))\n\n# separ los datos en train y eval\nx_train, x_eval, y_train, y_eval = train_test_split(X, y, test_size=0.35, \n                                                    train_size=0.65,\n                                                    random_state=1982)\n</pre> # Create a random dataset rng = np.random.RandomState(1) X = np.sort(5 * rng.rand(80, 1), axis=0) y = np.sin(X).ravel() y[::5] += 3 * (0.5 - rng.rand(16))  # separ los datos en train y eval x_train, x_eval, y_train, y_eval = train_test_split(X, y, test_size=0.35,                                                      train_size=0.65,                                                     random_state=1982) In\u00a0[3]: Copied! <pre># Fit regression model\nregr_1 = DecisionTreeRegressor(max_depth=2)\nregr_2 = DecisionTreeRegressor(max_depth=10)\n\nregr_1.fit(x_train,y_train)\nregr_2.fit(x_train,y_train)\n\n# Predict\nX_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\ny_1 = regr_1.predict(X_test)\ny_2 = regr_2.predict(X_test)\ny_3 = regr_1.predict(X_test)\n\n# Plot the results\nfig, ax = plt.subplots(figsize=(11, 8.5))\nplt.scatter(X, y, s=20, edgecolor=\"black\",\n            c=\"darkorange\", label=\"data\")\nplt.plot(X_test, y_1, color=\"cornflowerblue\",\n         label=\"max_depth=2\", linewidth=2)\nplt.plot(X_test, y_2, color=\"yellowgreen\", label=\"max_depth=10\", linewidth=2)\nplt.xlabel(\"data\")\nplt.ylabel(\"target\")\nplt.title(\"Decision Tree Regression\")\nplt.legend()\nplt.show()\n</pre> # Fit regression model regr_1 = DecisionTreeRegressor(max_depth=2) regr_2 = DecisionTreeRegressor(max_depth=10)  regr_1.fit(x_train,y_train) regr_2.fit(x_train,y_train)  # Predict X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis] y_1 = regr_1.predict(X_test) y_2 = regr_2.predict(X_test) y_3 = regr_1.predict(X_test)  # Plot the results fig, ax = plt.subplots(figsize=(11, 8.5)) plt.scatter(X, y, s=20, edgecolor=\"black\",             c=\"darkorange\", label=\"data\") plt.plot(X_test, y_1, color=\"cornflowerblue\",          label=\"max_depth=2\", linewidth=2) plt.plot(X_test, y_2, color=\"yellowgreen\", label=\"max_depth=10\", linewidth=2) plt.xlabel(\"data\") plt.ylabel(\"target\") plt.title(\"Decision Tree Regression\") plt.legend() plt.show() <p>Basado en los gr\u00e1ficos, el modelo de DT con profundidad 2, no se ajuste muy bien a los datos, mientras que el modelo DT con profundidad 10 se ajuste excesivamente demasiado a ellos.</p> <p>Para ver el ajuste de cada modelo, estudiaremos su precisi\u00f3n (score) sobre los conjunto de entrenamiento y de testeo.</p> In\u00a0[4]: Copied! <pre>result  = pd.DataFrame({\n    \n    'model': ['dt_depth_2','dt_depth_10'],\n    'train_score': [ regr_1.score(x_train, y_train), regr_2.score(x_train, y_train)],\n    'test_score': [ regr_1.score(x_eval, y_eval), regr_2.score(x_eval, y_eval)]\n})\nresult\n</pre> result  = pd.DataFrame({          'model': ['dt_depth_2','dt_depth_10'],     'train_score': [ regr_1.score(x_train, y_train), regr_2.score(x_train, y_train)],     'test_score': [ regr_1.score(x_eval, y_eval), regr_2.score(x_eval, y_eval)] }) result Out[4]: model train_score test_score 0 dt_depth_2 0.766363 0.719137 1 dt_depth_10 1.000000 0.661186 <p>Como es de esperar, para el modelo DT con profundidad 10, la precisi\u00f3n sobre el conjunto de entrenamiento es perfecta (igual a 1), no obstante, esta disminuye considerablemente al obtener la presici\u00f3n sobre los datos de testeo (igual a 0.66), por lo que esto es una evidencia para decir que el modelo tiene overfitting.</p> <p>Caso contrario es el modelo  DT con profundidad 2, puesto que es un caso t\u00edpico de underfitting. Cabe destacar que el modelo de underfitting tiene una presici\u00f3n similar tanto para el conjunto de entrenamiento como para el conjunto de testo.</p> <p>Conclusiones del caso</p> <p>Ambos modelos no ajuste de la mejor manera, pero lo hacen de distintas perspectivas. Se debe poner mucho \u00e9nfasis al momento de separar el conjunto de entrenamiento y de testeo, puesto que los resultados se pueden ver altamente sesgado (caso del overfitting). Particularmente para este caso, el ajuste era complejo de realizar puesto que eliminabamos un monto de datos \"significativos\", que hacian que los modelos no captar\u00e1n la continuidad de la funci\u00f3n sinusoidal.</p> In\u00a0[5]: Copied! <pre># Ejemplo en python - \u00e1rboles de decisi\u00f3n\n# dummy data con 100 atributos y 2 clases\nX, y = make_classification(10000, 100, n_informative=3, n_classes=2,\n                          random_state=1982)\n\n# separ los datos en train y eval\nx_train, x_eval, y_train, y_eval = train_test_split(X, y, test_size=0.35, \n                                                    train_size=0.65,\n                                                    random_state=1982)\n\n# Grafico de ajuste del \u00e1rbol de decisi\u00f3n\ntrain_prec =  []\neval_prec = []\nmax_deep_list = list(range(2, 20))\n\n# Entrenar con arboles de distinta profundidad\nfor deep in max_deep_list:\n    model = DecisionTreeClassifier( max_depth=deep)\n    model.fit(x_train, y_train)\n    train_prec.append(model.score(x_train, y_train))\n    eval_prec.append(model.score(x_eval, y_eval))\n</pre> # Ejemplo en python - \u00e1rboles de decisi\u00f3n # dummy data con 100 atributos y 2 clases X, y = make_classification(10000, 100, n_informative=3, n_classes=2,                           random_state=1982)  # separ los datos en train y eval x_train, x_eval, y_train, y_eval = train_test_split(X, y, test_size=0.35,                                                      train_size=0.65,                                                     random_state=1982)  # Grafico de ajuste del \u00e1rbol de decisi\u00f3n train_prec =  [] eval_prec = [] max_deep_list = list(range(2, 20))  # Entrenar con arboles de distinta profundidad for deep in max_deep_list:     model = DecisionTreeClassifier( max_depth=deep)     model.fit(x_train, y_train)     train_prec.append(model.score(x_train, y_train))     eval_prec.append(model.score(x_eval, y_eval)) In\u00a0[6]: Copied! <pre># graficar los resultados.\n\nsns.set(rc={'figure.figsize':(12,9)})\n\ndf1 = pd.DataFrame({'numero_nodos':max_deep_list,\n                   'precision':train_prec,\n                   'datos':'entrenamiento'})\n\ndf2 = pd.DataFrame({'numero_nodos':max_deep_list,\n                   'precision':eval_prec,\n                   'datos':'evaluacion'})\n\ndf_graph = pd.concat([df1,df2])\n\nsns.lineplot(data=df_graph,\n             x='numero_nodos',\n             y='precision',\n             hue='datos',\n             palette=\"Set1\")\n</pre> # graficar los resultados.  sns.set(rc={'figure.figsize':(12,9)})  df1 = pd.DataFrame({'numero_nodos':max_deep_list,                    'precision':train_prec,                    'datos':'entrenamiento'})  df2 = pd.DataFrame({'numero_nodos':max_deep_list,                    'precision':eval_prec,                    'datos':'evaluacion'})  df_graph = pd.concat([df1,df2])  sns.lineplot(data=df_graph,              x='numero_nodos',              y='precision',              hue='datos',              palette=\"Set1\") Out[6]: <pre>&lt;Axes: xlabel='numero_nodos', ylabel='precision'&gt;</pre> <p>El gr\u00e1fico que acabamos de construir se llama gr\u00e1fico de ajuste y muestra la precisi\u00f3n del modelo en funci\u00f3n de su complejidad.</p> <p>El punto con mayor precisi\u00f3n, en los datos de evaluaci\u00f3n, lo obtenemos con un nivel de profundidad de aproximadamente 6 nodos; a partir de all\u00ed el modelo pierde en generalizaci\u00f3n y comienza a estar sobreajustado.</p> <p>Tambi\u00e9n podemos crear un gr\u00e1fico similar con la ayuda de Scikit-learn, utilizando <code>validation_curve</code>.</p> In\u00a0[7]: Copied! <pre># utilizando validation curve de sklearn\nfrom sklearn.model_selection import validation_curve\n\ntrain_prec, eval_prec = validation_curve(estimator=model, X=x_train,\n                                        y=y_train, param_name='max_depth',\n                                        param_range=max_deep_list, cv=5)\n\ntrain_mean = np.mean(train_prec, axis=1)\ntrain_std = np.std(train_prec, axis=1)\ntest_mean = np.mean(eval_prec, axis=1)\ntest_std = np.std(eval_prec, axis=1)\n</pre> # utilizando validation curve de sklearn from sklearn.model_selection import validation_curve  train_prec, eval_prec = validation_curve(estimator=model, X=x_train,                                         y=y_train, param_name='max_depth',                                         param_range=max_deep_list, cv=5)  train_mean = np.mean(train_prec, axis=1) train_std = np.std(train_prec, axis=1) test_mean = np.mean(eval_prec, axis=1) test_std = np.std(eval_prec, axis=1) In\u00a0[8]: Copied! <pre># graficando las curvas\nplt.plot(max_deep_list, train_mean, color='r', marker='o', markersize=5,\n         label='entrenamiento')\nplt.fill_between(max_deep_list, train_mean + train_std, \n                 train_mean - train_std, alpha=0.15, color='r')\nplt.plot(max_deep_list, test_mean, color='b', linestyle='--', \n         marker='s', markersize=5, label='evaluacion')\nplt.fill_between(max_deep_list, test_mean + test_std, \n                 test_mean - test_std, alpha=0.15, color='b')\nplt.legend(loc='center right')\nplt.xlabel('numero_nodos')\nplt.ylabel('precision')\nplt.show()\n</pre> # graficando las curvas plt.plot(max_deep_list, train_mean, color='r', marker='o', markersize=5,          label='entrenamiento') plt.fill_between(max_deep_list, train_mean + train_std,                   train_mean - train_std, alpha=0.15, color='r') plt.plot(max_deep_list, test_mean, color='b', linestyle='--',           marker='s', markersize=5, label='evaluacion') plt.fill_between(max_deep_list, test_mean + test_std,                   test_mean - test_std, alpha=0.15, color='b') plt.legend(loc='center right') plt.xlabel('numero_nodos') plt.ylabel('precision') plt.show()"},{"location":"lectures/machine_learning/over_01/#overfitting-i","title":"Overfitting I\u00b6","text":""},{"location":"lectures/machine_learning/over_01/#ejemplo-con-arboles-de-decision","title":"Ejemplo con  \u00c1rboles de Decisi\u00f3n\u00b6","text":"<p>Los \u00c1rboles de Decisi\u00f3n pueden ser muchas veces una herramienta muy precisa, pero tambi\u00e9n con mucha tendencia al sobreajuste. Para construir estos modelos aplicamos un procedimiento recursivo para encontrar los atributos que nos proporcionan m\u00e1s informaci\u00f3n sobre distintos subconjuntos de datos, cada vez m\u00e1s peque\u00f1os.</p> <p>Si aplicamos este procedimiento en forma reiterada, eventualmente podemos llegar a un \u00e1rbol en el que cada hoja tenga una sola instancia de nuestra variable objetivo a clasificar.</p> <p>En este caso extremo, el \u00c1rbol de Decisi\u00f3n va a tener una pobre generalizaci\u00f3n y estar bastante sobreajustado; ya que cada instancia de los datos de entrenamiento va a encontrar el camino que lo lleve eventualmente a la hoja que lo contiene, alcanzando as\u00ed una precisi\u00f3n del 100% con los datos de entrenamiento.</p>"},{"location":"lectures/machine_learning/over_01/#ejemplo-funcion-sinusoidal","title":"Ejemplo funci\u00f3n sinusoidal\u00b6","text":"<p>Veamos un ejemplo sencillo con la ayuda de python, tratemos de ajustar un modelo de DT sobre una funci\u00f3n senusoidal.</p>"},{"location":"lectures/machine_learning/over_01/#equilibrio-en-el-ajuste-de-modelos","title":"Equilibrio en el ajuste de modelos\u00b6","text":"<p>A continuaci\u00f3n ocuparemos otro conjunto de entrenamientos (make_classification) para mostrar una forma de encoentrar un un equilibrio en la complejidad del modelo y su ajuste a los datos.</p> <p>Siguiendo con el ejemplo de los modelos de \u00e1rbol de decisi\u00f3n, analizaremos la presici\u00f3n (score) para distintas profundidades sobre los distintos conjuntos (entrenamiento y testeo).</p>"},{"location":"lectures/machine_learning/over_01/#referencias","title":"Referencias\u00b6","text":"<ol> <li>Underfitting vs Underfitting</li> <li>Overfitting and Underfitting With Machine Learning Algorithms</li> </ol>"},{"location":"lectures/machine_learning/over_02/","title":"Overfitting II","text":"<p>Veamos un ejemplo en python, ocupando el conjunto de datos make_classification.</p> In\u00a0[1]: Copied! <pre># librerias \n\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_classification\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\nimport random\n\nrandom.seed(1982) # semilla\n\n# graficos incrustados\n%matplotlib inline\n\n# parametros esteticos de seaborn\nsns.set_palette(\"deep\", desat=.6)\nsns.set_context(rc={\"figure.figsize\": (12, 4)})\n</pre> # librerias   import pandas as pd import numpy as np  import matplotlib.pyplot as plt  import seaborn as sns  from sklearn.model_selection import train_test_split from sklearn.datasets import make_classification from sklearn.svm import SVC from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor import random  random.seed(1982) # semilla  # graficos incrustados %matplotlib inline  # parametros esteticos de seaborn sns.set_palette(\"deep\", desat=.6) sns.set_context(rc={\"figure.figsize\": (12, 4)}) In\u00a0[2]: Copied! <pre># Ejemplo en python - \u00e1rboles de decisi\u00f3n\n# dummy data con 100 atributos y 2 clases\nX, y = make_classification(10000, 100, n_informative=3, n_classes=2,\n                          random_state=1982)\n\n# separ los datos en train y eval\nx_train, x_eval, y_train, y_eval = train_test_split(X, y, test_size=0.35, \n                                                    train_size=0.65,\n                                                    random_state=1982)\n\n# Grafico de ajuste del \u00e1rbol de decisi\u00f3n\ntrain_prec =  []\neval_prec = []\nmax_deep_list = list(range(2, 20))\n</pre> # Ejemplo en python - \u00e1rboles de decisi\u00f3n # dummy data con 100 atributos y 2 clases X, y = make_classification(10000, 100, n_informative=3, n_classes=2,                           random_state=1982)  # separ los datos en train y eval x_train, x_eval, y_train, y_eval = train_test_split(X, y, test_size=0.35,                                                      train_size=0.65,                                                     random_state=1982)  # Grafico de ajuste del \u00e1rbol de decisi\u00f3n train_prec =  [] eval_prec = [] max_deep_list = list(range(2, 20)) In\u00a0[3]: Copied! <pre># Ejemplo cross-validation\nfrom sklearn.model_selection import cross_validate,StratifiedKFold\n\n# creando pliegues\n\nskf = StratifiedKFold(n_splits=20)\nprecision = []\nmodel =  DecisionTreeClassifier(criterion='entropy', max_depth=5)\n\nskf.get_n_splits(x_train, y_train)\nfor k, (train_index, test_index) in enumerate(skf.split(X, y)):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    model.fit(X_train,y_train) \n    score = model.score(X_test,y_test)\n    precision.append(score)\n    print('Pliegue: {0:}, Dist Clase: {1:}, Prec: {2:.3f}'.format(k+1,\n                        np.bincount(y_train), score))\n</pre> # Ejemplo cross-validation from sklearn.model_selection import cross_validate,StratifiedKFold  # creando pliegues  skf = StratifiedKFold(n_splits=20) precision = [] model =  DecisionTreeClassifier(criterion='entropy', max_depth=5)  skf.get_n_splits(x_train, y_train) for k, (train_index, test_index) in enumerate(skf.split(X, y)):     X_train, X_test = X[train_index], X[test_index]     y_train, y_test = y[train_index], y[test_index]     model.fit(X_train,y_train)      score = model.score(X_test,y_test)     precision.append(score)     print('Pliegue: {0:}, Dist Clase: {1:}, Prec: {2:.3f}'.format(k+1,                         np.bincount(y_train), score))      <pre>Pliegue: 1, Dist Clase: [4763 4737], Prec: 0.928\nPliegue: 2, Dist Clase: [4763 4737], Prec: 0.914\nPliegue: 3, Dist Clase: [4763 4737], Prec: 0.916\nPliegue: 4, Dist Clase: [4763 4737], Prec: 0.938\nPliegue: 5, Dist Clase: [4763 4737], Prec: 0.924\nPliegue: 6, Dist Clase: [4763 4737], Prec: 0.938\nPliegue: 7, Dist Clase: [4763 4737], Prec: 0.924\nPliegue: 8, Dist Clase: [4762 4738], Prec: 0.938\nPliegue: 9, Dist Clase: [4762 4738], Prec: 0.936\nPliegue: 10, Dist Clase: [4762 4738], Prec: 0.908\nPliegue: 11, Dist Clase: [4762 4738], Prec: 0.936\nPliegue: 12, Dist Clase: [4762 4738], Prec: 0.938\nPliegue: 13, Dist Clase: [4762 4738], Prec: 0.934\nPliegue: 14, Dist Clase: [4762 4738], Prec: 0.922\nPliegue: 15, Dist Clase: [4762 4738], Prec: 0.930\nPliegue: 16, Dist Clase: [4762 4738], Prec: 0.928\nPliegue: 17, Dist Clase: [4762 4738], Prec: 0.924\nPliegue: 18, Dist Clase: [4762 4738], Prec: 0.926\nPliegue: 19, Dist Clase: [4762 4738], Prec: 0.936\nPliegue: 20, Dist Clase: [4762 4738], Prec: 0.920\n</pre> <p>En este ejemplo, utilizamos el iterador <code>StratifiedKFold</code> que nos proporciona Scikit-learn. Este iterador es una versi\u00f3n mejorada de la validaci\u00f3n cruzada, ya que cada pliegue va a estar estratificado para mantener las proporciones entre las clases del conjunto de datos original, lo que suele dar mejores estimaciones del sesgo y la varianza del modelo.</p> <p>Tambi\u00e9n podr\u00edamos utilizar <code>cross_val_score</code> que ya nos proporciona los resultados de la precisi\u00f3n que tuvo el modelo en cada pliegue.</p> In\u00a0[4]: Copied! <pre># Ejemplo con cross_val_score\nfrom sklearn.model_selection import cross_val_score\n\n# separ los datos en train y eval\nx_train, x_eval, y_train, y_eval = train_test_split(X, y, test_size=0.35, \n                                                    train_size=0.65,\n                                                    random_state=1982)\n\n\nmodel = DecisionTreeClassifier(criterion='entropy',\n                               max_depth=5)\n\n\nprecision = cross_val_score(estimator=model,\n                            X=x_train,\n                            y=y_train,\n                            cv=20)\n</pre> # Ejemplo con cross_val_score from sklearn.model_selection import cross_val_score  # separ los datos en train y eval x_train, x_eval, y_train, y_eval = train_test_split(X, y, test_size=0.35,                                                      train_size=0.65,                                                     random_state=1982)   model = DecisionTreeClassifier(criterion='entropy',                                max_depth=5)   precision = cross_val_score(estimator=model,                             X=x_train,                             y=y_train,                             cv=20) In\u00a0[5]: Copied! <pre>precision = [round(x,2) for x in precision]\nprint('Precisiones: {} '.format(precision))\nprint('Precision promedio: {0: .3f} +/- {1: .3f}'.format(np.mean(precision),\n                                          np.std(precision)))\n</pre> precision = [round(x,2) for x in precision] print('Precisiones: {} '.format(precision)) print('Precision promedio: {0: .3f} +/- {1: .3f}'.format(np.mean(precision),                                           np.std(precision))) <pre>Precisiones: [0.93, 0.94, 0.92, 0.94, 0.93, 0.9, 0.92, 0.94, 0.94, 0.93, 0.94, 0.92, 0.91, 0.91, 0.93, 0.94, 0.93, 0.93, 0.92, 0.93] \nPrecision promedio:  0.927 +/-  0.011\n</pre> <p>Para graficar las curvas de aprendizaje es necesario ocupar el comando de sklearn llamado <code>learning_curve</code>.</p> In\u00a0[6]: Copied! <pre># Ejemplo Curvas de aprendizaje\nfrom sklearn.model_selection import  learning_curve\n\ntrain_sizes, train_scores, test_scores = learning_curve(\n                        estimator=model,\n                        X=x_train,\n                        y=y_train, \n                        train_sizes=np.linspace(0.1, 1.0, 20),\n                        cv=10,\n                        n_jobs=-1\n                        )\n\n# calculo de metricas\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n</pre> # Ejemplo Curvas de aprendizaje from sklearn.model_selection import  learning_curve  train_sizes, train_scores, test_scores = learning_curve(                         estimator=model,                         X=x_train,                         y=y_train,                          train_sizes=np.linspace(0.1, 1.0, 20),                         cv=10,                         n_jobs=-1                         )  # calculo de metricas train_mean = np.mean(train_scores, axis=1) train_std = np.std(train_scores, axis=1) test_mean = np.mean(test_scores, axis=1) test_std = np.std(test_scores, axis=1) <p>Veamos que el comando <code>learning_curve</code> va creando conjunto de datos, pero de distintos tama\u00f1os.</p> In\u00a0[7]: Copied! <pre># tamano conjunto de entrenamiento\nfor k in range(len(train_sizes)):\n    print('Tama\u00f1o Conjunto {}: {}'.format(k+1,train_sizes[k]))\n</pre> # tamano conjunto de entrenamiento for k in range(len(train_sizes)):     print('Tama\u00f1o Conjunto {}: {}'.format(k+1,train_sizes[k])) <pre>Tama\u00f1o Conjunto 1: 585\nTama\u00f1o Conjunto 2: 862\nTama\u00f1o Conjunto 3: 1139\nTama\u00f1o Conjunto 4: 1416\nTama\u00f1o Conjunto 5: 1693\nTama\u00f1o Conjunto 6: 1970\nTama\u00f1o Conjunto 7: 2247\nTama\u00f1o Conjunto 8: 2524\nTama\u00f1o Conjunto 9: 2801\nTama\u00f1o Conjunto 10: 3078\nTama\u00f1o Conjunto 11: 3356\nTama\u00f1o Conjunto 12: 3633\nTama\u00f1o Conjunto 13: 3910\nTama\u00f1o Conjunto 14: 4187\nTama\u00f1o Conjunto 15: 4464\nTama\u00f1o Conjunto 16: 4741\nTama\u00f1o Conjunto 17: 5018\nTama\u00f1o Conjunto 18: 5295\nTama\u00f1o Conjunto 19: 5572\nTama\u00f1o Conjunto 20: 5850\n</pre> <p>Finalmente, graficamos las precisiones tanto para el conjunto de entranamiento como de evaluaci\u00f3n para los distintos conjuntos de datos generados.</p> In\u00a0[8]: Copied! <pre># graficando las curvas\nplt.figure(figsize=(12,8))\n\nplt.plot(train_sizes, train_mean, color='r', marker='o', markersize=5,\n         label='entrenamiento')\nplt.fill_between(train_sizes, train_mean + train_std, \n                 train_mean - train_std, alpha=0.15, color='r')\nplt.plot(train_sizes, test_mean, color='b', linestyle='--', \n         marker='s', markersize=5, label='evaluacion')\nplt.fill_between(train_sizes, test_mean + test_std, \n                 test_mean - test_std, alpha=0.15, color='b')\nplt.grid()\nplt.title('Curva de aprendizaje')\nplt.legend(loc='upper right')\nplt.xlabel('Cant de ejemplos de entrenamiento')\nplt.ylabel('Precision')\nplt.show()\n</pre> # graficando las curvas plt.figure(figsize=(12,8))  plt.plot(train_sizes, train_mean, color='r', marker='o', markersize=5,          label='entrenamiento') plt.fill_between(train_sizes, train_mean + train_std,                   train_mean - train_std, alpha=0.15, color='r') plt.plot(train_sizes, test_mean, color='b', linestyle='--',           marker='s', markersize=5, label='evaluacion') plt.fill_between(train_sizes, test_mean + test_std,                   test_mean - test_std, alpha=0.15, color='b') plt.grid() plt.title('Curva de aprendizaje') plt.legend(loc='upper right') plt.xlabel('Cant de ejemplos de entrenamiento') plt.ylabel('Precision') plt.show() <p>En este gr\u00e1fico podemos concluir que:</p> <ul> <li><p>Con pocos datos la precisi\u00f3n entre los datos de entrenamiento y los de evaluaci\u00f3n son muy distintas y luego a medida que la cantidad de datos va aumentando, el modelo puede generalizar mucho mejor y las precisiones se comienzan a emparejar.</p> </li> <li><p>Este gr\u00e1fico tambi\u00e9n puede ser importante a la hora de decidir invertir en la obtenci\u00f3n de m\u00e1s datos, ya que por ejemplo nos indica que a partir las 2500 muestras, el modelo ya no gana mucha m\u00e1s precisi\u00f3n a pesar de obtener m\u00e1s datos.</p> </li> </ul> In\u00a0[9]: Copied! <pre># Ejemplo de grid search con SVM.\nfrom sklearn.model_selection import GridSearchCV\n\n# creaci\u00f3n del modelo\nmodel = DecisionTreeClassifier()\n\n# rango de parametros\nrango_criterion = ['gini','entropy']\nrango_max_depth =np.array( [4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150])\nparam_grid = dict(criterion=rango_criterion, max_depth=rango_max_depth)\nparam_grid\n</pre> # Ejemplo de grid search con SVM. from sklearn.model_selection import GridSearchCV  # creaci\u00f3n del modelo model = DecisionTreeClassifier()  # rango de parametros rango_criterion = ['gini','entropy'] rango_max_depth =np.array( [4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]) param_grid = dict(criterion=rango_criterion, max_depth=rango_max_depth) param_grid Out[9]: <pre>{'criterion': ['gini', 'entropy'],\n 'max_depth': array([  4,   5,   6,   7,   8,   9,  10,  11,  12,  15,  20,  30,  40,\n         50,  70,  90, 120, 150])}</pre> In\u00a0[10]: Copied! <pre># aplicar greed search\n\ngs = GridSearchCV(estimator=model, \n                  param_grid=param_grid, \n                  scoring='accuracy',\n                  cv=5,\n                  n_jobs=-1)\n\ngs = gs.fit(x_train, y_train)\n</pre> # aplicar greed search  gs = GridSearchCV(estimator=model,                    param_grid=param_grid,                    scoring='accuracy',                   cv=5,                   n_jobs=-1)  gs = gs.fit(x_train, y_train) In\u00a0[11]: Copied! <pre># imprimir resultados\nprint(gs.best_score_)\nprint(gs.best_params_)\n</pre> # imprimir resultados print(gs.best_score_) print(gs.best_params_) <pre>0.9332307692307692\n{'criterion': 'entropy', 'max_depth': 6}\n</pre> In\u00a0[12]: Copied! <pre># utilizando el mejor modelo\nmejor_modelo = gs.best_estimator_\nmejor_modelo.fit(x_train, y_train)\nprint('Precisi\u00f3n: {0:.3f}'.format(mejor_modelo.score(x_eval, y_eval)))\n</pre> # utilizando el mejor modelo mejor_modelo = gs.best_estimator_ mejor_modelo.fit(x_train, y_train) print('Precisi\u00f3n: {0:.3f}'.format(mejor_modelo.score(x_eval, y_eval))) <pre>Precisi\u00f3n: 0.939\n</pre> <p>En este ejemplo, primero utilizamos el objeto <code>GridSearchCV</code> que nos permite realizar grid search junto con validaci\u00f3n cruzada, luego comenzamos a ajustar el modelo con las diferentes combinaciones de los valores de los par\u00e1metros <code>criterion</code> y <code>max_depth</code>. Finalmente imprimimos el mejor resultado de precisi\u00f3n y los valores de los par\u00e1metros que utilizamos para obtenerlos; por \u00faltimo utilizamos este mejor modelo para realizar las predicciones con los datos de evaluaci\u00f3n.</p> <p>Podemos ver que la precisi\u00f3n que obtuvimos con los datos de evaluaci\u00f3n es casi id\u00e9ntica a la que nos indic\u00f3 grid search, lo que indica que el modelo generaliza muy bien.</p> <p>Algoritmos para selecci\u00f3n de atributos</p> <p>Podemos encontrar dos clases generales de algoritmos de selecci\u00f3n de atributos: los m\u00e9todos de filtrado, y los m\u00e9todos empaquetados.</p> <ul> <li><p>M\u00e9todos de filtrado:  Estos m\u00e9todos aplican una medida estad\u00edstica para asignar una puntuaci\u00f3n a cada atributo. Los atributos luego son clasificados de acuerdo a su puntuaci\u00f3n y son, o bien seleccionados para su conservaci\u00f3n o eliminados del conjunto de datos. Los m\u00e9todos de filtrado son a menudo univariantes y consideran a cada atributo en forma independiente, o con respecto a la variable dependiente.</p> <ul> <li>Ejemplos : prueba de Chi cuadrado, prueba F de Fisher, ratio de ganancia de informaci\u00f3n y los coeficientes de correlaci\u00f3n.</li> </ul> </li> <li><p>M\u00e9todos empaquetados: Estos m\u00e9todos consideran la selecci\u00f3n de un conjunto de atributos como un problema de b\u00fasqueda, en donde las diferentes combinaciones son evaluadas y comparadas. Para hacer estas evaluaciones se utiliza un modelo predictivo y luego se asigna una puntuaci\u00f3n a cada combinaci\u00f3n basada en la precisi\u00f3n del modelo.</p> <ul> <li>Un ejemplo de este m\u00e9todo es el algoritmo de eliminaci\u00f3n recursiva de atributos.</li> </ul> </li> </ul> <p>Un m\u00e9todo popular en sklearn es el m\u00e9todo SelectKBest, el cual selecciona las  caracter\u00edsticas de acuerdo con las $k$ puntuaciones m\u00e1s altas (de acuerdo al criterio escogido).</p> <p>Para entender este conceptos, transformemos el conjunto de datos anterior a formato pandas DataFrame.</p> In\u00a0[13]: Copied! <pre># Datos\nX, y = make_classification(10000, 100, n_informative=3, n_classes=2,\n                          random_state=1982)\n\ndf = pd.DataFrame(X)\ndf.columns = [f'V{k}' for k in range(1,X.shape[1]+1)]\ndf['y']=y\ndf.head()\n</pre> # Datos X, y = make_classification(10000, 100, n_informative=3, n_classes=2,                           random_state=1982)  df = pd.DataFrame(X) df.columns = [f'V{k}' for k in range(1,X.shape[1]+1)] df['y']=y df.head() Out[13]: V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 ... V92 V93 V94 V95 V96 V97 V98 V99 V100 y 0 0.949283 -1.075706 -0.105733 -0.000047 -0.278974 0.510083 -0.778030 -1.976158 -1.201534 -1.047384 ... -0.630209 -0.331225 -0.202422 -1.786323 1.540031 1.119424 0.507775 -0.848286 -0.027485 1 1 0.183904 0.524554 -1.561357 -1.950628 1.077846 -0.598287 0.153160 -1.206113 0.673170 -0.843770 ... -1.015067 0.319214 0.240570 -2.205400 -0.430933 -0.313175 0.752012 -0.070265 1.390394 0 2 0.499151 -0.625950 2.977037 0.612030 -0.102034 2.076814 1.661343 1.310895 -1.115465 -0.544276 ... 0.311830 -1.130865 0.247865 -0.499241 -1.595737 -0.496805 -0.917257 0.976909 -1.518979 0 3 -0.172063 -0.599516 0.154253 -0.593797 0.931374 0.939714 1.107241 0.146723 -0.446275 0.095896 ... -1.641808 -1.170021 0.815094 -0.722564 -0.263476 -0.715898 1.962313 1.076288 -2.259682 0 4 -0.396408 0.876210 -0.791795 0.999677 0.046859 -0.166211 -0.549437 0.344644 0.349981 -0.207106 ... 1.307020 0.876912 0.882497 -0.704791 -0.743942 -0.075060 0.622693 0.751576 0.907325 0 <p>5 rows \u00d7 101 columns</p> <p>Comencemos con un simple algoritmo univariante que aplica el m\u00e9todo de filtrado. Para esto vamos a utilizar los objetos <code>SelectKBest</code> y <code>f_classif</code> del paquete <code>sklearn.feature_selection</code>.</p> <p>Este algoritmo selecciona a los mejores atributos bas\u00e1ndose en una prueba estad\u00edstica univariante. Al objeto <code>SelectKBest</code> le pasamos la prueba estad\u00edstica que vamos a a aplicar, en este caso una prueba F definida por el objeto <code>f_classif</code>, junto con el n\u00famero de atributos a seleccionar. El algoritmo va a aplicar la prueba a todos los atributos y va a seleccionar los que mejor resultado obtuvieron.</p> In\u00a0[14]: Copied! <pre>from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n</pre> from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import f_classif In\u00a0[15]: Copied! <pre># Separamos las columnas objetivo\nx_training = df.drop(['y',], axis=1)\ny_training = df['y']\n\n# Aplicando el algoritmo univariante de prueba F.\nk = 15  # n\u00famero de atributos a seleccionar\ncolumnas = list(x_training.columns.values)\nseleccionadas = SelectKBest(f_classif, k=k).fit(x_training, y_training)\n</pre> # Separamos las columnas objetivo x_training = df.drop(['y',], axis=1) y_training = df['y']  # Aplicando el algoritmo univariante de prueba F. k = 15  # n\u00famero de atributos a seleccionar columnas = list(x_training.columns.values) seleccionadas = SelectKBest(f_classif, k=k).fit(x_training, y_training) In\u00a0[16]: Copied! <pre>catrib = seleccionadas.get_support()\natributos = [columnas[i] for i in list(catrib.nonzero()[0])]\natributos\n</pre> catrib = seleccionadas.get_support() atributos = [columnas[i] for i in list(catrib.nonzero()[0])] atributos Out[16]: <pre>['V1',\n 'V42',\n 'V46',\n 'V49',\n 'V62',\n 'V64',\n 'V66',\n 'V68',\n 'V69',\n 'V75',\n 'V82',\n 'V86',\n 'V89',\n 'V98',\n 'V100']</pre> <p>Como podemos ver, el algoritmo nos seleccion\u00f3 la cantidad de atributos que le indicamos; en este ejemplo decidimos seleccionar solo 15; obviamente, cuando armemos nuestro modelo final vamos a tomar un n\u00famero mayor de atributos. Ahora se proceder\u00e1 a comparar los resultados de entrenar un modelo en particular con todas las variables y el subconjunto de variables seleccionadas.</p> In\u00a0[23]: Copied! <pre>import time\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n</pre> import time from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression In\u00a0[18]: Copied! <pre>from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n\n# Evaluar las m\u00e9tricas\ndef classification_metrics(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Aplicar las distintas m\u00e9tricas definidas\n    :param df: dataframe con las columnas: ['y', 'yhat']\n    :return: dataframe con las m\u00e9tricas especificadas\n    \"\"\"\n    y_true = df['y']\n    y_pred = df['yhat']\n\n    accuracy = round(accuracy_score(y_true, y_pred), 4)\n    recall = round(recall_score(y_true, y_pred, average='macro'), 4)\n    precision = round(precision_score(y_true, y_pred, average='macro'), 4)\n    fscore = round(f1_score(y_true, y_pred, average='macro'), 4)\n\n    df_result = pd.DataFrame({'accuracy': [accuracy],\n                              'recall': [recall],\n                              'precision': [precision],\n                              'fscore': [fscore]})\n\n    return df_result\n</pre> from sklearn.metrics import confusion_matrix from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score  # Evaluar las m\u00e9tricas def classification_metrics(df: pd.DataFrame) -&gt; pd.DataFrame:     \"\"\"     Aplicar las distintas m\u00e9tricas definidas     :param df: dataframe con las columnas: ['y', 'yhat']     :return: dataframe con las m\u00e9tricas especificadas     \"\"\"     y_true = df['y']     y_pred = df['yhat']      accuracy = round(accuracy_score(y_true, y_pred), 4)     recall = round(recall_score(y_true, y_pred, average='macro'), 4)     precision = round(precision_score(y_true, y_pred, average='macro'), 4)     fscore = round(f1_score(y_true, y_pred, average='macro'), 4)      df_result = pd.DataFrame({'accuracy': [accuracy],                               'recall': [recall],                               'precision': [precision],                               'fscore': [fscore]})      return df_result In\u00a0[24]: Copied! <pre># Record start time\nstart_time = time.time()\n\n# Entrenamiento con todas las variables \nX = df.drop('y', axis=1)\nY = df['y']\n\n# split dataset\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2)\n\n# Creando el modelo\nrlog = LogisticRegression()\nrlog.fit(X_train, Y_train) # ajustando el modelo\n\npredicciones = rlog.predict(X_test)\n\ndf_pred = pd.DataFrame({\n    'y': Y_test,\n    'yhat': predicciones\n})\n\ndf_s1 = classification_metrics(df_pred).assign(name='Todas las variables')\n\n# Calculate the elapsed time\nelapsed_time = time.time() - start_time\nprint(\"Elapsed time:\", elapsed_time, \"seconds\")\n</pre> # Record start time start_time = time.time()  # Entrenamiento con todas las variables  X = df.drop('y', axis=1) Y = df['y']  # split dataset X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2)  # Creando el modelo rlog = LogisticRegression() rlog.fit(X_train, Y_train) # ajustando el modelo  predicciones = rlog.predict(X_test)  df_pred = pd.DataFrame({     'y': Y_test,     'yhat': predicciones })  df_s1 = classification_metrics(df_pred).assign(name='Todas las variables')  # Calculate the elapsed time elapsed_time = time.time() - start_time print(\"Elapsed time:\", elapsed_time, \"seconds\") <pre>Elapsed time: 0.027977705001831055 seconds\n</pre> In\u00a0[25]: Copied! <pre># Record start time\nstart_time = time.time()\n\n# Entrenamiento con las variables seleccionadas\nX = df[atributos]\nY = df['y']\n\n# split dataset\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2)\n\n# Creando el modelo\nrlog = LogisticRegression()\nrlog.fit(X_train, Y_train) # ajustando el modelo\n\npredicciones = rlog.predict(X_test)\n\ndf_pred = pd.DataFrame({\n    'y': Y_test,\n    'yhat': predicciones\n})\n\ndf_s2 = classification_metrics(df_pred).assign(name='Variables Seleccionadas')\n\n# Calculate the elapsed time\nelapsed_time = time.time() - start_time\nprint(\"Elapsed time:\", elapsed_time, \"seconds\")\n</pre> # Record start time start_time = time.time()  # Entrenamiento con las variables seleccionadas X = df[atributos] Y = df['y']  # split dataset X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2)  # Creando el modelo rlog = LogisticRegression() rlog.fit(X_train, Y_train) # ajustando el modelo  predicciones = rlog.predict(X_test)  df_pred = pd.DataFrame({     'y': Y_test,     'yhat': predicciones })  df_s2 = classification_metrics(df_pred).assign(name='Variables Seleccionadas')  # Calculate the elapsed time elapsed_time = time.time() - start_time print(\"Elapsed time:\", elapsed_time, \"seconds\")  <pre>Elapsed time: 0.025048255920410156 seconds\n</pre> <p>Juntando ambos resultados:</p> In\u00a0[26]: Copied! <pre># juntar resultados en formato dataframe\npd.concat([df_s1,df_s2])\n</pre> # juntar resultados en formato dataframe pd.concat([df_s1,df_s2]) Out[26]: accuracy recall precision fscore name 0 0.8905 0.8906 0.8907 0.8905 Todas las variables 0 0.8985 0.8986 0.8986 0.8985 Variables Seleccionadas <p>Las m\u00e9tricas para ambos casos son parecidas y el tiempo de ejecuci\u00f3n del modelo con menos variable resulta ser menor (lo cual era algo esperable). Lo cual nos muestra que trabajando con menos variables, se puede captar las caracter\u00edsticas m\u00e1s relevante del problema, y en la medida que se trabaje con m\u00e1s datos, las mejoras a nivel de capacidad de c\u00f3mputo tendr\u00e1n un mejor desempe\u00f1o.</p>"},{"location":"lectures/machine_learning/over_02/#overfitting-ii","title":"Overfitting II\u00b6","text":"<p>Algunas de las t\u00e9cnicas que podemos utilizar para reducir el overfitting, son:</p> <ul> <li>Recolectar m\u00e1s datos.</li> <li>Introducir una penalizaci\u00f3n a la complejidad con alguna t\u00e9cnica de regularizaci\u00f3n.</li> <li>Utilizar modelos ensamblados.</li> <li>Utilizar validaci\u00f3n cruzada.</li> <li>Optimizar los par\u00e1metros del modelo con grid search.</li> <li>Reducir la dimensi\u00f3n de los datos.</li> <li>Aplicar t\u00e9cnicas de selecci\u00f3n de atributos.</li> </ul> <p>Veremos ejemplos de algunos m\u00e9todos para reducir el sobreajuste (overfitting).</p>"},{"location":"lectures/machine_learning/over_02/#validacion-cruzada","title":"Validaci\u00f3n cruzada\u00b6","text":"<p>La validaci\u00f3n cruzada se inicia mediante el fraccionamiento de un conjunto de datos en un n\u00famero $k$ de particiones (generalmente entre 5 y 10) llamadas pliegues.</p> <p>La validaci\u00f3n cruzada luego itera entre los datos de evaluaci\u00f3n y entrenamiento $k$ veces, de un modo particular. En cada iteraci\u00f3n de la validaci\u00f3n cruzada, un pliegue diferente se elige como los datos de evaluaci\u00f3n. En esta iteraci\u00f3n, los otros pliegues $k-1$ se combinan para formar los datos de entrenamiento. Por lo tanto, en cada iteraci\u00f3n tenemos $(k-1) / k$ de los datos utilizados para el entrenamiento y $1 / k$ utilizado para la evaluaci\u00f3n.</p> <p>Cada iteraci\u00f3n produce un modelo, y por lo tanto una estimaci\u00f3n del rendimiento de la generalizaci\u00f3n, por ejemplo, una estimaci\u00f3n de la precisi\u00f3n. Una vez finalizada la validaci\u00f3n cruzada, todos los ejemplos se han utilizado s\u00f3lo una vez para evaluar pero $k -1$ veces para entrenar. En este punto tenemos estimaciones de rendimiento de todos los pliegues y podemos calcular la media y la desviaci\u00f3n est\u00e1ndar de la precisi\u00f3n del modelo.</p> <p></p>"},{"location":"lectures/machine_learning/over_02/#mas-datos-y-curvas-de-aprendizaje","title":"M\u00e1s datos y curvas de aprendizaje\u00b6","text":"<ul> <li>Muchas veces, reducir el Sobreajuste es tan f\u00e1cil como conseguir m\u00e1s datos, dame m\u00e1s datos y te predecir\u00e9 el futuro!.</li> <li>En la vida real nunca es una tarea tan sencilla conseguir m\u00e1s datos.</li> <li>Una t\u00e9cnica para reducir el sobreajuste son las curvas de aprendizaje, las cuales grafican la precisi\u00f3n en funci\u00f3n del tama\u00f1o de los datos de entrenamiento.</li> </ul>"},{"location":"lectures/machine_learning/over_02/#optimizacion-de-parametros-con-grid-search","title":"Optimizaci\u00f3n de par\u00e1metros con Grid Search\u00b6","text":"<p>La mayor\u00eda de los modelos de Machine Learning cuentan con varios par\u00e1metros para ajustar su comportamiento, por lo tanto, otra alternativa que tenemos para reducir el Sobreajuste es optimizar estos par\u00e1metros por medio de un proceso conocido como grid search e intentar encontrar la combinaci\u00f3n ideal que nos proporcione mayor precisi\u00f3n.</p> <p>El enfoque que utiliza grid search es bastante simple, se trata de una b\u00fasqueda exhaustiva por el paradigma de fuerza bruta en el que se especifica una lista de valores para diferentes par\u00e1metros, y la computadora eval\u00faa el rendimiento del modelo para cada combinaci\u00f3n de \u00e9stos par\u00e1metros para obtener el conjunto \u00f3ptimo que nos brinda el mayor rendimiento.</p> <p></p>"},{"location":"lectures/machine_learning/over_02/#reduccion-de-dimensionalidad","title":"Reducci\u00f3n de dimensionalidad\u00b6","text":"<p>La reducci\u00f3n de dimensiones es frecuentemente usada como una etapa de preproceso en el entrenamiento de sistemas, y consiste en escoger un subconjunto de variables, de tal manera, que el espacio de caracter\u00edsticas quede \u00f3ptimamente reducido de acuerdo a un criterio de evaluaci\u00f3n, cuyo fin es distinguir el subconjunto que representa mejor el espacio inicial de entrenamiento.</p> <p>Como cada caracter\u00edstica que se incluye en el an\u00e1lisis, puede incrementar el costo y el tiempo de proceso de los sistemas, hay una fuerte motivaci\u00f3n para dise\u00f1ar e implementar sistemas con peque\u00f1os conjuntos de caracter\u00edsticas. Sin dejar de lado, que al mismo tiempo, hay una opuesta necesidad de incluir un conjunto suficiente de caracter\u00edsticas para lograr un alto rendimiento.</p> <p>La reducci\u00f3n de dimensionalidad se puede separar en dos tipos: Extracci\u00f3n de atributos y  Selecci\u00f3n de aributos.</p>"},{"location":"lectures/machine_learning/over_02/#extraccion-de-atributos","title":"Extracci\u00f3n de atributos\u00b6","text":"<p>La extracci\u00f3n de atributos comienza a partir de un conjunto inicial de datos medidos y crea valores derivados (caracter\u00edsticas) destinados a ser informativos y no redundantes, lo que facilita los pasos de aprendizaje y generalizaci\u00f3n posteriores, y en algunos casos conduce a a mejores interpretaciones humanas.</p> <p>Cuando los datos de entrada a un algoritmo son demasiado grandes para ser procesados y se sospecha que son redundantes (por ejemplo, la misma medici\u00f3n en pies y metros, o la repetitividad de las im\u00e1genes presentadas como p\u00edxeles), entonces se puede transformar en un conjunto reducido de caracter\u00edsticas (tambi\u00e9n denominado un vector de caracter\u00edsticas).</p> <p>Estos algoritmos fueron analizados con profundidad en la secci\u00f3n de An\u00e1lisis no supervisados - Reducci\u00f3n de la dimensionalidad.</p>"},{"location":"lectures/machine_learning/over_02/#seleccion-de-atributos","title":"Selecci\u00f3n de atributos\u00b6","text":"<p>Proceso por el cual seleccionamos un subconjunto de atributos (representados por cada una de las columnas en un datasetde forma tabular) que son m\u00e1s relevantes para la construcci\u00f3n del modelo predictivo sobre el que estamos trabajando.</p> <p>El objetivo de la selecci\u00f3n de atributos es :</p> <ul> <li>mejorar la capacidad predictiva de nuestro modelo,</li> <li>proporcionando modelos predictivos m\u00e1s r\u00e1pidos y eficientes,</li> <li>proporcionar una mejor comprensi\u00f3n del proceso subyacente que gener\u00f3 los datos.</li> </ul> <p>Los m\u00e9todos de selecci\u00f3n de atributos se pueden utilizar para identificar y eliminar los atributos innecesarios, irrelevantes y redundantes que no contribuyen a la exactitud del modelo predictivo o incluso puedan disminuir su precisi\u00f3n.</p>"},{"location":"lectures/machine_learning/over_02/#referencias","title":"Referencias\u00b6","text":"<ol> <li>K-Fold Cross Validation</li> <li>Cross Validation and Grid Search for Model Selection in Python</li> <li>Feature selection for supervised models using SelectKBest</li> </ol>"},{"location":"lectures/machine_learning/reg_01/","title":"Regresi\u00f3n I","text":"<p>Existen algunas situaciones donde los modelos lineales no son apropiados:</p> <ul> <li>El rango de valores de $Y$ est\u00e1 restringido (ejemplo: datos binarios o de conteos).</li> <li>La varianza de $Y$ depende de la media.</li> </ul> <p>La metodolog\u00eda para encontrar los par\u00e1metros $\\beta$ para el caso de la regresi\u00f3n lineal multiple se extienden de manera natural del modelo de regresi\u00f3n lineal multiple, cuya soluci\u00f3n viene dada por:</p> <p>$$\\beta = (XX^{\\top})^{-1}X^{\\top}y$$</p> <ol> <li><p>M\u00e9tricas absolutas: Las m\u00e9tricas absolutas o no escalada miden el error sin escalar los valores. Las m\u00e9trica absolutas m\u00e1s ocupadas son:</p> <ul> <li>Mean Absolute Error (MAE)</li> </ul> <p>$$\\textrm{MAE}(y,\\hat{y}) = \\dfrac{1}{n}\\sum_{t=1}^{n}\\left | y_{t}-\\hat{y}_{t}\\right |$$</p> <ul> <li>Mean squared error (MSE):</li> </ul> <p>$$\\textrm{MSE}(y,\\hat{y}) =\\dfrac{1}{n}\\sum_{t=1}^{n}\\left ( y_{t}-\\hat{y}_{t}\\right )^2$$</p> </li> </ol> <ol> <li><p>M\u00e9tricas Porcentuales: Las m\u00e9tricas porcentuales o escaladas miden el error de manera escalada, es decir, se busca acotar el error entre valores de 0 a 1, donde 0 significa que el ajuste es perfecto, mientras que 1 ser\u00eda un mal ajuste. Cabe destacar que muchas veces las m\u00e9tricas porcentuales puden tener valores mayores a 1.Las m\u00e9trica Porcentuales m\u00e1s ocupadas son:</p> <ul> <li>Mean absolute percentage error (MAPE):</li> </ul> <p>$$\\textrm{MAPE}(y,\\hat{y}) = \\dfrac{1}{n}\\sum_{t=1}^{n}\\left | \\frac{y_{t}-\\hat{y}_{t}}{y_{t}} \\right |$$</p> <ul> <li>Symmetric mean absolute percentage error (sMAPE):</li> </ul> <p>$$\\textrm{sMAPE}(y,\\hat{y}) = \\dfrac{1}{n}\\sum_{t=1}^{n} \\frac{\\left |y_{t}-\\hat{y}_{t}\\right |}{(\\left | y_{t} \\right |^2+\\left | \\hat{y}_{t} \\right |^2)/2}$$</p> </li> </ol> <p>IMPORTANTE:</p> <ul> <li><p>Cabe destacar que el coeficiente $r^2$ funciona bien en el contexto del mundo de las regresiones lineales. Para el an\u00e1lisis de modelos no lineales, esto coeficiente pierde su interpretaci\u00f3n.</p> </li> <li><p>Se deja la siguiente refrerencia para comprender conceptos claves de test de hip\u00f3tesis, intervalos de confianza, p-valor. Estos t\u00e9rminos son escenciales para comprender la significancia del ajuste realizado.</p> </li> <li><p>Existen muchas m\u00e1s m\u00e9tricas, pero estas son las m\u00e1s usulaes de encontrar. En el archivo metrics.py se definen las distintas m\u00e9tricas presentadas, las cuales serpan de utilidad m\u00e1s adelante.</p> </li> </ul> In\u00a0[1]: Copied! <pre># librerias\n \nimport os\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \npd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes\n\n# Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab\n%matplotlib inline\n</pre> # librerias   import os import numpy as np import pandas as pd  import matplotlib.pyplot as plt import seaborn as sns  pd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes  # Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab %matplotlib inline In\u00a0[2]: Copied! <pre># ejemplo sencillo\n\nn = 100 \nnp.random.seed(n)\n\nbeta = np.array([1,1]) # coeficientes\nx =  np.random.rand(n) # variable independiente\n\nmu, sigma = 0, 0.1 # media y desviacion estandar\nepsilon = np.random.normal(mu, sigma, n) # ruido blanco\n\ny = np.dot(np.c_[ np.ones(n),x]  , beta) + epsilon # variables dependientes\n\n# generar dataframe\ndf = pd.DataFrame({\n    'x':x,\n    'y':y\n})\ndf.head()\n</pre> # ejemplo sencillo  n = 100  np.random.seed(n)  beta = np.array([1,1]) # coeficientes x =  np.random.rand(n) # variable independiente  mu, sigma = 0, 0.1 # media y desviacion estandar epsilon = np.random.normal(mu, sigma, n) # ruido blanco  y = np.dot(np.c_[ np.ones(n),x]  , beta) + epsilon # variables dependientes  # generar dataframe df = pd.DataFrame({     'x':x,     'y':y }) df.head() Out[2]: x y 0 0.543405 1.612417 1 0.278369 1.347058 2 0.424518 1.267849 3 0.844776 1.935274 4 0.004719 1.082601 <p>Grafiquemos los puntos en el plano cartesiano.</p> In\u00a0[3]: Copied! <pre># grafico de puntos\nsns.set(rc={'figure.figsize':(10,8)})\nsns.scatterplot(\n    x='x',\n    y='y',\n    data=df,\n)  \nplt.show()\n</pre> # grafico de puntos sns.set(rc={'figure.figsize':(10,8)}) sns.scatterplot(     x='x',     y='y',     data=df, )   plt.show() <p>Lo primero que debemos hacer es separar nuestro datos en los conjuntos de training set y test set. Concepto de  Train set y Test set</p> <p>Al momento de entrenar los modelos de machine leraning, se debe tener un conjunto para poder entrenar el modelo y otro conjunto para poder evaluar el modelo. Es por esto que el conjunto de datos se separ\u00e1 en dos conjuntos:</p> <ul> <li><p>Train set: Conjunto de entrenamiento con el cual se entrenar\u00e1n los algoritmos de machine learning.</p> </li> <li><p>Test set: Conjunto de testeo para averiguar la confiabilidad del modelo, es decir, cuan bueno es el ajuste del modelo.</p> </li> </ul> <p></p> <p>Tama\u00f1o ideal de cada conjunto</p> <p>La respuesta depende fuertemente del tama\u00f1o del conjunto de datos. A modo de regla emp\u00edrica, se considerar\u00e1 el tama\u00f1o \u00f3ptimo basado en la siguiente tabla:</p> n\u00famero de filas train set test set entre 100-1000 67% 33% entre 1.000- 100.000 80% 20% mayor a 100.000 99% 1% In\u00a0[4]: Copied! <pre>from sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\n# import some data to play with\n\nX = df[['x']] # we only take the first two features.\ny = df['y']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# print rows train and test sets\nprint('Separando informacion:\\n')\nprint('numero de filas data original : ',len(X))\nprint('numero de filas train set     : ',len(X_train))\nprint('numero de filas test set      : ',len(X_test))\n</pre> from sklearn import datasets from sklearn.model_selection import train_test_split  # import some data to play with  X = df[['x']] # we only take the first two features. y = df['y']  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # print rows train and test sets print('Separando informacion:\\n') print('numero de filas data original : ',len(X)) print('numero de filas train set     : ',len(X_train)) print('numero de filas test set      : ',len(X_test)) <pre>Separando informacion:\n\nnumero de filas data original :  100\nnumero de filas train set     :  80\nnumero de filas test set      :  20\n</pre> <p>Existen varias librer\u00edas para poder aplicar modelos de regresi\u00f3n, de los cuales la atenci\u00f3n estar\u00e1 enfocada en las librer\u00edas de <code>statsmodels</code> y <code>sklearn</code>.</p> In\u00a0[5]: Copied! <pre>import statsmodels.api as sm\n\nmodel = sm.OLS(y_train, sm.add_constant(X_train))\nresults = model.fit()\n</pre> import statsmodels.api as sm  model = sm.OLS(y_train, sm.add_constant(X_train)) results = model.fit() <p>En <code>statsmodel</code> existe un comando para ver informaci\u00f3n del modelo en estudio mediante el comando <code>summary</code></p> In\u00a0[6]: Copied! <pre># resultados del modelo\nprint(results.summary())\n</pre> # resultados del modelo print(results.summary()) <pre>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.894\nModel:                            OLS   Adj. R-squared:                  0.893\nMethod:                 Least Squares   F-statistic:                     658.4\nDate:                Sat, 22 Jul 2023   Prob (F-statistic):           8.98e-40\nTime:                        18:28:07   Log-Likelihood:                 69.472\nNo. Observations:                  80   AIC:                            -134.9\nDf Residuals:                      78   BIC:                            -130.2\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.9805      0.021     46.338      0.000       0.938       1.023\nx              1.0099      0.039     25.659      0.000       0.932       1.088\n==============================================================================\nOmnibus:                        0.424   Durbin-Watson:                   1.753\nProb(Omnibus):                  0.809   Jarque-Bera (JB):                0.587\nSkew:                           0.102   Prob(JB):                        0.746\nKurtosis:                       2.633   Cond. No.                         4.17\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n</pre> <p>A continuaci\u00f3n se dara una interpretaci\u00f3n de esta tabla:</p> <p>Descripci\u00f3n del Modelo</p> <p>Estos son estad\u00edsticas relacionadas a la ejecuci\u00f3n del modelo.</p> Variable Descripi\u00f3n Dep. Variable Nombre de la variables dependiente Model Nombre del modelo ocupado Method M\u00e9todo para encontrar los par\u00e1metros \u00f3ptimos Date Fecha de ejecuci\u00f3n No. Observations N\u00famero de observaciones Df Residuals Grados de libertas de los residuos Df Model Grados de libertad del modelo Covariance Type Tipo de covarianza <p>Ajustes del Modelo</p> <p>Estos son estad\u00edsticas relacionadas con la verosimilitud y la confiabilidad del modelo.</p> Variable Descripi\u00f3n R-squared Valor del R-cuadrado Adj. R-squared Valor del R-cuadrado ajustado F-statistic Test para ver si todos los par\u00e1metros son iguales a cero Prob (F-statistic) Probabilidad Asociada al test Log-Likelihood Logaritmo de la funci\u00f3n de verosimilitud AIC Valor del estad\u00edstico AIC BIC Valor del estad\u00edstico BIC <p>En este caso, tanto el r-cuadrado como el r-cuadrado ajustado est\u00e1n cerca del 0.9, se tiene un buen ajuste lineal de los datos. Adem\u00e1s, el test F nos da una probabilidad menor al 0.05, se rechaza la hip\u00f3tess nula que los coeficientes son iguales de cero.</p> <p>Par\u00e1metros del modelo</p> <p>La tabla muestra los valores asociados a los par\u00e1metros del modelo</p> coef std err t P&gt;|t| [0.025 0.975] const 0.9805 0.021 46.338 0.000 0.938 1.023 x 1.0099 0.039 25.659 0.000 0.932 1.088 <p>Ac\u00e1 se tiene:</p> <ul> <li>Variables: Las variables en estudio son <code>const</code> (intercepto) y <code>x</code>.</li> <li>coef: Valor estimado del coeficiente.</li> <li>std err: Desviaci\u00f3n estandar del estimador.</li> <li>t: t = estimate/std error.</li> <li>P&gt;|t|:p-valor individual para cada par\u00e1metro para aceptar o rechazar hip\u00f3tesis nula (par\u00e1metros significativamente distinto de cero).</li> <li>[0.025 | 0.975]: Intervalo de confianza de los par\u00e1metros</li> </ul> <p>En este caso, los valores estimados son cercanos a 1 (algo esperable debido a la simulaci\u00f3n realizadas), adem\u00e1s, se observa que cada uno de los par\u00e1metros es significativamente distinto de cero.</p> <p>Estad\u00edsticos interesantes del modelo</p> Variable Descripci\u00f3n Omnibus Prueba de la asimetr\u00eda y curtosis de los residuos Prob(Omnibus) Probabilidad de que los residuos se distribuyan normalmente Skew Medida de simetr\u00eda de los datos Kurtosis Medida de curvatura de los datos Durbin-Watson Pruebas de homocedasticidad Jarque-Bera (JB) Como la prueba Omnibus, prueba tanto el sesgo como la curtosis. Prob(JB) Probabilidad de que los residuos se distribuyan normalmente Cond. No. N\u00famero de condici\u00f3n. Mide la sensibilidad de la salida de una funci\u00f3n en comparaci\u00f3n con su entrada <p>En este caso:</p> <ul> <li><p>Tanto el test de Omnibus como el test  Jarque-Bera nos arroja una probabilidad cercana a uno, lo cual confirma la hip\u00f3tesis que los residuos se distribuyen de manera normal.</p> </li> <li><p>Para el test de Durbin-Watson, basados en la tablas de valores(tama\u00f1o de la muestra 80 y n\u00famero de variables 2), se tiene que los l\u00edmites para asumir que no existe correlaci\u00f3n en los residuos es de: $[d_u,4-d_u]=[1.66,2.34]$, dado que el valor obtenido (1.753) se encuentra dentro de este rango, se concluye que no hay autocorrelaci\u00f3n de los residuos.</p> </li> <li><p>El n\u00famero de condici\u00f3n es peque\u00f1o (podemos asumir que menor a 30 es un buen resultado) por lo que podemos asumir que no hay colinealidad de los datos.</p> </li> </ul> <p>Ahora, para convencernos de manera visual de los resultados, realicemos un gr\u00e1fico con el ajuste lineal:</p> In\u00a0[7]: Copied! <pre># grafico de puntos\nsns.lmplot(\n    x='x',\n    y='y',\n    data=df,\n    height = 8,\n)  \nplt.show()\n</pre> # grafico de puntos sns.lmplot(     x='x',     y='y',     data=df,     height = 8, )   plt.show() In\u00a0[8]: Copied! <pre># predicciones\ny_pred = results.predict(sm.add_constant(X_test))\n</pre> # predicciones y_pred = results.predict(sm.add_constant(X_test)) <p>Ahora, analizaremos las m\u00e9tricas de error asociado a las predicciones del modelo:</p> In\u00a0[9]: Copied! <pre>from sklearn.metrics import mean_absolute_error, mean_squared_error\n\ndef mean_absolute_percentage_error(y_true, y_pred):\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\ndef regression_metrics(df):\n    \"\"\"\n    Aplicar las distintas m\u00e9tricas definidas\n    :param df: DataFrame con las columnas: ['y', 'yhat']\n    :return: DataFrame con las m\u00e9tricas especificadas\n    \"\"\"\n    df_result = pd.DataFrame()\n\n    y_true = df['y']\n    y_pred = df['yhat']\n\n    df_result['mae'] = [round(mean_absolute_error(y_true, y_pred), 4)]\n    df_result['mse'] = [round(mean_squared_error(y_true, y_pred), 4)]\n    df_result['rmse'] = [round(np.sqrt(mean_squared_error(y_true, y_pred)), 4)]\n    df_result['mape'] = [round(mean_absolute_percentage_error(y_true, y_pred), 4)]\n    df_result['smape'] = [round(2 * mean_absolute_percentage_error(y_true, y_pred) / (mean_absolute_percentage_error(y_true, y_pred) + 100), 4)]\n\n    return df_result\n</pre> from sklearn.metrics import mean_absolute_error, mean_squared_error  def mean_absolute_percentage_error(y_true, y_pred):     return np.mean(np.abs((y_true - y_pred) / y_true)) * 100  def regression_metrics(df):     \"\"\"     Aplicar las distintas m\u00e9tricas definidas     :param df: DataFrame con las columnas: ['y', 'yhat']     :return: DataFrame con las m\u00e9tricas especificadas     \"\"\"     df_result = pd.DataFrame()      y_true = df['y']     y_pred = df['yhat']      df_result['mae'] = [round(mean_absolute_error(y_true, y_pred), 4)]     df_result['mse'] = [round(mean_squared_error(y_true, y_pred), 4)]     df_result['rmse'] = [round(np.sqrt(mean_squared_error(y_true, y_pred)), 4)]     df_result['mape'] = [round(mean_absolute_percentage_error(y_true, y_pred), 4)]     df_result['smape'] = [round(2 * mean_absolute_percentage_error(y_true, y_pred) / (mean_absolute_percentage_error(y_true, y_pred) + 100), 4)]      return df_result In\u00a0[10]: Copied! <pre>from sklearn.metrics import r2_score\n\n# ejemplo \ndf_temp = pd.DataFrame(\n    {\n        'y':y_test,\n        'yhat': y_pred\n        }\n)\n\nprint('\\nMetricas para el regresor consumo_litros_milla:\\n')\nregression_metrics(df_temp)\n</pre> from sklearn.metrics import r2_score  # ejemplo  df_temp = pd.DataFrame(     {         'y':y_test,         'yhat': y_pred         } )  print('\\nMetricas para el regresor consumo_litros_milla:\\n') regression_metrics(df_temp) <pre>\nMetricas para el regresor consumo_litros_milla:\n\n</pre> Out[10]: mae mse rmse mape smape 0 0.1028 0.0171 0.1309 6.7733 0.1269 <p>Funci\u00f3n de Autocorrelaci\u00f3n</p> <p>La funci\u00f3n de autocorrelaci\u00f3n muestra que los residuos se encuentra dentro de la banda de valores cr\u00edticos $(-0.2,0.2)$, concluyendo que no existe correlaci\u00f3n entre los residuos.</p> In\u00a0[11]: Copied! <pre>from statsmodels.graphics.tsaplots import plot_acf\nsns.set(rc={'figure.figsize':(12,8)})\n\n# funcion de autocorrelation\nplot_acf(results.resid)\nplt.show()\n</pre> from statsmodels.graphics.tsaplots import plot_acf sns.set(rc={'figure.figsize':(12,8)})  # funcion de autocorrelation plot_acf(results.resid) plt.show() <p>QQ-plot</p> <p>La gr\u00e1fica de qq-plot nos muestra una comparaci\u00f3n en las distribuci\u00f3n de los residuos respecto a una poblaci\u00f3n con una distribuci\u00f3n normal. En este caso, los puntos (que representan la distribuci\u00f3n de los errores) se encuentran cercana a la recta (distribuci\u00f3n normal), concluyendo que la distribuci\u00f3n de los residuos sigue una distribuci\u00f3n normal.</p> In\u00a0[12]: Copied! <pre>import scipy.stats as stats\nfig = sm.qqplot(results.resid, stats.t, fit=True, line=\"45\")\nplt.show()\n</pre> import scipy.stats as stats fig = sm.qqplot(results.resid, stats.t, fit=True, line=\"45\") plt.show() <p>Histograma</p> <p>Esta es una comparaci\u00f3n directa enntre la distribuci\u00f3n de los residuos versus la distribuci\u00f3n de una variable normal mediante un histograma.</p> In\u00a0[13]: Copied! <pre>df_hist = pd.DataFrame({'error':results.resid})\nsns.histplot(\n    x='error',\n    data=df_hist,\n    kde=True,\n     bins=15\n)  \nplt.show()\n</pre> df_hist = pd.DataFrame({'error':results.resid}) sns.histplot(     x='error',     data=df_hist,     kde=True,      bins=15 )   plt.show() <p>A modo de conclusi\u00f3n, es correcto asumir que los errores siguen la distribuci\u00f3n de un ruido blanco, cumpliendo correctamente con los supuestos de la regresi\u00f3n lineal.</p> In\u00a0[14]: Copied! <pre># ejemplo sencillo\n\nn = 100 \nnp.random.seed(n)\n\nbeta = np.array([1,1]) # coeficientes\nx =  np.random.rand(n) # variable independiente\n</pre> # ejemplo sencillo  n = 100  np.random.seed(n)  beta = np.array([1,1]) # coeficientes x =  np.random.rand(n) # variable independiente  In\u00a0[15]: Copied! <pre>mu, sigma = 0, 0.1 # media y desviacion estandar\nepsilon = np.random.normal(mu, sigma, n) # ruido blanco\n\ny = np.dot(np.c_[ np.ones(n),x]  , beta) + epsilon # variables dependientes\n\ny[:10] = 3.1 # contaminacion\nx[10] = x[10]-1\ny[10]= y[10]-1\nx[11] = x[11] +1\ny[11] = y[11]+1\n\n# etiqueta\noutlier = np.zeros(n)\noutlier[:10] = 1\noutlier[10:12] = 2\n\n# generar dataframe\ndf = pd.DataFrame({\n    'x':x,\n    'y':y,\n    'outlier':outlier\n})\n</pre> mu, sigma = 0, 0.1 # media y desviacion estandar epsilon = np.random.normal(mu, sigma, n) # ruido blanco  y = np.dot(np.c_[ np.ones(n),x]  , beta) + epsilon # variables dependientes  y[:10] = 3.1 # contaminacion x[10] = x[10]-1 y[10]= y[10]-1 x[11] = x[11] +1 y[11] = y[11]+1  # etiqueta outlier = np.zeros(n) outlier[:10] = 1 outlier[10:12] = 2  # generar dataframe df = pd.DataFrame({     'x':x,     'y':y,     'outlier':outlier }) In\u00a0[16]: Copied! <pre># grafico de puntos\nsns.set(rc={'figure.figsize':(10,8)})\nsns.scatterplot(\n    x='x',\n    y='y',\n    hue='outlier',\n    data=df,\n    palette = ['blue','red','black']\n)  \nplt.show()\nplt.show()\n</pre>  # grafico de puntos sns.set(rc={'figure.figsize':(10,8)}) sns.scatterplot(     x='x',     y='y',     hue='outlier',     data=df,     palette = ['blue','red','black'] )   plt.show() plt.show() <p>En este caso, se tiene dos tipos de outliers en este caso:</p> <ul> <li>Significativos: Aquellos outliers que afectan la regresi\u00f3n cambiando la tendencia a este grupo de outliers (puntos rojos).</li> <li>No significativo: Si bien son datos at\u00edpicos  puesto que se encuentran fuera de la nube de puntos, el ajuste de la regresi\u00f3n lineal no se ve afectado (puntos negros).</li> </ul> <p>Veamos el ajuste lineal.</p> In\u00a0[17]: Copied! <pre># grafico de puntos\nsns.lmplot(\n    x='x',\n    y='y',\n    data=df,\n    height = 8,\n)  \nplt.show()\n</pre> # grafico de puntos sns.lmplot(     x='x',     y='y',     data=df,     height = 8, )   plt.show() <p>Otro gr\u00e1fico de inter\u00e9s, es el gr\u00e1fico de influencia, que analiza la distancia de Cook de los residuos.</p> In\u00a0[18]: Copied! <pre># modelos de influencia\nX = df[['x']] # we only take the first two features.\ny = df['y']\nmodel = sm.OLS(y, sm.add_constant(X))\nresults = model.fit()\nsm.graphics.influence_plot(results)\nplt.show()\n</pre> # modelos de influencia X = df[['x']] # we only take the first two features. y = df['y'] model = sm.OLS(y, sm.add_constant(X)) results = model.fit() sm.graphics.influence_plot(results) plt.show() <p>Los puntos grandes se interpretan como puntos que tienen una alta influencia sobre la regresi\u00f3n lineal, mientras aquellos puntos peque\u00f1os tienen una influencia menor.</p> <p>En este caso, la recta se ve fuertemente afectadas por estos valores. Para estos casos se pueden hacer varias cosas:</p> <ul> <li><p>Eliminaci\u00f3n de los outliers: Una vez identificado los outliers (algo que no es tan trivial de identificar para datos multivariables), se puden eliminar y seguir con el paso de modelado.</p> <ul> <li>Ventajas: F\u00e1cil de trabajar la data para los modelos que dependen fuertemente de la media de los datos.</li> <li>Desventajas: Para el caso multivariables no es t\u00e1n trivial encontrar outliers.</li> </ul> </li> <li><p>Modelos m\u00e1s robustos a outliers: Se pueden aplicar otros modelos de regresi\u00f3n cuya estimaci\u00f3n de los par\u00e1metros, no se vea afectado por los valores de outliers.</p> <ul> <li>Ventajas: El an\u00e1lisis se vuelve independiente de los datos.</li> <li>Desventajas: Modelos m\u00e1s costoso computacionalmente y/o m\u00e1s complejos de implementar.</li> </ul> </li> </ul>"},{"location":"lectures/machine_learning/reg_01/#regresion-i","title":"Regresi\u00f3n I\u00b6","text":""},{"location":"lectures/machine_learning/reg_01/#regression-lineal","title":"Regressi\u00f3n Lineal\u00b6","text":"<p>El modelo de regresio\u0301n lineal general o modelo de regresi\u00f3n multiple,  supone que, $\\boldsymbol{Y} =  \\boldsymbol{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon},$ donde:</p> <ul> <li>$\\boldsymbol{X} = (x_1,...,x_n)^{T}$: variable explicativa</li> <li>$\\boldsymbol{Y} = (y_1,...,y_n)^{T}$: variable respuesta</li> <li>$\\boldsymbol{\\epsilon} = (\\epsilon_1,...,\\epsilon_n)^{T}$: error se asume un ruido blanco, es decir, $\\epsilon \\sim \\mathcal{N}( \\boldsymbol{0},\\sigma^2I)$</li> <li>$\\boldsymbol{\\beta} = (\\beta_1,...,\\beta_n)^{T}$: coeficientes de regresio\u0301n.</li> </ul> <p>La idea es tratar de establecer la relaci\u00f3n entre las variables independientes y dependientes por medio de ajustar el mejor hyper plano con respecto a los puntos.</p> <p>Por ejemplo, para el caso de la regresi\u00f3n lineal simple, se tiene la siguiente estructura: $y_i=\\beta_0+\\beta_1x_i+\\epsilon_i.$ En este caso, la regresi\u00f3n lineal corresponder\u00e1 a la recta que mejor pasa por los puntos observados.</p>"},{"location":"lectures/machine_learning/reg_01/#mejores-paremetros-metodo-de-minimos-cudrados","title":"Mejores par\u00e9metros: M\u00e9todo de minimos cudrados\u00b6","text":"<p>El m\u00e9todo de m\u00ednimos cudrados es un m\u00e9todo de optimizaci\u00f3n que busca encontrar la mejor aproximaci\u00f3n mediante la minimizaci\u00f3n de los residuos al cuadrado, es decir, se buscar encontrar:</p> <p>$$(P)\\ \\min \\sum_{i=1}^n e_{i}^2 =\\sum_{i=1}^n (y_{i}-f_{i}(x;\\beta))^2   $$</p> <p>Para el caso de la regresi\u00f3n lineal simple, se busca una funci\u00f3n $$f(x;\\beta) = \\beta_{0} + \\beta_{1}x,$$</p> <p>por lo tanto el problema que se debe resolver es el siguiente:</p> <p>$$(P)\\ \\min \\sum_{i=1}^n e_{i}^2 =\\dfrac{1}{n}\\sum_{i=1}^{n}\\left ( y_{i}-(\\beta_{0} + \\beta_{1}x_{i})\\right )^2$$</p> <p>Lo que significa, que para este problema, se debe encontrar $\\beta = (\\beta_{0},\\beta_{1})$ que minimicen el problema de optimizaci\u00f3n. En este caso la soluci\u00f3n viene dada por:</p> <p>$$\\hat{\\beta}_{1} = \\dfrac{\\sum(x-\\bar{x})(y-\\bar{y})}{\\sum(x-\\bar{x})^2} = \\rho (x,y)\\ ; \\  \\hat{\\beta}_{0} = \\bar{y}-\\hat{\\beta}_{1} \\bar{x} $$</p>"},{"location":"lectures/machine_learning/reg_01/#seleccion-de-modelos","title":"Selecci\u00f3n de modelos\u00b6","text":""},{"location":"lectures/machine_learning/reg_01/#criterio-de-informacion-de-akaike-aic","title":"Criterio de informaci\u00f3n de Akaike (AIC)\u00b6","text":"<p>El criterio de informaci\u00f3n de Akaike (AIC) es una medida de la calidad relativa de un modelo estad\u00edstico, para un conjunto dado de datos. Como tal, el AIC proporciona un medio para la selecci\u00f3n del modelo.</p> <p>AIC maneja un trade-off entre la bondad de ajuste del modelo y la complejidad del modelo. Se basa en la entrop\u00eda de informaci\u00f3n: se ofrece una estimaci\u00f3n relativa de la informaci\u00f3n perdida cuando se utiliza un modelo determinado para representar el proceso que genera los datos.</p> <p>AIC no proporciona una prueba de un modelo en el sentido de probar una hip\u00f3tesis nula, es decir AIC no puede decir nada acerca de la calidad del modelo en un sentido absoluto. Si todos los modelos candidatos encajan mal, AIC no dar\u00e1 ning\u00fan aviso de ello.</p> <p>En el caso general, el AIC es</p> <p>$$AIC = 2k-2\\ln(L)$$</p> <p>donde $k$ es el n\u00famero de par\u00e1metros en el modelo estad\u00edstico , y $L$ es el m\u00e1ximo valor de la funci\u00f3n de verosimilitud para el modelo estimado.</p>"},{"location":"lectures/machine_learning/reg_01/#criterio-de-informacion-bayesiano-bic","title":"Criterio de informaci\u00f3n bayesiano (BIC)\u00b6","text":"<p>En estad\u00edstica, el criterio de informaci\u00f3n bayesiano (BIC) o el m\u00e1s general criterio de Schwarz (SBC tambi\u00e9n, SBIC) es un criterio para la selecci\u00f3n de modelos entre un conjunto finito de modelos. Se basa, en parte, de la funci\u00f3n de probabilidad y que est\u00e1 estrechamente relacionado con el Criterio de Informaci\u00f3n de Akaike (AIC).</p> <p>Cuando el ajuste de modelos, es posible aumentar la probabilidad mediante la adici\u00f3n de par\u00e1metros, pero si lo hace puede resultar en sobreajuste. Tanto el BIC y AIC resuelven este problema mediante la introducci\u00f3n de un t\u00e9rmino de penalizaci\u00f3n para el n\u00famero de par\u00e1metros en el modelo, el t\u00e9rmino de penalizaci\u00f3n es mayor en el BIC que en el AIC.</p> <p>El BIC fue desarrollado por Gideon E. Schwarz, quien dio un argumento bayesiano a favor de su adopci\u00f3n.1\u200b Akaike tambi\u00e9n desarroll\u00f3 su propio formalismo Bayesiano, que ahora se conoce como la ABIC por Criterio de Informaci\u00f3n Bayesiano de Akaike \"</p> <p>En el caso general, el BIC es</p> <p>$$BIC =k\\ln(n)-2\\ln(L)$$</p> <p>donde $k$ es el n\u00famero de par\u00e1metros en el modelo estad\u00edstico, $n$ es la cantidad de datos disponibles y $L$ es el m\u00e1ximo valor de la funci\u00f3n de verosimilitud para el modelo estimado.</p>"},{"location":"lectures/machine_learning/reg_01/#r-cuadrado","title":"R-cuadrado\u00b6","text":"<p>El coeficiente de determinaci\u00f3n o R-cuadrado ($r^2$ ) , es un estad\u00edstico usado en el contexto de un modelo estad\u00edstico cuyo principal prop\u00f3sito es predecir futuros resultados o probar una hip\u00f3tesis. El coeficiente determina la calidad del modelo para replicar los resultados, y la proporci\u00f3n de variaci\u00f3n de los resultados que puede explicarse por el modelo.</p> <p>El valor del $r^2$ habitualmente entre 0 y 1, donde 0 significa una mala calidad de ajuste en el modelo y 1 corresponde a un ajuste lineal perfecto. A menudo, este estad\u00edstico es ocupado para modelos lineales.</p> <p>Se define por la f\u00f3rmula:</p> <p>$$r^2 = \\dfrac{SS_{reg}}{SS_{tot}} = 1 - \\dfrac{SS_{res}}{SS_{tot}},$$</p> <p>donde:</p> <ul> <li><p>$SS_{reg}$ ( suma explicada de cuadrados (ESS)): $\\sum_{i}(\\hat{y}-\\bar{y})^2$</p> </li> <li><p>$SS_{res}$: ( suma residual de cuadrados (RSS)): $\\sum_{i}(y_{i}-\\hat{y})^2 = \\sum_{i}e_{i}^2$</p> </li> <li><p>$SS_{tot}$: ( varianza): $\\sum_{i}(y_{i}-\\bar{y})$, donde: $SS_{tot}=SS_{reg}+SS_{res}$</p> </li> </ul> <p>En una forma general, se puede ver que $r^2$ est\u00e1 relacionado con la fracci\u00f3n de varianza inexplicada (FVU), ya que el segundo t\u00e9rmino compara la varianza inexplicada (varianza de los errores del modelo) con la varianza total (de los datos).</p> <p></p> <ul> <li><p>Las \u00e1reas de los cuadrados azules representan los residuos cuadrados con respecto a la regresi\u00f3n lineal ($SS_{tot}$).</p> </li> <li><p>Las \u00e1reas de los cuadrados rojos representan los residuos al cuadrado con respecto al valor promedio ($SS_{res}$).</p> </li> </ul> <p>Por otro lado, a medida que m\u00e1s variables explicativas se agregan al modelo, el $r^2$ aumenta de forma autom\u00e1tica, es decir, entre m\u00e1s variables explicativas se agreguen, mejor ser\u00e1 la calidad ser\u00e1 el ajuste (un falso argumento).</p> <p>Es por ello que se define el R cuadrado ajustado, que viene a ser  una modificaci\u00f3n del $r^2$, ajustando por el n\u00famero de variables explicativas en un modelo ($p$) en relaci\u00f3n con el n\u00famero de puntos de datos ($n$).</p> <p>$$r^2_{ajustado} = 1-(1-r^2)\\dfrac{n-1}{n-p-1} ,$$</p>"},{"location":"lectures/machine_learning/reg_01/#error-de-un-modelo","title":"Error de un modelo\u00b6","text":""},{"location":"lectures/machine_learning/reg_01/#definicion","title":"Definici\u00f3n\u00b6","text":"<p>El error corresponde a la diferencia entre el valor original y el valor predicho,es decir:</p> <p>$$e_{i}=y_{i}-\\hat{y}_{i} $$</p> <p></p>"},{"location":"lectures/machine_learning/reg_01/#formas-de-medir-el-error-de-un-modelo","title":"Formas de medir el error de un modelo\u00b6","text":"<p>Para medir el ajuste de un modelo se ocupan las denominadas funciones de distancias o m\u00e9tricas. Existen varias m\u00e9tricas, dentro de las cuales encontramos:</p>"},{"location":"lectures/machine_learning/reg_01/#otros-estadisticos-interesantes-del-modelo","title":"Otros estad\u00edsticos interesantes del modelo\u00b6","text":""},{"location":"lectures/machine_learning/reg_01/#test-f","title":"Test F\u00b6","text":"<p>EL test F para regresi\u00f3n lineal prueba si alguna de las variables independientes en un modelo de regresi\u00f3n lineal m\u00faltiple es significativa.</p> <p>En t\u00e9rminos de test de hip\u00f3tesis, se quiere contrastar lo siguiente:</p> <ul> <li>$H_0: \\beta_1 = \\beta_2 = ... = \\beta_{p-1} = 0$</li> <li>$H_1: \\beta_j \u2260 0$, para al menos un valor de $j$</li> </ul>"},{"location":"lectures/machine_learning/reg_01/#test-omnibus","title":"Test Omnibus\u00b6","text":"<p>EL test Omnibusesta relacionado con la simetr\u00eda y curtosis del resido. Se espera ver un valor cercano a cero que indicar\u00eda normalidad. El Prob (Omnibus) realiza una prueba estad\u00edstica que indica la probabilidad de que los residuos se distribuyan normalmente.</p>"},{"location":"lectures/machine_learning/reg_01/#test-durbin-watson","title":"Test Durbin-Watson\u00b6","text":"<p>El Test Durbin-Watson es un test de homocedasticidad. Para ver los l\u00edmites relacionados de este test, se puede consultar la siguiente tablas de valores.</p>"},{"location":"lectures/machine_learning/reg_01/#test-jarque-bera","title":"Test Jarque-Bera\u00b6","text":"<p>Como el test Omnibus en que prueba tanto el sesgo como la curtosis. Esperamos ver en esta prueba una confirmaci\u00f3n de la prueba Omnibus.</p>"},{"location":"lectures/machine_learning/reg_01/#aplicacion-con-python","title":"Aplicaci\u00f3n con python\u00b6","text":""},{"location":"lectures/machine_learning/reg_01/#ejemplo-sencillo","title":"Ejemplo sencillo\u00b6","text":"<p>Para comprender los modelos de regresi\u00f3n lineal, mostraremos un caso sencillo de uso. Para ello realizaremos un simulaci\u00f3n de una recta, en el cual le agregaremos un ruido blanco.</p>"},{"location":"lectures/machine_learning/reg_01/#ejemplo-con-statsmodel","title":"Ejemplo con Statsmodel\u00b6","text":"<p>Para trabajar los modelos de <code>statsmodel</code>, basta con instanciar el comando <code>OLS</code>. El modelo no considera intercepto, por lo tanto, para agregar el intercepto, a las variables independientes se le debe agregar un vector de unos (tanto para el conjunto de entranamiento como de testeo).</p>"},{"location":"lectures/machine_learning/reg_01/#analisis-del-error","title":"An\u00e1lisis del error\u00b6","text":""},{"location":"lectures/machine_learning/reg_01/#predicciones","title":"Predicciones\u00b6","text":"<p>Ahora que ya se tiene el modelo entrenado y se ha analizado sus principales caracter\u00edsticas, se pueden realizar predicciones de los valores que se desconocen, de la siguiente manera:</p>"},{"location":"lectures/machine_learning/reg_01/#normalidad-de-los-residuos","title":"Normalidad de los residuos\u00b6","text":"<p>Basados en los distintos test (Durbin-Watson,Omnibus,Jarque-Bera ) se concluye que los residuos del modelo son un ruido blanco. Para convencernos de esto de manera gr\u00e1fica, se realizan los siguientes gr\u00e1ficos de inter\u00e9s.</p>"},{"location":"lectures/machine_learning/reg_01/#outliers","title":"Outliers\u00b6","text":"<p>Un outlier (o valor at\u00edpico) una observaci\u00f3n que es num\u00e9ricamente distante del resto de los datos. Las estad\u00edsticas derivadas de los conjuntos de datos que incluyen valores at\u00edpicos ser\u00e1n frecuentemente enga\u00f1osas. Estos valores pueden afectar fuertemente al modelo de regresi\u00f3n log\u00edstica. Veamos un ejemplo:</p>"},{"location":"lectures/machine_learning/reg_01/#que-hacer-ante-la-presencia-de-outliers","title":"\u00bf Qu\u00e9 hacer ante la presencia de outliers?\u00b6","text":""},{"location":"lectures/machine_learning/reg_01/#conclusion","title":"Conclusi\u00f3n\u00b6","text":"<ul> <li>Los modelos de regresi\u00f3n lineal son una gran herramienta para realizar predicciones.</li> <li>Los outliers afectan considerablemente a la regresi\u00f3n lineal, por lo que se debn buscar estrategias para abordar esta problem\u00e1tica.</li> <li>En esta oportunidad se hizo un detalle t\u00e9cnico de disntintos est\u00e1disticos asociados a la regresi\u00f3n l\u00edneal (apuntando a un an\u00e1lisis inferencial ), no obstante, en los pr\u00f3ximos modelos, se estar\u00e1 interesado en analizar las predicciones del modelo y los errores asociados a ella, por lo cual los aspectos t\u00e9cnico quedar\u00e1n como lecturas complementarias.</li> <li>Existen varios casos donde los modelos de regresi\u00f3n l\u00edneal no realizan un correcto ajuste de los datos, pero es una gran herramienta para comenzar.</li> </ul>"},{"location":"lectures/machine_learning/reg_01/#referencias","title":"Referencias\u00b6","text":"<ol> <li>Linear Regression in Python</li> </ol>"},{"location":"lectures/machine_learning/reg_02/","title":"Regressi\u00f3n II","text":"In\u00a0[1]: Copied! <pre># librerias\n \nimport os\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \npd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes\n\n# Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab\n%matplotlib inline\n</pre> # librerias   import os import numpy as np import pandas as pd  import matplotlib.pyplot as plt import seaborn as sns  pd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes  # Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab %matplotlib inline In\u00a0[2]: Copied! <pre># sklearn models\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import fetch_openml\n</pre> # sklearn models from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.datasets import fetch_openml In\u00a0[3]: Copied! <pre># Load the Boston Housing dataset using fetch_openml\ndata = fetch_openml(data_id=531)\n\n# Create a DataFrame from the data\nboston_df = pd.DataFrame(data.data, columns=data.feature_names)\nboston_df['TARGET'] = data.target\nboston_df = boston_df.astype(float)\nboston_df.head()\n</pre> # Load the Boston Housing dataset using fetch_openml data = fetch_openml(data_id=531)  # Create a DataFrame from the data boston_df = pd.DataFrame(data.data, columns=data.feature_names) boston_df['TARGET'] = data.target boston_df = boston_df.astype(float) boston_df.head() <pre>C:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\db-connectors-RsEieBu8-py3.8\\lib\\site-packages\\sklearn\\datasets\\_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n  warn(\n</pre> Out[3]: CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT TARGET 0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.98 24.0 1 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.14 21.6 2 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.03 34.7 3 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.94 33.4 4 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33 36.2 In\u00a0[4]: Copied! <pre># descripcion del conjunto de datos\nboston_df.describe()\n</pre> # descripcion del conjunto de datos boston_df.describe() Out[4]: CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT TARGET count 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 mean 3.613524 11.363636 11.136779 0.069170 0.554695 6.284634 68.574901 3.795043 9.549407 408.237154 18.455534 356.674032 12.653063 22.532806 std 8.601545 23.322453 6.860353 0.253994 0.115878 0.702617 28.148861 2.105710 8.707259 168.537116 2.164946 91.294864 7.141062 9.197104 min 0.006320 0.000000 0.460000 0.000000 0.385000 3.561000 2.900000 1.129600 1.000000 187.000000 12.600000 0.320000 1.730000 5.000000 25% 0.082045 0.000000 5.190000 0.000000 0.449000 5.885500 45.025000 2.100175 4.000000 279.000000 17.400000 375.377500 6.950000 17.025000 50% 0.256510 0.000000 9.690000 0.000000 0.538000 6.208500 77.500000 3.207450 5.000000 330.000000 19.050000 391.440000 11.360000 21.200000 75% 3.677083 12.500000 18.100000 0.000000 0.624000 6.623500 94.075000 5.188425 24.000000 666.000000 20.200000 396.225000 16.955000 25.000000 max 88.976200 100.000000 27.740000 1.000000 0.871000 8.780000 100.000000 12.126500 24.000000 711.000000 22.000000 396.900000 37.970000 50.000000 In\u00a0[5]: Copied! <pre>boston_df.info()\n</pre> boston_df.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 506 entries, 0 to 505\nData columns (total 14 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   CRIM     506 non-null    float64\n 1   ZN       506 non-null    float64\n 2   INDUS    506 non-null    float64\n 3   CHAS     506 non-null    float64\n 4   NOX      506 non-null    float64\n 5   RM       506 non-null    float64\n 6   AGE      506 non-null    float64\n 7   DIS      506 non-null    float64\n 8   RAD      506 non-null    float64\n 9   TAX      506 non-null    float64\n 10  PTRATIO  506 non-null    float64\n 11  B        506 non-null    float64\n 12  LSTAT    506 non-null    float64\n 13  TARGET   506 non-null    float64\ndtypes: float64(14)\nmemory usage: 55.5 KB\n</pre> In\u00a0[6]: Copied! <pre>#matriz de correlacion\ncorr_mat=boston_df.corr(method='pearson')\nplt.figure(figsize=(20,10))\nsns.heatmap(corr_mat,vmax=1,square=True,annot=True,cmap='cubehelix',fmt='.2f')\nplt.show()\n</pre> #matriz de correlacion corr_mat=boston_df.corr(method='pearson') plt.figure(figsize=(20,10)) sns.heatmap(corr_mat,vmax=1,square=True,annot=True,cmap='cubehelix',fmt='.2f') plt.show() <p>Apliquemos el modelo de regresi\u00f3n lineal multiple con sklearn</p> In\u00a0[7]: Copied! <pre># datos para la regresion lineal simple\nX = boston_df.drop(\"TARGET\",axis=1) \nY = boston_df[\"TARGET\"]\n\n# split dataset\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state = 2) \n\n# ajustar el modelo\nmodel_rl = LinearRegression() # Creando el modelo.\nmodel_rl.fit(X_train, Y_train) # ajustando el modelo\n\n# prediciones\nY_predict = model_rl.predict(X_test)\n</pre> # datos para la regresion lineal simple X = boston_df.drop(\"TARGET\",axis=1)  Y = boston_df[\"TARGET\"]  # split dataset X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state = 2)   # ajustar el modelo model_rl = LinearRegression() # Creando el modelo. model_rl.fit(X_train, Y_train) # ajustando el modelo  # prediciones Y_predict = model_rl.predict(X_test) In\u00a0[8]: Copied! <pre>from sklearn.metrics import mean_absolute_error, mean_squared_error\n\ndef mean_absolute_percentage_error(y_true, y_pred):\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\ndef regression_metrics(df):\n    \"\"\"\n    Aplicar las distintas m\u00e9tricas definidas\n    :param df: DataFrame con las columnas: ['y', 'yhat']\n    :return: DataFrame con las m\u00e9tricas especificadas\n    \"\"\"\n    df_result = pd.DataFrame()\n\n    y_true = df['y']\n    y_pred = df['yhat']\n\n    df_result['mae'] = [round(mean_absolute_error(y_true, y_pred), 4)]\n    df_result['mse'] = [round(mean_squared_error(y_true, y_pred), 4)]\n    df_result['rmse'] = [round(np.sqrt(mean_squared_error(y_true, y_pred)), 4)]\n    df_result['mape'] = [round(mean_absolute_percentage_error(y_true, y_pred), 4)]\n    df_result['smape'] = [round(2 * mean_absolute_percentage_error(y_true, y_pred) / (mean_absolute_percentage_error(y_true, y_pred) + 100), 4)]\n\n    return df_result\n</pre> from sklearn.metrics import mean_absolute_error, mean_squared_error  def mean_absolute_percentage_error(y_true, y_pred):     return np.mean(np.abs((y_true - y_pred) / y_true)) * 100  def regression_metrics(df):     \"\"\"     Aplicar las distintas m\u00e9tricas definidas     :param df: DataFrame con las columnas: ['y', 'yhat']     :return: DataFrame con las m\u00e9tricas especificadas     \"\"\"     df_result = pd.DataFrame()      y_true = df['y']     y_pred = df['yhat']      df_result['mae'] = [round(mean_absolute_error(y_true, y_pred), 4)]     df_result['mse'] = [round(mean_squared_error(y_true, y_pred), 4)]     df_result['rmse'] = [round(np.sqrt(mean_squared_error(y_true, y_pred)), 4)]     df_result['mape'] = [round(mean_absolute_percentage_error(y_true, y_pred), 4)]     df_result['smape'] = [round(2 * mean_absolute_percentage_error(y_true, y_pred) / (mean_absolute_percentage_error(y_true, y_pred) + 100), 4)]      return df_result In\u00a0[9]: Copied! <pre>from sklearn.metrics import r2_score\n\n# ejemplo: boston df\ndf_temp = pd.DataFrame(\n    {\n        'y':Y_test,\n        'yhat': model_rl.predict(X_test)\n        }\n)\n\ndf_metrics = regression_metrics(df_temp)\ndf_metrics['r2'] =  round(r2_score(Y_test, model_rl.predict(X_test)),4)\n\nprint('\\nMetricas para el regresor CRIM:')\ndf_metrics\n</pre> from sklearn.metrics import r2_score  # ejemplo: boston df df_temp = pd.DataFrame(     {         'y':Y_test,         'yhat': model_rl.predict(X_test)         } )  df_metrics = regression_metrics(df_temp) df_metrics['r2'] =  round(r2_score(Y_test, model_rl.predict(X_test)),4)  print('\\nMetricas para el regresor CRIM:') df_metrics <pre>\nMetricas para el regresor CRIM:\n</pre> Out[9]: mae mse rmse mape smape r2 0 3.113 18.4954 4.3006 16.036 0.2764 0.7789 <p>Cuando se aplica el modelo de regresi\u00f3n lineal con todas las variables regresoras, las m\u00e9tricas disminuyen considerablemente, lo implica una mejora en el modelo</p> <p>Un problema que se tiene, a diferencia de la regresi\u00f3n lineal simple,es que no se puede ver gr\u00e1ficamente la calidad del ajuste, por lo que solo se puede confiar en las m\u00e9tricas calculadas. Adem\u00e1s, se dejan las siguientes preguntas:</p> <ul> <li>\u00bf Entre m\u00e1s regresores, mejor ser\u00e1 el modelo de regresi\u00f3n lineal?</li> <li>\u00bf Qu\u00e9 se debe tener en cuenta antes de agregar otro variable regresora al modelo de regresi\u00f3n lineal ?</li> <li>\u00bf Qu\u00e9 sucede si se tienen outliers ?</li> <li>\u00bf Existen otros modelos mejor que la regresi\u00f3n lineal ?</li> </ul> <p>Ya se han discutido algunos de estos puntos, por lo que la atenci\u00f3n estar\u00e1 en abordar otros modelos.</p> In\u00a0[10]: Copied! <pre>from sklearn import linear_model\nfrom sklearn import tree\nfrom sklearn import svm\nfrom sklearn import neighbors\n</pre> from sklearn import linear_model from sklearn import tree from sklearn import svm from sklearn import neighbors In\u00a0[11]: Copied! <pre>class SklearnRegressionModels:\n    def __init__(self,model,name_model):\n\n        self.model = model\n        self.name_model = name_model\n        \n    @staticmethod\n    def test_train_model(X,y,n_size):\n        X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=n_size , random_state=42)\n        return X_train, X_test, y_train, y_test\n    \n    def fit_model(self,X,y,test_size):\n        X_train, X_test, y_train, y_test = self.test_train_model(X,y,test_size )\n        return self.model.fit(X_train, y_train) \n    \n    def df_testig(self,X,y,test_size):\n        X_train, X_test, y_train, y_test = self.test_train_model(X,y,test_size )\n        model_fit = self.model.fit(X_train, y_train)\n        preds = model_fit.predict(X_test)\n        df_temp = pd.DataFrame(\n            {\n                'y':y_test,\n                'yhat': model_fit.predict(X_test)\n            }\n        )\n        \n        return df_temp\n    \n    def metrics(self,X,y,test_size):\n        df_temp = self.df_testig(X,y,test_size)\n        df_metrics = regression_metrics(df_temp)\n        df_metrics['r2'] =  round(r2_score(df_temp['y'],df_temp['yhat']),4)\n\n        df_metrics['model'] = self.name_model\n        \n        return df_metrics\n\n    def parameters(self,X,y,test_size):\n        model_fit = self.fit_model(X,y,test_size)\n        \n        list_betas = [\n             ('beta_0',model_fit.intercept_)\n                ]\n            \n        betas = model_fit.coef_\n        \n        for num, beta in enumerate(betas):\n            name_beta = f'beta_{num+1}'\n            list_betas.append((name_beta,round(beta,2)))\n\n        result = pd.DataFrame(\n            columns = ['coef','value'],\n            data = list_betas\n        )\n        \n        result['model'] = self.name_model\n        return result\n</pre> class SklearnRegressionModels:     def __init__(self,model,name_model):          self.model = model         self.name_model = name_model              @staticmethod     def test_train_model(X,y,n_size):         X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=n_size , random_state=42)         return X_train, X_test, y_train, y_test          def fit_model(self,X,y,test_size):         X_train, X_test, y_train, y_test = self.test_train_model(X,y,test_size )         return self.model.fit(X_train, y_train)           def df_testig(self,X,y,test_size):         X_train, X_test, y_train, y_test = self.test_train_model(X,y,test_size )         model_fit = self.model.fit(X_train, y_train)         preds = model_fit.predict(X_test)         df_temp = pd.DataFrame(             {                 'y':y_test,                 'yhat': model_fit.predict(X_test)             }         )                  return df_temp          def metrics(self,X,y,test_size):         df_temp = self.df_testig(X,y,test_size)         df_metrics = regression_metrics(df_temp)         df_metrics['r2'] =  round(r2_score(df_temp['y'],df_temp['yhat']),4)          df_metrics['model'] = self.name_model                  return df_metrics      def parameters(self,X,y,test_size):         model_fit = self.fit_model(X,y,test_size)                  list_betas = [              ('beta_0',model_fit.intercept_)                 ]                      betas = model_fit.coef_                  for num, beta in enumerate(betas):             name_beta = f'beta_{num+1}'             list_betas.append((name_beta,round(beta,2)))          result = pd.DataFrame(             columns = ['coef','value'],             data = list_betas         )                  result['model'] = self.name_model         return result  In\u00a0[12]: Copied! <pre># boston dataframe\nX = boston_df.drop(\"TARGET\",axis=1) \nY = boston_df[\"TARGET\"]\n</pre> # boston dataframe X = boston_df.drop(\"TARGET\",axis=1)  Y = boston_df[\"TARGET\"] In\u00a0[13]: Copied! <pre># models\nreg_lineal = linear_model.LinearRegression()\nreg_ridge = linear_model.Ridge(alpha=.5)\nreg_lasso = linear_model.Lasso(alpha=0.1)\n\nreg_knn = neighbors.KNeighborsRegressor(5,weights='uniform')\nreg_bayesian = linear_model.BayesianRidge()\nreg_tree = tree.DecisionTreeRegressor(max_depth=5)\nreg_svm = svm.SVR(kernel='linear')\n\n\nlist_models =[\n    [reg_lineal,'lineal'],\n    [reg_ridge,'ridge'],\n    [reg_lasso,'lasso'],\n    [reg_knn,'knn'],\n    [reg_bayesian,'bayesian'],\n    [reg_tree,'decision_tree'],\n    [reg_svm,'svm'],\n]\n</pre> # models reg_lineal = linear_model.LinearRegression() reg_ridge = linear_model.Ridge(alpha=.5) reg_lasso = linear_model.Lasso(alpha=0.1)  reg_knn = neighbors.KNeighborsRegressor(5,weights='uniform') reg_bayesian = linear_model.BayesianRidge() reg_tree = tree.DecisionTreeRegressor(max_depth=5) reg_svm = svm.SVR(kernel='linear')   list_models =[     [reg_lineal,'lineal'],     [reg_ridge,'ridge'],     [reg_lasso,'lasso'],     [reg_knn,'knn'],     [reg_bayesian,'bayesian'],     [reg_tree,'decision_tree'],     [reg_svm,'svm'], ] In\u00a0[14]: Copied! <pre>frames_metrics = []\nframes_coef = []\n\nfor model,name_models in list_models:\n    fit_model =  SklearnRegressionModels( model,name_models)\n    frames_metrics.append(fit_model.metrics(X,Y,0.2))\n    if name_models in ['lineal','ridge','lasso']:\n        frames_coef.append(fit_model.parameters(X,Y,0.2))\n</pre> frames_metrics = [] frames_coef = []  for model,name_models in list_models:     fit_model =  SklearnRegressionModels( model,name_models)     frames_metrics.append(fit_model.metrics(X,Y,0.2))     if name_models in ['lineal','ridge','lasso']:         frames_coef.append(fit_model.parameters(X,Y,0.2)) In\u00a0[15]: Copied! <pre># juntar resultados: metricas\npd.concat(frames_metrics).sort_values('rmse')\n</pre> # juntar resultados: metricas pd.concat(frames_metrics).sort_values('rmse') Out[15]: mae mse rmse mape smape r2 model 0 2.6062 20.3563 4.5118 15.0760 0.2620 0.7224 decision_tree 0 3.1891 24.2911 4.9286 16.8664 0.2886 0.6688 lineal 0 3.1493 24.3776 4.9374 16.6837 0.2860 0.6676 ridge 0 3.1251 24.6471 4.9646 16.5449 0.2839 0.6639 bayesian 0 3.1452 25.1556 5.0155 16.7519 0.2870 0.6570 lasso 0 3.6639 25.8601 5.0853 18.8859 0.3177 0.6474 knn 0 3.1404 29.4359 5.4255 16.7713 0.2873 0.5986 svm <p>Basados en los distintos estad\u00edsticos, el mejor modelo corresponde al modelo de decision_tree. Por otro lado, podemos analizar los coeficientes de los modelos l\u00edneales ordinarios,Ridge y Lasso.</p> In\u00a0[16]: Copied! <pre># juntar resultados: coeficientes\npd.concat(frames_coef)\n</pre> # juntar resultados: coeficientes pd.concat(frames_coef) Out[16]: coef value model 0 beta_0 30.246751 lineal 1 beta_1 -0.110000 lineal 2 beta_2 0.030000 lineal 3 beta_3 0.040000 lineal 4 beta_4 2.780000 lineal 5 beta_5 -17.200000 lineal 6 beta_6 4.440000 lineal 7 beta_7 -0.010000 lineal 8 beta_8 -1.450000 lineal 9 beta_9 0.260000 lineal 10 beta_10 -0.010000 lineal 11 beta_11 -0.920000 lineal 12 beta_12 0.010000 lineal 13 beta_13 -0.510000 lineal 0 beta_0 26.891132 ridge 1 beta_1 -0.110000 ridge 2 beta_2 0.030000 ridge 3 beta_3 0.020000 ridge 4 beta_4 2.640000 ridge 5 beta_5 -12.270000 ridge 6 beta_6 4.460000 ridge 7 beta_7 -0.010000 ridge 8 beta_8 -1.380000 ridge 9 beta_9 0.250000 ridge 10 beta_10 -0.010000 ridge 11 beta_11 -0.860000 ridge 12 beta_12 0.010000 ridge 13 beta_13 -0.520000 ridge 0 beta_0 19.859769 lasso 1 beta_1 -0.100000 lasso 2 beta_2 0.030000 lasso 3 beta_3 -0.020000 lasso 4 beta_4 0.920000 lasso 5 beta_5 -0.000000 lasso 6 beta_6 4.310000 lasso 7 beta_7 -0.020000 lasso 8 beta_8 -1.150000 lasso 9 beta_9 0.240000 lasso 10 beta_10 -0.010000 lasso 11 beta_11 -0.730000 lasso 12 beta_12 0.010000 lasso 13 beta_13 -0.560000 lasso <p>Al comparar los resultados entre ambos modelos, se observa que hay coeficientes en la regresi\u00f3n Lasso que se van a cero directamente, pudiendo eliminar estas variables del modelo. Por otro lado, queda como tarea para el lector, hacer una eliminaci\u00f3n de outliers del modelo y probar estos modelos lineales para ver si existe alg\u00fan tipo de diferencia.</p>"},{"location":"lectures/machine_learning/reg_02/#regression-ii","title":"Regressi\u00f3n II\u00b6","text":""},{"location":"lectures/machine_learning/reg_02/#regression-multiple","title":"Regressi\u00f3n M\u00faltiple\u00b6","text":"<p>Los modelos de regresi\u00f3n multiple son los m\u00e1s utilizados en el mundo de machine learning, puestos que se dispone de varios caracter\u00edstica de la poblaci\u00f3n objetivo. A menudo, se estar\u00e1 abordando estos modelos de la perspectiva de los modelos lineales, por lo cual se debetener en mente algunos supuestos antes de comenzar:</p> <ul> <li>No colinialidad o multicolinialidad: En los modelos lineales m\u00faltiples los predictores deben ser independientes, no debe de haber colinialidad entre ellos</li> <li>Parsimonia: Este t\u00e9rmino hace referencia a que el mejor modelo es aquel capaz de explicar con mayor precisi\u00f3n la variabilidad observada en la variable respuesta empleando el menor n\u00famero de predictores, por lo tanto, con menos asunciones.</li> <li>Homocedasticidad:La varianza de los residuos debe de ser constante en todo el rango de observaciones.</li> <li>Otros Factores:<ul> <li>Distribuci\u00f3n normal de los residuos</li> <li>No autocorrelaci\u00f3n (Independencia)</li> <li>Valores at\u00edpicos, con alto leverage o influyentes</li> <li>Tama\u00f1o de la muestra</li> </ul> </li> </ul> <p>Por otro lado, existen otros tipos de modelos de regresi\u00f3n, en los cuales se  necesitan menos supuestos que los modelos de regresi\u00f3n lineal, a cambio se pierde un poco de interpretabilidad en sus par\u00e1metros y centran su atenci\u00f3n en los resultados obtenidos de las predicciones.</p>"},{"location":"lectures/machine_learning/reg_02/#aplicacion-con-python","title":"Aplicaci\u00f3n con python\u00b6","text":""},{"location":"lectures/machine_learning/reg_02/#dataset-boston-house-prices","title":"Dataset  Boston house prices\u00b6","text":"<p>En este ejemplo se va utilizar el dataset Boston que ya viene junto con sklearn y es ideal para practicar con Regresiones Lineales; el mismo contiene precios de casas de varias \u00e1reas de la ciudad de Boston.</p>"},{"location":"lectures/machine_learning/reg_02/#otros-modelos-de-regresion","title":"Otros modelos de Regresi\u00f3n\u00b6","text":"<p>Existen varios modelos de regresi\u00f3n, sin embargo, la intepretaci\u00f3n de sus par\u00e1metros y el an\u00e1lisis de confiabilidad no es tan directo como los modelos de regresi\u00f3n lineal. Por este motivo, la atenci\u00f3n estar\u00e1 centrada en la predicci\u00f3n m\u00e1s que en la confiabilidad como tal del modelo.</p>"},{"location":"lectures/machine_learning/reg_02/#modelos-lineales","title":"Modelos lineales\u00b6","text":"<p>Existen varios modelos lineales que podemos trabajar en sklearn (ver referencia), los cualeas podemos utilizar e ir comparando unos con otros.</p> <p>De lo modelos lineales, destacamos los siguientes:</p> <ul> <li>regresi\u00f3n lineal cl\u00e1sica: regresi\u00f3n cl\u00e1sica por m\u00ednimos cudrados. $$(P)\\ \\min \\sum_{i=1}^n (y_{i}-f_{i}(x;\\beta))^2   $$</li> <li>lasso): se ocupa cuando tenemos un gran n\u00famero de regresores y queremos que disminuya el problema de colinealidad (es decir, estimar como cero los par\u00e1metros poco relevantes). $$(P)\\ \\min \\sum_{i=1}^n (y_{i}-f_{i}(x;\\beta))^2 + \\lambda \\sum_{i=1}^n |\\beta_{i}| $$</li> <li>ridge: tambi\u00e9n sirve para disminuir el problema de colinealidad, y adem\u00e1s trata de que los coeficientes sean m\u00e1s rocuesto bajo outliers. $$(P)\\ \\min \\sum_{i=1}^n (y_{i}-f_{i}(x;\\beta))^2  + \\lambda \\sum_{i=1}^n \\beta_{i}^2 $$</li> </ul> <p>Dado que en sklearn, la forma de entrenar, estimar y predecir modelos de regresi\u00f3n siguen una misma estructura, para fectos pr\u00e1cticos, definimos una rutina para estimar las distintas m\u00e9tricas de la siguiente manera:</p>"},{"location":"lectures/machine_learning/reg_02/#bayesian-regression","title":"Bayesian Regression\u00b6","text":"<p>En estad\u00edstica, la regresi\u00f3n lineal bayesiana es un enfoque de regresi\u00f3n lineal en el que el an\u00e1lisis estad\u00edstico se realiza dentro del contexto de la inferencia bayesiana. Cuando el modelo de regresi\u00f3n tiene errores que tienen una distribuci\u00f3n normal, y si se asume una forma particular de distribuci\u00f3n previa, los resultados expl\u00edcitos est\u00e1n disponibles para las distribuciones de probabilidad posteriores de los par\u00e1metros del modelo.</p> <p></p>"},{"location":"lectures/machine_learning/reg_02/#k-vecinos-mas-cercanos-knn","title":"k-vecinos m\u00e1s cercanos  Knn\u00b6","text":"<p>El m\u00e9todo de los $k$ vecinos m\u00e1s cercanos (en ingl\u00e9s, k-nearest neighbors, abreviado $knn$) es un m\u00e9todo de clasificaci\u00f3n supervisada (Aprendizaje, estimaci\u00f3n basada en un conjunto de entrenamiento y prototipos) que sirve para estimar la funci\u00f3n de densidad $F(x/C_j)$ de las predictoras $x$ por cada clase  $C_{j}$.</p> <p>Este es un m\u00e9todo de clasificaci\u00f3n no param\u00e9trico, que estima el valor de la funci\u00f3n de densidad de probabilidad o directamente la probabilidad a posteriori de que un elemento $x$ pertenezca a la clase $C_j$ a partir de la informaci\u00f3n proporcionada por el conjunto de prototipos. En el proceso de aprendizaje no se hace ninguna suposici\u00f3n acerca de la distribuci\u00f3n de las variables predictoras.</p> <p>En el reconocimiento de patrones, el algoritmo $knn$ es usado como m\u00e9todo de clasificaci\u00f3n de objetos (elementos) basado en un entrenamiento mediante ejemplos cercanos en el espacio de los elementos. $knn$ es un tipo de aprendizaje vago (lazy learning), donde la funci\u00f3n se aproxima solo localmente y todo el c\u00f3mputo es diferido a la clasificaci\u00f3n. La normalizaci\u00f3n de datos puede mejorar considerablemente la exactitud del algoritmo $knn$.</p> <p></p>"},{"location":"lectures/machine_learning/reg_02/#decision-tree-regressor","title":"Decision Tree Regressor\u00b6","text":"<p>Un \u00e1rbol de decisi\u00f3n es un modelo de predicci\u00f3n utilizado en diversos \u00e1mbitos que van desde la inteligencia artificial hasta la Econom\u00eda. Dado un conjunto de datos se fabrican diagramas de construcciones l\u00f3gicas, muy similares a los sistemas de predicci\u00f3n basados en reglas, que sirven para representar y categorizar una serie de condiciones que ocurren de forma sucesiva, para la resoluci\u00f3n de un problema.</p> <p></p> <p>Vamos a explicar c\u00f3mo se construye un \u00e1rbol de decisi\u00f3n. Para ello, vamos a hacer hincapi\u00e9 en varios aspectos</p>"},{"location":"lectures/machine_learning/reg_02/#elementos","title":"Elementos\u00b6","text":"<p>Los \u00e1rboles de decisi\u00f3n est\u00e1n formados por nodos, vectores de n\u00fameros, flechas y etiquetas.</p> <ul> <li>Cada nodo se puede definir como el momento en el que se ha de tomar una decisi\u00f3n de entre varias posibles, lo que va haciendo que a medida que aumenta el n\u00famero de nodos aumente el n\u00famero de posibles finales a los que puede llegar el individuo. Esto hace que un \u00e1rbol con muchos nodos sea complicado de dibujar a mano y de analizar debido a la existencia de numerosos caminos que se pueden seguir.</li> <li>Los vectores de n\u00fameros ser\u00edan la soluci\u00f3n final a la que se llega en funci\u00f3n de las diversas posibilidades que se tienen, dan las utilidades en esa soluci\u00f3n.</li> <li>Las flechas son las uniones entre un nodo y otro y representan cada acci\u00f3n distinta.</li> <li>Las etiquetas se encuentran en cada nodo y cada flecha y dan nombre a cada acci\u00f3n.</li> </ul>"},{"location":"lectures/machine_learning/reg_02/#conceptos","title":"Conceptos\u00b6","text":"<p>Cuando tratemos en el desarrollo de \u00e1rboles utilizaremos frecuentemente estos conceptos:</p> <ul> <li>Costo. Se refiere a dos conceptos diferentes: el costo de medici\u00f3n para determinar el valor de una determinada propiedad (atributo) exhibida por el objeto y el costo de clasificaci\u00f3n err\u00f3nea al decidir que el objeto pertenece a la clase $X$ cuando su clase real es $Y$.</li> <li>Sobreajuste (Overfitting). Se produce cuando los datos de entrenamiento son pocos o contienen incoherencias. Al tomar un espacio de hip\u00f3tesis $H$, se dice que una hip\u00f3tesis $h \u2208 H$ sobreajusta un conjunto de entrenamiento $C$ si existe alguna hip\u00f3tesis alternativa $h' \u2208 H$ tal que $h$ clasifica mejor que $h'$ los elementos del conjunto de entrenamiento, pero $h'$ clasifica mejor que h el conjunto completo de posibles instancias.</li> <li>Poda (Prunning). La poda consiste en eliminar una rama de un nodo transform\u00e1ndolo en una hoja (terminal), asign\u00e1ndole la clasificaci\u00f3n m\u00e1s com\u00fan de los ejemplos de entrenamiento considerados en ese nodo.</li> <li>La validaci\u00f3n cruzada. Es el proceso de construir un \u00e1rbol con la mayor\u00eda de los datos y luego usar la parte restante de los datos para probar la precisi\u00f3n del \u00e1rbol.</li> </ul>"},{"location":"lectures/machine_learning/reg_02/#reglas","title":"Reglas\u00b6","text":"<p>En los \u00e1rboles de decisi\u00f3n se tiene que cumplir una serie de reglas.</p> <ol> <li>Al comienzo del juego se da un nodo inicial que no es apuntado por ninguna flecha, es el \u00fanico del juego con esta caracter\u00edstica.</li> <li>El resto de los nodos del juego son apuntados por una \u00fanica flecha.</li> <li>De esto se deduce que hay un \u00fanico camino para llegar del nodo inicial a cada uno de los nodos del juego. No hay varias formas de llegar a la misma soluci\u00f3n final, las decisiones son excluyentes.</li> </ol> <p>En los \u00e1rboles de decisiones las decisiones que se eligen son lineales, a medida que vas seleccionando entre varias opciones se van cerrando otras, lo que implica normalmente que no hay marcha atr\u00e1s. En general se podr\u00eda decir que las normas siguen una forma condicional:</p> <p>$$\\textrm{Opci\u00f3n }1-&gt;\\textrm{opci\u00f3n }2-&gt;\\textrm{opci\u00f3n }3-&gt;\\textrm{Resultado Final }X$$</p> <p>Estas reglas suelen ir impl\u00edcitas en el conjunto de datos a ra\u00edz del cual se construye el \u00e1rbol de decisi\u00f3n.</p>"},{"location":"lectures/machine_learning/reg_02/#svm","title":"SVM\u00b6","text":"<p>Las m\u00e1quinas de vectores de soporte  (del ingl\u00e9s Support Vector Machines, SVM) son un conjunto de algoritmos de aprendizaje supervisado desarrollados por Vladimir Vapnik y su equipo en los laboratorios AT&amp;T.</p> <p>Estos m\u00e9todos est\u00e1n propiamente relacionados con problemas de clasificaci\u00f3n y regresi\u00f3n. Dado un conjunto de ejemplos de entrenamiento (de muestras) podemos etiquetar las clases y entrenar una SVM para construir un modelo que prediga la clase de una nueva muestra. Intuitivamente, una SVM es un modelo que representa a los puntos de muestra en el espacio, separando las clases a 2 espacios lo m\u00e1s amplios posibles mediante un hiperplano de separaci\u00f3n definido como el vector entre los 2 puntos, de las 2 clases, m\u00e1s cercanos al que se llama vector soporte. Cuando las nuevas muestras se ponen en correspondencia con dicho modelo, en funci\u00f3n de los espacios a los que pertenezcan, pueden ser clasificadas a una o la otra clase.</p> <p>M\u00e1s formalmente, una SVM construye un hiperplano o conjunto de hiperplanos en un espacio de dimensionalidad muy alta (o incluso infinita) que puede ser utilizado en problemas de clasificaci\u00f3n o regresi\u00f3n. Una buena separaci\u00f3n entre las clases permitir\u00e1 una clasificaci\u00f3n correcta.</p> <p></p>"},{"location":"lectures/machine_learning/reg_02/#idea-basica","title":"Idea B\u00e1sica\u00b6","text":"<p>Dado un conjunto de puntos, subconjunto de un conjunto mayor (espacio), en el que cada uno de ellos pertenece a una de dos posibles categor\u00edas, un algoritmo basado en SVM construye un modelo capaz de predecir si un punto nuevo (cuya categor\u00eda desconocemos) pertenece a una categor\u00eda o a la otra.</p> <p>Como en la mayor\u00eda de los m\u00e9todos de clasificaci\u00f3n supervisada, los datos de entrada (los puntos) son vistos como un vector $p-dimensional$ (una lista ordenada de $p$ n\u00fameros).</p> <p>La SVM busca un hiperplano que separe de forma \u00f3ptima a los puntos de una clase de la de otra, que eventualmente han podido ser previamente proyectados a un espacio de dimensionalidad superior.</p> <p>En ese concepto de \"separaci\u00f3n \u00f3ptima\" es donde reside la caracter\u00edstica fundamental de las SVM: este tipo de algoritmos buscan el hiperplano que tenga la m\u00e1xima distancia (margen) con los puntos que est\u00e9n m\u00e1s cerca de \u00e9l mismo. Por eso tambi\u00e9n a veces se les conoce a las SVM como clasificadores de margen m\u00e1ximo. De esta forma, los puntos del vector que son etiquetados con una categor\u00eda estar\u00e1n a un lado del hiperplano y los casos que se encuentren en la otra categor\u00eda estar\u00e1n al otro lado.</p> <p>Los algoritmos SVM pertenecen a la familia de los clasificadores lineales. Tambi\u00e9n pueden ser considerados un caso especial de la regularizaci\u00f3n de Tikhonov.</p> <p>En la literatura de las SVM, se llama atributo a la variable predictora y caracter\u00edstica a un atributo transformado que es usado para definir el hiperplano. La elecci\u00f3n de la representaci\u00f3n m\u00e1s adecuada del universo estudiado, se realiza mediante un proceso denominado selecci\u00f3n de caracter\u00edsticas.</p> <p>Al vector formado por los puntos m\u00e1s cercanos al hiperplano se le llama vector de soporte.</p> <p>Los modelos basados en SVM est\u00e1n estrechamente relacionados con las redes neuronales. Usando una funci\u00f3n kernel, resultan un m\u00e9todo de entrenamiento alternativo para clasificadores polinomiales, funciones de base radial y perceptr\u00f3n multicapa.</p> <p></p>"},{"location":"lectures/machine_learning/reg_02/#aplicando-varios-modelos-al-mismo-tiempo","title":"Aplicando varios modelos al mismo tiempo\u00b6","text":"<p>Veremos el performance de los distintos modelos estudiados.</p>"},{"location":"lectures/machine_learning/reg_02/#conclusion","title":"Conclusi\u00f3n\u00b6","text":"<ul> <li>Existen distintos modelos de regresi\u00f3n lineal: normal, Ridge y Lasso. Cada uno con sus respectivs ventajas y desventajas.</li> <li>Existen otros tipos de modelos de regresi\u00f3n (bayesiano, knn, arboles de decisi\u00f3n, svm, entre otros). Por ahora, nos interesa saber como funcionan, para poder configurar los hiperpar\u00e1metros de los modelos ocupados en python (principalmente de la librer\u00eda sklearn).</li> <li>En el mundo del machine learning se estar\u00e1 interesado m\u00e1s en predecir con el menor error posible (siempre tomando como referencia alguna de las m\u00e9tricas mencionadas) que hacer un an\u00e1lisis exhaustivo de la confiabilidad del modelo. Siendo este el caso y si la capacidad computacional lo permite, lo ideal es probar varios modelos al mismo tiempo y poder discriminar bajo un determinado criterio (a menudo el error cuadr\u00e1tico medio (rmse) o el mape).</li> </ul>"},{"location":"lectures/machine_learning/reg_02/#referencias","title":"Referencias\u00b6","text":"<ol> <li>Supervised learning</li> </ol>"},{"location":"lectures/machine_learning/ts_01/","title":"Series Temporales I","text":"<ul> <li><p>Una serie temporal o cronol\u00f3gica es una sucesi\u00f3n de datos medidos en determinados momentos y ordenados cronol\u00f3gicamente. Los datos pueden estar espaciados a intervalos iguales (como la temperatura en un observatorio meteorol\u00f3gico en d\u00edas sucesivos al mediod\u00eda) o desiguales (como el peso de una persona en sucesivas mediciones en el consultorio m\u00e9dico, la farmacia, etc.).</p> </li> <li><p>Uno de los usos m\u00e1s habituales de las series de datos temporales es su an\u00e1lisis para predicci\u00f3n y pron\u00f3stico (as\u00ed se hace por ejemplo con los datos clim\u00e1ticos, las acciones de bolsa, o las series de datos demogr\u00e1ficos). Resulta dif\u00edcil imaginar una rama de las ciencias en la que no aparezcan datos que puedan ser considerados como series temporales. Las series temporales se estudian en estad\u00edstica, procesamiento de se\u00f1ales, econometr\u00eda y muchas otras \u00e1reas.</p> </li> </ul> In\u00a0[1]: Copied! <pre># librerias \n\nimport os\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\n# graficos incrustados\nsns.set_style(\"whitegrid\")\n%matplotlib inline\n\n# parametros esteticos de seaborn\nsns.set_palette(\"deep\", desat=.6)\nsns.set_context(rc={\"figure.figsize\": (12, 4)})\n</pre> # librerias   import os import pandas as pd import numpy as np  import matplotlib.pyplot as plt  import seaborn as sns   # graficos incrustados sns.set_style(\"whitegrid\") %matplotlib inline  # parametros esteticos de seaborn sns.set_palette(\"deep\", desat=.6) sns.set_context(rc={\"figure.figsize\": (12, 4)}) In\u00a0[2]: Copied! <pre># cargar datos\nurl='https://drive.google.com/file/d/1a9e0hoBLYof4mJfCeifOm_-H4J12pP-b/view?usp=drive_link'\nurl='https://drive.google.com/uc?id=' + url.split('/')[-2]\n\ndf = pd.read_csv(url, sep=\",\")\ndf.columns = ['date','passengers']\ndf.head()\n</pre> # cargar datos url='https://drive.google.com/file/d/1a9e0hoBLYof4mJfCeifOm_-H4J12pP-b/view?usp=drive_link' url='https://drive.google.com/uc?id=' + url.split('/')[-2]  df = pd.read_csv(url, sep=\",\") df.columns = ['date','passengers'] df.head() Out[2]: date passengers 0 1949-01 112 1 1949-02 118 2 1949-03 132 3 1949-04 129 4 1949-05 121 In\u00a0[3]: Copied! <pre># resumen\ndf.describe()\n</pre> # resumen df.describe() Out[3]: passengers count 144.000000 mean 280.298611 std 119.966317 min 104.000000 25% 180.000000 50% 265.500000 75% 360.500000 max 622.000000 In\u00a0[4]: Copied! <pre># fechas\nprint('Fecha Inicio: {}\\nFecha Fin:    {}'.format(df.date.min(),df.date.max()))\n</pre> # fechas print('Fecha Inicio: {}\\nFecha Fin:    {}'.format(df.date.min(),df.date.max())) <pre>Fecha Inicio: 1949-01\nFecha Fin:    1960-12\n</pre> In\u00a0[5]: Copied! <pre># formato datetime de las fechas\ndf['date'] = pd.to_datetime(df['date'], format='%Y-%m')\n\n# dejar en formato ts\ny = df.set_index('date').resample('M').mean()\n\ny.head()\n</pre> # formato datetime de las fechas df['date'] = pd.to_datetime(df['date'], format='%Y-%m')  # dejar en formato ts y = df.set_index('date').resample('M').mean()  y.head() Out[5]: passengers date 1949-01-31 112.0 1949-02-28 118.0 1949-03-31 132.0 1949-04-30 129.0 1949-05-31 121.0 In\u00a0[6]: Copied! <pre># graficar datos\ny.plot(figsize=(15, 3),color = 'blue')\nplt.show()\n</pre> # graficar datos y.plot(figsize=(15, 3),color = 'blue') plt.show() <p>Basado en el gr\u00e1fico, uno podria decir que tiene un comportamiento estacionario en el tiempo, es decir, que se repita cada cierto instante de tiempo. Adem\u00e1s, la demanda ha tendido a incrementar a\u00f1o a a\u00f1o.</p> <p>Por otro lado, podemos agregar valor a nuestro entendimiento de nuestra serie temporal, realizando un diagrama de box-plot a los distintos a\u00f1os.</p> In\u00a0[7]: Copied! <pre># Create the boxplot\nfig, ax = plt.subplots(figsize=(15, 6))\nsns.boxplot(x=y.index.year, y=y[\"passengers\"], data=y, ax=ax)\n\n# Set labels and title\nplt.xlabel(\"Year\")\nplt.ylabel(\"Passengers\")\nplt.title(\"Boxplot of Passengers by Year\")\n\n# Display the plot\nplt.show()\n</pre>  # Create the boxplot fig, ax = plt.subplots(figsize=(15, 6)) sns.boxplot(x=y.index.year, y=y[\"passengers\"], data=y, ax=ax)  # Set labels and title plt.xlabel(\"Year\") plt.ylabel(\"Passengers\") plt.title(\"Boxplot of Passengers by Year\")  # Display the plot plt.show() In\u00a0[8]: Copied! <pre>from pylab import rcParams\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nrcParams['figure.figsize'] = 18, 8\ndecomposition = sm.tsa.seasonal_decompose(y, model='multiplicative')\nfig = decomposition.plot()\nplt.show()\n</pre> from pylab import rcParams import statsmodels.api as sm import matplotlib.pyplot as plt  rcParams['figure.figsize'] = 18, 8 decomposition = sm.tsa.seasonal_decompose(y, model='multiplicative') fig = decomposition.plot() plt.show() <p>Analicemos cada uno de estos gr\u00e1ficos:</p> <ul> <li><p>gr\u00e1fico 01 (serie original): este gr\u00e1fico simplemente nos muestra la serie original graficada en el tiempo.</p> </li> <li><p>gr\u00e1fico 02 (tendencia): este gr\u00e1fico nos muestra la tendencia de la serie, para este caso, se tiene una tendencial lineal positiva.</p> </li> <li><p>gr\u00e1fico 03 (estacionariedad): este gr\u00e1fico nos muestra la estacionariedad de la serie, para este caso, se muestra una estacionariedad a\u00f1o a a\u00f1o, esta estacionariedad se puede ver como una curva invertida (funci\u00f3n cuadr\u00e1tica negativa), en donde a aumenta hasta hasta a mediados de a\u00f1os (o un poco m\u00e1s) y luego esta cae suavemente a final de a\u00f1o.</p> </li> <li><p>gr\u00e1fico 04 (error): este gr\u00e1fico nos muestra el error de la serie, para este caso, el error oscila entre 0 y 1. En general se busca que el error sea siempre lo m\u00e1s peque\u00f1o posible y que tenga el comportamiento de una distribuci\u00f3n normal. Cuando el error sigue una distribuci\u00f3n normal con media cero y varianza 1, se dice que el error es un ruido blanco.</p> </li> </ul> <p>\u00bf c\u00f3mo es un ruido blanco?, mostremos un ruido blanco en python y veamos como este luce:</p> In\u00a0[9]: Copied! <pre># grafico: lineplot \n\nnp.random.seed(42) # fijar semilla\n\nmean = 0\nstd = 1 \nnum_samples = 300\n\n\nsamples = np.random.normal(mean, std, size=num_samples)\n\nplt.plot(samples)\nplt.title(\"Ruido blanco: N(0,1)\")\nplt.show()\n</pre> # grafico: lineplot   np.random.seed(42) # fijar semilla  mean = 0 std = 1  num_samples = 300   samples = np.random.normal(mean, std, size=num_samples)  plt.plot(samples) plt.title(\"Ruido blanco: N(0,1)\") plt.show() <p>Observemos que el ruido blanco oscila sobre el valor 0 y tiene una varianza constante (igual a 1).</p> <p>Veamos su histograma:</p> In\u00a0[10]: Copied! <pre># grafico: histograma\nplt.hist(samples,bins = 10)\nplt.title(\"Ruido blanco: N(0,1)\")\nplt.show()\n</pre> # grafico: histograma plt.hist(samples,bins = 10) plt.title(\"Ruido blanco: N(0,1)\") plt.show() <p>EL histograma de una variable normal, se caracteriza por esa forma de campana sim\u00e9trica entorno a un valor, en este caso, entorno al valor 0.</p> <p></p> <p>La imagen A: Este es un excelente ejemplo de una serie estacionaria, a simple vista se puede ver que los valores se encuentran oscilando en un rango acotado (tendencia constante) y la variabilidad es constante.</p> <p>Las im\u00e1genes B, C y D no son estacionarias porque:</p> <ul> <li><p>imagen B: existe una una tendencia no contante, para este caso lineal (similar al caso que estamos analizando).</p> </li> <li><p>imagen C: existe varianza no constante, si bien varia dentro de valores acotados, la oscilaciones son err\u00e1ticas.</p> </li> <li><p>imagen D: existe codependencia entre los distintos instantes de tiempo.</p> </li> </ul> <p>\u00bf Por qu\u00e9 es importante este concepto ?</p> <ul> <li>Supuesto base de muchos modelos de series temporales (desde el punto de vista estad\u00edstico)</li> <li>No se requiere muchas complicaciones extras para que las predicciones sean \"buenas\".</li> </ul> <p>Autocorrelaci\u00f3n (ACF) y autocorrelaci\u00f3n parcial PACF</p> <p>Definamos a grandes rasgos estos conceptos:</p> <ul> <li><p>Funci\u00f3n de autocorrelaci\u00f3n (ACF). En el retardo $k$, es la autocorrelaci\u00f3n entre los valores de las series que se encuentran a $k$ intervalos de distancia.</p> </li> <li><p>Funci\u00f3n de autocorrelaci\u00f3n parcial (PACF). En el retardo $k$, es la autocorrelaci\u00f3n entre los valores de las series que se encuentran a $k$ intervalos de distancia, teniendo en cuenta los valores de los intervalos intermedios.</p> </li> </ul> <p>Si la serie temporal es estacionaria, los gr\u00e1ficos ACF / PACF mostrar\u00e1n una r\u00e1pida disminuci\u00f3n de la correlaci\u00f3n despu\u00e9s de un peque\u00f1o retraso entre los puntos.</p> <p>Gr\u00e1fiquemos la acf y pacf de nuestra serie temporal ocupando los comandos plot_acf y plot_pacf, respectivamente.</p> In\u00a0[11]: Copied! <pre>from statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\nfrom matplotlib import pyplot\npyplot.figure(figsize=(12,9))\n\n# acf\npyplot.subplot(211)\nplot_acf(y.passengers, ax=pyplot.gca(), lags = 30)\n\n#pacf\npyplot.subplot(212)\nplot_pacf(y.passengers, ax=pyplot.gca(), lags = 30)\npyplot.show()\n</pre> from statsmodels.graphics.tsaplots import plot_acf from statsmodels.graphics.tsaplots import plot_pacf from matplotlib import pyplot pyplot.figure(figsize=(12,9))  # acf pyplot.subplot(211) plot_acf(y.passengers, ax=pyplot.gca(), lags = 30)  #pacf pyplot.subplot(212) plot_pacf(y.passengers, ax=pyplot.gca(), lags = 30) pyplot.show() <pre>C:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\db-connectors-RsEieBu8-py3.8\\lib\\site-packages\\statsmodels\\graphics\\tsaplots.py:348: FutureWarning: The default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n  warnings.warn(\n</pre> <p>Se observa de ambas imagenes, que estas no decaen r\u00e1pidamente a cero, por lo cual se puede decir que la serie en estudio no es estacionaria.</p> <p>Prueba  Dickey-Fuller</p> <p>En estad\u00edstica, la prueba Dickey-Fuller prueba la hip\u00f3tesis nula de que una ra\u00edz unitaria est\u00e1 presente en un modelo autorregresivo. La hip\u00f3tesis alternativa es diferente seg\u00fan la versi\u00f3n de la prueba utilizada, pero generalmente es estacionariedad o tendencia-estacionaria. Lleva el nombre de los estad\u00edsticos David Dickey y Wayne Fuller, quienes desarrollaron la prueba en 1979.</p> <p>Para efectos pr\u00e1ticos, para el caso de estacionariedad se puede definir el test como:</p> <ul> <li>Hip\u00f3tesis nula: la serie temporal no es estacionaria.</li> <li>Hip\u00f3tesis alternativa: la serie temporal es alternativa.</li> </ul> <p>Rechazar la hip\u00f3tesis nula (es decir, un valor p muy bajo) indicar\u00e1 estacionariedad</p> In\u00a0[12]: Copied! <pre>from statsmodels.tsa.stattools import adfuller\n\n#test Dickey-Fulle:\nprint ('Resultados del test de Dickey-Fuller:')\ndftest = adfuller(y.passengers, autolag='AIC')\ndfoutput = pd.Series(dftest[0:4], \n                     index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n\nprint(dfoutput)\n</pre> from statsmodels.tsa.stattools import adfuller  #test Dickey-Fulle: print ('Resultados del test de Dickey-Fuller:') dftest = adfuller(y.passengers, autolag='AIC') dfoutput = pd.Series(dftest[0:4],                       index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])  print(dfoutput) <pre>Resultados del test de Dickey-Fuller:\nTest Statistic                   0.815369\np-value                          0.991880\n#Lags Used                      13.000000\nNumber of Observations Used    130.000000\ndtype: float64\n</pre> <p>Dado que el p-value es 0.991880, se concluye que la serie temporal no es estacionaria.</p> <p>\u00bf Qu\u00e9 se puede hacer si la serie no es etacionaria ?</p> <p>Nuestro caso en estudio resulto ser una serie no estacionaria, no obstante, se pueden realizar tranformaciones para solucionar este problema.</p> <p>Como es de esperar, estas tranformaciones deben cumplir ciertas porpiedades (inyectividad, diferenciables, etc.). A continuaci\u00f3n, se presentan algunas de las tranformaciones m\u00e1s ocupadas en el \u00e1mbito de series temporales:</p> <p>Sea $X_{t}$ una serie temporal, entonces uno puede definir una nueva serie temporal $Y_{t}$ de la siguiente manera:</p> <ul> <li><p>diferenciaci\u00f3n:  $Y_{t} =\\Delta X_{t}  =X_{t}-X_{t-1}$.</p> </li> <li><p>transformaciones de Box-Cox: $$Y_{t} = \\left\\{\\begin{matrix} \\dfrac{X_{t}^{\\lambda}-1}{\\lambda}, \\ \\  \\textrm{si }  \\lambda &gt; 0\\\\  \\textrm{log}(X_{t}), \\ \\  \\textrm{si }  \\lambda = 0 \\end{matrix}\\right.$$</p> </li> </ul> <p>Ayudemonos con python para ver el resultado de las distintas transformaciones.</p> In\u00a0[13]: Copied! <pre># diferenciacion\n\nY_diff = y.diff()\n\nrcParams['figure.figsize'] = 12, 4\nplt.plot(Y_diff)\nplt.title(\"Serie diferenciada\")\nplt.show()\n</pre> # diferenciacion  Y_diff = y.diff()  rcParams['figure.figsize'] = 12, 4 plt.plot(Y_diff) plt.title(\"Serie diferenciada\") plt.show() In\u00a0[14]: Copied! <pre>def box_transformations(y,param):\n    if param&gt;0:\n        return y.apply(lambda x: (x**(param)-1)/param)\n    elif param==0:\n        return np.log(y)\n    else:\n        print(\"lambda es negativo, se devulve la serie original\")\n        return y\n</pre> def box_transformations(y,param):     if param&gt;0:         return y.apply(lambda x: (x**(param)-1)/param)     elif param==0:         return np.log(y)     else:         print(\"lambda es negativo, se devulve la serie original\")         return y In\u00a0[15]: Copied! <pre># logaritmo\n\nY_log = box_transformations(y,0)\n\nrcParams['figure.figsize'] = 12, 4\nplt.plot(Y_log)\nplt.title(\"Serie logaritmica\")\nplt.show()\n</pre> # logaritmo  Y_log = box_transformations(y,0)  rcParams['figure.figsize'] = 12, 4 plt.plot(Y_log) plt.title(\"Serie logaritmica\") plt.show() In\u00a0[16]: Copied! <pre># cuadratica\n\nY_quad = box_transformations(y,2)\n\nrcParams['figure.figsize'] = 12, 4\nplt.plot(Y_log)\nplt.title(\"Serie cuadratica\")\nplt.show()\n</pre> # cuadratica  Y_quad = box_transformations(y,2)  rcParams['figure.figsize'] = 12, 4 plt.plot(Y_log) plt.title(\"Serie cuadratica\") plt.show() <p>Se deja como tarea al lector profundizar m\u00e1s en estos conceptos.</p> In\u00a0[17]: Copied! <pre>target_date =  '1958-01-01'\n\n# crear conjunto de entrenamiento y de testeo\nmask_ds = y.index &lt; target_date\n\ny_train = y[mask_ds]\ny_test = y[~mask_ds]\n\n#plotting the data\ny_train['passengers'].plot()\ny_test['passengers'].plot()\nplt.show()\n</pre> target_date =  '1958-01-01'  # crear conjunto de entrenamiento y de testeo mask_ds = y.index &lt; target_date  y_train = y[mask_ds] y_test = y[~mask_ds]  #plotting the data y_train['passengers'].plot() y_test['passengers'].plot() plt.show() <p>Una pregunta natural que surgue es: \u00bf por qu\u00e9 no se toman datos de manera aleatoria?.</p> <p>La respuesta es que como se trabaja el la variable tiempo, por lo tanto los datos siguen un orden natural de los sucesos, en cambio, en los problemas de regresi\u00f3n no existe orden en los sucesos, por cada par de punto es de cierta forma independiente uno con otros. Adem\u00e1s, si se sacar\u00e1n puntos de testeo de manera aleatoria, podr\u00eda romper con la tendencia y estacionariedad original de la serie.</p> <p>Veamos un ejemplo sensillo de este caso en python:</p> In\u00a0[18]: Copied! <pre>from statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\ndef mean_absolute_percentage_error(y_true, y_pred):\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\ndef regression_metrics(df):\n    \"\"\"\n    Aplicar las distintas m\u00e9tricas definidas\n    :param df: DataFrame con las columnas: ['y', 'yhat']\n    :return: DataFrame con las m\u00e9tricas especificadas\n    \"\"\"\n    df_result = pd.DataFrame()\n\n    y_true = df['y']\n    y_pred = df['yhat']\n\n    df_result['mae'] = [round(mean_absolute_error(y_true, y_pred), 4)]\n    df_result['mse'] = [round(mean_squared_error(y_true, y_pred), 4)]\n    df_result['rmse'] = [round(np.sqrt(mean_squared_error(y_true, y_pred)), 4)]\n    df_result['mape'] = [round(mean_absolute_percentage_error(y_true, y_pred), 4)]\n    df_result['smape'] = [round(2 * mean_absolute_percentage_error(y_true, y_pred) / (mean_absolute_percentage_error(y_true, y_pred) + 100), 4)]\n\n    return df_result\n</pre> from statsmodels.tsa.statespace.sarimax import SARIMAX from sklearn.metrics import mean_absolute_error, mean_squared_error  def mean_absolute_percentage_error(y_true, y_pred):     return np.mean(np.abs((y_true - y_pred) / y_true)) * 100  def regression_metrics(df):     \"\"\"     Aplicar las distintas m\u00e9tricas definidas     :param df: DataFrame con las columnas: ['y', 'yhat']     :return: DataFrame con las m\u00e9tricas especificadas     \"\"\"     df_result = pd.DataFrame()      y_true = df['y']     y_pred = df['yhat']      df_result['mae'] = [round(mean_absolute_error(y_true, y_pred), 4)]     df_result['mse'] = [round(mean_squared_error(y_true, y_pred), 4)]     df_result['rmse'] = [round(np.sqrt(mean_squared_error(y_true, y_pred)), 4)]     df_result['mape'] = [round(mean_absolute_percentage_error(y_true, y_pred), 4)]     df_result['smape'] = [round(2 * mean_absolute_percentage_error(y_true, y_pred) / (mean_absolute_percentage_error(y_true, y_pred) + 100), 4)]      return df_result In\u00a0[19]: Copied! <pre># parametros\nparam = [(1,0,0),(0,0,0,12)]\n\n# modelo\nmodel = SARIMAX(y_train,\n                        order=param[0],\n                        seasonal_order=param[1],\n                        enforce_stationarity=False,\n                        enforce_invertibility=False)\n# ajustar modelo\nmodel_fit = model.fit(disp=0)\n\n# fecha de las predicciones        \nstart_index = y_test.index.min()\nend_index = y_test.index.max()\n\npreds = model_fit.get_prediction(start=start_index,end=end_index, dynamic=False)\ndf_temp = pd.DataFrame(\n            {\n                'y':y_test['passengers'],\n                'yhat': preds.predicted_mean\n            }\n        )\n\n# resultados del ajuste\ndf_temp.head()\n</pre> # parametros param = [(1,0,0),(0,0,0,12)]  # modelo model = SARIMAX(y_train,                         order=param[0],                         seasonal_order=param[1],                         enforce_stationarity=False,                         enforce_invertibility=False) # ajustar modelo model_fit = model.fit(disp=0)  # fecha de las predicciones         start_index = y_test.index.min() end_index = y_test.index.max()  preds = model_fit.get_prediction(start=start_index,end=end_index, dynamic=False) df_temp = pd.DataFrame(             {                 'y':y_test['passengers'],                 'yhat': preds.predicted_mean             }         )  # resultados del ajuste df_temp.head() Out[19]: y yhat 1958-01-31 340.0 336.756041 1958-02-28 318.0 337.513783 1958-03-31 362.0 338.273230 1958-04-30 348.0 339.034386 1958-05-31 363.0 339.797254 In\u00a0[20]: Copied! <pre># resultados de las m\u00e9tricas\ndf_metrics = regression_metrics(df_temp)\ndf_metrics['model'] = f\"SARIMA_{param[0]}X{param[1]}\".replace(' ','')\ndf_metrics\n</pre> # resultados de las m\u00e9tricas df_metrics = regression_metrics(df_temp) df_metrics['model'] = f\"SARIMA_{param[0]}X{param[1]}\".replace(' ','') df_metrics Out[20]: mae mse rmse mape smape model 0 81.8529 11619.4305 107.7935 17.0068 0.2907 SARIMA_(1,0,0)X(0,0,0,12) In\u00a0[21]: Copied! <pre># graficamos resultados\n\npreds = df_temp['yhat']\nax = y['1949':].plot(label='observed')\npreds.plot(ax=ax, label='Forecast', alpha=.7, figsize=(14, 7))\nax.set_xlabel('Date')\nax.set_ylabel('Passengers')\nplt.legend()\nplt.show()\n</pre> # graficamos resultados  preds = df_temp['yhat'] ax = y['1949':].plot(label='observed') preds.plot(ax=ax, label='Forecast', alpha=.7, figsize=(14, 7)) ax.set_xlabel('Date') ax.set_ylabel('Passengers') plt.legend() plt.show() <p>Observamos a simple vista que el ajuste no es tan bueno que digamos (analizando las m\u00e9tricas y el gr\u00e1fico).</p> <p>Entonces, \u00bf qu\u00e9 se puede hacer?. La respuesta es simple \u00a1 probar varios modelos sarima !.</p> <p>Ahora, \u00bf c\u00f3mo lo hacemos?. Lo primero es definir una clase llamada <code>SarimaModels</code> que automatice el proceso anterior, y nos quedamos con aquel modelo que minimice alguna de las m\u00e9tricas propuestas, por ejemplo, minimizar las m\u00e9tricas de mae y mape</p> In\u00a0[22]: Copied! <pre># creando clase SarimaModels\n\nclass SarimaModels:\n    def __init__(self,params):\n\n        self.params = params\n        \n        \n    @property\n    def name_model(self):\n        return f\"SARIMA_{self.params[0]}X{self.params[1]}\".replace(' ','')\n    \n    @staticmethod\n    def test_train_model(y,date):\n        mask_ds = y.index &lt; date\n\n        y_train = y[mask_ds]\n        y_test = y[~mask_ds]        \n        \n        return y_train, y_test\n    \n    def fit_model(self,y,date):\n        y_train, y_test = self.test_train_model(y,date )\n        model = SARIMAX(y_train,\n                        order=self.params[0],\n                        seasonal_order=self.params[1],\n                        enforce_stationarity=False,\n                        enforce_invertibility=False)\n        \n        model_fit = model.fit(disp=0)\n\n        return model_fit\n    \n    def df_testig(self,y,date):\n        y_train, y_test = self.test_train_model(y,date )\n        model = SARIMAX(y_train,\n                        order=self.params[0],\n                        seasonal_order=self.params[1],\n                        enforce_stationarity=False,\n                        enforce_invertibility=False)\n        \n        model_fit = model.fit(disp=0)\n        \n        start_index = y_test.index.min()\n        end_index = y_test.index.max()\n\n        preds = model_fit.get_prediction(start=start_index,end=end_index, dynamic=False)\n        df_temp = pd.DataFrame(\n            {\n                'y':y_test['passengers'],\n                'yhat': preds.predicted_mean\n            }\n        )\n        \n        return df_temp\n    \n    def metrics(self,y,date):\n        df_temp = self.df_testig(y,date)\n        df_metrics = regression_metrics(df_temp)\n        df_metrics['model'] = self.name_model\n        \n        return df_metrics\n</pre> # creando clase SarimaModels  class SarimaModels:     def __init__(self,params):          self.params = params                       @property     def name_model(self):         return f\"SARIMA_{self.params[0]}X{self.params[1]}\".replace(' ','')          @staticmethod     def test_train_model(y,date):         mask_ds = y.index &lt; date          y_train = y[mask_ds]         y_test = y[~mask_ds]                          return y_train, y_test          def fit_model(self,y,date):         y_train, y_test = self.test_train_model(y,date )         model = SARIMAX(y_train,                         order=self.params[0],                         seasonal_order=self.params[1],                         enforce_stationarity=False,                         enforce_invertibility=False)                  model_fit = model.fit(disp=0)          return model_fit          def df_testig(self,y,date):         y_train, y_test = self.test_train_model(y,date )         model = SARIMAX(y_train,                         order=self.params[0],                         seasonal_order=self.params[1],                         enforce_stationarity=False,                         enforce_invertibility=False)                  model_fit = model.fit(disp=0)                  start_index = y_test.index.min()         end_index = y_test.index.max()          preds = model_fit.get_prediction(start=start_index,end=end_index, dynamic=False)         df_temp = pd.DataFrame(             {                 'y':y_test['passengers'],                 'yhat': preds.predicted_mean             }         )                  return df_temp          def metrics(self,y,date):         df_temp = self.df_testig(y,date)         df_metrics = regression_metrics(df_temp)         df_metrics['model'] = self.name_model                  return df_metrics In\u00a0[23]: Copied! <pre># definir parametros \n\nimport itertools\n\np = d = q = range(0, 2)\npdq = list(itertools.product(p, d, q))\nseasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n\nparams = list(itertools.product(pdq,seasonal_pdq))\ntarget_date = '1958-01-01'\n</pre> # definir parametros   import itertools  p = d = q = range(0, 2) pdq = list(itertools.product(p, d, q)) seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]  params = list(itertools.product(pdq,seasonal_pdq)) target_date = '1958-01-01' In\u00a0[24]: Copied! <pre># iterar para los distintos escenarios\n\nframes = []\nfor param in params:\n    try:\n        sarima_model = SarimaModels(param)\n        df_metrics = sarima_model.metrics(y,target_date)\n        frames.append(df_metrics)\n    except:\n        pass\n</pre> # iterar para los distintos escenarios  frames = [] for param in params:     try:         sarima_model = SarimaModels(param)         df_metrics = sarima_model.metrics(y,target_date)         frames.append(df_metrics)     except:         pass In\u00a0[25]: Copied! <pre># juntar resultados de las m\u00e9tricas y comparar\ndf_metrics_result = pd.concat(frames)\ndf_metrics_result.sort_values(['mae','mape'])\n</pre> # juntar resultados de las m\u00e9tricas y comparar df_metrics_result = pd.concat(frames) df_metrics_result.sort_values(['mae','mape']) Out[25]: mae mse rmse mape smape model 0 16.4272 406.5114 20.1621 4.1002 0.0788 SARIMA_(0,1,0)X(1,0,0,12) 0 17.7173 469.8340 21.6757 4.2001 0.0806 SARIMA_(0,1,1)X(1,1,1,12) 0 17.7204 480.9764 21.9312 4.1431 0.0796 SARIMA_(1,1,0)X(1,1,1,12) 0 17.8053 501.0603 22.3844 4.1164 0.0791 SARIMA_(1,1,1)X(0,1,0,12) 0 17.8056 505.4167 22.4815 4.1061 0.0789 SARIMA_(0,1,0)X(0,1,0,12) ... ... ... ... ... ... ... 0 94.9444 14674.5556 121.1386 19.8867 0.3318 SARIMA_(0,1,0)X(0,0,0,12) 0 360.7115 150709.0664 388.2127 82.0329 0.9013 SARIMA_(0,0,1)X(0,0,1,12) 0 366.5303 153175.7993 391.3768 83.7030 0.9113 SARIMA_(0,0,0)X(0,0,1,12) 0 422.9626 187068.9748 432.5147 98.3713 0.9918 SARIMA_(0,0,1)X(0,0,0,12) 0 428.5000 189730.5556 435.5807 100.0000 1.0000 SARIMA_(0,0,0)X(0,0,0,12) <p>64 rows \u00d7 6 columns</p> <p>En este caso el mejor modelo resulta ser el modelo $SARIMA(0,1,0)X(1,0,0,12)$. Veamos gr\u00e1ficamente que tal el ajuste de este modelo.</p> In\u00a0[26]: Copied! <pre># ajustar mejor modelo\n\nparam = [(0,1,0),(1,0,0,12)]\nsarima_model =  SarimaModels(param)\nmodel_fit = sarima_model.fit_model(y,target_date)\nbest_model = sarima_model.df_testig(y,target_date)\nbest_model.head()\n</pre> # ajustar mejor modelo  param = [(0,1,0),(1,0,0,12)] sarima_model =  SarimaModels(param) model_fit = sarima_model.fit_model(y,target_date) best_model = sarima_model.df_testig(y,target_date) best_model.head() Out[26]: y yhat 1958-01-31 340.0 345.765806 1958-02-28 318.0 330.574552 1958-03-31 362.0 390.254475 1958-04-30 348.0 381.573759 1958-05-31 363.0 389.169386 In\u00a0[27]: Copied! <pre># graficar mejor modelo\n\npreds = best_model['yhat']\nax = y['1949':].plot(label='observed')\npreds.plot(ax=ax, label='Forecast', alpha=.7, figsize=(14, 7))\nax.set_xlabel('Date')\nax.set_ylabel('Passengers')\nplt.legend()\nplt.show()\n</pre> # graficar mejor modelo  preds = best_model['yhat'] ax = y['1949':].plot(label='observed') preds.plot(ax=ax, label='Forecast', alpha=.7, figsize=(14, 7)) ax.set_xlabel('Date') ax.set_ylabel('Passengers') plt.legend() plt.show() <p>Para este caso, el mejor modelo encontrado se ajusta bastante bien a los datos.</p> <p>Finalmente, vemos algunos resultados del error asosciado al modelo. Para esto ocupamos la herramienta plot_diagnostics, el cual nos arroja cuatro gr\u00e1ficos analizando el error de diferentes manera.</p> In\u00a0[28]: Copied! <pre># resultados del error \nmodel_fit.plot_diagnostics(figsize=(16, 8))\nplt.show()\n</pre> # resultados del error  model_fit.plot_diagnostics(figsize=(16, 8)) plt.show() <ul> <li><p>gr\u00e1fico 01 (standarized residual): Este gr\u00e1fico nos muestra el error estandarizado en el tiempo. En este caso se observa que esta nueva serie de tiempo corresponde a una serie estacionaria que oscila entorno al cero, es decir, un ruido blanco.</p> </li> <li><p>gr\u00e1fico 02 (histogram plus estimated density): Este gr\u00e1fico nos muestra el histograma del error. En este caso, el histograma es muy similar al histograma de una variable $\\mathcal{N}(0,1)$ (ruido blanco).</p> </li> <li><p>gr\u00e1fico 03 (normal QQ):  el gr\u00e1fico Q-Q (\"Q\" viene de cuantil) es un m\u00e9todo gr\u00e1fico para el diagn\u00f3stico de diferencias entre la distribuci\u00f3n de probabilidad de una poblaci\u00f3n de la que se ha extra\u00eddo una muestra aleatoria y una distribuci\u00f3n usada para la comparaci\u00f3n. En este caso se comparar la distribuci\u00f3n del error versus una distribuci\u00f3n normal. Cuando mejor es el ajuste lineal sobre los puntos, m\u00e1s parecida es la distribuci\u00f3n entre la muestra obtenida y la distribuci\u00f3n de prueba (distribuci\u00f3n normal).</p> </li> <li><p>gr\u00e1fico 04 (correlogram): Este gr\u00e1fico nos muestra el gr\u00e1fico de autocorrelaci\u00f3n entre las variables del error, se observa que no hay correlaci\u00f3n entre ninguna de las variables, por lo que se puedan dar indicios de independencia entre las variables.</p> </li> </ul> <p>En conclusi\u00f3n, el error asociado al modelo en estudio corresponde a un ruido blanco.</p> In\u00a0[29]: Copied! <pre>from prophet import Prophet\n</pre> from prophet import Prophet <pre>C:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\db-connectors-RsEieBu8-py3.8\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[30]: Copied! <pre># rename \ny_train_prophet = y_train.reset_index()\ny_train_prophet.columns = [\"ds\",\"y\"]\n\ny_test_prophet = y_test.reset_index()\ny_test_prophet.columns = [\"ds\",\"y\"]\n</pre> # rename  y_train_prophet = y_train.reset_index() y_train_prophet.columns = [\"ds\",\"y\"]  y_test_prophet = y_test.reset_index() y_test_prophet.columns = [\"ds\",\"y\"] In\u00a0[31]: Copied! <pre># model\nm = Prophet()\nm.fit(y_train_prophet)\n</pre> # model m = Prophet() m.fit(y_train_prophet) <pre>21:45:53 - cmdstanpy - INFO - Chain [1] start processing\n21:45:53 - cmdstanpy - INFO - Chain [1] done processing\n</pre> Out[31]: <pre>&lt;prophet.forecaster.Prophet at 0x20c23ff6d60&gt;</pre> In\u00a0[32]: Copied! <pre># forecast\nfuture = m.make_future_dataframe(periods=365*4)\nforecast = m.predict(future)[['ds', 'yhat']]\nforecast.tail()\n</pre> # forecast future = m.make_future_dataframe(periods=365*4) forecast = m.predict(future)[['ds', 'yhat']] forecast.tail() Out[32]: ds yhat 1563 1961-12-26 535.962819 1564 1961-12-27 534.927951 1565 1961-12-28 533.244132 1566 1961-12-29 530.943853 1567 1961-12-30 528.076593 In\u00a0[33]: Copied! <pre># metrics\nresult = y_test_prophet.merge(forecast,on = 'ds',how='inner')\nregression_metrics(result)\n</pre> # metrics result = y_test_prophet.merge(forecast,on = 'ds',how='inner') regression_metrics(result) Out[33]: mae mse rmse mape smape 0 39.9557 2013.1356 44.868 9.8525 0.1794 In\u00a0[34]: Copied! <pre>preds = result[['ds','yhat']].set_index(\"ds\")\nax = y['1949':].plot(label='observed')\npreds.plot(ax=ax, label='Forecast', alpha=.7, figsize=(14, 7))\n\nax.set_xlabel('Date')\nax.set_ylabel('Passengers')\nplt.legend()\nplt.show()\n</pre> preds = result[['ds','yhat']].set_index(\"ds\") ax = y['1949':].plot(label='observed') preds.plot(ax=ax, label='Forecast', alpha=.7, figsize=(14, 7))  ax.set_xlabel('Date') ax.set_ylabel('Passengers') plt.legend() plt.show()"},{"location":"lectures/machine_learning/ts_01/#series-temporales-i","title":"Series Temporales I\u00b6","text":""},{"location":"lectures/machine_learning/ts_01/#introduccion","title":"Introducci\u00f3n\u00b6","text":""},{"location":"lectures/machine_learning/ts_01/#analisis-de-series-temporales","title":"An\u00e1lisis de series temporales\u00b6","text":"<p>Para entender mejor el comportamiento de una serie temporal, nos iremos ayudando con python.</p>"},{"location":"lectures/machine_learning/ts_01/#descripcion-del-problema","title":"Descripci\u00f3n del problema\u00b6","text":"<p>El conjunto de datos se llama <code>AirPassengers.csv</code>, el cual contiene la informaci\u00f3n del total de pasajeros (a nivel de mes) entre los a\u00f1o 1949 y 1960.</p> <p>En terminos  estad\u00edsticos, el problema puede ser presentado por la serie temporal $\\left \\{ X_t:  t \\in T \\right \\}$ , donde:</p> <ul> <li>$X_{t}$: corresponde al total de pasajeros en el tiempo t</li> <li>$t$: tiempo medido a nivel de mes.</li> </ul> <p>Comparando la serie temporal con un dataframe, este ser\u00eda un dataframe de una sola columna, en cuyo \u00edndice se encuentra las distintas fechas.</p> <p>El objetivo es poder desarrollar un modelo predictivo que me indique el n\u00famero de pasajeros para los pr\u00f3ximos dos a\u00f1os. Antes de ajustar el modelo, se debe entender el comportamiento de la serie de tiempo en estudio y con esta informaci\u00f3n, encontrar el modelo que mejor se puede ajustar (en caso que exista).</p> <p>Como son muchos los conceptos que se presentar\u00e1n, es necesario ir apoyandose con alguna herramienta de programaci\u00f3n, en nuestro caso python. Dentro de python, la librer\u00eda statsmodels es ideal para hacer este tipo de an\u00e1lisis.</p> <p>Lo primero es cargar, transformar y visualizar el conjunto de datos.</p>"},{"location":"lectures/machine_learning/ts_01/#descomposicion-stl","title":"Descomposici\u00f3n STL\u00b6","text":"<p>Una serie temporal la podemos descomponer en tres componentes:</p> <ul> <li>tendencia ($T$): trayectoria de los datos en el tiempo (direcci\u00f3n positiva o negativa).</li> <li>estacionalidad($S$):  fluctuaciones regulares y predecibles en un periodo determinado (anual, semestral,etc.)</li> <li>ruido($e$): error intr\u00ednsico al tomar una serie temporal (instrumenos, medici\u00f3n humana, etc.)</li> </ul> <p>Cuando un descompone la serie temporla en sus tres componenctes (tendencia, estacionalidad, ruido) se habla de descompocisi\u00f3n STL. En muchas ocasiones no es posible descomponer adecuadamente la serie temporal, puesto que la muestra obtenida no presenta un comportamiento ciclico o repetitivo en el periodo de tiempo analizado.</p> <p>Por otro lado, esta descomposici\u00f3n se puede realizar de dos maneras diferentes:</p> <ul> <li>Aditiva: $$X_{t} = T_{t} + S_{t} + e_{t}$$</li> <li>Multiplicativa: $$X_{t} = T_{t} * S_{t} * e_{t}$$</li> </ul> <p>Por suepuesto esta no es la \u00fanica forma de descomponer una serie, pero sirve como punto de partida para comprender nuestra serie en estudio.</p> <p>Realizaremos un descomposici\u00f3n del tipo multiplicativa, ocupando el comando de statsmodels seasonal_decompose.</p>"},{"location":"lectures/machine_learning/ts_01/#series-estacionarias","title":"Series Estacionarias\u00b6","text":"<p>Un concepto importante para el \u00e1nalisis de series temporales, es el concepto de estacionariedad.</p>"},{"location":"lectures/machine_learning/ts_01/#definicion","title":"Definici\u00f3n\u00b6","text":"<p>Sea $\\left \\{ X_t: t \\in T \\right \\}$ una serie temporal. Se dice que una serie temporal es d\u00e9bilmente estacionaria si:</p> <ul> <li>$\\mathbb{V}(X_t) &lt; \\infty$, para todo $t \\in T$.</li> <li>$\\mu_{X}(t)= \\mu$, para todo $t \\in T$.</li> <li>$\\gamma_{X}(r,s)= \\gamma_{X}(r+h,s+h)=\\gamma_{X}(h)  $, para todo $r,s,h\\in T$.</li> </ul> <p>En palabras simple, una serie temporal es d\u00e9bilmente estacionaria si var\u00eda poco respecto a su propia media.</p> <p>Veamos las siguientes im\u00e1genes:</p>"},{"location":"lectures/machine_learning/ts_01/#formas-de-detectar-la-estacionariedad","title":"Formas de detectar la estacionariedad\u00b6","text":"<p>La manera m\u00e1s simple es gr\u00e1ficarla e inferir el comportamiento de esta. La ventaja que este m\u00e9todo es r\u00e1pido, sin embargo, se encuentra sesgado por el criterio del ojo humano.</p> <p>Por otro lado existen algunas alternativas que aqu\u00ed presentamos:</p>"},{"location":"lectures/machine_learning/ts_01/#modelos-de-forecast-pronostico","title":"Modelos de forecast  (Pron\u00f3stico)\u00b6","text":"<p>Para realizar el pron\u00f3stico de series, existen varios modelos cl\u00e1sicos para analizar series temporales:</p> <p>Modelos con variabilidad (varianza) constante</p> <ul> <li><p>Modelos de media m\u00f3vil (MA(d)): el modelo queda en funci\u00f3n de los ruidos $e_{1},e_{2},...,e_{d}$</p> </li> <li><p>Modelos autorregresivos (AR(q)): el modelo queda en funci\u00f3n de los ruidos $X_{1},X_{2},...,X_{q}$</p> </li> <li><p>Modelos ARMA  (ARMA(p,q)): Mezcla de los modelos $MA(d)$ y $AR(q)$</p> </li> <li><p>Modelos ARIMA (ARIMA(p,d,q)):: Mezcla de los modelos $MA(d)$ y $AR(q)$ sobre la serie diferenciada $d$ veces.</p> </li> <li><p>Modelos SARIMA (SARIMA(p,d,q)x(P,D,Q,S)): Mezcla de los modelos ARIMA(p,d,q) agregando componentes de estacionariedad ($S$).</p> </li> </ul> <p>Dentro de estos modelos, se tiene que uno son un caso particular de otro m\u00e1s general:</p> <p>$$MA(d),AR(q) \\subset ARMA(p,q) \\subset ARIMA(p,d,q)  \\subset SARIMA(p,d,q)x(P,D,Q,S)  $$</p> <p>Modelos de volatibilidad</p> <ul> <li><p>Arch</p> </li> <li><p>Garch</p> </li> <li><p>Modelos de espacio estado</p> </li> </ul>"},{"location":"lectures/machine_learning/ts_01/#realizar-pronostico-con-statsmodels","title":"Realizar pron\u00f3stico con statsmodels\u00b6","text":"<p>El pron\u00f3stico lo realizaremos ocupando los modelos disponible en statsmodels, particularmen los modelos $SARIMA(p,d,q)x(P,D,Q,S)$.</p> <p>Como todo buen proceso de machine learning es necesario separar nuestro conjunto de datos en dos (entrenamiento y testeo). \u00bf C\u00f3mo se realiza esto con series temporales ?.</p> <p>El camino correcto para considerar una fecha objetivo (target_date), el cual separ\u00e9 en dos conjuntos, de la siguiente manera:</p> <ul> <li>y_train: todos los datos hasta la fecha target_date</li> <li>y_test: todos los datos despu\u00e9s la fecha target_date</li> </ul>"},{"location":"lectures/machine_learning/ts_01/#prophet","title":"Prophet\u00b6","text":"<p>Prophet es un procedimiento para pronosticar datos de series temporales basado en un modelo aditivo en el que las tendencias no lineales se ajustan a la estacionalidad anual, semanal y diaria, adem\u00e1s de los efectos de las vacaciones. Funciona mejor con series temporales que tienen fuertes efectos estacionales y varias temporadas de datos hist\u00f3ricos. Prophet es resistente a los datos faltantes y los cambios en la tendencia, y por lo general maneja bien los valores at\u00edpicos.</p> <p>Prophet es un software de c\u00f3digo abierto lanzado por el equipo Core Data Science de Facebook. Est\u00e1 disponible para descargar en CRAN y PyPI.</p> <p>Nota: Para entender mejor este algoritmo, puede leer el siguiente paper.</p>"},{"location":"lectures/machine_learning/ts_01/#conclusion","title":"Conclusi\u00f3n\u00b6","text":"<p>Este fue una introducci\u00f3n amigable a los conceptos claves de series temporales, a medida que m\u00e1s se profundice en la teor\u00eda, mejor ser\u00e1n las t\u00e9cnicas empleadas sobre la serie temporal en fin de obtener el mejor pron\u00f3stico posible.</p> <p>En esta secci\u00f3n nos limitamos a algunos modelos y algunos criterios de verificaci\u00f3n de estacionariedad. En la literatura existen muchas m\u00e1s, pero con los mostrados de momento y un poco de expertice en el tema, se pueden abordar casi todos los problemas de series temporales.</p>"},{"location":"lectures/machine_learning/ts_01/#referencias","title":"Referencias\u00b6","text":"<ol> <li>A comprehensive beginner\u2019s guide to create a Time Series Forecast (with Codes in Python and R)</li> <li>A Gentle Introduction to SARIMA for Time Series Forecasting in Python</li> <li>An Introductory Study on Time Series Modeling and Forecasting </li> </ol>"},{"location":"lectures/toolkit/bt_intro/","title":"Introducci\u00f3n","text":""},{"location":"lectures/toolkit/bt_intro/#sistema-operativo","title":"Sistema Operativo","text":"<p>Los sistemas operativos son programas de software que gestionan los recursos de hardware y  proporcionan servicios comunes para otros programas que se ejecutan en una computadora.</p> <p>B\u00e1sicamente, act\u00faan como intermediarios entre el hardware de la computadora y  el software de aplicaci\u00f3n. Los sistemas operativos realizan tareas como la gesti\u00f3n de memoria, la gesti\u00f3n de archivos, el control de dispositivos, la administraci\u00f3n de procesos y la interfaz de usuario.</p> <p>Algunos ejemplos comunes de sistemas operativos son Windows, macOS, Linux, Android e iOS.</p> <p>\ud83d\udca1 Recomendaciones</p> <ul> <li>Personalmente recomiendo Linux, en particular distribuciones como Ubuntu, Mint o Fedora por su facilidad a la hora de instalar.</li> <li>En ocasiones las implementaciones en Windows no est\u00e1n completamente integradas e inclusive en ocasiones no est\u00e1n disponibles.<ul> <li>Una alternativa es Windows Subsystem for Linux, pero lamentablemente no se asegura un 100% de compatibilidad.</li> </ul> </li> <li>En el caso que poseas un equipo con macOS no deber\u00eda haber problema.</li> </ul> <p>Interfaz de L\u00ednea de Comandos (Command Line Interface / CLI)</p> <p>La Interfaz de L\u00ednea de Comandos es un tipo de interfaz de usuario que permite a los usuarios interactuar con un sistema inform\u00e1tico mediante comandos de texto introducidos  a trav\u00e9s de una l\u00ednea de comando. En lugar de utilizar elementos gr\u00e1ficos como ventanas,  botones y men\u00fas, los usuarios ingresan comandos espec\u00edficos en un s\u00edmbolo del sistema o terminal.</p> <p>En una CLI, los usuarios escriben comandos y argumentos de texto plano para ejecutar diversas tareas, como administrar archivos, manipular configuraciones del sistema, ejecutar programas y realizar diversas operaciones inform\u00e1ticas. </p> <p>Esto proporciona un nivel de control y flexibilidad avanzado para usuarios experimentados, aunque puede tener una curva de aprendizaje m\u00e1s pronunciada en comparaci\u00f3n con las interfaces gr\u00e1ficas de usuario (GUI).</p> <p></p>"},{"location":"lectures/toolkit/bt_intro/#python","title":"Python","text":"<p>Python es un lenguaje de programaci\u00f3n interpretado que destaca por su claridad y legibilidad en el c\u00f3digo. Es multiparadigma, lo que significa que admite diferentes estilos de programaci\u00f3n, incluyendo orientaci\u00f3n a objetos, programaci\u00f3n imperativa y, en menor medida, programaci\u00f3n funcional. Adem\u00e1s, es din\u00e1mico y multiplataforma, lo que lo hace vers\u00e1til y ampliamente utilizado en diversos entornos de desarrollo.</p> <p>En el \u00e1mbito cient\u00edfico y de an\u00e1lisis de datos, Python cuenta con una variedad de bibliotecas especializadas. Algunas de las m\u00e1s importantes son:</p> <ul> <li>Numpy: para computaci\u00f3n cient\u00edfica.</li> <li>Pandas: ideal para el an\u00e1lisis de datos.</li> <li>Matplotlib: proporciona herramientas para visualizaci\u00f3n de datos.</li> <li>Scikit-Learn: una biblioteca esencial para machine learning.</li> </ul> <p>Adem\u00e1s, durante el curso se explorar\u00e1n otras bibliotecas complementarias, como scipy, seaborn y statsmodels, entre otras. Estas herramientas ampl\u00edan las capacidades de Python en \u00e1reas espec\u00edficas, como estad\u00edsticas, visualizaci\u00f3n avanzada y an\u00e1lisis cient\u00edfico.</p>"},{"location":"lectures/toolkit/bt_intro/#entorno-virtual","title":"Entorno Virtual","text":"<p>Problemas recurrentes: - Incompatibilidad entre dependencias de librer\u00edas (packages). - Dificultad para compartir y reproducir resultados debido a la falta de conocimiento sobre las versiones de las librer\u00edas instaladas. - Tedioso y costoso tener que usar una m\u00e1quina virtual diferente para cada desarrollo. - Temor constante a la interrupci\u00f3n del funcionamiento del script al instalar nuevas herramientas o librer\u00edas.</p> <p>Soluci\u00f3n: Aislar el desarrollo para mejorar la compatibilidad y la reproducibilidad de los resultados.</p> <p>Para el curso (recomendado):</p> <p></p> <p>Gesti\u00f3n de paquetes, dependencias y entornos para cualquier lenguaje: Python, R, Ruby, Lua, Scala, Java, JavaScript, C/C++, FORTRAN.</p> <p>\u00bfPor qu\u00e9 Conda? - Es de c\u00f3digo abierto. - Permite gestionar tanto librer\u00edas como entornos virtuales. - Compatible con Linux, Windows y macOS. - No est\u00e1 limitado a un solo lenguaje de programaci\u00f3n (aunque se desarroll\u00f3 inicialmente para Python). - F\u00e1cil de instalar y utilizar.</p> <p>Otras alternativas: - <code>pip + virtualenv</code>: Pip es el gestor de librer\u00edas preferido en Python, mientras que Virtualenv es un gestor de entornos virtuales. Sin embargo, esta combinaci\u00f3n es exclusiva de Python. - <code>Pipenv</code> o <code>Poetry</code>: Librer\u00edas especializadas en la gesti\u00f3n de dependencias (altamente recomendadas).</p> <p>\ud83d\udd11 Nota: Las sugerencias anteriores est\u00e1n orientadas principalmente al trabajo en entornos locales. Sin embargo, dado que todo el curso est\u00e1 dise\u00f1ado para trabajar en Google Colab.</p>"},{"location":"lectures/toolkit/bt_intro/#project-jupyter","title":"Project Jupyter","text":"<p>Project Jupyter exists to develop open-source software, open-standards, and services for interactive computing across dozens of programming languages.*</p> <p></p>"},{"location":"lectures/toolkit/bt_intro/#jupyter-notebook","title":"Jupyter Notebook","text":"<p>Es una aplicaci\u00f3n web que permite crear y compartir documentos que contienen c\u00f3digo, ecuaciones, visualizaciones y texto. Entre sus usos se encuentra:</p> <ul> <li>Limpieza de datos</li> <li>Transformaci\u00f3n de datos</li> <li>Simulaciones num\u00e9ricas</li> <li>Modelamiendo Estad\u00edstico</li> <li>Visualizaci\u00f3n de Datos</li> <li>Machine Learning</li> <li>Mucho m\u00e1s.</li> </ul>"},{"location":"lectures/toolkit/bt_intro/#jupyter-lab","title":"Jupyter Lab","text":"<ul> <li>Es la siguiente generaci\u00f3n de la interfaz de usuario de Project Jupyter.</li> <li>Similar a Jupyter Notebook cuenta con la facilidad de editar archivos .ipynb (notebooks) y heramientas como una terminal, editor de texto, explorador de archivos, etc.</li> <li>Eventualmente Jupyter Lab reemplazar\u00e1 a Jupyter Notebok (aunque la versi\u00f3n estable fue liberada hace algunos meses).</li> <li>Cuenta con una serie de extensiones que puedes instalar (y desarrollar inclurisve.</li> <li>M\u00e1s informaci\u00f3n en: https://github.com/jupyterlab/jupyterlab-demo</li> </ul>"},{"location":"lectures/toolkit/bt_intro/#otros-proyectos","title":"Otros Proyectos","text":"<p>Entre los m\u00e1s conocidos se encuentran:</p> <ul> <li>JupyterHub: Distribuir Jupyter Noterbooks a m\u00faltiples usuarios.</li> <li>nbviewer: Compartir Jupyter Notebooks.</li> <li>Jupyter Book: Construir y publicar libros de t\u00f3picos computacionales.</li> <li>Jupyter Docker Stacks: Im\u00e1genes de Jupyter para utilizar en Docker.</li> </ul>"},{"location":"lectures/toolkit/bt_intro/#versionamiento-de-codigo","title":"Versionamiento de C\u00f3digo","text":"<ul> <li> <p>Permite compartir el c\u00f3digo fuente de nuestros desarrollos y a la vez mantener un registro de los cambios por los que va pasando.</p> </li> <li> <p>Herramienta m\u00e1s importante y fundamental dentro del desarrollo.</p> </li> <li>Tipos de versionadores de c\u00f3digo:</li> <li>Sistemas Centralizados: Son los m\u00e1s \"tradicionales\", por ejemplo SVN, CVS, etc.</li> <li>Sistemas Distribuidos: son los que est\u00e1n en auge actualmente como: Git, Mercurial, Bazaar, etc.</li> </ul>"},{"location":"lectures/toolkit/bt_intro/#git","title":"Git","text":"<p>Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency.</p> <p>Es importante comprender que Git es la herramienta que permite versionar tus proyectos, sin embargo, a la hora de querer aprovechar m\u00e1s funcionalidades, como compartir o sincronizar tus trabajos se hace necesario utilizar servicios externos. Los m\u00e1s famosos son:</p> <ul> <li>GitHub</li> <li>GitLab</li> <li>Bitbucket</li> </ul> <p>Piensa lo siguiente, cualquiera podr\u00eda implementar un correo electr\u00f3nico entre dos computadoras conectadas entre ellas por LAN pero no conectadas a Internet. Sin embargo la gente utiliza servicios como Gmail, Outlook, etc. con tal de aprovechar de mejor manera las funcionalidades que ofrece la tecnolog\u00eda del correo electr\u00f3nico. Esta es una analog\u00eda perfecta entre las diferencias de Git y los servicios como GitHub o GitLab.</p>"},{"location":"lectures/toolkit/bt_intro/#github","title":"GitHub","text":"<p>GitHub is a development platform inspired by the way you work. From open source to business, you can host and review code, manage projects, and build software alongside 30 million developers.M</p>"},{"location":"lectures/toolkit/bt_intro/#configuraciones","title":"Configuraciones","text":"<ul> <li>GitHub: Crear una cuenta directamente en el sitio oficial.</li> <li>Google Colab: Herrramienta de trabajo oficial. Cada clase con c\u00f3digo tendr\u00e1 el siguiente logo (donde deben hacer click): </li> <li>Portafolio Personal: Para los entregables del curso, utilizaremos la plantilla de MAT281-Portfolio.</li> </ul> <p>\ud83d\udd11 Nota: Las instrucciones de uso se encuentran en el siguiente archivo.</p>"},{"location":"lectures/toolkit/git/","title":"Git","text":""},{"location":"lectures/toolkit/git/#que-es-git","title":"\u00bfQu\u00e9 es git?","text":"<p>Es importante dejar claro que git no es lo mismo que GitHub, git es un sistema de control de versiones Open Source creado en el a\u00f1o 2005 por Linus Torvalds, el creador de Linux, por otro lado GitHub es un servicio de alojamiento para proyectos versionados con git, de esta forma podemos tener una copia local y una remota de nuestros repositorios.</p> <p>Con git podemos entre otras cosas: tener un control de ediciones versionadas, regresar a versiones anteriores de nuestros proyectos, revisar el registro de cambios y crear ramas experimentales o mejoras paralelas al proyecto troncal.</p>"},{"location":"lectures/toolkit/git/#instalacion","title":"Instalaci\u00f3n","text":"<p>Primero debes descargar e instalar la versi\u00f3n de git para tu sistema operativo desde https://git-scm.com/downloads</p> <p>Para comprobar que la instalaci\u00f3n fue realizada en la terminal de comandos escribes <code>git --version</code>, si fue instalado ver\u00e1s la versi\u00f3n de tu instalaci\u00f3n de git.</p>"},{"location":"lectures/toolkit/git/#configuracion-basica","title":"Configuraci\u00f3n b\u00e1sica","text":"<p>Es importante configurar al menos un nombre de usuario y un correo electr\u00f3nico, para ello escribes en la terminal <code>git config --global user.name \u2018tunombre\u2019</code>, pasando entre comillas tu nombre de usuario y <code>git config --global user.email tuemail</code> pasando sin comillas tu email.</p>"},{"location":"lectures/toolkit/git/#inicializar-un-repositorio","title":"Inicializar un repositorio","text":"<p>Para que la carpeta de un proyecto en tu equipo pase a ser un repositorio de git debes inicializarla, en la terminal debes posicionarte en la carpeta del proyecto y escribir <code>git init</code>, esto crear\u00e1 una carpeta oculta de nombre .git la cual contiene toda la informaci\u00f3n del versionamiento.</p> <p>A continuaci\u00f3n se hace necesario comprender los estados de git en nuestro repositorio local y los comando necesarios para movernos entre ellos.</p> <p></p> <p>En la imagen de referencia se describen tres estados: el working directory, este es el sistema de archivos en el cual trabajamos, el staging area, un \u00e1rea temporal en el que a\u00f1adimos los archivos cuyos cambios estamos por enviar a git y el repository, donde se versiona nuestro trabajo.</p>"},{"location":"lectures/toolkit/git/#anadir-archivos-status-y-commit","title":"A\u00f1adir archivos, status y commit","text":"<p>Para a\u00f1adir cambios al staging area, lo haces con <code>git add .</code> para incluir los cambios realizados en todos tus archivos y <code>git add archivo.txt</code> si lo que quieres es incluir los cambios de un archivo en particular.</p> <p>Para comprobar el estado de los archivos escribimos <code>git status</code>, de esta forma sabr\u00e1s cuales est\u00e1n en el staging area listo para ser enviados al repositorio.</p> <p>Para a\u00f1adir los cambios que est\u00e1n en espera al repositorio escribes <code>git commit -m \u2018Mensaje que explique la funci\u00f3n del commit\u2019</code>, con el flag -m pasa un mensaje entre comillas que entregue detalles del cambio a\u00f1adido, el commit crea puntos de control en la l\u00ednea de tiempo de tu repositorio.</p> <p>Para utilizar un solo comado y realizar el git add + git commit puedes utilizar el flag -a, de la siguiente manera <code>git commit -a -m \u2018Mensaje que explique la funci\u00f3n del commit\u2019</code>.</p> <p>Si agregaste alg\u00fan cambio al staging area pero quieres quitarlo de ah\u00ed puedes escribir  <code>git reset HEAD archivo.txt</code></p>"},{"location":"lectures/toolkit/git/#ignorar-y-borrar-archivos","title":"Ignorar y borrar archivos","text":"<p>En cada proyecto hay archivos que no deseas versionar ya sea por su peso o su formato, para esto es que git te permite crear el archivo .gitignore dentro del cual debes detallar las carpetas o archivos que ser\u00e1n ignorados. A continuaci\u00f3n un ejemplo.</p> <p></p> <p>Para borrar los cambio realizados en un archivo, en otras palabras regresar a su \u00faltima versi\u00f3n guardada o si borraste por error un archivos que ya hab\u00eda sido enviado en un commit antes y quieres restaurarlo debes escribir en la l\u00ednea de comandos <code>git checkout archivo.txt</code>, entonces git lo regresar\u00e1 a su \u00faltima versi\u00f3n.</p> <p>Si lo que quieres es borrar un archivo del repositorio debes ejecutar el comando <code>git rm archivo.txt</code>, en el pr\u00f3ximo commit el archivo ser\u00e1 removido. </p>"},{"location":"lectures/toolkit/git/#usando-ramas","title":"Usando ramas","text":"<p>Con el comando <code>git log</code> puedes ver el historial de commits enviados al repositorio con los datos del usuario que los realiz\u00f3, un ID \u00fanico y el comentario de referencia que explica su funci\u00f3n. Si lo que deseas es retroceder hasta una versi\u00f3n \u2018x\u2019, solo debes escribir en la terminal el siguiente comando <code>git checkout IdCommit</code> donde IdCommit son los primeros 7 caracteres del ID proporcionado en el log. Para crear una rama de desarrollo desde ese punto o desde donde te encuentras debes ejecutar <code>git checkout -b NombreRama</code></p> <p>Ahora te encuentras en otra rama, puedes crear cuantas quieras y pasa saber sus nombre y navegar luego entre ellas es que puedes ejecutar <code>git branch</code></p> <p>Es importante mencionar que la rama troncal tiene por nombre master. Si lo que necesitas es regresar de una rama a master o simplemente moverte de una rama a otra ejecutas <code>git checkout NombreRama</code></p> <p>Para subir tu rama local al repositorio remoto (en el cual la rama no esta creada), ejecutas <code>git push origin NombreRama</code>, este comando se utiliza con m\u00e1s frecuencia para subir los cambios realizados en el repositorio local, luego darles commit, hacia el repositorio remoto. Para traer los cambios que otro integrante del equipo subi\u00f3 en una rama a tu repositorio local ejecutas <code>git pull origin NombreRama</code>, puedes traer cambios tu misma rama en su version remota o de otra rama con la cual quieras actualizar la tuya.</p> <p>En el caso de querer borrar una rama, ejecutas <code>git branch -D NombreRama</code> forzando el borrado de una rama sin haber ejecutado antes un merge. Para fusionar dos ramas o hacer un merge, debes estar sobre la rama que quieres que permanezca (ej: hacer un merge de un feature a la rama develop) y ejecutas el comando <code>git merge NombreRama</code> donde NombreRama el nombre de la rama que queremos integrar. Luego podemos si lo deseas, puedes borrar la rama ejecutando <code>git branch -d NombreRama</code></p>"},{"location":"lectures/toolkit/git/#repositorio-remoto","title":"Repositorio remoto","text":"<p>GitHub es una de la posibilidades que tienes para crear tu repositorio de forma remota. Para ello debes crear un repositorio, darle un nombre y si quieres escribes una descripci\u00f3n. El comando que te permite asociar tu repositorio local (previamente inicializado) con el remoto es:  <code>git remote add origin https://github.com/nombre-usuario/nombre-repositorio.git</code> luego para subir el repositorio local ejecutas git push -u origin master, origin master es como se nombra a este repositorio remoto.</p> <p>Si lo que deseas es descargar tu repositorio en otro equipo puedes clonar tu repositorio remoto con el comando:  <code>git clone https://github.com/nombre-usuario/nombre-repositorio.git</code> </p> <p>Por \u00faltimo ya que alguien m\u00e1s del equipo de desarrollo pudo haber subido alg\u00fan cambio, para no estar trabajando en una versi\u00f3n desactualizada debes bajar la \u00faltima actualizaci\u00f3n de tu repositorio remoto a tu local, esto se hace con <code>git pull origin master</code>. Pull trae los cambio y push los env\u00eda.</p>"},{"location":"lectures/toolkit/git/#resumen-de-comandos-git","title":"Resumen de comandos git","text":"comando funci\u00f3n git init inicializa un repositorio git add agrega cambios a staging area git status muestra el estado de los cambios git reset HEAD elimina un cambio del staging area git commit env\u00eda un cambio del staging al repositorio git rm borra cambio del repositorio git checkout trae la \u00faltima versi\u00f3n del cambio, crea y cambia de rama git branch muestra las ramas git merge fusiona las ramas git remote add origin relacionar repo local a un repo remoto git clone clonar repositorio remoto git pull trae la \u00faltima versi\u00f3n del repo remoto git push env\u00eda los cambio al repo remoto"},{"location":"lectures/toolkit/git/#resumen-comandos-consola","title":"Resumen comandos consola","text":"comando funci\u00f3n pwd (unix) - cd (windows) muestra el directorio actual cd  + directorio cambio de directorio ls (unix) - dir (windows) listar archivos del directorio cd .. sube un directorio"},{"location":"lectures/toolkit/git/#referencias","title":"Referencias","text":"<ul> <li>git para dummies</li> <li>Git - la gu\u00eda sencilla </li> <li>Te ense\u00f1o git en 30 minutos </li> <li>Git: Subir un proyecto a Github</li> <li>Curso gratis Devcode git y github</li> </ul>"},{"location":"lectures/toolkit/github/","title":"Github","text":"<p>\ud83d\udd11 Nota: Este tutorial se basa en el art\u00edculo publicado por Platzi - Qu\u00e9 es GitHub y c\u00f3mo usarlo para aprovechar sus beneficios.</p>"},{"location":"lectures/toolkit/github/#introduccion","title":"Introducci\u00f3n","text":"<p>GitHub es una plataforma de alojamiento, propiedad de Microsoft, que ofrece a los desarrolladores la posibilidad de crear repositorios de c\u00f3digo y guardarlos en la nube de forma segura, usando un sistema de control de versiones llamado Git.</p> <p>Facilita la organizaci\u00f3n de proyectos y permite la colaboraci\u00f3n de varios desarrolladores en tiempo real. Es decir, nos permite centralizar el contenido del repositorio para poder colaborar con los otros miembros de nuestra organizaci\u00f3n.</p> <p>GitHub esta basada en el sistema de control de versiones distribuida de Git, por lo que se puede contar con sus funciones y herramientas, aunque GitHub ofrece varias opciones adicionales y su interfaz es mucho m\u00e1s f\u00e1cil de manejar, por lo que no es absolutamente necesario que las personas que lo usan tengan un gran conocimiento t\u00e9cnico. Aqu\u00ed puedes conocer m\u00e1s sobre su historia.</p> <p>Si t\u00fa ya has utilizado este servicio de alojamiento para tus proyectos, cu\u00e9ntame en los comentarios qu\u00e9 tal te ha funcionado y cu\u00e1les son tus mejores tips. Si a\u00fan no lo conoces o te gustar\u00eda aprovecharlo mejor, recuerda que en Platzi tenemos un curso de GitHub ideal para ti y que puedes empezar sin costo.</p> <p></p>"},{"location":"lectures/toolkit/github/#ventajas-de-github","title":"Ventajas de GitHub","text":"<p>Existe un gran n\u00famero de razones que convierten a GitHub en una gran opci\u00f3n para el control y gesti\u00f3n de tus proyectos de c\u00f3digo. Aqu\u00ed algunas de ellas:</p> <ul> <li>GitHub permite que alojemos proyectos en repositorios de forma gratuita</li> <li>Te brinda la posibilidad de personalizar tu perfil en la plataforma</li> <li>Los repositorios son p\u00fablicos por defecto. Sin embargo, GitHub te permite tambi\u00e9n alojar tus proyectos de forma privada</li> <li>Puedes crear y compartir p\u00e1ginas web est\u00e1ticas con GitHub Pages</li> <li>Facilita compartir tus proyectos de una forma mucho m\u00e1s f\u00e1cil y crear un portafolio</li> <li>Te permite colaborar para mejorar los proyectos de otros y a otros mejorar o aportar a los tuyos</li> <li>Ayuda reducir significativamente los errores humanos y escribir tu c\u00f3digo m\u00e1s r\u00e1pido con GitHub Copilot</li> </ul>"},{"location":"lectures/toolkit/github/#como-funciona-github","title":"C\u00f3mo funciona GitHub","text":"<p>GitHub te permite subir tus repositorios de c\u00f3digo para que sean  almacenados en la nube a trav\u00e9s del sistema de control de versiones de Git y  participar tambi\u00e9n en el desarrollo de proyectos de terceros, lo que significa  que cualquiera en el mundo que use GitHub puede encontrar tu c\u00f3digo, aprender de \u00e9l y,  por qu\u00e9 no, mejorarlo. Al igual que t\u00fa puedes acceder a los repositorios de c\u00f3digo  de otras personas.</p>"},{"location":"lectures/toolkit/github/#que-es-el-control-de-versiones","title":"\u00bfQu\u00e9 es el control de versiones?","text":"<p>Se le llama control de versiones a la administraci\u00f3n  de los cambios que se realizan sobre los elementos o la configuraci\u00f3n de alg\u00fan proyecto. En otras palabras, el control de versiones sirve para conocer  y autorizar los cambios que realicen los colaboradores en tu proyecto, guardando informaci\u00f3n  de qu\u00e9 incluyen los cambios y cu\u00e1ndo se hicieron. </p> <p>Este control comienza con una versi\u00f3n b\u00e1sica del documento y luego va guardando los cambios que se realicen a lo largo del proyecto.</p> <p>El control de versiones es una herramienta valios\u00edsima, pues con ella puedes tener acceso a las versiones anteriores de tu proyecto si es que en alg\u00fan momento no llega a funcionar de forma correcta.</p>"},{"location":"lectures/toolkit/github/#que-es-git","title":"\u00bfQu\u00e9 es Git?","text":"<p>Es el sistema de control de versiones m\u00e1s moderno y popular del mundo.  Fue creado por Linus Torvalds para garantizar la eficiencia y confiabilidad del mantenimiento \u00edntegro de versiones de un proyecto, aun cuando este tenga un gran n\u00famero de archivos o ramas en su c\u00f3digo fuente.</p> <p>Git proporciona herramientas que facilitan el desarrollo del c\u00f3digo entre un equipo de desarrolladores de manera r\u00e1pida e inteligente. Adem\u00e1s, podr\u00e1s comparar los cambios realizados a lo largo del tiempo, ver qui\u00e9n modific\u00f3 algo en el c\u00f3digo del software y conocer en qu\u00e9 momento se introdujo un cambio que gener\u00f3 un error en tu proyecto, facilitando la opci\u00f3n de revertirlo.</p>"},{"location":"lectures/toolkit/github/#entonces-cual-es-la-diferencia-entre-git-y-github","title":"Entonces, cu\u00e1l es la diferencia entre Git y GitHub","text":"<p>Debes tener en cuenta que Git es un sistema que permite establecer un control de versiones, mientras que GitHub es una plataforma que ofrece un grupo de funciones que facilitan el uso de Git y la colaboraci\u00f3n en tiempo real, as\u00ed como el almacenamiento en la nube.</p>"},{"location":"lectures/toolkit/github/#por-que-github-es-tan-popular","title":"\u00bfPor qu\u00e9 GitHub es tan popular?","text":"<p>GitHub gana su popularidad gracias a que es la principal plataforma para la creaci\u00f3n de trabajos colaborativos. De igual manera, no censura o discrimina lenguaje de programaci\u00f3n alguno, los acepta a todos sin inconveniente, por lo que le facilita el trabajo a la gran mayor\u00eda de los desarrolladores. En esta plataforma viven y crecen millones de proyectos de c\u00f3digo abierto que te permiten aprender y colaborar en la creaci\u00f3n de grandes iniciativas.</p> <p>Tambi\u00e9n es considerada una gran red social, pues esta herramienta te permite conocer otros perfiles, unirse a sus proyectos, ver su trabajo y realizar trabajo colaborativo.</p>"},{"location":"lectures/toolkit/github/#como-empezar-a-usar-github","title":"\u00bfC\u00f3mo empezar a usar GitHub?","text":"<p>Una vez que ya conocimos todo sobre GitHub y sus bondades, es hora de ponernos manos a la obra para adentrarnos en este interesante mundo:</p>"},{"location":"lectures/toolkit/github/#crear-un-repositorio-de-github","title":"Crear un Repositorio de GitHub","text":"<p>Lo primero que debes tener es una cuenta creada en GitHub. Registrarse es gratuito.\u00a0Una vez que tengas la cuenta, inicia sesi\u00f3n con tu usuario y clave. Luego, sigue estos pasos:</p> <ul> <li> <p>En la esquina superior derecha de cualquier p\u00e1gina, encontrar\u00e1s un signo de + que sirve para realizar las acciones de la p\u00e1gina. Das clic en el s\u00edmbolo y creas un nuevo repositorio (new repository).</p> </li> <li> <p>Una vez realizado eso, debes llenar los datos que se solicitan a continuaci\u00f3n. Darle un nombre, que de preferencia debe ser claro, definir si ser\u00e1 p\u00fablico o privado y colocar una peque\u00f1a descripci\u00f3n sobre tu repositorio. Este campo es opcional, pero te recomiendo que lo llenes para organizarte mejor y que los dem\u00e1s usuarios tengan una idea sobre lo que trata el repositorio que est\u00e1s creando.</p> </li> <li> <p>Activa el checkbox que dice iniciar tu repositorio con un\u00a0README, este ser\u00e1 tu primer archivo, la presentaci\u00f3n de tu proyecto.</p> </li> <li> <p>Presiona el bot\u00f3n de \u201ccrear repositorio\u201d y listo. Ya tienes tu primer repositorio creado.</p> </li> </ul> <p>\ud83d\udd11 Nota: Conoce aqu\u00ed una gu\u00eda de buenas pr\u00e1cticas para trabajar con GitHub.</p>"},{"location":"lectures/toolkit/github/#crear-ramas-branches-en-github","title":"Crear ramas (branches) en GitHub","text":"<p>Branch se puede ver como el mapa lineal de los commits que has realizado al archivo. Cuando empezamos un proyecto con GitHub, autom\u00e1ticamente nos crea una rama llamada master, a partir de la cual comenzaremos a crear nuestras propias ramas.</p> <ul> <li>Entra en tu repositorio.</li> <li>En la parte superior de la p\u00e1gina da clic en Rama actual. En la lista de rama selecciona la rama llamada master, que ser\u00e1 nuestra base para la que estamos creando.</li> <li>Luego presiona New Branch (Nueva Rama)</li> <li>A\u00f1ade el nombre de tu nueva rama</li> <li>Selecciona la rama actual (master) en la que se basara la nueva rama</li> <li>Presiona Create Branch (Crear Rama)</li> </ul> <p>Por ac\u00e1 encontrar\u00e1s tips y trucos para GitHub que te har\u00e1n ser un pro del control de versiones.</p>"},{"location":"lectures/toolkit/github/#entender-los-commits-de-github","title":"Entender los commits de GitHub","text":"<p>Commit es la denominaci\u00f3n que se le da a los cambios guardados en Github. En otras palabras, commit es la acci\u00f3n de subir los archivos con los cambios realizados en tus repositorios y guardarlos.</p> <p>Para realizar el commits de Github debes seguir los siguientes pasos:</p> <p>1.- Se debe verificar el estado de nuestro repositorio ejecutando el siguiente comando: <pre><code>git status\n</code></pre></p> <p>Una vez realizado el comando anterior, te aparecer\u00e1 una lista con los archivos que fueron modificados y con los que est\u00e1n agregados al \u00edndice, listos para subir.</p> <p>2.-  Si a\u00fan existen archivos sin agregar al \u00edndice, debes ejecutar el siguiente comando: <pre><code>git add\n</code></pre></p> <p>De esta forma se a\u00f1aden todos los cambios pendientes.</p> <p>3.- Ahora vamos a generar el commit ejecutando el siguiente comando: <pre><code>git commit -m \"Un comentario de los cambios realizados\"\n</code></pre></p> <p>Es importante que en este paso agregues una descripci\u00f3n clara, esta se guardar\u00e1 en el historial y podremos entender mejor los cambios m\u00e1s adelante. Cuando no se describen bien los cambios que se realizaron, volver a reparar un bug (error) puede ser una pesadilla.</p> <p>Y eso es todo. Son pasos bastante sencillos, pero nunca est\u00e1 dem\u00e1s revisar un par de veces lo que vamos a subir a nuestro repositorio remoto y seguir practicando.</p>"},{"location":"lectures/toolkit/github/#crear-solicitudes-de-extraccion-pull-requests-en-github","title":"Crear solicitudes de extracci\u00f3n (pull requests) en GitHub","text":"<p>Las solicitudes de extracci\u00f3n o pull requests son el formato para contribuir con los cambios que realizaste a un c\u00f3digo base para que sean fusionados.</p> <p>Los pasos que debes seguir son los siguientes:</p> <ol> <li>Haz clic en el bot\u00f3n bifurcaci\u00f3n (Fork) en la p\u00e1gina de GitHub que contiene la base de c\u00f3digo original al que deseas contribuir, para crear una copia del mismo en tu cuenta. Bifurcar un repositorio te permite realizar cambios y experimentos sin la preocupaci\u00f3n de afectar el proyecto original.</li> <li>Obt\u00e9n la URL de la bifurcaci\u00f3n que acabas de crear</li> <li>Usa el comando git clone para clonar la base de c\u00f3digo de Fork en la p\u00e1gina de tu Github en tu computadora local</li> <li>Realiza los cambios que consideres necesarios en tu repositorio local. Agrega y modifica archivos.</li> <li>Ahora inserta el repositorio de c\u00f3digo local en el repositorio de c\u00f3digo de Fork en Github, con los siguientes comandos:     <pre><code>git add\ngit commit\ngit push\n</code></pre></li> <li>Vuelve a la p\u00e1gina de tu fork en Github</li> <li>Presiona el bot\u00f3n Solicitud de Extracci\u00f3n (pull request)</li> <li>Dale un nombre a tu solicitud de extracci\u00f3n, colocando los detalles de los cambios que realizaste y finalmente presiona el bot\u00f3n enviar. \u00a1Felicitaciones! Has colaborado oficialmente con un proyecto y tu solicitud ser\u00e1 agregada si el administrador del proyecto la considera adecuada.</li> </ol>"},{"location":"lectures/toolkit/google_colab/","title":"Google Colab","text":"<p>Si ya conoces Colab, echa un vistazo a este v\u00eddeo para obtener informaci\u00f3n sobre las tablas interactivas, la vista del historial de c\u00f3digo ejecutado y la paleta de comandos.</p> In\u00a0[1]: Copied! <pre>seconds_in_a_day = 24 * 60 * 60\nseconds_in_a_day\n</pre> seconds_in_a_day = 24 * 60 * 60 seconds_in_a_day Out[1]: <pre>86400</pre> <p>Si quieres ejecutar el c\u00f3digo de la celda anterior, haz clic para seleccionarlo y pulsa el bot\u00f3n de reproducir situado a la izquierda del c\u00f3digo o usa la combinaci\u00f3n de teclas \"Comando/Ctrl\u00a0+\u00a0Intro\". Para editar el c\u00f3digo, solo tienes que hacer clic en la celda.</p> <p>Las variables que definas en una celda se pueden usar despu\u00e9s en otras celdas:</p> In\u00a0[2]: Copied! <pre>seconds_in_a_week = 7 * seconds_in_a_day\nseconds_in_a_week\n</pre> seconds_in_a_week = 7 * seconds_in_a_day seconds_in_a_week Out[2]: <pre>604800</pre> <p>Los cuadernos de Colab te permiten combinar c\u00f3digo ejecutable y texto enriquecido en un mismo documento, adem\u00e1s de im\u00e1genes, HTML, LaTeX y mucho m\u00e1s. Los cuadernos que creas en Colab se almacenan en tu cuenta de Google\u00a0Drive. Puedes compartir tus cuadernos de Colab f\u00e1cilmente con compa\u00f1eros de trabajo o amigos, lo que les permite comentarlos o incluso editarlos. Consulta m\u00e1s informaci\u00f3n en Informaci\u00f3n general sobre Colab. Para crear un cuaderno de Colab, puedes usar el men\u00fa Archivo que aparece arriba o bien acceder al enlace para crear un cuaderno de Colab.</p> <p>Los cuadernos de Colab son cuadernos de Jupyter alojados en Colab. Para obtener m\u00e1s informaci\u00f3n sobre el proyecto Jupyter, visita jupyter.org.</p> In\u00a0[3]: Copied! <pre>import numpy as np\nimport IPython.display as display\nfrom matplotlib import pyplot as plt\nimport io\nimport base64\n\nys = 200 + np.random.randn(100)\nx = [x for x in range(len(ys))]\n\nfig = plt.figure(figsize=(4, 3), facecolor='w')\nplt.plot(x, ys, '-')\nplt.fill_between(x, ys, 195, where=(ys &gt; 195), facecolor='g', alpha=0.6)\nplt.title(\"Sample Visualization\", fontsize=10)\n\ndata = io.BytesIO()\nplt.savefig(data)\nimage = F\"data:image/png;base64,{base64.b64encode(data.getvalue()).decode()}\"\nalt = \"Sample Visualization\"\ndisplay.display(display.Markdown(F\"\"\"![{alt}]({image})\"\"\"))\nplt.close(fig)\n</pre> import numpy as np import IPython.display as display from matplotlib import pyplot as plt import io import base64  ys = 200 + np.random.randn(100) x = [x for x in range(len(ys))]  fig = plt.figure(figsize=(4, 3), facecolor='w') plt.plot(x, ys, '-') plt.fill_between(x, ys, 195, where=(ys &gt; 195), facecolor='g', alpha=0.6) plt.title(\"Sample Visualization\", fontsize=10)  data = io.BytesIO() plt.savefig(data) image = F\"data:image/png;base64,{base64.b64encode(data.getvalue()).decode()}\" alt = \"Sample Visualization\" display.display(display.Markdown(F\"\"\"![{alt}]({image})\"\"\")) plt.close(fig) <p></p> <p>Puedes importar tus propios datos a los cuadernos de Colab desde tu cuenta de Google Drive, incluidas las hojas de c\u00e1lculo, y tambi\u00e9n desde GitHub y muchas fuentes m\u00e1s. Para obtener m\u00e1s informaci\u00f3n sobre c\u00f3mo importar datos y c\u00f3mo se puede usar Colab en la ciencia de datos, consulta los enlaces que aparecen en la secci\u00f3n Trabajar con datos m\u00e1s abajo.</p> <p>Colab es una herramienta muy utilizada en la comunidad de aprendizaje autom\u00e1tico. Estos son algunos ejemplos de las aplicaciones que tiene Colab:</p> <ul> <li>Dar los primeros pasos con TensorFlow</li> <li>Desarrollar y entrenar redes neuronales</li> <li>Experimentar con TPUs</li> <li>Divulgar datos de investigaci\u00f3n sobre IA</li> <li>Crear tutoriales</li> </ul> <p>Para ver cuadernos de Colab que demuestran las aplicaciones del aprendizaje autom\u00e1tico, consulta los ejemplos de aprendizaje autom\u00e1tico de abajo.</p>"},{"location":"lectures/toolkit/google_colab/#google-colab","title":"Google Colab\u00b6","text":"<p>\ud83d\udca1 (Novedad) Prueba la API de Gemini</p> <ul> <li>Generate a Gemini API key</li> <li>Talk to Gemini with the Speech-to-Text API</li> <li>Gemini API: Quickstart with Python</li> <li>Gemini API code sample</li> <li>Compare Gemini with ChatGPT</li> <li>More notebooks</li> </ul>"},{"location":"lectures/toolkit/google_colab/#que-es-colaboratory","title":"\u00bfQu\u00e9 es Colaboratory?\u00b6","text":"<p>Colab, tambi\u00e9n conocido como \"Colaboratory\", te permite programar y ejecutar Python en tu navegador con las siguientes ventajas:</p> <ul> <li>No requiere configuraci\u00f3n</li> <li>Acceso a GPUs sin coste adicional</li> <li>Permite compartir contenido f\u00e1cilmente</li> </ul> <p>Colab puede facilitar tu trabajo, ya seas estudiante, cient\u00edfico de datos o investigador de IA. No te pierdas el v\u00eddeo de Introducci\u00f3n a Colab para obtener m\u00e1s informaci\u00f3n. O simplemente empieza con los pasos descritos m\u00e1s abajo.</p>"},{"location":"lectures/toolkit/google_colab/#primeros-pasos","title":"Primeros pasos\u00b6","text":"<p>El documento que est\u00e1s leyendo no es una p\u00e1gina web est\u00e1tica, sino un entorno interactivo denominado cuaderno de Colab que te permite escribir y ejecutar c\u00f3digo.</p> <p>Por ejemplo, a continuaci\u00f3n se muestra una celda de c\u00f3digo con una breve secuencia de comandos de Python que calcula un valor, lo almacena en una variable e imprime el resultado:</p>"},{"location":"lectures/toolkit/google_colab/#ciencia-de-datos","title":"Ciencia de datos\u00b6","text":"<p>Con Colab, puedes aprovechar toda la potencia de las bibliotecas m\u00e1s populares de Python para analizar y visualizar datos. La celda de c\u00f3digo de abajo utiliza NumPy para generar datos aleatorios y Matplotlib para visualizarlos. Para editar el c\u00f3digo, solo tienes que hacer clic en la celda.</p>"},{"location":"lectures/toolkit/google_colab/#aprendizaje-automatico","title":"Aprendizaje autom\u00e1tico\u00b6","text":"<p>Con Colab, puedes importar un conjunto de datos de im\u00e1genes, entrenar un clasificador de im\u00e1genes con dicho conjunto de datos y evaluar el modelo con tan solo usar unas pocas l\u00edneas de c\u00f3digo. Los cuadernos de Colab ejecutan c\u00f3digo en los servidores en la nube de Google, lo que te permite aprovechar la potencia del hardware de Google, incluidas las GPU y TPU, independientemente de la potencia de tu equipo. Lo \u00fanico que necesitas es un navegador.</p>"},{"location":"lectures/toolkit/google_colab/#mas-recursos","title":"M\u00e1s recursos\u00b6","text":""},{"location":"lectures/toolkit/google_colab/#trabajar-con-cuadernos-en-colab","title":"Trabajar con cuadernos en Colab\u00b6","text":"<ul> <li>Informaci\u00f3n general sobre Colaboratory</li> <li>Gu\u00eda de Markdown</li> <li>Importar bibliotecas e instalar dependencias</li> <li>Guardar y cargar cuadernos en GitHub</li> <li>Formularios interactivos</li> <li>Widgets interactivos</li> </ul>"},{"location":"lectures/toolkit/google_colab/#trabajar-con-datos","title":"Trabajar con datos\u00b6","text":"<ul> <li>Cargar datos: Drive, Hojas de c\u00e1lculo y Google Cloud Storage</li> <li>Gr\u00e1ficos: visualizaci\u00f3n de datos</li> <li>Primeros pasos con BigQuery</li> </ul>"},{"location":"lectures/toolkit/google_colab/#curso-intensivo-de-aprendizaje-automatico","title":"Curso intensivo de aprendizaje autom\u00e1tico\u00b6","text":"<p>A continuaci\u00f3n, se muestran algunos cuadernos del curso online de Google sobre aprendizaje autom\u00e1tico. Para obtener m\u00e1s informaci\u00f3n, consulta el sitio web del curso completo.</p> <ul> <li>Introducci\u00f3n a Pandas DataFrame</li> <li>Regresi\u00f3n lineal con tf.keras usando datos sint\u00e9ticos</li> </ul>"},{"location":"lectures/toolkit/google_colab/#uso-de-hardware-acelerado","title":"Uso de hardware acelerado\u00b6","text":"<ul> <li>TensorFlow con GPUs</li> <li>TensorFlow con TPUs</li> </ul>"},{"location":"lectures/toolkit/google_colab/#ejemplos-destacados","title":"Ejemplos destacados\u00b6","text":"<ul> <li><p>Reemplaza voces con NeMo: usa NeMo, el kit de herramientas de IA conversacional de Nvidia, para sustituir una voz de un fragmento de audio por otra generada por ordenador.</p> </li> <li><p>Reentrenamiento de un clasificador de im\u00e1genes: crea un modelo de Keras sobre un clasificador de im\u00e1genes preparado previamente para que distinga flores.</p> </li> <li><p>Clasificaci\u00f3n de textos: clasifica las rese\u00f1as de pel\u00edculas de IMDb en positivas o negativas.</p> </li> <li><p>Transferencia de estilo: utiliza el aprendizaje profundo para transferir el estilo de una imagen a otra.</p> </li> <li><p>Codificador universal de frases multiling\u00fce para preguntas y respuestas: utiliza un modelo de aprendizaje autom\u00e1tico para contestar preguntas con el conjunto de datos SQuAD.</p> </li> <li><p>Interpolaci\u00f3n de v\u00eddeo: predice lo que ocurre entre el primer y el \u00faltimo fotograma de un v\u00eddeo.</p> </li> </ul>"},{"location":"lectures/toolkit/markdown/","title":"Markdown","text":""},{"location":"lectures/toolkit/markdown/#introduccion","title":"Introducci\u00f3n","text":"<p>Markdown es un lenguaje de marcado ligero que facilita el formato de texto de manera sencilla y legible. Fue creado en 2004 por John Gruber y Aaron Swartz. Su principal ventaja es que es f\u00e1cil de escribir y leer, y se convierte f\u00e1cilmente a HTML y otros formatos. Es ampliamente utilizado en documentaci\u00f3n, blogs, y plataformas como GitHub para archivos README.</p>"},{"location":"lectures/toolkit/markdown/#elementos-basicos-de-markdown","title":"Elementos B\u00e1sicos de Markdown","text":""},{"location":"lectures/toolkit/markdown/#1-encabezados","title":"1. Encabezados","text":"<p>Crea encabezados con el s\u00edmbolo <code>#</code>. M\u00e1s <code>#</code> indican un encabezado de nivel inferior.</p>"},{"location":"lectures/toolkit/markdown/#ejemplo","title":"Ejemplo:","text":"<pre><code># T\u00edtulo Principal (H1)\n## Subt\u00edtulo (H2)\n### Sub-subt\u00edtulo (H3)\n</code></pre>"},{"location":"lectures/toolkit/markdown/#2-parrafos","title":"2. P\u00e1rrafos","text":"<p>Escribe texto seguido de una l\u00ednea en blanco para crear un nuevo p\u00e1rrafo.</p>"},{"location":"lectures/toolkit/markdown/#ejemplo_1","title":"Ejemplo:","text":"<pre><code>Este es un p\u00e1rrafo simple.\n\nEste es otro p\u00e1rrafo, separado del anterior por una l\u00ednea en blanco.\n</code></pre>"},{"location":"lectures/toolkit/markdown/#3-negrita-y-cursiva","title":"3. Negrita y Cursiva","text":"<p>Destaca el texto usando negrita o cursiva:</p> <ul> <li>Negrita: Usa <code>**texto**</code> o <code>__texto__</code>.</li> <li>Cursiva: Usa <code>*texto*</code> o <code>_texto_</code>.</li> </ul>"},{"location":"lectures/toolkit/markdown/#ejemplo_2","title":"Ejemplo:","text":"<pre><code>**Texto en negrita**\n*Texto en cursiva*\n</code></pre>"},{"location":"lectures/toolkit/markdown/#4-listas","title":"4. Listas","text":"<p>Crea listas ordenadas y no ordenadas f\u00e1cilmente.</p> <ul> <li>No ordenadas: Usa <code>*</code>, <code>-</code>, o <code>+</code>.</li> <li>Ordenadas: Usa n\u00fameros seguidos de un punto.</li> </ul>"},{"location":"lectures/toolkit/markdown/#ejemplo_3","title":"Ejemplo:","text":"<pre><code>- Manzanas\n- Naranjas\n  - Valencia\n  - Navel\n</code></pre> <pre><code>1. Primer paso\n2. Segundo paso\n3. Tercer paso\n</code></pre>"},{"location":"lectures/toolkit/markdown/#5-enlaces","title":"5. Enlaces","text":"<p>Inserta enlaces con la siguiente sintaxis:</p>"},{"location":"lectures/toolkit/markdown/#ejemplo_4","title":"Ejemplo:","text":"<pre><code>[Visita Google](https://www.google.com)\n</code></pre>"},{"location":"lectures/toolkit/markdown/#6-imagenes","title":"6. Im\u00e1genes","text":"<p>Inserta im\u00e1genes de manera similar a los enlaces, pero precede con <code>!</code>.</p>"},{"location":"lectures/toolkit/markdown/#ejemplo_5","title":"Ejemplo:","text":"<pre><code>![Logo de Markdown](https://downloads.marketplace.jetbrains.com/files/18897/166369/icon/pluginIcon.png)\n</code></pre>"},{"location":"lectures/toolkit/markdown/#7-citas","title":"7. Citas","text":"<p>Destaca texto con citas utilizando <code>&gt;</code> al principio de la l\u00ednea.</p>"},{"location":"lectures/toolkit/markdown/#ejemplo_6","title":"Ejemplo:","text":"<pre><code>&gt; Esto es una cita importante.\n</code></pre>"},{"location":"lectures/toolkit/markdown/#8-codigo-en-python","title":"8. C\u00f3digo en Python","text":"<p>Markdown permite resaltar c\u00f3digo en l\u00ednea y en bloques. Es ideal para compartir scripts de Python.</p>"},{"location":"lectures/toolkit/markdown/#ejemplo-de-codigo-en-linea","title":"Ejemplo de c\u00f3digo en l\u00ednea:","text":"<pre><code>El comando `print(\"Hola, mundo\")` se utiliza para mostrar texto en Python.\n</code></pre>"},{"location":"lectures/toolkit/markdown/#ejemplo-de-bloque-de-codigo","title":"Ejemplo de bloque de c\u00f3digo:","text":"<pre><code>def saludar(nombre):\n    return f\"Hola, {nombre}!\"\n\nnombre = \"John\"\nprint(saludar(nombre))\n</code></pre>"},{"location":"lectures/toolkit/markdown/#9-formulas-matematicas","title":"9. F\u00f3rmulas Matem\u00e1ticas","text":"<p>Incorpora expresiones matem\u00e1ticas utilizando LaTeX con MathJax.</p>"},{"location":"lectures/toolkit/markdown/#ejemplo-de-formula-en-linea","title":"Ejemplo de f\u00f3rmula en l\u00ednea:","text":"<pre><code>La ecuaci\u00f3n de Einstein es $E = mc^2$.\n</code></pre>"},{"location":"lectures/toolkit/markdown/#ejemplo-de-formula-en-bloque","title":"Ejemplo de f\u00f3rmula en bloque:","text":"<pre><code>$$\n\\int_0^\\infty \\frac{\\sin x}{x}\\,dx = \\frac{\\pi}{2}\n$$\n</code></pre>"},{"location":"lectures/toolkit/python/","title":"Python","text":"In\u00a0[1]: Copied! <pre># imprimir \"Hola Mundo!\"\nprint(\"Hola Mundo!\");\n</pre> # imprimir \"Hola Mundo!\" print(\"Hola Mundo!\"); <pre>Hola Mundo!\n</pre> <p>Variables</p> <p>Las variables son contenedores que se utilizan para almacenar datos. Estos datos pueden ser n\u00fameros, cadenas de texto, listas, diccionarios, y m\u00e1s. En Python, no necesitas declarar el tipo de variable antes de asignarle un valor; el tipo de variable se determina autom\u00e1ticamente seg\u00fan el valor que se le asigne.</p> <p>Adem\u00e1s, las variables pueden contener una amplia variedad de tipos de datos, que determinan el tipo de valores que pueden almacenar y las operaciones que se pueden realizar con ellos.</p> In\u00a0[2]: Copied! <pre># Variables num\u00e9ricas\nnumero_entero = 10\nnumero_flotante = 3.14\n\n# Variables de cadena de texto\nnombre = \"Juan\"\nmensaje = 'Hola, mundo!'\n\n# Variables booleanas\nverdadero = True\nfalso = False\n\n# Variables nulas\nmi_variable_nula = None\n\n# Imprimiendo los valores de las variables\nprint(\"Valor de numero_entero:\", numero_entero)\nprint(\"Valor de numero_flotante:\", numero_flotante)\nprint(\"Valor de nombre:\", nombre)\nprint(\"Valor de mensaje:\", mensaje)\nprint(\"Valor de verdadero:\", verdadero)\nprint(\"Valor de falso:\", falso)\nprint(\"Valor de mi_variable_nula:\", mi_variable_nula)\n</pre> # Variables num\u00e9ricas numero_entero = 10 numero_flotante = 3.14  # Variables de cadena de texto nombre = \"Juan\" mensaje = 'Hola, mundo!'  # Variables booleanas verdadero = True falso = False  # Variables nulas mi_variable_nula = None  # Imprimiendo los valores de las variables print(\"Valor de numero_entero:\", numero_entero) print(\"Valor de numero_flotante:\", numero_flotante) print(\"Valor de nombre:\", nombre) print(\"Valor de mensaje:\", mensaje) print(\"Valor de verdadero:\", verdadero) print(\"Valor de falso:\", falso) print(\"Valor de mi_variable_nula:\", mi_variable_nula) <pre>Valor de numero_entero: 10\nValor de numero_flotante: 3.14\nValor de nombre: Juan\nValor de mensaje: Hola, mundo!\nValor de verdadero: True\nValor de falso: False\nValor de mi_variable_nula: None\n</pre> <p>Operaciones Aritm\u00e9ticas</p> <p>Las operaciones aritm\u00e9ticas se utilizan para realizar c\u00e1lculos matem\u00e1ticos simples. Algunas de las operaciones aritm\u00e9ticas b\u00e1sicas incluyen:</p> <ul> <li>Suma (+)</li> <li>Resta (-)</li> <li>Multiplicaci\u00f3n (*)</li> <li>Divisi\u00f3n (/)</li> <li>Potenciaci\u00f3n (**)</li> <li>Divisi\u00f3n entera (//)</li> <li>M\u00f3dulo (%)</li> </ul> In\u00a0[3]: Copied! <pre># Operaciones Aritm\u00e9ticas\na = 10\nb = 3\n\nsuma = a + b\nresta = a - b\nmultiplicacion = a * b\ndivision = a / b\npotenciacion = a ** b\ndivision_entera = a // b\nmodulo = a % b\n\nprint(\"Operaciones Aritm\u00e9ticas:\")\nprint(\"Suma:\", suma)\nprint(\"Resta:\", resta)\nprint(\"Multiplicaci\u00f3n:\", multiplicacion)\nprint(\"Divisi\u00f3n:\", division)\nprint(\"Potenciaci\u00f3n:\", potenciacion)\nprint(\"Divisi\u00f3n entera:\", division_entera)\nprint(\"M\u00f3dulo:\", modulo)\n</pre> # Operaciones Aritm\u00e9ticas a = 10 b = 3  suma = a + b resta = a - b multiplicacion = a * b division = a / b potenciacion = a ** b division_entera = a // b modulo = a % b  print(\"Operaciones Aritm\u00e9ticas:\") print(\"Suma:\", suma) print(\"Resta:\", resta) print(\"Multiplicaci\u00f3n:\", multiplicacion) print(\"Divisi\u00f3n:\", division) print(\"Potenciaci\u00f3n:\", potenciacion) print(\"Divisi\u00f3n entera:\", division_entera) print(\"M\u00f3dulo:\", modulo) <pre>Operaciones Aritm\u00e9ticas:\nSuma: 13\nResta: 7\nMultiplicaci\u00f3n: 30\nDivisi\u00f3n: 3.3333333333333335\nPotenciaci\u00f3n: 1000\nDivisi\u00f3n entera: 3\nM\u00f3dulo: 1\n</pre> <p>Operaciones de Comparaci\u00f3n</p> <p>Las operaciones de comparaci\u00f3n se utilizan para comparar valores y devolver un valor booleano (Verdadero o Falso). Algunas operaciones de comparaci\u00f3n comunes son:</p> <ul> <li>Igualdad (==)</li> <li>Desigualdad (!=)</li> <li>Mayor que (&gt;)</li> <li>Menor que (&lt;)</li> <li>Mayor o igual que (&gt;=)</li> <li>Menor o igual que (&lt;=)</li> </ul> In\u00a0[4]: Copied! <pre># Operaciones de Comparaci\u00f3n\nigualdad = (a == b)\ndesigualdad = (a != b)\nmayor_que = (a &gt; b)\nmenor_que = (a &lt; b)\nmayor_igual = (a &gt;= b)\nmenor_igual = (a &lt;= b)\n\nprint(\"Operaciones de Comparaci\u00f3n:\")\nprint(\"Igualdad:\", igualdad)\nprint(\"Desigualdad:\", desigualdad)\nprint(\"Mayor que:\", mayor_que)\nprint(\"Menor que:\", menor_que)\nprint(\"Mayor o igual que:\", mayor_igual)\nprint(\"Menor o igual que:\", menor_igual)\n</pre> # Operaciones de Comparaci\u00f3n igualdad = (a == b) desigualdad = (a != b) mayor_que = (a &gt; b) menor_que = (a &lt; b) mayor_igual = (a &gt;= b) menor_igual = (a &lt;= b)  print(\"Operaciones de Comparaci\u00f3n:\") print(\"Igualdad:\", igualdad) print(\"Desigualdad:\", desigualdad) print(\"Mayor que:\", mayor_que) print(\"Menor que:\", menor_que) print(\"Mayor o igual que:\", mayor_igual) print(\"Menor o igual que:\", menor_igual) <pre>Operaciones de Comparaci\u00f3n:\nIgualdad: False\nDesigualdad: True\nMayor que: True\nMenor que: False\nMayor o igual que: True\nMenor o igual que: False\n</pre> <p>Operaciones L\u00f3gicas</p> <p>Las operaciones l\u00f3gicas se utilizan para realizar operaciones booleanas en valores booleanos. Algunas operaciones l\u00f3gicas comunes incluyen:</p> <ul> <li>Y l\u00f3gico (and)</li> <li>O l\u00f3gico (or)</li> <li>Negaci\u00f3n l\u00f3gica (not)</li> </ul> In\u00a0[5]: Copied! <pre># Operaciones L\u00f3gicas\nverdadero = True\nfalso = False\n\ny_logico = verdadero and falso\no_logico = verdadero or falso\nnegacion = not verdadero\n\nprint(\"Operaciones L\u00f3gicas:\")\nprint(\"Y l\u00f3gico:\", y_logico)\nprint(\"O l\u00f3gico:\", o_logico)\nprint(\"Negaci\u00f3n:\", negacion)\n</pre> # Operaciones L\u00f3gicas verdadero = True falso = False  y_logico = verdadero and falso o_logico = verdadero or falso negacion = not verdadero  print(\"Operaciones L\u00f3gicas:\") print(\"Y l\u00f3gico:\", y_logico) print(\"O l\u00f3gico:\", o_logico) print(\"Negaci\u00f3n:\", negacion) <pre>Operaciones L\u00f3gicas:\nY l\u00f3gico: False\nO l\u00f3gico: True\nNegaci\u00f3n: False\n</pre> <p>Operaciones de Asignaci\u00f3n</p> <p>Las operaciones de asignaci\u00f3n se utilizan para asignar valores a variables. Algunas operaciones de asignaci\u00f3n pueden combinar operaciones aritm\u00e9ticas con la asignaci\u00f3n. Por ejemplo:</p> In\u00a0[6]: Copied! <pre># Operaciones de Asignaci\u00f3n\nx = 5\nx += 2  # Equivalente a x = x + 2\n\nprint(\"Operaciones de Asignaci\u00f3n:\")\nprint(\"Valor de x despu\u00e9s de la asignaci\u00f3n:\", x)\n</pre> # Operaciones de Asignaci\u00f3n x = 5 x += 2  # Equivalente a x = x + 2  print(\"Operaciones de Asignaci\u00f3n:\") print(\"Valor de x despu\u00e9s de la asignaci\u00f3n:\", x) <pre>Operaciones de Asignaci\u00f3n:\nValor de x despu\u00e9s de la asignaci\u00f3n: 7\n</pre> <p>Operaciones con strings</p> <p>Puedes realizar varias operaciones con cadenas de texto (strings) para manipular y trabajar con ellas de diversas formas. A continuaci\u00f3n, se describen algunas operaciones comunes con ejemplos y resultados:</p> In\u00a0[7]: Copied! <pre>cadena1 = \"Hola\"\ncadena2 = \"mundo\"\n\n# Concatenaci\u00f3n de cadenas\nresultado_concatenacion = cadena1 + \" \" + cadena2\nprint(\"Concatenaci\u00f3n:\", resultado_concatenacion)\n</pre> cadena1 = \"Hola\" cadena2 = \"mundo\"  # Concatenaci\u00f3n de cadenas resultado_concatenacion = cadena1 + \" \" + cadena2 print(\"Concatenaci\u00f3n:\", resultado_concatenacion) <pre>Concatenaci\u00f3n: Hola mundo\n</pre> In\u00a0[8]: Copied! <pre>cadena = \"Python\"\n# Repetici\u00f3n de una cadena\nresultado_repeticion = cadena * 3\nprint(\"Repetici\u00f3n:\", resultado_repeticion)\n</pre> cadena = \"Python\" # Repetici\u00f3n de una cadena resultado_repeticion = cadena * 3 print(\"Repetici\u00f3n:\", resultado_repeticion) <pre>Repetici\u00f3n: PythonPythonPython\n</pre> In\u00a0[9]: Copied! <pre># Indexaci\u00f3n para obtener el primer car\u00e1cter de la cadena\nprimer_caracter = cadena[0]\nprint(\"Indexaci\u00f3n:\", primer_caracter)\n</pre> # Indexaci\u00f3n para obtener el primer car\u00e1cter de la cadena primer_caracter = cadena[0] print(\"Indexaci\u00f3n:\", primer_caracter) <pre>Indexaci\u00f3n: P\n</pre> In\u00a0[10]: Copied! <pre># Obtenci\u00f3n de la longitud de la cadena\nlongitud = len(cadena)\nprint(\"Longitud:\", longitud)\n</pre> # Obtenci\u00f3n de la longitud de la cadena longitud = len(cadena) print(\"Longitud:\", longitud) <pre>Longitud: 6\n</pre> In\u00a0[11]: Copied! <pre># Slicing para obtener una subcadena de la cadena original\nsubcadena = cadena[2:5]\nprint(\"Slicing:\", subcadena)\n</pre> # Slicing para obtener una subcadena de la cadena original subcadena = cadena[2:5] print(\"Slicing:\", subcadena) <pre>Slicing: tho\n</pre> In\u00a0[12]: Copied! <pre>edad = 20\n\nif edad &lt; 18:\n    print(\"Eres menor de edad\")\nelif edad &gt;= 18 and edad &lt; 65:\n    print(\"Eres adulto\")\nelse:\n    print(\"Eres un adulto mayor\")\n</pre> edad = 20  if edad &lt; 18:     print(\"Eres menor de edad\") elif edad &gt;= 18 and edad &lt; 65:     print(\"Eres adulto\") else:     print(\"Eres un adulto mayor\") <pre>Eres adulto\n</pre> In\u00a0[13]: Copied! <pre>contador = 0\n\nwhile contador &lt; 5:\n    print(\"El contador es:\", contador)\n    contador += 1\n\nprint(\"\u00a1Bucle completado!\")\n</pre> contador = 0  while contador &lt; 5:     print(\"El contador es:\", contador)     contador += 1  print(\"\u00a1Bucle completado!\") <pre>El contador es: 0\nEl contador es: 1\nEl contador es: 2\nEl contador es: 3\nEl contador es: 4\n\u00a1Bucle completado!\n</pre> In\u00a0[14]: Copied! <pre>for i in range(5):\n    print(\"El valor de i es:\", i)\n\nprint(\"\u00a1Ciclo completado!\")\n</pre> for i in range(5):     print(\"El valor de i es:\", i)  print(\"\u00a1Ciclo completado!\") <pre>El valor de i es: 0\nEl valor de i es: 1\nEl valor de i es: 2\nEl valor de i es: 3\nEl valor de i es: 4\n\u00a1Ciclo completado!\n</pre> In\u00a0[15]: Copied! <pre># Ejemplo de salir de un ciclo con break\n\ncontador = 0\n\nwhile True:\n    contador += 1\n\n    # Salir del ciclo si el contador llega a 5\n    if contador == 5:\n        print(\"\u00a1El contador lleg\u00f3 a 5! Saliendo del ciclo.\")\n        break\n\n    print(\"El valor del contador es:\", contador)\n\nprint(\"\u00a1Ciclo completado!\")\n</pre> # Ejemplo de salir de un ciclo con break  contador = 0  while True:     contador += 1      # Salir del ciclo si el contador llega a 5     if contador == 5:         print(\"\u00a1El contador lleg\u00f3 a 5! Saliendo del ciclo.\")         break      print(\"El valor del contador es:\", contador)  print(\"\u00a1Ciclo completado!\")  <pre>El valor del contador es: 1\nEl valor del contador es: 2\nEl valor del contador es: 3\nEl valor del contador es: 4\n\u00a1El contador lleg\u00f3 a 5! Saliendo del ciclo.\n\u00a1Ciclo completado!\n</pre> <p>Continuar con la Siguiente Iteraci\u00f3n con <code>continue</code></p> <p>La palabra clave <code>continue</code> se utiliza para saltar a la siguiente iteraci\u00f3n del ciclo si se cumple una cierta condici\u00f3n. Cuando se encuentra una instrucci\u00f3n <code>continue</code>, el ciclo se detiene temporalmente y se pasa a la siguiente iteraci\u00f3n del ciclo, omitiendo cualquier c\u00f3digo restante dentro del bloque de ciclo para esa iteraci\u00f3n en particular.</p> <p>Ejemplo</p> In\u00a0[16]: Copied! <pre># Ejemplo de continuar con la siguiente iteraci\u00f3n con continue\n\ncontador = 0\n\nwhile contador &lt; 5:\n    contador += 1\n\n    # Saltar a la siguiente iteraci\u00f3n si el contador es par\n    if contador % 2 == 0:\n        print(\"El contador es par. Continuando con la siguiente iteraci\u00f3n.\")\n        continue\n\n    print(\"El valor del contador es:\", contador)\n\nprint(\"\u00a1Ciclo completado!\")\n</pre> # Ejemplo de continuar con la siguiente iteraci\u00f3n con continue  contador = 0  while contador &lt; 5:     contador += 1      # Saltar a la siguiente iteraci\u00f3n si el contador es par     if contador % 2 == 0:         print(\"El contador es par. Continuando con la siguiente iteraci\u00f3n.\")         continue      print(\"El valor del contador es:\", contador)  print(\"\u00a1Ciclo completado!\")  <pre>El valor del contador es: 1\nEl contador es par. Continuando con la siguiente iteraci\u00f3n.\nEl valor del contador es: 3\nEl contador es par. Continuando con la siguiente iteraci\u00f3n.\nEl valor del contador es: 5\n\u00a1Ciclo completado!\n</pre> <p>Existen varias estructuras de datos integradas que te permiten almacenar y organizar colecciones de elementos de diferentes maneras. Algunas de las estructuras de datos m\u00e1s comunes son las listas, las tuplas, los conjuntos y los diccionarios. A continuaci\u00f3n, se explican cada una de ellas con ejemplos y resultados impresos.</p> In\u00a0[17]: Copied! <pre># Crear una lista vac\u00eda\nlista_vacia = []\nprint(\"Lista vac\u00eda:\", lista_vacia)\n</pre> # Crear una lista vac\u00eda lista_vacia = [] print(\"Lista vac\u00eda:\", lista_vacia) <pre>Lista vac\u00eda: []\n</pre> In\u00a0[18]: Copied! <pre># Crear una lista con elementos de diferentes tipos\nlista_mixta = [1, \"dos\", 3.0, True]\nprint(\"Lista mixta:\", lista_mixta)\n</pre> # Crear una lista con elementos de diferentes tipos lista_mixta = [1, \"dos\", 3.0, True] print(\"Lista mixta:\", lista_mixta) <pre>Lista mixta: [1, 'dos', 3.0, True]\n</pre> In\u00a0[19]: Copied! <pre># Acceder a elementos de una lista utilizando \u00edndices positivos y negativos\nmi_lista = ['a', 'b', 'c', 'd', 'e']\nprint(\"Primer elemento:\", mi_lista[0])\nprint(\"\u00daltimo elemento:\", mi_lista[-1])\n</pre> # Acceder a elementos de una lista utilizando \u00edndices positivos y negativos mi_lista = ['a', 'b', 'c', 'd', 'e'] print(\"Primer elemento:\", mi_lista[0]) print(\"\u00daltimo elemento:\", mi_lista[-1]) <pre>Primer elemento: a\n\u00daltimo elemento: e\n</pre> In\u00a0[20]: Copied! <pre># Rebanado (slicing) de listas\nprint(\"Rebanado:\", mi_lista[1:3])\n</pre> # Rebanado (slicing) de listas print(\"Rebanado:\", mi_lista[1:3]) <pre>Rebanado: ['b', 'c']\n</pre> In\u00a0[21]: Copied! <pre># Modificar elementos de una lista\nmi_lista[0] = 'z'\nprint(\"Lista modificada:\", mi_lista)\n</pre> # Modificar elementos de una lista mi_lista[0] = 'z' print(\"Lista modificada:\", mi_lista) <pre>Lista modificada: ['z', 'b', 'c', 'd', 'e']\n</pre> In\u00a0[22]: Copied! <pre># Agregar elementos a una lista\nmi_lista.append('f')\nprint(\"Lista despu\u00e9s de agregar un elemento:\", mi_lista)\n</pre> # Agregar elementos a una lista mi_lista.append('f') print(\"Lista despu\u00e9s de agregar un elemento:\", mi_lista) <pre>Lista despu\u00e9s de agregar un elemento: ['z', 'b', 'c', 'd', 'e', 'f']\n</pre> In\u00a0[23]: Copied! <pre># Extender una lista con otra lista\nmi_otra_lista = ['g', 'h', 'i']\nmi_lista.extend(mi_otra_lista)\nprint(\"Lista despu\u00e9s de extenderla con otra lista:\", mi_lista)\n</pre> # Extender una lista con otra lista mi_otra_lista = ['g', 'h', 'i'] mi_lista.extend(mi_otra_lista) print(\"Lista despu\u00e9s de extenderla con otra lista:\", mi_lista) <pre>Lista despu\u00e9s de extenderla con otra lista: ['z', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i']\n</pre> In\u00a0[24]: Copied! <pre># Insertar un elemento en una posici\u00f3n espec\u00edfica\nmi_lista.insert(2, 'nuevo')\nprint(\"Lista despu\u00e9s de insertar un nuevo elemento:\", mi_lista)\n</pre> # Insertar un elemento en una posici\u00f3n espec\u00edfica mi_lista.insert(2, 'nuevo') print(\"Lista despu\u00e9s de insertar un nuevo elemento:\", mi_lista) <pre>Lista despu\u00e9s de insertar un nuevo elemento: ['z', 'b', 'nuevo', 'c', 'd', 'e', 'f', 'g', 'h', 'i']\n</pre> In\u00a0[25]: Copied! <pre># Eliminar un elemento de una lista\nmi_lista.remove('c')\nprint(\"Lista despu\u00e9s de eliminar un elemento:\", mi_lista)\n</pre> # Eliminar un elemento de una lista mi_lista.remove('c') print(\"Lista despu\u00e9s de eliminar un elemento:\", mi_lista) <pre>Lista despu\u00e9s de eliminar un elemento: ['z', 'b', 'nuevo', 'd', 'e', 'f', 'g', 'h', 'i']\n</pre> In\u00a0[26]: Copied! <pre># Eliminar el \u00faltimo elemento de una lista y devolverlo\nultimo_elemento = mi_lista.pop()\nprint(\"\u00daltimo elemento eliminado:\", ultimo_elemento)\nprint(\"Lista despu\u00e9s de eliminar el \u00faltimo elemento:\", mi_lista)\n</pre> # Eliminar el \u00faltimo elemento de una lista y devolverlo ultimo_elemento = mi_lista.pop() print(\"\u00daltimo elemento eliminado:\", ultimo_elemento) print(\"Lista despu\u00e9s de eliminar el \u00faltimo elemento:\", mi_lista) <pre>\u00daltimo elemento eliminado: i\nLista despu\u00e9s de eliminar el \u00faltimo elemento: ['z', 'b', 'nuevo', 'd', 'e', 'f', 'g', 'h']\n</pre> In\u00a0[27]: Copied! <pre># Buscar el \u00edndice de un elemento en una lista\nindice = mi_lista.index('e')\nprint(\"\u00cdndice del elemento 'e':\", indice)\n</pre> # Buscar el \u00edndice de un elemento en una lista indice = mi_lista.index('e') print(\"\u00cdndice del elemento 'e':\", indice) <pre>\u00cdndice del elemento 'e': 4\n</pre> In\u00a0[28]: Copied! <pre># Contar la cantidad de veces que aparece un elemento en una lista\ncantidad = mi_lista.count('a')\nprint(\"Cantidad de veces que aparece 'a':\", cantidad)\n</pre> # Contar la cantidad de veces que aparece un elemento en una lista cantidad = mi_lista.count('a') print(\"Cantidad de veces que aparece 'a':\", cantidad) <pre>Cantidad de veces que aparece 'a': 0\n</pre> In\u00a0[29]: Copied! <pre># Revertir el orden de los elementos en una lista\nmi_lista.reverse()\nprint(\"Lista despu\u00e9s de revertir el orden:\", mi_lista)\n</pre> # Revertir el orden de los elementos en una lista mi_lista.reverse() print(\"Lista despu\u00e9s de revertir el orden:\", mi_lista) <pre>Lista despu\u00e9s de revertir el orden: ['h', 'g', 'f', 'e', 'd', 'nuevo', 'b', 'z']\n</pre> In\u00a0[30]: Copied! <pre># Ordenar los elementos de una lista (si son del mismo tipo)\nnumeros = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3]\nnumeros.sort()\nprint(\"Lista de n\u00fameros ordenada:\", numeros)\n</pre> # Ordenar los elementos de una lista (si son del mismo tipo) numeros = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3] numeros.sort() print(\"Lista de n\u00fameros ordenada:\", numeros) <pre>Lista de n\u00fameros ordenada: [1, 1, 2, 3, 3, 4, 5, 5, 6, 9]\n</pre> In\u00a0[31]: Copied! <pre># Copiar una lista\ncopia_lista = mi_lista.copy()\nprint(\"Copia de la lista:\", copia_lista)\n</pre> # Copiar una lista copia_lista = mi_lista.copy() print(\"Copia de la lista:\", copia_lista) <pre>Copia de la lista: ['h', 'g', 'f', 'e', 'd', 'nuevo', 'b', 'z']\n</pre> In\u00a0[32]: Copied! <pre># Vaciar una lista\nmi_lista.clear()\nprint(\"Lista despu\u00e9s de vaciarla:\", mi_lista)\n</pre> # Vaciar una lista mi_lista.clear() print(\"Lista despu\u00e9s de vaciarla:\", mi_lista) <pre>Lista despu\u00e9s de vaciarla: []\n</pre> In\u00a0[33]: Copied! <pre># Crear una tupla vac\u00eda\ntupla_vacia = ()\nprint(\"Tupla vac\u00eda:\", tupla_vacia)\n</pre> # Crear una tupla vac\u00eda tupla_vacia = () print(\"Tupla vac\u00eda:\", tupla_vacia) <pre>Tupla vac\u00eda: ()\n</pre> In\u00a0[34]: Copied! <pre># Crear una tupla con elementos de diferentes tipos\ntupla_mixta = (1, \"dos\", 3.0, True)\nprint(\"Tupla mixta:\", tupla_mixta)\n</pre> # Crear una tupla con elementos de diferentes tipos tupla_mixta = (1, \"dos\", 3.0, True) print(\"Tupla mixta:\", tupla_mixta) <pre>Tupla mixta: (1, 'dos', 3.0, True)\n</pre> In\u00a0[35]: Copied! <pre># Acceder a elementos de una tupla utilizando \u00edndices positivos y negativos\nmi_tupla = ('a', 'b', 'c', 'd', 'e')\nprint(\"Primer elemento:\", mi_tupla[0])\nprint(\"\u00daltimo elemento:\", mi_tupla[-1])\n</pre> # Acceder a elementos de una tupla utilizando \u00edndices positivos y negativos mi_tupla = ('a', 'b', 'c', 'd', 'e') print(\"Primer elemento:\", mi_tupla[0]) print(\"\u00daltimo elemento:\", mi_tupla[-1]) <pre>Primer elemento: a\n\u00daltimo elemento: e\n</pre> In\u00a0[36]: Copied! <pre># Rebanado (slicing) de tuplas\nprint(\"Rebanado:\", mi_tupla[1:3])\n# No se puede modificar una tupla despu\u00e9s de crearla\n# mi_tupla[0] = 'z'  # Esto producir\u00e1 un error\n</pre> # Rebanado (slicing) de tuplas print(\"Rebanado:\", mi_tupla[1:3]) # No se puede modificar una tupla despu\u00e9s de crearla # mi_tupla[0] = 'z'  # Esto producir\u00e1 un error <pre>Rebanado: ('b', 'c')\n</pre> In\u00a0[37]: Copied! <pre># Concatenar tuplas\notra_tupla = ('f', 'g')\nconcatenacion = mi_tupla + otra_tupla\nprint(\"Concatenaci\u00f3n de tuplas:\", concatenacion)\n</pre> # Concatenar tuplas otra_tupla = ('f', 'g') concatenacion = mi_tupla + otra_tupla print(\"Concatenaci\u00f3n de tuplas:\", concatenacion) <pre>Concatenaci\u00f3n de tuplas: ('a', 'b', 'c', 'd', 'e', 'f', 'g')\n</pre> In\u00a0[38]: Copied! <pre># Multiplicar una tupla\nmultiplicacion = otra_tupla * 3\nprint(\"Multiplicaci\u00f3n de tupla:\", multiplicacion)\n</pre> # Multiplicar una tupla multiplicacion = otra_tupla * 3 print(\"Multiplicaci\u00f3n de tupla:\", multiplicacion) <pre>Multiplicaci\u00f3n de tupla: ('f', 'g', 'f', 'g', 'f', 'g')\n</pre> In\u00a0[39]: Copied! <pre># Obtener la longitud de una tupla\nlongitud = len(mi_tupla)\nprint(\"Longitud de la tupla:\", longitud)\n</pre> # Obtener la longitud de una tupla longitud = len(mi_tupla) print(\"Longitud de la tupla:\", longitud) <pre>Longitud de la tupla: 5\n</pre> In\u00a0[40]: Copied! <pre># Buscar un elemento en una tupla\nif 'c' in mi_tupla:\n    print(\"El elemento 'c' est\u00e1 en la tupla.\")\nelse:\n    print(\"El elemento 'c' no est\u00e1 en la tupla.\")\n</pre> # Buscar un elemento en una tupla if 'c' in mi_tupla:     print(\"El elemento 'c' est\u00e1 en la tupla.\") else:     print(\"El elemento 'c' no est\u00e1 en la tupla.\") <pre>El elemento 'c' est\u00e1 en la tupla.\n</pre> In\u00a0[41]: Copied! <pre># Contar la cantidad de veces que aparece un elemento en una tupla\ncantidad = mi_tupla.count('b')\nprint(\"Cantidad de veces que aparece 'b':\", cantidad)\n</pre> # Contar la cantidad de veces que aparece un elemento en una tupla cantidad = mi_tupla.count('b') print(\"Cantidad de veces que aparece 'b':\", cantidad) <pre>Cantidad de veces que aparece 'b': 1\n</pre> In\u00a0[42]: Copied! <pre># Encontrar el \u00edndice de un elemento en una tupla\nindice = mi_tupla.index('d')\nprint(\"\u00cdndice del elemento 'd':\", indice)\n</pre> # Encontrar el \u00edndice de un elemento en una tupla indice = mi_tupla.index('d') print(\"\u00cdndice del elemento 'd':\", indice) <pre>\u00cdndice del elemento 'd': 3\n</pre> In\u00a0[43]: Copied! <pre># Crear un conjunto vac\u00edo\nconjunto_vacio = set()\nprint(\"Conjunto vac\u00edo:\", conjunto_vacio)\n</pre> # Crear un conjunto vac\u00edo conjunto_vacio = set() print(\"Conjunto vac\u00edo:\", conjunto_vacio) <pre>Conjunto vac\u00edo: set()\n</pre> In\u00a0[44]: Copied! <pre># Crear un conjunto con elementos\nmi_conjunto = {1, 2, 3, 4, 5}\nprint(\"Conjunto:\", mi_conjunto)\n</pre> # Crear un conjunto con elementos mi_conjunto = {1, 2, 3, 4, 5} print(\"Conjunto:\", mi_conjunto) <pre>Conjunto: {1, 2, 3, 4, 5}\n</pre> In\u00a0[45]: Copied! <pre># Agregar elementos a un conjunto\nmi_conjunto.add(6)\nprint(\"Conjunto despu\u00e9s de agregar un elemento:\", mi_conjunto)\n</pre> # Agregar elementos a un conjunto mi_conjunto.add(6) print(\"Conjunto despu\u00e9s de agregar un elemento:\", mi_conjunto) <pre>Conjunto despu\u00e9s de agregar un elemento: {1, 2, 3, 4, 5, 6}\n</pre> In\u00a0[46]: Copied! <pre># Eliminar un elemento de un conjunto\nmi_conjunto.remove(3)\nprint(\"Conjunto despu\u00e9s de eliminar un elemento:\", mi_conjunto)\n</pre> # Eliminar un elemento de un conjunto mi_conjunto.remove(3) print(\"Conjunto despu\u00e9s de eliminar un elemento:\", mi_conjunto) <pre>Conjunto despu\u00e9s de eliminar un elemento: {1, 2, 4, 5, 6}\n</pre> In\u00a0[47]: Copied! <pre># Verificar si un elemento est\u00e1 en un conjunto\nif 4 in mi_conjunto:\n    print(\"El elemento 4 est\u00e1 en el conjunto.\")\nelse:\n    print(\"El elemento 4 no est\u00e1 en el conjunto.\")\n</pre> # Verificar si un elemento est\u00e1 en un conjunto if 4 in mi_conjunto:     print(\"El elemento 4 est\u00e1 en el conjunto.\") else:     print(\"El elemento 4 no est\u00e1 en el conjunto.\") <pre>El elemento 4 est\u00e1 en el conjunto.\n</pre> In\u00a0[48]: Copied! <pre># Realizar operaciones de conjunto\nconjunto1 = {1, 2, 3, 4, 5}\nconjunto2 = {4, 5, 6, 7, 8}\n</pre> # Realizar operaciones de conjunto conjunto1 = {1, 2, 3, 4, 5} conjunto2 = {4, 5, 6, 7, 8} In\u00a0[49]: Copied! <pre># Uni\u00f3n de conjuntos\nunion = conjunto1.union(conjunto2)\nprint(\"Uni\u00f3n de conjuntos:\", union)\n</pre> # Uni\u00f3n de conjuntos union = conjunto1.union(conjunto2) print(\"Uni\u00f3n de conjuntos:\", union) <pre>Uni\u00f3n de conjuntos: {1, 2, 3, 4, 5, 6, 7, 8}\n</pre> In\u00a0[50]: Copied! <pre># Intersecci\u00f3n de conjuntos\ninterseccion = conjunto1.intersection(conjunto2)\nprint(\"Intersecci\u00f3n de conjuntos:\", interseccion)\n</pre> # Intersecci\u00f3n de conjuntos interseccion = conjunto1.intersection(conjunto2) print(\"Intersecci\u00f3n de conjuntos:\", interseccion) <pre>Intersecci\u00f3n de conjuntos: {4, 5}\n</pre> In\u00a0[51]: Copied! <pre># Diferencia entre conjuntos\ndiferencia = conjunto1.difference(conjunto2)\nprint(\"Diferencia entre conjuntos 1 y 2:\", diferencia)\n</pre> # Diferencia entre conjuntos diferencia = conjunto1.difference(conjunto2) print(\"Diferencia entre conjuntos 1 y 2:\", diferencia) <pre>Diferencia entre conjuntos 1 y 2: {1, 2, 3}\n</pre> In\u00a0[52]: Copied! <pre># Comprobar si un conjunto es subconjunto de otro\nif conjunto1.issubset(conjunto2):\n    print(\"El conjunto 1 es un subconjunto del conjunto 2.\")\nelse:\n    print(\"El conjunto 1 no es un subconjunto del conjunto 2.\")\n</pre> # Comprobar si un conjunto es subconjunto de otro if conjunto1.issubset(conjunto2):     print(\"El conjunto 1 es un subconjunto del conjunto 2.\") else:     print(\"El conjunto 1 no es un subconjunto del conjunto 2.\") <pre>El conjunto 1 no es un subconjunto del conjunto 2.\n</pre> In\u00a0[53]: Copied! <pre># Vaciar un conjunto\nconjunto1.clear()\nprint(\"Conjunto 1 despu\u00e9s de vaciarlo:\", conjunto1)\n</pre> # Vaciar un conjunto conjunto1.clear() print(\"Conjunto 1 despu\u00e9s de vaciarlo:\", conjunto1) <pre>Conjunto 1 despu\u00e9s de vaciarlo: set()\n</pre> In\u00a0[54]: Copied! <pre># Crear un diccionario vac\u00edo\ndiccionario_vacio = {}\nprint(\"Diccionario vac\u00edo:\", diccionario_vacio)\n</pre> # Crear un diccionario vac\u00edo diccionario_vacio = {} print(\"Diccionario vac\u00edo:\", diccionario_vacio) <pre>Diccionario vac\u00edo: {}\n</pre> In\u00a0[55]: Copied! <pre># Crear un diccionario con elementos\nmi_diccionario = {\"nombre\": \"Juan\", \"edad\": 30, \"ciudad\": \"Madrid\"}\nprint(\"Diccionario:\", mi_diccionario)\n</pre> # Crear un diccionario con elementos mi_diccionario = {\"nombre\": \"Juan\", \"edad\": 30, \"ciudad\": \"Madrid\"} print(\"Diccionario:\", mi_diccionario) <pre>Diccionario: {'nombre': 'Juan', 'edad': 30, 'ciudad': 'Madrid'}\n</pre> In\u00a0[56]: Copied! <pre># Acceder a valores utilizando claves\nprint(\"Nombre:\", mi_diccionario[\"nombre\"])\nprint(\"Edad:\", mi_diccionario[\"edad\"])\n</pre> # Acceder a valores utilizando claves print(\"Nombre:\", mi_diccionario[\"nombre\"]) print(\"Edad:\", mi_diccionario[\"edad\"]) <pre>Nombre: Juan\nEdad: 30\n</pre> In\u00a0[57]: Copied! <pre># Modificar valores en un diccionario\nmi_diccionario[\"edad\"] = 31\nprint(\"Diccionario despu\u00e9s de modificar la edad:\", mi_diccionario)\n</pre> # Modificar valores en un diccionario mi_diccionario[\"edad\"] = 31 print(\"Diccionario despu\u00e9s de modificar la edad:\", mi_diccionario) <pre>Diccionario despu\u00e9s de modificar la edad: {'nombre': 'Juan', 'edad': 31, 'ciudad': 'Madrid'}\n</pre> In\u00a0[58]: Copied! <pre># Agregar un nuevo par clave-valor\nmi_diccionario[\"profesi\u00f3n\"] = \"Ingeniero\"\nprint(\"Diccionario despu\u00e9s de agregar una profesi\u00f3n:\", mi_diccionario)\n</pre> # Agregar un nuevo par clave-valor mi_diccionario[\"profesi\u00f3n\"] = \"Ingeniero\" print(\"Diccionario despu\u00e9s de agregar una profesi\u00f3n:\", mi_diccionario) <pre>Diccionario despu\u00e9s de agregar una profesi\u00f3n: {'nombre': 'Juan', 'edad': 31, 'ciudad': 'Madrid', 'profesi\u00f3n': 'Ingeniero'}\n</pre> In\u00a0[59]: Copied! <pre># Eliminar un par clave-valor\ndel mi_diccionario[\"ciudad\"]\nprint(\"Diccionario despu\u00e9s de eliminar la ciudad:\", mi_diccionario)\n</pre> # Eliminar un par clave-valor del mi_diccionario[\"ciudad\"] print(\"Diccionario despu\u00e9s de eliminar la ciudad:\", mi_diccionario) <pre>Diccionario despu\u00e9s de eliminar la ciudad: {'nombre': 'Juan', 'edad': 31, 'profesi\u00f3n': 'Ingeniero'}\n</pre> In\u00a0[60]: Copied! <pre># Verificar si una clave est\u00e1 en el diccionario\nif \"nombre\" in mi_diccionario:\n    print(\"La clave 'nombre' est\u00e1 en el diccionario.\")\nelse:\n    print(\"La clave 'nombre' no est\u00e1 en el diccionario.\")\n</pre> # Verificar si una clave est\u00e1 en el diccionario if \"nombre\" in mi_diccionario:     print(\"La clave 'nombre' est\u00e1 en el diccionario.\") else:     print(\"La clave 'nombre' no est\u00e1 en el diccionario.\") <pre>La clave 'nombre' est\u00e1 en el diccionario.\n</pre> In\u00a0[61]: Copied! <pre># Obtener todas las claves y valores del diccionario\nclaves = mi_diccionario.keys()\nprint(\"Claves del diccionario:\", claves)\nvalores = mi_diccionario.values()\nprint(\"Valores del diccionario:\", valores)\n</pre> # Obtener todas las claves y valores del diccionario claves = mi_diccionario.keys() print(\"Claves del diccionario:\", claves) valores = mi_diccionario.values() print(\"Valores del diccionario:\", valores) <pre>Claves del diccionario: dict_keys(['nombre', 'edad', 'profesi\u00f3n'])\nValores del diccionario: dict_values(['Juan', 31, 'Ingeniero'])\n</pre> In\u00a0[62]: Copied! <pre># Obtener pares clave-valor del diccionario como tuplas\nitems = mi_diccionario.items()\nprint(\"Pares clave-valor del diccionario:\", items)\n</pre> # Obtener pares clave-valor del diccionario como tuplas items = mi_diccionario.items() print(\"Pares clave-valor del diccionario:\", items) <pre>Pares clave-valor del diccionario: dict_items([('nombre', 'Juan'), ('edad', 31), ('profesi\u00f3n', 'Ingeniero')])\n</pre> In\u00a0[63]: Copied! <pre># Copiar un diccionario\ncopia_diccionario = mi_diccionario.copy()\nprint(\"Copia del diccionario:\", copia_diccionario)\n</pre> # Copiar un diccionario copia_diccionario = mi_diccionario.copy() print(\"Copia del diccionario:\", copia_diccionario) <pre>Copia del diccionario: {'nombre': 'Juan', 'edad': 31, 'profesi\u00f3n': 'Ingeniero'}\n</pre> In\u00a0[64]: Copied! <pre># Vaciar un diccionario\nmi_diccionario.clear()\nprint(\"Diccionario despu\u00e9s de vaciarlo:\", mi_diccionario)\n</pre> # Vaciar un diccionario mi_diccionario.clear() print(\"Diccionario despu\u00e9s de vaciarlo:\", mi_diccionario) <pre>Diccionario despu\u00e9s de vaciarlo: {}\n</pre> In\u00a0[65]: Copied! <pre># Definir una funci\u00f3n sin par\u00e1metros y sin valor de retorno\ndef saludar():\n    print(\"\u00a1Hola!\")\n\n# Llamar a la funci\u00f3n\nsaludar()\n</pre> # Definir una funci\u00f3n sin par\u00e1metros y sin valor de retorno def saludar():     print(\"\u00a1Hola!\")  # Llamar a la funci\u00f3n saludar() <pre>\u00a1Hola!\n</pre> In\u00a0[66]: Copied! <pre># Definir una funci\u00f3n con par\u00e1metros y sin valor de retorno\ndef saludar_persona(nombre):\n    print(\"\u00a1Hola,\", nombre, \"!\")\n\n# Llamar a la funci\u00f3n con un argumento\nsaludar_persona(\"Juan\")\n</pre> # Definir una funci\u00f3n con par\u00e1metros y sin valor de retorno def saludar_persona(nombre):     print(\"\u00a1Hola,\", nombre, \"!\")  # Llamar a la funci\u00f3n con un argumento saludar_persona(\"Juan\") <pre>\u00a1Hola, Juan !\n</pre> In\u00a0[67]: Copied! <pre># Definir una funci\u00f3n con par\u00e1metros y con valor de retorno\ndef sumar(a, b):\n    return a + b\n\n# Llamar a la funci\u00f3n y almacenar el resultado en una variable\nresultado = sumar(3, 5)\nprint(\"Resultado de la suma:\", resultado)\n</pre> # Definir una funci\u00f3n con par\u00e1metros y con valor de retorno def sumar(a, b):     return a + b  # Llamar a la funci\u00f3n y almacenar el resultado en una variable resultado = sumar(3, 5) print(\"Resultado de la suma:\", resultado) <pre>Resultado de la suma: 8\n</pre> In\u00a0[68]: Copied! <pre># Funci\u00f3n con par\u00e1metros con valores predeterminados\ndef multiplicar(a, b=2):\n    return a * b\n\n# Llamar a la funci\u00f3n con un solo argumento\nresultado1 = multiplicar(3)\nprint(\"Resultado de la multiplicaci\u00f3n (con valor predeterminado):\", resultado1)\n\n# Llamar a la funci\u00f3n con dos argumentos\nresultado2 = multiplicar(3, 4)\nprint(\"Resultado de la multiplicaci\u00f3n (con argumento espec\u00edfico):\", resultado2)\n</pre> # Funci\u00f3n con par\u00e1metros con valores predeterminados def multiplicar(a, b=2):     return a * b  # Llamar a la funci\u00f3n con un solo argumento resultado1 = multiplicar(3) print(\"Resultado de la multiplicaci\u00f3n (con valor predeterminado):\", resultado1)  # Llamar a la funci\u00f3n con dos argumentos resultado2 = multiplicar(3, 4) print(\"Resultado de la multiplicaci\u00f3n (con argumento espec\u00edfico):\", resultado2) <pre>Resultado de la multiplicaci\u00f3n (con valor predeterminado): 6\nResultado de la multiplicaci\u00f3n (con argumento espec\u00edfico): 12\n</pre> In\u00a0[69]: Copied! <pre># Funci\u00f3n con un n\u00famero variable de argumentos\ndef promedio(*args):\n    return sum(args) / len(args)\n\n# Llamar a la funci\u00f3n con diferentes cantidades de argumentos\nresultado1 = promedio(2, 4, 6)\nprint(\"Promedio de 2, 4 y 6:\", resultado1)\n\nresultado2 = promedio(1, 3, 5, 7, 9)\nprint(\"Promedio de 1, 3, 5, 7 y 9:\", resultado2)\n</pre> # Funci\u00f3n con un n\u00famero variable de argumentos def promedio(*args):     return sum(args) / len(args)  # Llamar a la funci\u00f3n con diferentes cantidades de argumentos resultado1 = promedio(2, 4, 6) print(\"Promedio de 2, 4 y 6:\", resultado1)  resultado2 = promedio(1, 3, 5, 7, 9) print(\"Promedio de 1, 3, 5, 7 y 9:\", resultado2) <pre>Promedio de 2, 4 y 6: 4.0\nPromedio de 1, 3, 5, 7 y 9: 5.0\n</pre> In\u00a0[70]: Copied! <pre># Funci\u00f3n con argumentos de palabra clave\ndef mostrar_info(nombre, edad):\n    print(\"Nombre:\", nombre)\n    print(\"Edad:\", edad)\n\n# Llamar a la funci\u00f3n utilizando argumentos de palabra clave\nmostrar_info(nombre=\"Ana\", edad=25)\n</pre> # Funci\u00f3n con argumentos de palabra clave def mostrar_info(nombre, edad):     print(\"Nombre:\", nombre)     print(\"Edad:\", edad)  # Llamar a la funci\u00f3n utilizando argumentos de palabra clave mostrar_info(nombre=\"Ana\", edad=25) <pre>Nombre: Ana\nEdad: 25\n</pre> In\u00a0[71]: Copied! <pre># Funci\u00f3n con retorno de m\u00faltiples valores\ndef dividir(a, b):\n    cociente = a // b\n    resto = a % b\n    return cociente, resto\n\n# Llamar a la funci\u00f3n y almacenar los valores de retorno en variables separadas\ncociente, resto = dividir(10, 3)\nprint(\"Cociente:\", cociente)\nprint(\"Resto:\", resto)\n</pre> # Funci\u00f3n con retorno de m\u00faltiples valores def dividir(a, b):     cociente = a // b     resto = a % b     return cociente, resto  # Llamar a la funci\u00f3n y almacenar los valores de retorno en variables separadas cociente, resto = dividir(10, 3) print(\"Cociente:\", cociente) print(\"Resto:\", resto) <pre>Cociente: 3\nResto: 1\n</pre> In\u00a0[72]: Copied! <pre># Funciones anidadas\ndef exterior():\n    print(\"Funci\u00f3n exterior\")\n    \n    def interior():\n        print(\"Funci\u00f3n interior\")\n    \n    interior()\n\n# Llamar a la funci\u00f3n exterior, que tambi\u00e9n llama a la funci\u00f3n interior\nexterior()\n</pre> # Funciones anidadas def exterior():     print(\"Funci\u00f3n exterior\")          def interior():         print(\"Funci\u00f3n interior\")          interior()  # Llamar a la funci\u00f3n exterior, que tambi\u00e9n llama a la funci\u00f3n interior exterior() <pre>Funci\u00f3n exterior\nFunci\u00f3n interior\n</pre> <p>En Python, los errores se dividen principalmente en tres categor\u00edas: errores de sintaxis, excepciones y errores sem\u00e1nticos. Cada uno de estos tipos de errores tiene sus propias caracter\u00edsticas y se manejan de manera diferente en el c\u00f3digo.</p> <p>Falta de par\u00e9ntesis, corchetes o llaves</p> In\u00a0[73]: Copied! <pre># Error de sintaxis: Falta un par\u00e9ntesis de cierre\nprint(\"Hola mundo\"\n</pre> # Error de sintaxis: Falta un par\u00e9ntesis de cierre print(\"Hola mundo\" <pre>\n  Cell In[73], line 2\n    print(\"Hola mundo\"\n                      ^\nSyntaxError: unexpected EOF while parsing\n</pre> <p>Falta de comillas</p> In\u00a0[74]: Copied! <pre># Error de sintaxis: Falta una comilla de cierre\nmensaje = \"Hola mundo\n</pre> # Error de sintaxis: Falta una comilla de cierre mensaje = \"Hola mundo <pre>\n  Cell In[74], line 2\n    mensaje = \"Hola mundo\n                         ^\nSyntaxError: EOL while scanning string literal\n</pre> <p>Falta de dos puntos</p> In\u00a0[75]: Copied! <pre># Error de sintaxis: Falta dos puntos al final de la declaraci\u00f3n if\nif True\n    print(\"Es verdadero\")\n# Soluci\u00f3n: Agregar dos puntos al final de la declaraci\u00f3n if\nif True:\n    print(\"Es verdadero\")\n</pre> # Error de sintaxis: Falta dos puntos al final de la declaraci\u00f3n if if True     print(\"Es verdadero\") # Soluci\u00f3n: Agregar dos puntos al final de la declaraci\u00f3n if if True:     print(\"Es verdadero\") <pre>\n  Cell In[75], line 2\n    if True\n           ^\nSyntaxError: invalid syntax\n</pre> <p>Indentaci\u00f3n incorrecta</p> In\u00a0[76]: Copied! <pre># Error de sintaxis: Declaraci\u00f3n indentada incorrectamente\nif True:\nprint(\"Es verdadero\")\n</pre> # Error de sintaxis: Declaraci\u00f3n indentada incorrectamente if True: print(\"Es verdadero\") <pre>\n  Cell In[76], line 3\n    print(\"Es verdadero\")\n    ^\nIndentationError: expected an indented block\n</pre> <p>Uso incorrecto de palabras clave</p> In\u00a0[77]: Copied! <pre># Error de sintaxis: Uso incorrecto de la palabra clave 'else'\nif True:\n    print(\"Es verdadero\")\nelsee:\n    print(\"Es falso\")\n</pre> # Error de sintaxis: Uso incorrecto de la palabra clave 'else' if True:     print(\"Es verdadero\") elsee:     print(\"Es falso\") <pre>\n  Cell In[77], line 4\n    elsee:\n          ^\nSyntaxError: invalid syntax\n</pre> <p>Excepci\u00f3n de tipo <code>SyntaxError</code></p> In\u00a0[78]: Copied! <pre># Error de sintaxis: Falta un par\u00e9ntesis de cierre\nprint(\"Hola mundo\"\n</pre> # Error de sintaxis: Falta un par\u00e9ntesis de cierre print(\"Hola mundo\" <pre>\n  Cell In[78], line 2\n    print(\"Hola mundo\"\n                      ^\nSyntaxError: unexpected EOF while parsing\n</pre> <p>Excepci\u00f3n de tipo <code>NameError</code></p> In\u00a0[79]: Copied! <pre># Error de nombre: Variable no definida\nprint(variable_no_definida)\n</pre> # Error de nombre: Variable no definida print(variable_no_definida) <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[79], line 2\n      1 # Error de nombre: Variable no definida\n----&gt; 2 print(variable_no_definida)\n\nNameError: name 'variable_no_definida' is not defined</pre> <p>Excepci\u00f3n de tipo <code>TypeError</code></p> In\u00a0[80]: Copied! <pre># Error de tipo: Suma entre tipos incompatibles\nresultado = 10 + \"20\"\n</pre> # Error de tipo: Suma entre tipos incompatibles resultado = 10 + \"20\" <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[80], line 2\n      1 # Error de tipo: Suma entre tipos incompatibles\n----&gt; 2 resultado = 10 + \"20\"\n\nTypeError: unsupported operand type(s) for +: 'int' and 'str'</pre> <p>Excepci\u00f3n de tipo <code>ValueError</code></p> In\u00a0[81]: Copied! <pre># Error de valor: Argumento fuera de rango\nimport math\nraiz_negativa = math.sqrt(-9)\n</pre> # Error de valor: Argumento fuera de rango import math raiz_negativa = math.sqrt(-9) <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[81], line 3\n      1 # Error de valor: Argumento fuera de rango\n      2 import math\n----&gt; 3 raiz_negativa = math.sqrt(-9)\n\nValueError: math domain error</pre> <p>Excepci\u00f3n de tipo <code>ZeroDivisionError</code></p> In\u00a0[82]: Copied! <pre># Error de divisi\u00f3n por cero\ndivision = 10 / 0\n</pre> # Error de divisi\u00f3n por cero division = 10 / 0 <pre>\n---------------------------------------------------------------------------\nZeroDivisionError                         Traceback (most recent call last)\nCell In[82], line 2\n      1 # Error de divisi\u00f3n por cero\n----&gt; 2 division = 10 / 0\n\nZeroDivisionError: division by zero</pre> <p>Excepci\u00f3n de tipo <code>OverflowError</code></p> In\u00a0[83]: Copied! <pre># Error de desbordamiento: N\u00famero demasiado grande para ser representado\nnumero_grande = 20.0 ** 20.0 ** 20.0\n</pre> # Error de desbordamiento: N\u00famero demasiado grande para ser representado numero_grande = 20.0 ** 20.0 ** 20.0 <pre>\n---------------------------------------------------------------------------\nOverflowError                             Traceback (most recent call last)\nCell In[83], line 2\n      1 # Error de desbordamiento: N\u00famero demasiado grande para ser representado\n----&gt; 2 numero_grande = 20.0 ** 20.0 ** 20.0\n\nOverflowError: (34, 'Result too large')</pre> <p>Manejo de Excepciones</p> In\u00a0[84]: Copied! <pre>try:\n    # C\u00f3digo propenso a generar una excepci\u00f3n\n    resultado = 10 / 0\nexcept ZeroDivisionError:\n    # Manejar la excepci\u00f3n\n    print(\"Error: Divisi\u00f3n por cero\")\n</pre> try:     # C\u00f3digo propenso a generar una excepci\u00f3n     resultado = 10 / 0 except ZeroDivisionError:     # Manejar la excepci\u00f3n     print(\"Error: Divisi\u00f3n por cero\") <pre>Error: Divisi\u00f3n por cero\n</pre> <p>Operaciones incorrectas</p> In\u00a0[85]: Copied! <pre># Error sem\u00e1ntico: Operaci\u00f3n incorrecta\nresultado = 10 / 0  # Divisi\u00f3n por cero\n</pre> # Error sem\u00e1ntico: Operaci\u00f3n incorrecta resultado = 10 / 0  # Divisi\u00f3n por cero <pre>\n---------------------------------------------------------------------------\nZeroDivisionError                         Traceback (most recent call last)\nCell In[85], line 2\n      1 # Error sem\u00e1ntico: Operaci\u00f3n incorrecta\n----&gt; 2 resultado = 10 / 0  # Divisi\u00f3n por cero\n\nZeroDivisionError: division by zero</pre> <p>L\u00f3gica incorrecta</p> In\u00a0[86]: Copied! <pre># Error sem\u00e1ntico: L\u00f3gica incorrecta\ndef calcular_promedio(valores):\n    suma = 0\n    for valor in valores:\n        suma += valor\n    promedio = suma / len(valores)\n    return promedio\n\n# Llamar a la funci\u00f3n con una lista vac\u00eda\nvalores = []\npromedio = calcular_promedio(valores)\nprint(\"El promedio es:\", promedio)\n</pre> # Error sem\u00e1ntico: L\u00f3gica incorrecta def calcular_promedio(valores):     suma = 0     for valor in valores:         suma += valor     promedio = suma / len(valores)     return promedio  # Llamar a la funci\u00f3n con una lista vac\u00eda valores = [] promedio = calcular_promedio(valores) print(\"El promedio es:\", promedio) <pre>\n---------------------------------------------------------------------------\nZeroDivisionError                         Traceback (most recent call last)\nCell In[86], line 11\n      9 # Llamar a la funci\u00f3n con una lista vac\u00eda\n     10 valores = []\n---&gt; 11 promedio = calcular_promedio(valores)\n     12 print(\"El promedio es:\", promedio)\n\nCell In[86], line 6, in calcular_promedio(valores)\n      4 for valor in valores:\n      5     suma += valor\n----&gt; 6 promedio = suma / len(valores)\n      7 return promedio\n\nZeroDivisionError: division by zero</pre> <p>En este caso, si la lista <code>valores</code> est\u00e1 vac\u00eda, se generar\u00e1 un error al intentar dividir por cero, lo cual es un error sem\u00e1ntico en el contexto del programa.</p> <p>Uso incorrecto de estructuras de datos</p> In\u00a0[87]: Copied! <pre># Error sem\u00e1ntico: Uso incorrecto de una lista\nnumeros = [1, 2, 3, 4, 5]\nultimo_numero = numeros[10]  # Acceso a un \u00edndice fuera de rango\n</pre> # Error sem\u00e1ntico: Uso incorrecto de una lista numeros = [1, 2, 3, 4, 5] ultimo_numero = numeros[10]  # Acceso a un \u00edndice fuera de rango <pre>\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[87], line 3\n      1 # Error sem\u00e1ntico: Uso incorrecto de una lista\n      2 numeros = [1, 2, 3, 4, 5]\n----&gt; 3 ultimo_numero = numeros[10]  # Acceso a un \u00edndice fuera de rango\n\nIndexError: list index out of range</pre> <p>Valores incorrectos</p> In\u00a0[88]: Copied! <pre># Error sem\u00e1ntico: Asignaci\u00f3n incorrecta de valores\nedad = -10  # Edad negativa\n</pre> # Error sem\u00e1ntico: Asignaci\u00f3n incorrecta de valores edad = -10  # Edad negativa <p>L\u00f3gica de negocio incorrecta</p> In\u00a0[89]: Copied! <pre># Error sem\u00e1ntico: C\u00e1lculo incorrecto\nprecio = 100\ndescuento = 200  # Descuento mayor que el precio\nprecio_final = precio - descuento\n</pre> # Error sem\u00e1ntico: C\u00e1lculo incorrecto precio = 100 descuento = 200  # Descuento mayor que el precio precio_final = precio - descuento <p>C\u00f3mo Identificar Errores Sem\u00e1nticos</p> <p>Los errores sem\u00e1nticos generalmente se detectan cuando el programa produce resultados inesperados o incorrectos. Para identificar y corregir estos errores, es importante comprender la l\u00f3gica del programa y revisar cuidadosamente el c\u00f3digo en busca de posibles problemas. La depuraci\u00f3n paso a paso y las pruebas exhaustivas del programa pueden ayudar a identificar y corregir los errores sem\u00e1nticos de manera efectiva.</p>"},{"location":"lectures/toolkit/python/#python","title":"Python\u00b6","text":""},{"location":"lectures/toolkit/python/#introduccion","title":"Introducci\u00f3n\u00b6","text":"<p>Python es un lenguaje de programaci\u00f3n interpretado y de alto nivel. Fue creado por Guido van Rossum y lanzado por primera vez en 1991. Python se destaca por su legibilidad y sintaxis clara, lo que lo hace ideal tanto para principiantes como para programadores experimentados.</p> <p>Una de las caracter\u00edsticas m\u00e1s destacadas de Python es su filosof\u00eda de dise\u00f1o, que enfatiza la legibilidad del c\u00f3digo y la facilidad de uso. El lenguaje utiliza una sintaxis limpia y utiliza el espaciado en blanco de manera significativa para estructurar el c\u00f3digo, lo que lo hace f\u00e1cil de leer y entender.</p> <p>\ud83d\udd11 Nota: Para profundizar en conceptos b\u00e1sicos de Python, se recomienda estudiar del curso Introducci\u00f3n a Python.</p>"},{"location":"lectures/toolkit/python/#nomenclatura","title":"Nomenclatura\u00b6","text":""},{"location":"lectures/toolkit/python/#sintaxis","title":"Sintaxis\u00b6","text":"<p>Hola mundo!</p> <p>Escribamos nuestro primer programa de Python, \"\u00a1Hola, mundo!\". Es un programa simple que imprime Hello World! en el dispositivo de salida est\u00e1ndar (pantalla).</p>"},{"location":"lectures/toolkit/python/#operaciones","title":"Operaciones\u00b6","text":"<p>Puedes realizar una variedad de operaciones en diferentes tipos de datos. Estas operaciones pueden ser aritm\u00e9ticas, de comparaci\u00f3n, l\u00f3gicas, de asignaci\u00f3n, de pertenencia e identidad. A continuaci\u00f3n, se describen los principales tipos de operaciones:</p>"},{"location":"lectures/toolkit/python/#control-de-flujo","title":"Control de Flujo\u00b6","text":""},{"location":"lectures/toolkit/python/#condicional-if-elif-else","title":"Condicional if-elif-else\u00b6","text":"<p>Los condicionales <code>if</code>, <code>elif</code> y <code>else</code> son estructuras fundamentales en Python que te permiten ejecutar diferentes bloques de c\u00f3digo dependiendo de ciertas condiciones. A continuaci\u00f3n se explica c\u00f3mo funcionan:</p> <p>Estructura b\u00e1sica</p> <p>La estructura b\u00e1sica de un condicional <code>if</code> es la siguiente:</p> <pre>if condicion:\n    # C\u00f3digo a ejecutar si la condici\u00f3n es verdadera\n</pre> <p>Si la condici\u00f3n es verdadera, se ejecuta el bloque de c\u00f3digo indentado debajo de la declaraci\u00f3n <code>if</code>. Si la condici\u00f3n es falsa, este bloque de c\u00f3digo se omite.</p> <p>Condicional if-else</p> <p>Puedes agregar un bloque <code>else</code> para ejecutar un c\u00f3digo alternativo cuando la condici\u00f3n en el <code>if</code> es falsa:</p> <pre>if condicion:\n    # C\u00f3digo a ejecutar si la condici\u00f3n es verdadera\nelse:\n    # C\u00f3digo a ejecutar si la condici\u00f3n es falsa\n</pre> <p>Condicional if-elif-else</p> <p>Si tienes m\u00faltiples condiciones, puedes usar la estructura <code>elif</code> (abreviatura de \"else if\") para verificarlas en secuencia:</p> <pre>if condicion1:\n    # C\u00f3digo a ejecutar si la condicion1 es verdadera\nelif condicion2:\n    # C\u00f3digo a ejecutar si la condicion2 es verdadera\nelse:\n    # C\u00f3digo a ejecutar si ninguna de las condiciones anteriores es verdadera\n</pre> <p>Ejemplo</p>"},{"location":"lectures/toolkit/python/#bucle-while","title":"Bucle While\u00b6","text":"<p>El bucle <code>while</code> es una estructura de control que permite repetir un bloque de c\u00f3digo mientras una condici\u00f3n especificada sea verdadera. A continuaci\u00f3n se explica c\u00f3mo funciona:</p> <p>Estructura b\u00e1sica</p> <p>La estructura b\u00e1sica de un bucle <code>while</code> es la siguiente:</p> <pre>while condicion:\n    # C\u00f3digo a repetir mientras la condici\u00f3n sea verdadera\n</pre> <p>El bloque de c\u00f3digo indentado debajo del <code>while</code> se ejecutar\u00e1 repetidamente mientras la condici\u00f3n sea verdadera. Una vez que la condici\u00f3n se eval\u00faa como falsa, la ejecuci\u00f3n del bucle se detiene y el programa contin\u00faa con la siguiente instrucci\u00f3n despu\u00e9s del bucle.</p> <p>Ejemplo</p>"},{"location":"lectures/toolkit/python/#ciclo-for-con-la-funcion-range","title":"Ciclo <code>For</code> con la Funci\u00f3n Range()\u00b6","text":"<p>El ciclo <code>for</code> se utiliza para iterar sobre una secuencia (como una lista, una tupla, un diccionario, etc.) y realizar una acci\u00f3n en cada elemento de la secuencia. La funci\u00f3n <code>range()</code> es com\u00fanmente utilizada junto con un ciclo <code>for</code> para generar una secuencia de n\u00fameros en un rango espec\u00edfico. A continuaci\u00f3n se explica c\u00f3mo funciona:</p> <p>Estructura b\u00e1sica</p> <p>La estructura b\u00e1sica de un ciclo <code>for</code> con <code>range()</code> es la siguiente:</p> <pre>for variable in range(inicio, fin, paso):\n    # C\u00f3digo a ejecutar en cada iteraci\u00f3n\n</pre> <p>La funci\u00f3n <code>range()</code> genera una secuencia de n\u00fameros que comienza desde el valor de \"inicio\" hasta el valor de \"fin\" (sin incluirlo), con un incremento definido por el valor de \"paso\". El bloque de c\u00f3digo indentado debajo del <code>for</code> se ejecutar\u00e1 una vez para cada valor en la secuencia generada por <code>range()</code>.</p> <p>Ejemplo</p>"},{"location":"lectures/toolkit/python/#salir-o-continuar-un-ciclo","title":"Salir o Continuar un Ciclo\u00b6","text":"<p>Puedes controlar el flujo de ejecuci\u00f3n de un ciclo <code>for</code> o <code>while</code> utilizando las palabras clave <code>break</code> y <code>continue</code>. Estas palabras clave te permiten detener la ejecuci\u00f3n del ciclo de forma prematura o saltar a la siguiente iteraci\u00f3n del ciclo, respectivamente. A continuaci\u00f3n se explica c\u00f3mo funcionan:</p> <p>Salir de un Ciclo con <code>break</code></p> <p>La palabra clave <code>break</code> se utiliza para salir de un ciclo de forma prematura si se cumple una cierta condici\u00f3n. Cuando se encuentra una instrucci\u00f3n <code>break</code>, el ciclo se detiene inmediatamente y la ejecuci\u00f3n del programa contin\u00faa con la primera instrucci\u00f3n despu\u00e9s del ciclo.</p> <p>Ejemplo</p>"},{"location":"lectures/toolkit/python/#estructura-de-datos","title":"Estructura de datos\u00b6","text":""},{"location":"lectures/toolkit/python/#listas","title":"Listas\u00b6","text":"<p>Una lista en Python es una colecci\u00f3n ordenada y mutable de elementos. Puedes acceder a los elementos de una lista utilizando \u00edndices, y puedes modificar la lista agregando, eliminando o cambiando elementos.</p>"},{"location":"lectures/toolkit/python/#tuplas","title":"Tuplas\u00b6","text":"<p>Una tupla en Python es una colecci\u00f3n ordenada e inmutable de elementos. Puedes acceder a los elementos de una tupla utilizando \u00edndices, pero no puedes modificar la tupla despu\u00e9s de crearla.</p>"},{"location":"lectures/toolkit/python/#conjuntos","title":"Conjuntos\u00b6","text":"<p>Un conjunto en Python es una colecci\u00f3n desordenada y mutable de elementos \u00fanicos. Puedes realizar operaciones de conjuntos como uni\u00f3n, intersecci\u00f3n y diferencia entre conjuntos.</p>"},{"location":"lectures/toolkit/python/#diccionarios","title":"Diccionarios\u00b6","text":"<p>Un diccionario en Python es una colecci\u00f3n desordenada y mutable de pares clave-valor. Puedes acceder a los valores utilizando las claves, y puedes agregar, modificar o eliminar elementos del diccionario.</p>"},{"location":"lectures/toolkit/python/#funciones","title":"Funciones\u00b6","text":""},{"location":"lectures/toolkit/python/#definiciones","title":"Definiciones\u00b6","text":"<p>Una funci\u00f3n es un bloque de c\u00f3digo que realiza una tarea espec\u00edfica y puede ser llamado desde cualquier parte del programa. Las funciones pueden tener par\u00e1metros (datos que se pasan a la funci\u00f3n) y pueden devolver un resultado. Al utilizar funciones, podemos dividir el c\u00f3digo en partes m\u00e1s peque\u00f1as y manejables, lo que facilita la comprensi\u00f3n y la depuraci\u00f3n.</p>"},{"location":"lectures/toolkit/python/#sintaxis","title":"Sintaxis\u00b6","text":"<p>La sintaxis b\u00e1sica de una funci\u00f3n en Python es la siguiente:</p> <pre>def nombre_funcion(parametros):\n    # Cuerpo de la funci\u00f3n\n    # Realizar operaciones\n    return resultado\n</pre> <ul> <li>def: Es una palabra clave que indica que se est\u00e1 definiendo una funci\u00f3n.</li> <li>nombre_funcion: Es el nombre que le damos a la funci\u00f3n. Debe seguir las mismas reglas de nomenclatura que las variables.</li> <li>par\u00e1metros: Son los datos que la funci\u00f3n espera recibir. Pueden ser opcionales.</li> <li>return: Es una palabra clave que indica el valor que la funci\u00f3n devolver\u00e1. Puede devolver cero o m\u00e1s valores.</li> </ul>"},{"location":"lectures/toolkit/python/#excepciones","title":"Excepciones\u00b6","text":""},{"location":"lectures/toolkit/python/#errores-de-sintaxis","title":"Errores de Sintaxis\u00b6","text":"<p>Los errores de sintaxis en Python ocurren cuando el c\u00f3digo no sigue las reglas gramaticales del lenguaje. Estos errores se detectan durante la fase de an\u00e1lisis del c\u00f3digo y generalmente provocan que el int\u00e9rprete de Python detenga la ejecuci\u00f3n del programa y muestre un mensaje de error que indica la ubicaci\u00f3n y la naturaleza del error. Aqu\u00ed hay una lista de algunos errores de sintaxis comunes y ejemplos de c\u00f3mo corregirlos:</p>"},{"location":"lectures/toolkit/python/#excepciones","title":"Excepciones\u00b6","text":"<p>Las excepciones en Python son eventos que ocurren durante la ejecuci\u00f3n del programa y pueden alterar el flujo normal de ejecuci\u00f3n. Cuando una excepci\u00f3n se produce, Python detiene la ejecuci\u00f3n del programa y busca un bloque de c\u00f3digo que pueda manejar la excepci\u00f3n. Aqu\u00ed se presentan algunos tipos comunes de excepciones en Python y c\u00f3mo manejarlos:</p>"},{"location":"lectures/toolkit/python/#errores-semanticos","title":"Errores Sem\u00e1nticos\u00b6","text":"<p>Los errores sem\u00e1nticos en Python son m\u00e1s dif\u00edciles de detectar que los errores de sintaxis o las excepciones porque el c\u00f3digo puede ejecutarse sin generar mensajes de error, pero produce resultados inesperados o incorrectos debido a una l\u00f3gica incorrecta en el programa. Aqu\u00ed se presentan algunos ejemplos de errores sem\u00e1nticos comunes en Python:</p>"},{"location":"lectures/trading/trading_01/","title":"Estrategias de Trading con Medias M\u00f3viles","text":"In\u00a0[3]: Copied! <pre># import python utility libraries\nimport os as os\nimport datetime as dt\nimport itertools as it\n\n# import python data science libraries\nimport pandas as pd\nimport numpy as np\n\n# import the pandas financial data reader library\nimport pandas_datareader as dr\n\n# import the matplotlib and seaborn visualization library\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport bt as bt # library to backtest trading signals\n\nimport warnings\nwarnings.filterwarnings('ignore')\n</pre> # import python utility libraries import os as os import datetime as dt import itertools as it  # import python data science libraries import pandas as pd import numpy as np  # import the pandas financial data reader library import pandas_datareader as dr  # import the matplotlib and seaborn visualization library import matplotlib.pyplot as plt import seaborn as sns  import bt as bt # library to backtest trading signals  import warnings warnings.filterwarnings('ignore') In\u00a0[4]: Copied! <pre># leer datos\npath = 'data/ibm_data_1990_2017_daily.csv'\nibm_data =  pd.read_csv(path, sep=\",\" )#.dropna()\nibm_data['Date'] = pd.to_datetime(ibm_data['Date'])\nibm_data=ibm_data.set_index('Date')#.rename(columns = {'Adj_Close':'Adj Close'})\n</pre> # leer datos path = 'data/ibm_data_1990_2017_daily.csv' ibm_data =  pd.read_csv(path, sep=\",\" )#.dropna() ibm_data['Date'] = pd.to_datetime(ibm_data['Date']) ibm_data=ibm_data.set_index('Date')#.rename(columns = {'Adj_Close':'Adj Close'}) In\u00a0[6]: Copied! <pre>ibm_data.head()\n</pre> ibm_data.head() Out[6]: Open High Low Close Adj Close Volume Date 1990-12-31 27.097275 27.127151 26.977772 27.007648 12.125891 1930079 1991-01-02 26.977772 27.186901 26.798517 26.798517 12.031993 4341737 1991-01-03 26.858271 27.216778 26.828394 26.888145 12.072238 5470162 1991-01-04 26.947897 27.007648 26.738768 26.798517 12.031993 4540058 1991-01-07 26.619265 26.738768 26.290630 26.350382 11.830786 4976450 <p>Inspecciona visualmente el precio de cierre ajustado (<code>Adj Close</code>) de los datos descargados de <code>IBM</code>:</p> In\u00a0[13]: Copied! <pre>plt.rcParams['figure.figsize'] = [15, 5]\nfig = plt.figure()\n\n# trazar los precios de cierre ajustados diarios de IBM\nplt.plot(\n    ibm_data.index,\n    ibm_data['Adj Close'],\n    color='#9b59b6'\n)\n\n# establecer etiquetas de los ejes\nplt.xlabel('[tiempo]', fontsize=10)\nplt.ylabel('[precio de cierre ajustado]', fontsize=10)\n\n# establecer el t\u00edtulo del gr\u00e1fico\nplt.title('Corporaci\u00f3n Internacional de M\u00e1quinas Empresariales (IBM) - Precios Hist\u00f3ricos de Acciones', fontsize=10);\n</pre> plt.rcParams['figure.figsize'] = [15, 5] fig = plt.figure()  # trazar los precios de cierre ajustados diarios de IBM plt.plot(     ibm_data.index,     ibm_data['Adj Close'],     color='#9b59b6' )  # establecer etiquetas de los ejes plt.xlabel('[tiempo]', fontsize=10) plt.ylabel('[precio de cierre ajustado]', fontsize=10)  # establecer el t\u00edtulo del gr\u00e1fico plt.title('Corporaci\u00f3n Internacional de M\u00e1quinas Empresariales (IBM) - Precios Hist\u00f3ricos de Acciones', fontsize=10); <p>Una mejora del Cruce de Medias M\u00f3viles consiste en aplicar dos medias m\u00f3viles a un gr\u00e1fico: una media m\u00f3vil de largo plazo (por ejemplo, una SMAV de 200 d\u00edas) y una media m\u00f3vil de corto plazo (por ejemplo, una SMAV de 20 d\u00edas). Cuando la media m\u00f3vil de corto plazo cruza por encima de la media m\u00f3vil de largo plazo, se activa una se\u00f1al de Compra o Larga, ya que indica que la tendencia est\u00e1 cambiando hacia arriba (esto se conoce como \"cruce dorado\"). Por otro lado, cuando la media m\u00f3vil de corto plazo cruza por debajo de la media m\u00f3vil de largo plazo, se activa una se\u00f1al de Venta o Corta, ya que indica que la tendencia est\u00e1 cambiando hacia abajo (esto se conoce como \"cruce de muerte\").</p> <p>Comencemos a implementar esta estrategia de trading mejorada estableciendo los distintos tama\u00f1os de ventana de las medias m\u00f3viles que especifican el n\u00famero de precios de cierre ajustados diarios hist\u00f3ricos de las acciones de IBM que se deben considerar en el c\u00e1lculo de la media m\u00f3vil:</p> In\u00a0[14]: Copied! <pre>cross_mav_days_15 = 15  # establecer \"r\u00e1pido\" indicador de media m\u00f3vil de corto plazo, ventana de observaci\u00f3n = 15 d\u00edas\ncross_mav_days_60 = 60  # establecer \"lento\" indicador de media m\u00f3vil de corto plazo, ventana de observaci\u00f3n = 60 d\u00edas\ncross_mav_days_200 = 200  # establecer \"tendencia\" indicador de media m\u00f3vil de largo plazo, ventana de observaci\u00f3n = 200 d\u00edas\n</pre> cross_mav_days_15 = 15  # establecer \"r\u00e1pido\" indicador de media m\u00f3vil de corto plazo, ventana de observaci\u00f3n = 15 d\u00edas cross_mav_days_60 = 60  # establecer \"lento\" indicador de media m\u00f3vil de corto plazo, ventana de observaci\u00f3n = 60 d\u00edas cross_mav_days_200 = 200  # establecer \"tendencia\" indicador de media m\u00f3vil de largo plazo, ventana de observaci\u00f3n = 200 d\u00edas <p>Calcular las medias m\u00f3viles simples de las siguientes ventanas de observaci\u00f3n: 15 d\u00edas, 50 d\u00edas y 200 d\u00edas. En general, la \"Media M\u00f3vil Simple (SMAV)\" de un instrumento financiero $i$ (por ejemplo, una acci\u00f3n, materia prima, tipo de cambio) se define como la media de los \u00faltimos $n$ precios, formalmente denotada por:</p> <p>$$SMA_{i}(t)=\\frac{1}{n} \\sum_{k=0}^{n-1} p_{i}(t-k)$$</p> <p>donde $t$ denota el punto actual en el tiempo y $n$ la ventana de observaci\u00f3n.</p> <p>Podemos calcular la SMAV utilizando simplemente las funciones <code>rolling()</code> y <code>mean()</code> de Pandas:</p> In\u00a0[17]: Copied! <pre>cross_mav_15 = pd.Series(ibm_data['Adj Close'].rolling(window = cross_mav_days_15).mean(), name = 'SMAV_15')\ncross_mav_60 = pd.Series(ibm_data['Adj Close'].rolling(window = cross_mav_days_60).mean(), name = 'SMAV_60')\ncross_mav_200 = pd.Series(ibm_data['Adj Close'].rolling(window = cross_mav_days_200).mean(), name = 'SMAV_200')\n</pre> cross_mav_15 = pd.Series(ibm_data['Adj Close'].rolling(window = cross_mav_days_15).mean(), name = 'SMAV_15') cross_mav_60 = pd.Series(ibm_data['Adj Close'].rolling(window = cross_mav_days_60).mean(), name = 'SMAV_60') cross_mav_200 = pd.Series(ibm_data['Adj Close'].rolling(window = cross_mav_days_200).mean(), name = 'SMAV_200') <p>Combinar los valores de las medias m\u00f3viles simples con los datos originales del mercado (precios de cierre ajustados):</p> In\u00a0[18]: Copied! <pre>cross_mav_ibm_data = ibm_data.join(cross_mav_15)\ncross_mav_ibm_data = cross_mav_ibm_data.join(cross_mav_60)\ncross_mav_ibm_data = cross_mav_ibm_data.join(cross_mav_200)\n</pre> cross_mav_ibm_data = ibm_data.join(cross_mav_15) cross_mav_ibm_data = cross_mav_ibm_data.join(cross_mav_60) cross_mav_ibm_data = cross_mav_ibm_data.join(cross_mav_200) <p>Inspeccionar y validar los precios de cierre ajustados diarios de las acciones de IBM, as\u00ed como los valores derivados de las medias m\u00f3viles, comenzando desde el primer precio de mercado obtenido con una media m\u00f3vil de 200 d\u00edas:</p> In\u00a0[19]: Copied! <pre>cross_mav_ibm_data[['Adj Close', 'SMAV_15', 'SMAV_60', 'SMAV_200']].iloc[200:210]\n</pre> cross_mav_ibm_data[['Adj Close', 'SMAV_15', 'SMAV_60', 'SMAV_200']].iloc[200:210] Out[19]: Adj Close SMAV_15 SMAV_60 SMAV_200 Date 1991-10-15 11.566666 11.213472 11.078775 11.863328 1991-10-16 11.275419 11.188507 11.082853 11.859545 1991-10-17 11.081260 11.151524 11.083467 11.854590 1991-10-18 11.136729 11.137655 11.085463 11.850114 1991-10-21 11.122860 11.112691 11.087227 11.846574 1991-10-22 10.928700 11.082180 11.084156 11.842734 1991-10-23 10.928700 11.062763 11.080629 11.840035 1991-10-24 10.900956 11.057216 11.077324 11.836392 1991-10-25 10.873223 11.055367 11.073558 11.832744 1991-10-28 10.900956 11.050744 11.071852 11.829973 <p>Graficar los precios hist\u00f3ricos diarios de cierre ajustados de las acciones de IBM (azul), as\u00ed como sus medias m\u00f3viles de 15 d\u00edas (verde), 60 d\u00edas (rojo) y 200 d\u00edas (amarillo):</p> In\u00a0[21]: Copied! <pre>plt.rcParams['figure.figsize'] = [15, 5]\nfig = plt.figure()\n\n# graficar precios de cierre ajustados y medias m\u00f3viles\nplt.plot(cross_mav_ibm_data['Adj Close'], lw=1.0, color='#9b59b6', label='Precios de Cierre (morado)')\nplt.plot(cross_mav_ibm_data['SMAV_15'], color='C1',lw=1.0, label='MAV de 15 d\u00edas (verde)')\nplt.plot(cross_mav_ibm_data['SMAV_60'], color='C1',lw=1.0, label='MAV de 60 d\u00edas (rojo)')\nplt.plot(cross_mav_ibm_data['SMAV_200'], color='C4', lw=1.0, label='MAV de 200 d\u00edas (amarillo)')\n\n# establecer etiquetas de los ejes\nplt.xlabel('[tiempo]', fontsize=10)\nplt.ylabel('[precio de mercado]', fontsize=10)\n\n# establecer leyenda del gr\u00e1fico\nplt.legend(loc=\"upper left\", numpoints=1, fancybox=True)\n\n# establecer t\u00edtulo del gr\u00e1fico\nplt.title('Corporaci\u00f3n Internacional de M\u00e1quinas Empresariales (IBM) - Precios de Cierre Hist\u00f3ricos Diarios de Acciones', fontsize=10);\n</pre> plt.rcParams['figure.figsize'] = [15, 5] fig = plt.figure()  # graficar precios de cierre ajustados y medias m\u00f3viles plt.plot(cross_mav_ibm_data['Adj Close'], lw=1.0, color='#9b59b6', label='Precios de Cierre (morado)') plt.plot(cross_mav_ibm_data['SMAV_15'], color='C1',lw=1.0, label='MAV de 15 d\u00edas (verde)') plt.plot(cross_mav_ibm_data['SMAV_60'], color='C1',lw=1.0, label='MAV de 60 d\u00edas (rojo)') plt.plot(cross_mav_ibm_data['SMAV_200'], color='C4', lw=1.0, label='MAV de 200 d\u00edas (amarillo)')  # establecer etiquetas de los ejes plt.xlabel('[tiempo]', fontsize=10) plt.ylabel('[precio de mercado]', fontsize=10)  # establecer leyenda del gr\u00e1fico plt.legend(loc=\"upper left\", numpoints=1, fancybox=True)  # establecer t\u00edtulo del gr\u00e1fico plt.title('Corporaci\u00f3n Internacional de M\u00e1quinas Empresariales (IBM) - Precios de Cierre Hist\u00f3ricos Diarios de Acciones', fontsize=10); In\u00a0[24]: Copied! <pre># crear se\u00f1ales de seguimiento de tendencias 'r\u00e1pidas'\ncross_mav_ibm_data['SIGNAL_15'] = 0.0\ncross_mav_ibm_data.loc[cross_mav_ibm_data['SMAV_15'] &gt; cross_mav_ibm_data['SMAV_200'], 'SIGNAL_15'] = 1.0\ncross_mav_ibm_data.loc[cross_mav_ibm_data['SMAV_15'] &lt; cross_mav_ibm_data['SMAV_200'], 'SIGNAL_15'] = -1.0\n\n# crear se\u00f1ales de seguimiento de tendencias 'lentas'\ncross_mav_ibm_data['SIGNAL_60'] = 0.0\ncross_mav_ibm_data.loc[cross_mav_ibm_data['SMAV_60'] &gt; cross_mav_ibm_data['SMAV_200'], 'SIGNAL_60'] = 1.0\ncross_mav_ibm_data.loc[cross_mav_ibm_data['SMAV_60'] &lt; cross_mav_ibm_data['SMAV_200'], 'SIGNAL_60'] = -1.0\n</pre> # crear se\u00f1ales de seguimiento de tendencias 'r\u00e1pidas' cross_mav_ibm_data['SIGNAL_15'] = 0.0 cross_mav_ibm_data.loc[cross_mav_ibm_data['SMAV_15'] &gt; cross_mav_ibm_data['SMAV_200'], 'SIGNAL_15'] = 1.0 cross_mav_ibm_data.loc[cross_mav_ibm_data['SMAV_15'] &lt; cross_mav_ibm_data['SMAV_200'], 'SIGNAL_15'] = -1.0  # crear se\u00f1ales de seguimiento de tendencias 'lentas' cross_mav_ibm_data['SIGNAL_60'] = 0.0 cross_mav_ibm_data.loc[cross_mav_ibm_data['SMAV_60'] &gt; cross_mav_ibm_data['SMAV_200'], 'SIGNAL_60'] = 1.0 cross_mav_ibm_data.loc[cross_mav_ibm_data['SMAV_60'] &lt; cross_mav_ibm_data['SMAV_200'], 'SIGNAL_60'] = -1.0 <p>Adem\u00e1s, vamos a preparar una prueba retrospectiva de un \"referencia\" en t\u00e9rminos de una estrategia de trading comprar y mantener para fines de comparaci\u00f3n. Nuestra estrategia comprar y mantener env\u00eda una se\u00f1al \"larga\" (+1.0) para cada paso de tiempo:</p> In\u00a0[25]: Copied! <pre>cross_mav_ibm_data['SIGNAL_BASE'] = 1.0\n</pre> cross_mav_ibm_data['SIGNAL_BASE'] = 1.0 <p>Preparar los datos de las se\u00f1ales de trading para ser utilizados en la prueba retrospectiva de la estrategia de trading de medias m\u00f3viles a largo y corto plazo:</p> In\u00a0[26]: Copied! <pre># convertir las se\u00f1ales a un DataFrame de Pandas\ncross_mav_ibm_signal_data = pd.DataFrame(cross_mav_ibm_data[['SIGNAL_15', 'SIGNAL_60', 'SIGNAL_BASE']], columns=['SIGNAL_15', 'SIGNAL_60', 'SIGNAL_BASE'])\n\n# convertir el \u00edndice del DataFrame de Pandas al tipo de datos: datetime\ncross_mav_ibm_signal_data = cross_mav_ibm_signal_data.set_index(pd.to_datetime(ibm_data.index))\n</pre> # convertir las se\u00f1ales a un DataFrame de Pandas cross_mav_ibm_signal_data = pd.DataFrame(cross_mav_ibm_data[['SIGNAL_15', 'SIGNAL_60', 'SIGNAL_BASE']], columns=['SIGNAL_15', 'SIGNAL_60', 'SIGNAL_BASE'])  # convertir el \u00edndice del DataFrame de Pandas al tipo de datos: datetime cross_mav_ibm_signal_data = cross_mav_ibm_signal_data.set_index(pd.to_datetime(ibm_data.index)) <p>Inspeccionar las primeras filas de las se\u00f1ales de trading preparadas:</p> In\u00a0[28]: Copied! <pre>cross_mav_ibm_signal_data.head()\n</pre> cross_mav_ibm_signal_data.head() Out[28]: SIGNAL_15 SIGNAL_60 SIGNAL_BASE Date 1990-12-31 0.0 0.0 1.0 1991-01-02 0.0 0.0 1.0 1991-01-03 0.0 0.0 1.0 1991-01-04 0.0 0.0 1.0 1991-01-07 0.0 0.0 1.0 <p>Inspeccionar algunas de las desviaciones de se\u00f1al ejemplares entre las estrategias de trading de cruce de medias m\u00f3viles de 15 d\u00edas y 60 d\u00edas:</p> In\u00a0[29]: Copied! <pre>cross_mav_ibm_signal_data[cross_mav_ibm_signal_data['SIGNAL_15'] != cross_mav_ibm_signal_data['SIGNAL_60']].head()\n</pre> cross_mav_ibm_signal_data[cross_mav_ibm_signal_data['SIGNAL_15'] != cross_mav_ibm_signal_data['SIGNAL_60']].head() Out[29]: SIGNAL_15 SIGNAL_60 SIGNAL_BASE Date 1992-05-15 1.0 -1.0 1.0 1992-05-18 1.0 -1.0 1.0 1992-05-19 1.0 -1.0 1.0 1992-05-20 1.0 -1.0 1.0 1992-05-21 1.0 -1.0 1.0 <p>Visualizar las se\u00f1ales de trading preparadas:</p> In\u00a0[31]: Copied! <pre>plt.rcParams['figure.figsize'] = [15, 5]\nfig, ax = plt.subplots(ncols=1, nrows=3)\n\nax[0].plot(cross_mav_ibm_signal_data['SIGNAL_15'], lw=1.0, color='C2', label='SMAV 16 (red)')\nax[1].plot(cross_mav_ibm_signal_data['SIGNAL_60'], lw=1.0, color='C1', label='SMAV 60 (green)')\nax[2].plot(cross_mav_ibm_signal_data['SIGNAL_BASE'], lw=1.0, color='C3', label='BASE (purple)')\n\n# set axis labels\nplt.xlabel('[time]', fontsize=10)\nax[0].set_ylabel('[smav 15 signal]', fontsize=10)\nax[1].set_ylabel('[smav 60 signal]', fontsize=10)\nax[2].set_ylabel('[base signal]', fontsize=10)\n\n\n# set plot title\nax[0].set_title('International Business Machines Corporation (IBM) - 15 days Crossover Moving Average Trading Signals', fontsize=10)\nax[1].set_title('International Business Machines Corporation (IBM) - 60 days Crossover Moving Average Trading Signals', fontsize=10)\nax[2].set_title('International Business Machines Corporation (IBM) - Baseline Buy and Hold Trading Signals', fontsize=10)\n\n# reset plot layout\nplt.tight_layout()\n</pre> plt.rcParams['figure.figsize'] = [15, 5] fig, ax = plt.subplots(ncols=1, nrows=3)  ax[0].plot(cross_mav_ibm_signal_data['SIGNAL_15'], lw=1.0, color='C2', label='SMAV 16 (red)') ax[1].plot(cross_mav_ibm_signal_data['SIGNAL_60'], lw=1.0, color='C1', label='SMAV 60 (green)') ax[2].plot(cross_mav_ibm_signal_data['SIGNAL_BASE'], lw=1.0, color='C3', label='BASE (purple)')  # set axis labels plt.xlabel('[time]', fontsize=10) ax[0].set_ylabel('[smav 15 signal]', fontsize=10) ax[1].set_ylabel('[smav 60 signal]', fontsize=10) ax[2].set_ylabel('[base signal]', fontsize=10)   # set plot title ax[0].set_title('International Business Machines Corporation (IBM) - 15 days Crossover Moving Average Trading Signals', fontsize=10) ax[1].set_title('International Business Machines Corporation (IBM) - 60 days Crossover Moving Average Trading Signals', fontsize=10) ax[2].set_title('International Business Machines Corporation (IBM) - Baseline Buy and Hold Trading Signals', fontsize=10)  # reset plot layout plt.tight_layout() <p>Vamos a determinar el n\u00famero total de cambios de se\u00f1al larga-corta de las distintas estrategias de trading:</p> In\u00a0[32]: Copied! <pre># cambios de se\u00f1al de la estrategia de trading de cruce de medias m\u00f3viles de 15-200 d\u00edas\nlen(list(it.groupby(cross_mav_ibm_signal_data['SIGNAL_15'], lambda x: x &gt; 0)))\n</pre> # cambios de se\u00f1al de la estrategia de trading de cruce de medias m\u00f3viles de 15-200 d\u00edas len(list(it.groupby(cross_mav_ibm_signal_data['SIGNAL_15'], lambda x: x &gt; 0))) Out[32]: <pre>64</pre> In\u00a0[33]: Copied! <pre># cambios de se\u00f1al de la estrategia de trading de cruce de medias m\u00f3viles de 60-200 d\u00edas\nlen(list(it.groupby(cross_mav_ibm_signal_data['SIGNAL_60'], lambda x: x &gt; 0)))\n</pre> # cambios de se\u00f1al de la estrategia de trading de cruce de medias m\u00f3viles de 60-200 d\u00edas len(list(it.groupby(cross_mav_ibm_signal_data['SIGNAL_60'], lambda x: x &gt; 0))) Out[33]: <pre>44</pre> In\u00a0[34]: Copied! <pre># cambios de se\u00f1al de la estrategia de trading de comprar y mantener como referencia\nlen(list(it.groupby(cross_mav_ibm_signal_data['SIGNAL_BASE'], lambda x: x &gt; 0)))\n</pre> # cambios de se\u00f1al de la estrategia de trading de comprar y mantener como referencia len(list(it.groupby(cross_mav_ibm_signal_data['SIGNAL_BASE'], lambda x: x &gt; 0))) Out[34]: <pre>1</pre> In\u00a0[36]: Copied! <pre># extraer los precios de cierre de las acciones de IBM\nibm_market_data = pd.DataFrame(ibm_data['Adj Close'], columns=['Adj Close'])\n\n# renombrar la columna 'close' a 'ibm' (ya que esta es la columna que queremos asignar el Backtest)\nibm_market_data = ibm_market_data.rename(columns={'Adj Close': 'IBM'})\n\n# convertir el \u00edndice del DataFrame de Pandas al tipo de datos: datetime\nibm_market_data = ibm_market_data.set_index(pd.to_datetime(ibm_data.index))\n</pre> # extraer los precios de cierre de las acciones de IBM ibm_market_data = pd.DataFrame(ibm_data['Adj Close'], columns=['Adj Close'])  # renombrar la columna 'close' a 'ibm' (ya que esta es la columna que queremos asignar el Backtest) ibm_market_data = ibm_market_data.rename(columns={'Adj Close': 'IBM'})  # convertir el \u00edndice del DataFrame de Pandas al tipo de datos: datetime ibm_market_data = ibm_market_data.set_index(pd.to_datetime(ibm_data.index)) <p>Inspeccionar las primeras filas de los datos de mercado preparados:</p> In\u00a0[37]: Copied! <pre>ibm_market_data.head()\n</pre> ibm_market_data.head() Out[37]: IBM Date 1990-12-31 12.125891 1991-01-02 12.031993 1991-01-03 12.072238 1991-01-04 12.031993 1991-01-07 11.830786 <p>Implementar una estrategia de trading simple de medias m\u00f3viles mediante la interfaz de la clase Algo de Python <code>bt</code>:</p> In\u00a0[38]: Copied! <pre>class MovingAverageStrategy(bt.Algo):\n\n    # inits the strategy\n    def __init__(self, signals):\n\n        # set class signals\n        self.signals = signals\n\n    # calss the trading strategy\n    def __call__(self, target):\n\n        # case: current timestep in signals\n        if target.now in self.signals.index[1:]:\n\n            # get actual signal\n            signal = self.signals[target.now]\n\n            # set target weights according to signal\n            target.temp['weights'] = dict(IBM=signal)\n\n        # return 'True' since we want to move on to the next timestep\n        return True\n</pre> class MovingAverageStrategy(bt.Algo):      # inits the strategy     def __init__(self, signals):          # set class signals         self.signals = signals      # calss the trading strategy     def __call__(self, target):          # case: current timestep in signals         if target.now in self.signals.index[1:]:              # get actual signal             signal = self.signals[target.now]              # set target weights according to signal             target.temp['weights'] = dict(IBM=signal)          # return 'True' since we want to move on to the next timestep         return True <p>Definir el stack de algoritmos para el backtest de la estrategia de trading de medias m\u00f3viles.</p> <p>Nota: En la biblioteca Python <code>bt</code>, una estrategia de trading generalmente consiste en una stack de algoritmos. Para cada paso de tiempo de nuestro marco del backtest, la biblioteca <code>bt</code> ejecuta todos los algoritmos en orden secuencial. Cada estrategia de medias m\u00f3viles que pretendemos dise\u00f1ar y probar consiste en total de tres algoritmos, brevemente descritos a continuaci\u00f3n:</p> <ol> <li><code>bt.algos.SelectAll()</code>: Selecciona todas las acciones disponibles para trading excepto los precios de las acciones que corresponden a NaN o 0.00.</li> <li><code>MovingAverageStrategy()</code>: Asigna la se\u00f1al calculada en t\u00e9rminos de un valor de peso a las acciones de IBM.</li> <li><code>bt.algos.Rebalance()</code>: Rebalancea el capital disponible en funci\u00f3n de los pesos asignados a cada acci\u00f3n.</li> </ol> <p>Definir la stack de algoritmos para el backtest de la estrategia de trading de medias m\u00f3viles a largo y corto plazo:</p> In\u00a0[39]: Copied! <pre>cross_mav_ibm_strategy_15 = bt.Strategy(name='smav_15', algos=[bt.algos.SelectAll(), MovingAverageStrategy(cross_mav_ibm_signal_data['SIGNAL_15']), bt.algos.Rebalance()])\ncross_mav_ibm_strategy_60 = bt.Strategy(name='smav_60', algos=[bt.algos.SelectAll(), MovingAverageStrategy(cross_mav_ibm_signal_data['SIGNAL_60']), bt.algos.Rebalance()])\ncross_mav_ibm_strategy_base = bt.Strategy(name='base', algos=[bt.algos.SelectAll(), MovingAverageStrategy(cross_mav_ibm_signal_data['SIGNAL_BASE']), bt.algos.Rebalance()])\n</pre> cross_mav_ibm_strategy_15 = bt.Strategy(name='smav_15', algos=[bt.algos.SelectAll(), MovingAverageStrategy(cross_mav_ibm_signal_data['SIGNAL_15']), bt.algos.Rebalance()]) cross_mav_ibm_strategy_60 = bt.Strategy(name='smav_60', algos=[bt.algos.SelectAll(), MovingAverageStrategy(cross_mav_ibm_signal_data['SIGNAL_60']), bt.algos.Rebalance()]) cross_mav_ibm_strategy_base = bt.Strategy(name='base', algos=[bt.algos.SelectAll(), MovingAverageStrategy(cross_mav_ibm_signal_data['SIGNAL_BASE']), bt.algos.Rebalance()]) <p>Ahora definamos las comisiones de trading ('tarifas') utilizadas en cada paso de rebalanceo de un backtest. Para lograr esto, la biblioteca <code>bt</code> toma una funci\u00f3n que espera los siguientes dos par\u00e1metros como entrada:</p> <ul> <li>la 'cantidad', denominada <code>q</code>, de activos rebalanceados en un paso de tiempo de prueba retrospectiva;</li> <li>el 'precio', denominado <code>p</code>, de los activos rebalanceados en un paso de tiempo de prueba retrospectiva.</li> </ul> <p>Implementemos una funci\u00f3n invocable de este tipo que defina una tarifa de trading del 1% (0.01) por cantidad de activo rebalanceado (o una tarifa plana de USD 5.00 por operaci\u00f3n):</p> In\u00a0[40]: Copied! <pre># init trading fees function\ndef trading_fees_function(q, p):\n\n    # calcluate trading fees (rebalanced-quantity * trading-fee)\n    fees = q * 0.01 # non-flat fee of 1% per quantity of rebalanced asset\n    # fees = 5.00 # flat fee of USD 5.00 per trade\n\n    # return the total trading fees\n    return fees\n</pre> # init trading fees function def trading_fees_function(q, p):      # calcluate trading fees (rebalanced-quantity * trading-fee)     fees = q * 0.01 # non-flat fee of 1% per quantity of rebalanced asset     # fees = 5.00 # flat fee of USD 5.00 per trade      # return the total trading fees     return fees <p>Una vez completada la definici\u00f3n de las estrategias de medias m\u00f3viles a largo y corto plazo, procedamos a iniciar los backtest correspondientes utilizando (1) ambas estrategias, as\u00ed como (2) los datos de mercado que queremos evaluar durante backtestel backtest:</p> In\u00a0[41]: Copied! <pre>ibm_backtest_cross_mav_15 = bt.Backtest(strategy=cross_mav_ibm_strategy_15, data=ibm_market_data, name='ibm_backtest_smav_15', commissions=trading_fees_function, progress_bar=True)\nibm_backtest_cross_mav_60 = bt.Backtest(strategy=cross_mav_ibm_strategy_60, data=ibm_market_data, name='ibm_backtest_smav_60', commissions=trading_fees_function, progress_bar=True)\nibm_backtest_cross_mav_base = bt.Backtest(strategy=cross_mav_ibm_strategy_base, data=ibm_market_data, name='ibm_backtest_smav_base', commissions=trading_fees_function, progress_bar=True)\n</pre> ibm_backtest_cross_mav_15 = bt.Backtest(strategy=cross_mav_ibm_strategy_15, data=ibm_market_data, name='ibm_backtest_smav_15', commissions=trading_fees_function, progress_bar=True) ibm_backtest_cross_mav_60 = bt.Backtest(strategy=cross_mav_ibm_strategy_60, data=ibm_market_data, name='ibm_backtest_smav_60', commissions=trading_fees_function, progress_bar=True) ibm_backtest_cross_mav_base = bt.Backtest(strategy=cross_mav_ibm_strategy_base, data=ibm_market_data, name='ibm_backtest_smav_base', commissions=trading_fees_function, progress_bar=True) <p>Ahora, ejecutemos el backtest de ambas configuraciones de la estrategia de cruce de medias m\u00f3viles, as\u00ed como la referencia definida:</p> In\u00a0[42]: Copied! <pre>backtest_results_ibm = bt.run(ibm_backtest_cross_mav_15, ibm_backtest_cross_mav_60, ibm_backtest_cross_mav_base)\n</pre> backtest_results_ibm = bt.run(ibm_backtest_cross_mav_15, ibm_backtest_cross_mav_60, ibm_backtest_cross_mav_base) <pre>ibm_backtest_smav_15\n0% [############################# ] 100% | ETA: 00:00:00ibm_backtest_smav_60\n0% [############################# ] 100% | ETA: 00:00:00ibm_backtest_smav_base\n0% [############################# ] 100% | ETA: 00:00:00</pre> <p>Inspeccionar los resultados individuales del backtest y las medidas de rendimiento:</p> In\u00a0[43]: Copied! <pre>backtest_results_ibm.display()\n</pre> backtest_results_ibm.display() <pre>Stat                 ibm_backtest_smav_15    ibm_backtest_smav_60    ibm_backtest_smav_base\n-------------------  ----------------------  ----------------------  ------------------------\nStart                1990-12-30              1990-12-30              1990-12-30\nEnd                  2017-12-29              2017-12-29              2017-12-29\nRisk-free rate       0.00%                   0.00%                   0.00%\n\nTotal Return         35.24%                  -83.46%                 804.82%\nDaily Sharpe         0.18                    -0.10                   0.43\nDaily Sortino        0.28                    -0.16                   0.73\nCAGR                 1.12%                   -6.45%                  8.50%\nMax Drawdown         -86.02%                 -94.10%                 -66.83%\nCalmar Ratio         0.01                    -0.07                   0.13\n\nMTD                  0.12%                   1.45%                   -0.36%\n3m                   -7.55%                  -6.32%                  6.81%\n6m                   -3.19%                  -1.90%                  1.60%\nYTD                  -5.15%                  -9.75%                  -3.99%\n1Y                   -5.50%                  -10.08%                 -4.34%\n3Y (ann.)            -2.51%                  -6.21%                  2.10%\n5Y (ann.)            -4.39%                  -9.23%                  -1.42%\n10Y (ann.)           0.61%                   -5.48%                  6.08%\nSince Incep. (ann.)  1.12%                   -6.45%                  8.50%\n\nDaily Sharpe         0.18                    -0.10                   0.43\nDaily Sortino        0.28                    -0.16                   0.73\nDaily Mean (ann.)    4.92%                   -2.85%                  12.02%\nDaily Vol (ann.)     27.54%                  27.54%                  27.81%\nDaily Skew           -0.18                   -0.40                   0.22\nDaily Kurt           7.75                    7.73                    7.47\nBest Day             15.55%                  13.16%                  13.16%\nWorst Day            -14.95%                 -15.54%                 -15.54%\n\nMonthly Sharpe       0.18                    -0.11                   0.44\nMonthly Sortino      0.31                    -0.17                   0.82\nMonthly Mean (ann.)  4.75%                   -2.88%                  11.74%\nMonthly Vol (ann.)   26.93%                  27.22%                  26.89%\nMonthly Skew         0.09                    -0.03                   0.32\nMonthly Kurt         2.23                    2.14                    2.17\nBest Month           31.57%                  31.57%                  35.38%\nWorst Month          -29.66%                 -29.66%                 -26.19%\n\nYearly Sharpe        0.16                    -0.10                   0.42\nYearly Sortino       0.32                    -0.15                   1.08\nYearly Mean          3.72%                   -2.65%                  12.32%\nYearly Vol           23.57%                  25.59%                  29.61%\nYearly Skew          0.56                    -0.43                   0.33\nYearly Kurt          1.70                    -0.18                   -0.16\nBest Year            69.95%                  39.31%                  77.47%\nWorst Year           -45.02%                 -59.76%                 -40.04%\n\nAvg. Drawdown        -6.69%                  -10.58%                 -5.67%\nAvg. Drawdown Days   137.44                  286.82                  91.64\nAvg. Up Month        6.06%                   5.64%                   6.33%\nAvg. Down Month      -5.19%                  -5.70%                  -5.15%\nWin Year %           55.56%                  51.85%                  62.96%\nWin 12m %            53.50%                  44.27%                  63.06%\n</pre> <p>Recopilar el rendimiento detallado del backtest por paso de tiempo de la estrategia de cruce de medias m\u00f3viles \"r\u00e1pida\" de 15-200 d\u00edas:</p> In\u00a0[44]: Copied! <pre>backtest_sma_15_ibm_details = ibm_backtest_cross_mav_15.strategy.prices.to_frame(name='Rel. EQUITY')\nbacktest_sma_15_ibm_details['Abs. EQUITY'] = ibm_backtest_cross_mav_15.strategy.values # equity per timestep\nbacktest_sma_15_ibm_details['CASH'] = ibm_backtest_cross_mav_15.strategy.cash # cash per timestep\nbacktest_sma_15_ibm_details['POSITIONS'] = ibm_backtest_cross_mav_15.strategy.positions # positions per timestep\nbacktest_sma_15_ibm_details['FEES'] = ibm_backtest_cross_mav_15.strategy.fees # trading fees per timestep\n</pre> backtest_sma_15_ibm_details = ibm_backtest_cross_mav_15.strategy.prices.to_frame(name='Rel. EQUITY') backtest_sma_15_ibm_details['Abs. EQUITY'] = ibm_backtest_cross_mav_15.strategy.values # equity per timestep backtest_sma_15_ibm_details['CASH'] = ibm_backtest_cross_mav_15.strategy.cash # cash per timestep backtest_sma_15_ibm_details['POSITIONS'] = ibm_backtest_cross_mav_15.strategy.positions # positions per timestep backtest_sma_15_ibm_details['FEES'] = ibm_backtest_cross_mav_15.strategy.fees # trading fees per timestep <p>Inspeccionar los resultados detallados del backtest por paso de tiempo:</p> In\u00a0[45]: Copied! <pre>backtest_sma_15_ibm_details.head()\n</pre> backtest_sma_15_ibm_details.head() Out[45]: Rel. EQUITY Abs. EQUITY CASH POSITIONS FEES 1990-12-30 100.0 1000000.0 1000000.0 0.0 0.0 1990-12-31 100.0 1000000.0 1000000.0 0.0 0.0 1991-01-02 100.0 1000000.0 1000000.0 0.0 0.0 1991-01-03 100.0 1000000.0 1000000.0 0.0 0.0 1991-01-04 100.0 1000000.0 1000000.0 0.0 0.0 <p>Visualizar los rendimientos mensuales obtenidos por la estrategia de cruce de medias m\u00f3viles de 15-200 d\u00edas:</p> In\u00a0[46]: Copied! <pre>plt.rcParams['figure.figsize'] = [15, 10]\nfig = plt.figure()\nax = fig.add_subplot(111)\n\n# plot heatmap of monthly returns generated by the strategy\nax = sns.heatmap(ibm_backtest_cross_mav_15.stats.return_table, annot=True, cbar=True, vmin=-0.5, vmax=0.5)\n\n# set axis labels\nax.set_xlabel('[month]', fontsize=10)\nax.set_ylabel('[year]', fontsize=10)\n\n# set plot title\nax.set_title('International Business Machines Corporation (IBM) - Monthly Returns SMAV 15-200 Strategy', fontsize=10);\n</pre> plt.rcParams['figure.figsize'] = [15, 10] fig = plt.figure() ax = fig.add_subplot(111)  # plot heatmap of monthly returns generated by the strategy ax = sns.heatmap(ibm_backtest_cross_mav_15.stats.return_table, annot=True, cbar=True, vmin=-0.5, vmax=0.5)  # set axis labels ax.set_xlabel('[month]', fontsize=10) ax.set_ylabel('[year]', fontsize=10)  # set plot title ax.set_title('International Business Machines Corporation (IBM) - Monthly Returns SMAV 15-200 Strategy', fontsize=10); <p>Recopilar el rendimiento detallado del backtest por paso de tiempo de la estrategia de cruce de medias m\u00f3viles \"lenta\" de 60-200 d\u00edas:</p> In\u00a0[47]: Copied! <pre>backtest_sma_60_ibm_details = ibm_backtest_cross_mav_60.strategy.prices.to_frame(name='Rel. EQUITY')\nbacktest_sma_60_ibm_details['Abs. EQUITY'] = ibm_backtest_cross_mav_60.strategy.values # equity per timestep\nbacktest_sma_60_ibm_details['CASH'] = ibm_backtest_cross_mav_60.strategy.cash # cash per timestep\nbacktest_sma_60_ibm_details['POSITIONS'] = ibm_backtest_cross_mav_60.strategy.positions # positions per timestep\nbacktest_sma_60_ibm_details['FEES'] = ibm_backtest_cross_mav_60.strategy.fees # fees per timestep\n</pre> backtest_sma_60_ibm_details = ibm_backtest_cross_mav_60.strategy.prices.to_frame(name='Rel. EQUITY') backtest_sma_60_ibm_details['Abs. EQUITY'] = ibm_backtest_cross_mav_60.strategy.values # equity per timestep backtest_sma_60_ibm_details['CASH'] = ibm_backtest_cross_mav_60.strategy.cash # cash per timestep backtest_sma_60_ibm_details['POSITIONS'] = ibm_backtest_cross_mav_60.strategy.positions # positions per timestep backtest_sma_60_ibm_details['FEES'] = ibm_backtest_cross_mav_60.strategy.fees # fees per timestep <p>Inspeccionar los resultados detallados de la prueba retrospectiva por paso de tiempo:</p> In\u00a0[48]: Copied! <pre>backtest_sma_60_ibm_details.head()\n</pre> backtest_sma_60_ibm_details.head() Out[48]: Rel. EQUITY Abs. EQUITY CASH POSITIONS FEES 1990-12-30 100.0 1000000.0 1000000.0 0.0 0.0 1990-12-31 100.0 1000000.0 1000000.0 0.0 0.0 1991-01-02 100.0 1000000.0 1000000.0 0.0 0.0 1991-01-03 100.0 1000000.0 1000000.0 0.0 0.0 1991-01-04 100.0 1000000.0 1000000.0 0.0 0.0 <p>Visualizar los rendimientos mensuales obtenidos por la estrategia de cruce de medias m\u00f3viles de 60-200 d\u00edas:</p> In\u00a0[49]: Copied! <pre>plt.rcParams['figure.figsize'] = [15, 10]\nfig = plt.figure()\nax = fig.add_subplot(111)\n\n# plot heatmap of monthly returns generated by the strategy\nax = sns.heatmap(ibm_backtest_cross_mav_60.stats.return_table, annot=True, cbar=True, vmin=-0.5, vmax=0.5)\n\n# set axis labels\nax.set_xlabel('[month]', fontsize=10)\nax.set_ylabel('[year]', fontsize=10)\n\n# set plot title\nax.set_title('International Business Machines Corporation (IBM) - Monthly Returns SMAV 60-200 Strategy', fontsize=10);\n</pre> plt.rcParams['figure.figsize'] = [15, 10] fig = plt.figure() ax = fig.add_subplot(111)  # plot heatmap of monthly returns generated by the strategy ax = sns.heatmap(ibm_backtest_cross_mav_60.stats.return_table, annot=True, cbar=True, vmin=-0.5, vmax=0.5)  # set axis labels ax.set_xlabel('[month]', fontsize=10) ax.set_ylabel('[year]', fontsize=10)  # set plot title ax.set_title('International Business Machines Corporation (IBM) - Monthly Returns SMAV 60-200 Strategy', fontsize=10); <p>Recopilar el rendimiento detalladodel backtest por paso de tiempo de la estrategia de referencia \"solo largo\":</p> In\u00a0[51]: Copied! <pre>backtest_sma_base_ibm_details = ibm_backtest_cross_mav_base.strategy.prices.to_frame(name='Rel. EQUITY')\nbacktest_sma_base_ibm_details['Abs. EQUITY'] = ibm_backtest_cross_mav_base.strategy.values # equity per timestep\nbacktest_sma_base_ibm_details['CASH'] = ibm_backtest_cross_mav_base.strategy.cash # cash per timestep\nbacktest_sma_base_ibm_details['POSITIONS'] = ibm_backtest_cross_mav_base.strategy.positions # positions per timestep\nbacktest_sma_base_ibm_details['FEES'] = ibm_backtest_cross_mav_base.strategy.fees # fees per timestep\n</pre> backtest_sma_base_ibm_details = ibm_backtest_cross_mav_base.strategy.prices.to_frame(name='Rel. EQUITY') backtest_sma_base_ibm_details['Abs. EQUITY'] = ibm_backtest_cross_mav_base.strategy.values # equity per timestep backtest_sma_base_ibm_details['CASH'] = ibm_backtest_cross_mav_base.strategy.cash # cash per timestep backtest_sma_base_ibm_details['POSITIONS'] = ibm_backtest_cross_mav_base.strategy.positions # positions per timestep backtest_sma_base_ibm_details['FEES'] = ibm_backtest_cross_mav_base.strategy.fees # fees per timestep <p>Inspeccionar los resultados detallados del backtest por paso de tiempo:</p> In\u00a0[52]: Copied! <pre>backtest_sma_base_ibm_details.head()\n</pre> backtest_sma_base_ibm_details.head() Out[52]: Rel. EQUITY Abs. EQUITY CASH POSITIONS FEES 1990-12-30 100.000000 1.000000e+06 1000000.000000 0.0 0.00 1990-12-31 100.000000 1.000000e+06 1000000.000000 0.0 0.00 1991-01-02 99.916958 9.991696e+05 8.824577 83042.0 830.42 1991-01-03 100.251161 1.002512e+06 8.824577 83042.0 0.00 1991-01-04 99.916958 9.991696e+05 8.824577 83042.0 0.00 In\u00a0[53]: Copied! <pre>plt.rcParams['figure.figsize'] = [15, 10]\nfig = plt.figure()\nax = fig.add_subplot(111)\n\n# plot heatmap of monthly returns generated by the strategy\nax = sns.heatmap(ibm_backtest_cross_mav_base.stats.return_table, annot=True, cbar=True, vmin=-0.5, vmax=0.5)\n\n# set axis labels\nax.set_xlabel('[month]', fontsize=10)\nax.set_ylabel('[year]', fontsize=10)\n\n# set plot title\nax.set_title('International Business Machines Corporation (IBM) - Monthly Returns SMAV Baseline', fontsize=10);\n</pre> plt.rcParams['figure.figsize'] = [15, 10] fig = plt.figure() ax = fig.add_subplot(111)  # plot heatmap of monthly returns generated by the strategy ax = sns.heatmap(ibm_backtest_cross_mav_base.stats.return_table, annot=True, cbar=True, vmin=-0.5, vmax=0.5)  # set axis labels ax.set_xlabel('[month]', fontsize=10) ax.set_ylabel('[year]', fontsize=10)  # set plot title ax.set_title('International Business Machines Corporation (IBM) - Monthly Returns SMAV Baseline', fontsize=10); <p>Visualizar la progresi\u00f3n de la equidad de cada estrategia en el backtest a lo largo del tiempo:</p> In\u00a0[55]: Copied! <pre>plt.rcParams['figure.figsize'] = [15, 5]\nfig = plt.figure()\nax = fig.add_subplot(111)\n\n# plot equity progression of the distinct trading strategies\nax.plot(backtest_sma_15_ibm_details['Rel. EQUITY'], color='C2',lw=1.0, label='15-day SMAV strategy (red)')\nax.plot(backtest_sma_60_ibm_details['Rel. EQUITY'], color='C1',lw=1.0, label='60-day SMAV strategy (green)')\nax.plot(backtest_sma_base_ibm_details['Rel. EQUITY'], color='C3',lw=1.0, label='Base SMAV strategy (purple)')\n\n# rotate x-tick labels\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(45)\n\n# set axis labels\nax.set_xlabel('[time]', fontsize=10)\nax.set_xlim([start_date, end_date])\nax.set_ylabel('[rel. equity]', fontsize=10)\n\n# set plot legend\nplt.legend(loc=\"upper left\", numpoints=1, fancybox=True)\n\n# set plot title\nplt.title('International Business Machines Corporation (IBM) - Backtest % Equity Progression', fontsize=10);\n</pre> plt.rcParams['figure.figsize'] = [15, 5] fig = plt.figure() ax = fig.add_subplot(111)  # plot equity progression of the distinct trading strategies ax.plot(backtest_sma_15_ibm_details['Rel. EQUITY'], color='C2',lw=1.0, label='15-day SMAV strategy (red)') ax.plot(backtest_sma_60_ibm_details['Rel. EQUITY'], color='C1',lw=1.0, label='60-day SMAV strategy (green)') ax.plot(backtest_sma_base_ibm_details['Rel. EQUITY'], color='C3',lw=1.0, label='Base SMAV strategy (purple)')  # rotate x-tick labels for tick in ax.get_xticklabels():     tick.set_rotation(45)  # set axis labels ax.set_xlabel('[time]', fontsize=10) ax.set_xlim([start_date, end_date]) ax.set_ylabel('[rel. equity]', fontsize=10)  # set plot legend plt.legend(loc=\"upper left\", numpoints=1, fancybox=True)  # set plot title plt.title('International Business Machines Corporation (IBM) - Backtest % Equity Progression', fontsize=10); <p>1. Evaluaci\u00f3n de distintos par\u00e1metros de medias m\u00f3viles diarias.</p> <p>Evaluar la estrategia de trading simple de cruce de medias m\u00f3viles utilizando distintas ventanas de observaci\u00f3n de medias m\u00f3viles, por ejemplo, 10 d\u00edas, 30 d\u00edas, 50 d\u00edas, 200 d\u00edas y 300 d\u00edas. Comparar el rendimiento de las parametrizaciones de ventana de observaci\u00f3n en t\u00e9rminos de rendimiento total, progresi\u00f3n de la equidad y ratio de Sharpe anual. Obtener una intuici\u00f3n sobre los a\u00f1os en los que la estrategia no funcion\u00f3 bien y la posible raz\u00f3n de su bajo rendimiento.</p> In\u00a0[61]: Copied! <pre># Calcular medias m\u00f3viles diarias\nibm_market_data['MA_10'] = ibm_market_data['IBM'].rolling(window=10).mean()\nibm_market_data['MA_30'] = ibm_market_data['IBM'].rolling(window=30).mean()\nibm_market_data['MA_50'] = ibm_market_data['IBM'].rolling(window=50).mean()\nibm_market_data['MA_200'] = ibm_market_data['IBM'].rolling(window=200).mean()\nibm_market_data['MA_300'] = ibm_market_data['IBM'].rolling(window=300).mean()\n</pre> # Calcular medias m\u00f3viles diarias ibm_market_data['MA_10'] = ibm_market_data['IBM'].rolling(window=10).mean() ibm_market_data['MA_30'] = ibm_market_data['IBM'].rolling(window=30).mean() ibm_market_data['MA_50'] = ibm_market_data['IBM'].rolling(window=50).mean() ibm_market_data['MA_200'] = ibm_market_data['IBM'].rolling(window=200).mean() ibm_market_data['MA_300'] = ibm_market_data['IBM'].rolling(window=300).mean() In\u00a0[63]: Copied! <pre># Implementar la estrategia de cruce de medias m\u00f3viles\ndef moving_average_crossover_strategy(data):\n    signals = pd.DataFrame(index=data.index)\n    signals['Signal'] = 0.0\n    signals['MA_10'] = data['MA_10']\n    signals['MA_200'] = data['MA_200']\n    signals['Signal'][10:] = np.where(data['MA_10'][10:] &gt; data['MA_200'][10:], 1.0, 0.0)   \n    signals['Positions'] = signals['Signal'].diff()\n    return signals\n\n# Aplicar la estrategia de cruce de medias m\u00f3viles a cada ventana de observaci\u00f3n\nsignals_10_200 = moving_average_crossover_strategy(ibm_market_data[['MA_10', 'MA_200']])\nsignals_30_200 = moving_average_crossover_strategy(ibm_market_data[['MA_30', 'MA_200']])\nsignals_50_200 = moving_average_crossover_strategy(ibm_market_data[['MA_50', 'MA_200']])\nsignals_300_200 = moving_average_crossover_strategy(ibm_market_data[['MA_300', 'MA_200']])\n\n# Calcular el rendimiento total para cada estrategia\ndef calculate_total_return(signals, data):\n    positions = signals['Positions']\n    returns = data['Adj Close'].pct_change()\n    total_return = (positions.shift(-1) * returns).sum()\n    return total_return\n\ntotal_return_10_200 = calculate_total_return(signals_10_200, ibm_market_data)\ntotal_return_30_200 = calculate_total_return(signals_30_200, ibm_market_data)\ntotal_return_50_200 = calculate_total_return(signals_50_200, ibm_market_data)\ntotal_return_300_200 = calculate_total_return(signals_300_200, ibm_market_data)\n\n# Visualizar los resultados\nplt.figure(figsize=(10, 6))\nplt.plot(ibm_market_data['Date'], ibm_market_data['Adj Close'], label='Precio de Cierre Ajustado', color='black')\nplt.plot(ibm_market_data['Date'], ibm_market_data['MA_10'], label='MA 10', color='red')\nplt.plot(ibm_market_data['Date'], ibm_market_data['MA_200'], label='MA 200', color='blue')\nplt.scatter(signals_10_200.index, ibm_market_data['MA_10'][signals_10_200['Positions'] == 1.0], marker='^', color='g', label='Compra')\nplt.scatter(signals_10_200.index, ibm_market_data['MA_200'][signals_10_200['Positions'] == -1.0], marker='v', color='r', label='Venta')\nplt.xlabel('Fecha')\nplt.ylabel('Precio')\nplt.title('Precio de Cierre Ajustado y Medias M\u00f3viles')\nplt.legend()\nplt.show()\n\nprint(\"Rendimiento total para MA 10-200:\", total_return_10_200)\nprint(\"Rendimiento total para MA 30-200:\", total_return_30_200)\nprint(\"Rendimiento total para MA 50-200:\", total_return_50_200)\nprint(\"Rendimiento total para MA 300-200:\", total_return_300_200)\n</pre> # Implementar la estrategia de cruce de medias m\u00f3viles def moving_average_crossover_strategy(data):     signals = pd.DataFrame(index=data.index)     signals['Signal'] = 0.0     signals['MA_10'] = data['MA_10']     signals['MA_200'] = data['MA_200']     signals['Signal'][10:] = np.where(data['MA_10'][10:] &gt; data['MA_200'][10:], 1.0, 0.0)        signals['Positions'] = signals['Signal'].diff()     return signals  # Aplicar la estrategia de cruce de medias m\u00f3viles a cada ventana de observaci\u00f3n signals_10_200 = moving_average_crossover_strategy(ibm_market_data[['MA_10', 'MA_200']]) signals_30_200 = moving_average_crossover_strategy(ibm_market_data[['MA_30', 'MA_200']]) signals_50_200 = moving_average_crossover_strategy(ibm_market_data[['MA_50', 'MA_200']]) signals_300_200 = moving_average_crossover_strategy(ibm_market_data[['MA_300', 'MA_200']])  # Calcular el rendimiento total para cada estrategia def calculate_total_return(signals, data):     positions = signals['Positions']     returns = data['Adj Close'].pct_change()     total_return = (positions.shift(-1) * returns).sum()     return total_return  total_return_10_200 = calculate_total_return(signals_10_200, ibm_market_data) total_return_30_200 = calculate_total_return(signals_30_200, ibm_market_data) total_return_50_200 = calculate_total_return(signals_50_200, ibm_market_data) total_return_300_200 = calculate_total_return(signals_300_200, ibm_market_data)  # Visualizar los resultados plt.figure(figsize=(10, 6)) plt.plot(ibm_market_data['Date'], ibm_market_data['Adj Close'], label='Precio de Cierre Ajustado', color='black') plt.plot(ibm_market_data['Date'], ibm_market_data['MA_10'], label='MA 10', color='red') plt.plot(ibm_market_data['Date'], ibm_market_data['MA_200'], label='MA 200', color='blue') plt.scatter(signals_10_200.index, ibm_market_data['MA_10'][signals_10_200['Positions'] == 1.0], marker='^', color='g', label='Compra') plt.scatter(signals_10_200.index, ibm_market_data['MA_200'][signals_10_200['Positions'] == -1.0], marker='v', color='r', label='Venta') plt.xlabel('Fecha') plt.ylabel('Precio') plt.title('Precio de Cierre Ajustado y Medias M\u00f3viles') plt.legend() plt.show()  print(\"Rendimiento total para MA 10-200:\", total_return_10_200) print(\"Rendimiento total para MA 30-200:\", total_return_30_200) print(\"Rendimiento total para MA 50-200:\", total_return_50_200) print(\"Rendimiento total para MA 300-200:\", total_return_300_200) <pre>\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nFile ~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\db-connectors-RsEieBu8-py3.8\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802, in Index.get_loc(self, key, method, tolerance)\n   3801 try:\n-&gt; 3802     return self._engine.get_loc(casted_key)\n   3803 except KeyError as err:\n\nFile ~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\db-connectors-RsEieBu8-py3.8\\lib\\site-packages\\pandas\\_libs\\index.pyx:138, in pandas._libs.index.IndexEngine.get_loc()\n\nFile ~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\db-connectors-RsEieBu8-py3.8\\lib\\site-packages\\pandas\\_libs\\index.pyx:165, in pandas._libs.index.IndexEngine.get_loc()\n\nFile pandas\\_libs\\hashtable_class_helper.pxi:5745, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\nFile pandas\\_libs\\hashtable_class_helper.pxi:5753, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\nKeyError: 'MA_10'\n\nThe above exception was the direct cause of the following exception:\n\nKeyError                                  Traceback (most recent call last)\nCell In[63], line 13\n     11 # Aplicar la estrategia de cruce de medias m\u00f3viles a cada ventana de observaci\u00f3n\n     12 signals_10_200 = moving_average_crossover_strategy(ibm_market_data[['MA_10', 'MA_200']])\n---&gt; 13 signals_30_200 = moving_average_crossover_strategy(ibm_market_data[['MA_30', 'MA_200']])\n     14 signals_50_200 = moving_average_crossover_strategy(ibm_market_data[['MA_50', 'MA_200']])\n     15 signals_300_200 = moving_average_crossover_strategy(ibm_market_data[['MA_300', 'MA_200']])\n\nCell In[63], line 5, in moving_average_crossover_strategy(data)\n      3 signals = pd.DataFrame(index=data.index)\n      4 signals['Signal'] = 0.0\n----&gt; 5 signals['MA_10'] = data['MA_10']\n      6 signals['MA_200'] = data['MA_200']\n      7 signals['Signal'][10:] = np.where(data['MA_10'][10:] &gt; data['MA_200'][10:], 1.0, 0.0)   \n\nFile ~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\db-connectors-RsEieBu8-py3.8\\lib\\site-packages\\pandas\\core\\frame.py:3807, in DataFrame.__getitem__(self, key)\n   3805 if self.columns.nlevels &gt; 1:\n   3806     return self._getitem_multilevel(key)\n-&gt; 3807 indexer = self.columns.get_loc(key)\n   3808 if is_integer(indexer):\n   3809     indexer = [indexer]\n\nFile ~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\db-connectors-RsEieBu8-py3.8\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3804, in Index.get_loc(self, key, method, tolerance)\n   3802     return self._engine.get_loc(casted_key)\n   3803 except KeyError as err:\n-&gt; 3804     raise KeyError(key) from err\n   3805 except TypeError:\n   3806     # If we have a listlike key, _check_indexing_error will raise\n   3807     #  InvalidIndexError. Otherwise we fall through and re-raise\n   3808     #  the TypeError.\n   3809     self._check_indexing_error(key)\n\nKeyError: 'MA_10'</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"lectures/trading/trading_01/#estrategias-de-trading-con-medias-moviles","title":"Estrategias de Trading con Medias M\u00f3viles\u00b6","text":""},{"location":"lectures/trading/trading_01/#introduccion","title":"Introducci\u00f3n\u00b6","text":"<p>En este laboratorio introductorio, creamos nuestro primer proceso de ciencia de datos financieros. El objetivo principal de este laboratorio es guiarlo a trav\u00e9s del proceso general de implementaci\u00f3n y evaluaci\u00f3n de una estrategia de trading simple de seguimiento de tendencias. Para lograr esto, seguiremos los pasos distintos del proceso que se detallan a continuaci\u00f3n:</p>"},{"location":"lectures/trading/trading_01/#lectura-de-los-datos-financieros","title":"Lectura de los Datos Financieros\u00b6","text":"<p>En esta secci\u00f3n, utilizaremos los datos hist\u00f3ricos diarios del mercado de valores de la corporaci\u00f3n \"International Business Machines\" (IBM) (s\u00edmbolo burs\u00e1til: \"IBM\").</p>"},{"location":"lectures/trading/trading_01/#preprocesar-los-datos-financieros","title":"Preprocesar los Datos Financieros\u00b6","text":"<p>Inspeccionar los primeros 10 registros de los datos descargados de <code>IBM</code>:</p>"},{"location":"lectures/trading/trading_01/#analisis-de-datos-implementacion-de-la-estrategia-de-cruce-de-medias-moviles","title":"An\u00e1lisis de Datos - Implementaci\u00f3n de la Estrategia de Cruce de Medias M\u00f3viles\u00b6","text":"<p>Vamos a implementar una estrategia de trading simple de Cruce de Medias M\u00f3viles. En general, el trading por cruce se refiere a la idea de que los cambios en las situaciones del mercado pueden determinarse en funci\u00f3n de las \"rupturas\" de precios. Un cruce puede interpretarse como otra medida del impulso de instrumentos financieros. En el pasado, las se\u00f1ales de cruce han sido ampliamente utilizadas para determinar si es momento de comprar o vender el activo subyacente.</p> <p>Las se\u00f1ales de cruce de precios de una estrategia de trading simple de Cruce de Medias M\u00f3viles se desencadenan por los siguientes eventos:</p> <ul> <li>Generar una se\u00f1al de venta corta una vez que el precio de un instrumento financiero caiga por debajo de la tendencia general de precios, por ejemplo, el rango de medias m\u00f3viles de 100 d\u00edas (\"Se\u00f1al de Venta por Cruce\", imagen izquierda a continuaci\u00f3n).</li> <li>Generar una se\u00f1al de compra larga una vez que el precio de un instrumento financiero supere la tendencia general de precios, por ejemplo, el rango de medias m\u00f3viles de 100 d\u00edas (\"Se\u00f1al de Compra por Cruce\", imagen derecha a continuaci\u00f3n).</li> </ul>"},{"location":"lectures/trading/trading_01/#generacion-de-senales-de-cruce-de-medias-moviles","title":"Generaci\u00f3n de Se\u00f1ales de Cruce de Medias M\u00f3viles\u00b6","text":"<p>Derivar se\u00f1ales de trading de dos configuraciones distintas de estrategias de cruce de medias m\u00f3viles. Generaremos una se\u00f1al larga (+1.0) para los intervalos de tiempo donde las medias m\u00f3viles r\u00e1pidas est\u00e9n por encima de la media m\u00f3vil de 200 d\u00edas. Adem\u00e1s, generaremos una se\u00f1al corta (-1.0) para los intervalos de tiempo donde las medias m\u00f3viles r\u00e1pidas est\u00e9n por debajo de la media m\u00f3vil de 200 d\u00edas:</p>"},{"location":"lectures/trading/trading_01/#backtest-senal-de-cruce-de-medias-moviles","title":"Backtest - Se\u00f1al de Cruce de Medias M\u00f3viles\u00b6","text":"<p>Preparar los datos de mercado para utilizar backtesting de las configuraciones de estrategia de trading de cruce de medias m\u00f3viles:</p>"},{"location":"lectures/trading/trading_01/#ejercicios","title":"Ejercicios\u00b6","text":""},{"location":"lectures/trading/trading_02/","title":"Trading 02","text":"<p>In this introductory lab, we create our first financial data science process. The main objective of this lab is to walk you through the general process of implementing and evaluating a simple mean-reversion trading strategy. To achieve this, we will follow the distinct process steps as outlined below:</p> <p>As always, pls. don't hesitate to ask all your questions either during the lab or send us an email (using our fds.ai email addresses).</p> <p>After today's lab you should be able to:</p> <ol> <li>Implement a mean-reversion trading strategy and apply it to distinct financial instruments.</li> <li>Convert the trading strategy results into trade signals to be used in backtest.</li> <li>Understand how to use the python backtesting bt library to backtest the implemented strategy.</li> <li>Interpret the backtests results using the distinct backtest performance measures.</li> </ol> <p>Before we start let's watch a motivational video:</p> In\u00a0[1]: Copied! <pre>from IPython.display import YouTubeVideo\n# Nvidia GTC 2017: \"I Am AI\" Opening in Keynote\"\n# YouTubeVideo('SUNPrR4o5ZA', width=800, height=600)\n</pre> from IPython.display import YouTubeVideo # Nvidia GTC 2017: \"I Am AI\" Opening in Keynote\" # YouTubeVideo('SUNPrR4o5ZA', width=800, height=600) <p>We need to import a couple of Python libraries that allow for data analysis and data visualization. In this lab will use the <code>Pandas</code>, <code>NumPy</code>, <code>BT</code> and the <code>Matplotlib</code> library. Let's import the libraries by the execution of the statements below:</p> In\u00a0[4]: Copied! <pre># import python utility libraries\nimport os as os\nimport datetime as dt\nimport itertools as it\n\n# import python data science libraries\nimport pandas as pd\nimport numpy as np\n\n# import the pandas financial data reader library\nimport pandas_datareader as dr\n\n# import the matplotlib and seaborn visualization library\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n</pre> # import python utility libraries import os as os import datetime as dt import itertools as it  # import python data science libraries import pandas as pd import numpy as np  # import the pandas financial data reader library import pandas_datareader as dr  # import the matplotlib and seaborn visualization library import matplotlib.pyplot as plt import seaborn as sns <p>Install the Python <code>BT</code> backtesting library:</p> <p>Upon successful installation let's import the Python <code>BT</code> backtesting library:</p> In\u00a0[5]: Copied! <pre>import bt as bt # library to backtest trading signals\n</pre> import bt as bt # library to backtest trading signals <p>Let's also set a couple of general plot parameters:</p> In\u00a0[6]: Copied! <pre>import warnings\nwarnings.filterwarnings('ignore')\n</pre> import warnings warnings.filterwarnings('ignore') <p>Create a dataset sub-folder that we will use to store the financial data downloaded:</p> In\u00a0[7]: Copied! <pre>if not os.path.exists('./datasets'): os.makedirs('./datasets')\n</pre> if not os.path.exists('./datasets'): os.makedirs('./datasets') <p>In this section of the lab notebook, we will aquire historic daily stock market data of the Euro vs. US-Dollar foreign exchange rate (ticker symbol: \"EURUSD\"). Thereby, we will utilize the <code>datareader</code> of the <code>Pandas</code> library that provides the ability to interface the <code>Yahoo</code> finance API. Let's first specify the start date and end date of the data download. We aim to download the exchange rate data starting from the 31.12.2003 until the 31.12.2017 to develop and evaluate a simple mean-reversion trading strategy:</p> In\u00a0[8]: Copied! <pre># set to start and end date of the data download\nstart_date = dt.datetime(2003, 12, 31)\nend_date = dt.datetime(2017, 12, 31)\n</pre> # set to start and end date of the data download start_date = dt.datetime(2003, 12, 31) end_date = dt.datetime(2017, 12, 31) <p>Download the daily \"Euro vs. USD\" exchange rate data of the defined timeframe using the <code>datareader</code>'s <code>Yahoo</code> finance API:</p> In\u00a0[9]: Copied! <pre># download eurusd exchange rate data\n\npath = 'data/eurusd_data_2003_2017_daily.csv'\neurusd_data =  pd.read_csv(path, sep=\",\" )#.dropna()\neurusd_data['Date'] = pd.to_datetime(eurusd_data['Date'])\neurusd_data=eurusd_data.set_index('Date')#.rename(columns = {'Adj_Close':'Adj Close'})\n#ibm_data = ibm_data.iloc[1:]\n</pre> # download eurusd exchange rate data  path = 'data/eurusd_data_2003_2017_daily.csv' eurusd_data =  pd.read_csv(path, sep=\",\" )#.dropna() eurusd_data['Date'] = pd.to_datetime(eurusd_data['Date']) eurusd_data=eurusd_data.set_index('Date')#.rename(columns = {'Adj_Close':'Adj Close'}) #ibm_data = ibm_data.iloc[1:] <p>Inspect the top 10 records of the <code>EURUSD</code> data downloaded:</p> In\u00a0[10]: Copied! <pre>eurusd_data.head(10)\n</pre> eurusd_data.head(10) Out[10]: Open High Low Close Adj Close Volume Date 2003-12-31 1.255004 1.264894 1.253007 1.259002 1.259002 0 2004-01-01 1.259002 1.260796 1.247396 1.258194 1.258194 0 2004-01-02 1.258194 1.262802 1.252693 1.258194 1.258194 0 2004-01-05 1.263903 1.269406 1.263695 1.268698 1.268698 0 2004-01-06 1.268907 1.280803 1.267202 1.272103 1.272103 0 2004-01-07 1.272394 1.273999 1.262499 1.264095 1.264095 0 2004-01-08 1.264095 1.278707 1.256502 1.277498 1.277498 0 2004-01-09 1.277498 1.286703 1.271294 1.285892 1.285892 0 2004-01-12 1.284802 1.289707 1.273804 1.274096 1.274096 0 2004-01-13 1.274405 1.279705 1.271892 1.278593 1.278593 0 <p>Visually inspect the adjusted closing price of the downloaded <code>EURUSD</code> data:</p> In\u00a0[11]: Copied! <pre>plt.rcParams['figure.figsize'] = [15, 5]\nfig = plt.figure()\nax = fig.add_subplot(111)\n\n# plot eurusd daily adjusted closing prices\nax.plot(eurusd_data.index, eurusd_data['Adj Close'], color='#9b59b6')\n\n# rotate x-ticks\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(45)\n\n# set axis labels\nax.set_xlabel('[time]', fontsize=10)\nax.set_xlim([start_date, end_date])\nax.set_ylabel('[adjusted closing price]', fontsize=10)\n\n# set plot title\nplt.title('Euro vs. US-Dollar Exchange Rate - Historical Prices', fontsize=10);\n</pre> plt.rcParams['figure.figsize'] = [15, 5] fig = plt.figure() ax = fig.add_subplot(111)  # plot eurusd daily adjusted closing prices ax.plot(eurusd_data.index, eurusd_data['Adj Close'], color='#9b59b6')  # rotate x-ticks for tick in ax.get_xticklabels():     tick.set_rotation(45)  # set axis labels ax.set_xlabel('[time]', fontsize=10) ax.set_xlim([start_date, end_date]) ax.set_ylabel('[adjusted closing price]', fontsize=10)  # set plot title plt.title('Euro vs. US-Dollar Exchange Rate - Historical Prices', fontsize=10); <p>Save the downloaded <code>EURUSD</code> data to the local directory:</p> In\u00a0[12]: Copied! <pre>eurusd_data.to_csv('./datasets/eurusd_data_2003_2017_daily.csv', sep=';', encoding='utf-8')\n</pre> eurusd_data.to_csv('./datasets/eurusd_data_2003_2017_daily.csv', sep=';', encoding='utf-8') <p>Let's implement a simple Mean Reversion trading strategy. In general, mean reversion trading refers to the idea that extreme market movements are more likely followed by an \"average movement\" than by an even more extreme market movement.</p> <p>Mean reversion trading is derived from the observation that the price of financial instruments tend to revert to their mean price over time. It is assumed, that the price of a financial instrument is prone to random fluctuations around an underlying (potentially) stable trend. This behaviour can be frequently observed when analyzing price charts of foreign exchange rates such as the EUR to JPY fx-rate, as observable in the following illustration:</p> <p>In the context of mean reversion trading it is aimed to trade such (tiny) fluctuations around such more stable trends. To achieve this will, we apply a technique referred to as \"Bollinger Bands\" proposed by John Bollinger in the 1980's. Bollinger Bands characterize the price volatility of a financial instrument over time. In general, the bands are determined by:</p> <p>$$BB^{upper}(t, n, k) = SMA(t, n) + k \\cdot \\sigma(t)$$</p> <p>$$BB^{lower}(t, n, k) = SMA(t, n) - k \\cdot \\sigma(t)$$</p> <p>where $t$ denotes the current point in time and the distinct elements of the Bollinger Bands calculation correspond to:</p> <ul> <li>$SMA(t, n)$ denotes a simple moving average with a lookback window of $n$ historical prices $p_i$ defined by $SMA(t, n)=\\frac{1}{n} \\sum_{k=0}^{n-1} p_{i}(t-k)$, e.g. a 20, 50, or 100-days moving average lookback window;</li> <li>$BB^{upper}(t, n, k)$ denotes the upper Bollinger Band defined by adding $k$-times the positive standard deviation $\\sigma_i$ of the $n$ historical prices $p_i$ to the simple moving average $SMA(t, n)$; and,</li> <li>$BB^{lower}(t, n, k)$ denotes the lower Bollinger Band defined by subtracting $k$-times the positive standard deviation $\\sigma_i$ of the $n$ historical prices $p_i$ from the simple moving average $SMA(t, n)$.</li> </ul> <p>The following illustration shows the calculated Bollinger Bands $BB^{upper}$ and $BB^{lower}$ at distinct timesteps $t$ and different $k$ parametrizations:</p> <p>Let's start inspect the Bollinger Bands of the mean-reversion trading strategy by setting the moving average window lookback size:</p> In\u00a0[13]: Copied! <pre>mre_lookback_days_20 = 20   # set the mean-reversion moving average indicator lookback, days = 20\n</pre> mre_lookback_days_20 = 20   # set the mean-reversion moving average indicator lookback, days = 20 <p>We can calculate the moving (rolling) average using the Pandas <code>rolling()</code> and <code>mean()</code> function:</p> In\u00a0[14]: Copied! <pre>mre_mav_20 = pd.Series(eurusd_data['Adj Close'].rolling(window = mre_lookback_days_20).mean(), name = 'SMAV_20')\n</pre> mre_mav_20 = pd.Series(eurusd_data['Adj Close'].rolling(window = mre_lookback_days_20).mean(), name = 'SMAV_20') <p>Similarly, we can calculate the moving (rolling) standard deviation $\\sigma$ using the Pandas <code>rolling()</code> and <code>std()</code> function:</p> In\u00a0[15]: Copied! <pre>mre_std_20 = pd.Series(eurusd_data['Adj Close'].rolling(window = mre_lookback_days_20).std(), name = 'STD_20')\n</pre> mre_std_20 = pd.Series(eurusd_data['Adj Close'].rolling(window = mre_lookback_days_20).std(), name = 'STD_20') <p>Merge the obtained rolling moving average and standard deviation values with the original echange rate price data (adjusted closing prices):</p> In\u00a0[16]: Copied! <pre>mre_mav_eurusd_data = eurusd_data.join(mre_mav_20)\nmre_mav_eurusd_data = mre_mav_eurusd_data.join(mre_std_20)\n</pre> mre_mav_eurusd_data = eurusd_data.join(mre_mav_20) mre_mav_eurusd_data = mre_mav_eurusd_data.join(mre_std_20) <p>Inspect and validate the daily adjusted closing prices of the EURUSD exchange rate as well as the derived (i) moving average and (ii) standard deviation values starting from the first obtained 20-day moving average price:</p> In\u00a0[17]: Copied! <pre>mre_mav_eurusd_data[['Adj Close', 'SMAV_20', 'STD_20']].iloc[20:30]\n</pre> mre_mav_eurusd_data[['Adj Close', 'SMAV_20', 'STD_20']].iloc[20:30] Out[17]: Adj Close SMAV_20 STD_20 Date 2004-01-28 1.244199 1.262504 0.013107 2004-01-29 1.241003 1.261645 0.013941 2004-01-30 1.246805 1.261075 0.014317 2004-02-02 1.243302 1.259805 0.014726 2004-02-03 1.253494 1.258875 0.014494 2004-02-04 1.253997 1.258370 0.014479 2004-02-05 1.257197 1.257355 0.013761 2004-02-06 1.270002 1.256561 0.012420 2004-02-09 1.269406 1.256326 0.012112 2004-02-10 1.268504 1.255822 0.011320 <p>To gain an even more detailed intuition let's determine and visualize different degrees of rolling standard deviation obtainable from the 20-day moving average price. Obtain a rolling adjusted closing price standard deviation of $\\sigma = \\pm 1$:</p> In\u00a0[18]: Copied! <pre># one standard deviations\nmre_mav_eurusd_data['POS_STD1_20'] = mre_mav_eurusd_data['Adj Close'] + (1.0 * mre_mav_eurusd_data['STD_20'])\nmre_mav_eurusd_data['NEG_STD1_20'] = mre_mav_eurusd_data['Adj Close'] - (1.0 * mre_mav_eurusd_data['STD_20'])\n</pre> # one standard deviations mre_mav_eurusd_data['POS_STD1_20'] = mre_mav_eurusd_data['Adj Close'] + (1.0 * mre_mav_eurusd_data['STD_20']) mre_mav_eurusd_data['NEG_STD1_20'] = mre_mav_eurusd_data['Adj Close'] - (1.0 * mre_mav_eurusd_data['STD_20']) <p>Similarly, obtain a rolling adjusted closing price standard deviation of $\\sigma = \\pm 2$:</p> In\u00a0[19]: Copied! <pre># two standard deviations\nmre_mav_eurusd_data['POS_STD2_20'] = mre_mav_eurusd_data['Adj Close'] + (2.0 * mre_mav_eurusd_data['STD_20'])\nmre_mav_eurusd_data['NEG_STD2_20'] = mre_mav_eurusd_data['Adj Close'] - (2.0 * mre_mav_eurusd_data['STD_20'])\n</pre> # two standard deviations mre_mav_eurusd_data['POS_STD2_20'] = mre_mav_eurusd_data['Adj Close'] + (2.0 * mre_mav_eurusd_data['STD_20']) mre_mav_eurusd_data['NEG_STD2_20'] = mre_mav_eurusd_data['Adj Close'] - (2.0 * mre_mav_eurusd_data['STD_20']) <p>And finally, obtain a rolling adjusted closing price standard deviation of $\\sigma = \\pm 3$:</p> In\u00a0[20]: Copied! <pre># three standard deviations\nmre_mav_eurusd_data['POS_STD3_20'] = mre_mav_eurusd_data['Adj Close'] + (3.0 * mre_mav_eurusd_data['STD_20'])\nmre_mav_eurusd_data['NEG_STD3_20'] = mre_mav_eurusd_data['Adj Close'] - (3.0 * mre_mav_eurusd_data['STD_20'])\n</pre> # three standard deviations mre_mav_eurusd_data['POS_STD3_20'] = mre_mav_eurusd_data['Adj Close'] + (3.0 * mre_mav_eurusd_data['STD_20']) mre_mav_eurusd_data['NEG_STD3_20'] = mre_mav_eurusd_data['Adj Close'] - (3.0 * mre_mav_eurusd_data['STD_20']) <p>Inspect and validate the daily adjusted closing prices of the EURUSD exchange rate as well the different degrees of deviating standard deviations starting from the first obtained 20-day moving average price:</p> In\u00a0[21]: Copied! <pre>mre_mav_eurusd_data[['Adj Close', 'SMAV_20', 'STD_20', 'POS_STD1_20', 'NEG_STD1_20', 'POS_STD2_20', 'NEG_STD2_20', 'POS_STD3_20', 'NEG_STD3_20']].iloc[20:30]\n</pre> mre_mav_eurusd_data[['Adj Close', 'SMAV_20', 'STD_20', 'POS_STD1_20', 'NEG_STD1_20', 'POS_STD2_20', 'NEG_STD2_20', 'POS_STD3_20', 'NEG_STD3_20']].iloc[20:30] Out[21]: Adj Close SMAV_20 STD_20 POS_STD1_20 NEG_STD1_20 POS_STD2_20 NEG_STD2_20 POS_STD3_20 NEG_STD3_20 Date 2004-01-28 1.244199 1.262504 0.013107 1.257306 1.231092 1.270412 1.217985 1.283519 1.204879 2004-01-29 1.241003 1.261645 0.013941 1.254944 1.227061 1.268886 1.213120 1.282827 1.199178 2004-01-30 1.246805 1.261075 0.014317 1.261122 1.232488 1.275440 1.218170 1.289757 1.203853 2004-02-02 1.243302 1.259805 0.014726 1.258028 1.228576 1.272754 1.213849 1.287480 1.199123 2004-02-03 1.253494 1.258875 0.014494 1.267988 1.239000 1.282483 1.224506 1.296977 1.210011 2004-02-04 1.253997 1.258370 0.014479 1.268476 1.239518 1.282955 1.225040 1.297433 1.210561 2004-02-05 1.257197 1.257355 0.013761 1.270958 1.243437 1.284719 1.229676 1.298480 1.215915 2004-02-06 1.270002 1.256561 0.012420 1.282422 1.257582 1.294842 1.245162 1.307262 1.232742 2004-02-09 1.269406 1.256326 0.012112 1.281518 1.257294 1.293630 1.245182 1.305742 1.233070 2004-02-10 1.268504 1.255822 0.011320 1.279824 1.257184 1.291144 1.245864 1.302464 1.234544 <p>Plot the historical daily adjusted closing prices of the EUR vs. US-Dollar (blue) as well as its rolling 20 days standard deviations of $\\sigma=1$ standard deviations (top), $\\sigma=2$ standard deviations (middle) as well as $\\sigma=3$ standard deviations (bottom):</p> In\u00a0[22]: Copied! <pre>plt.rcParams['figure.figsize'] = [15, 15]\nfig, ax = plt.subplots(ncols=1, nrows=3)\n\n#### plot the standard deviation of 1\n\n# plot moving average adjusted closing price standard deviation of 1\nax[0].fill_between(mre_mav_eurusd_data.index, mre_mav_eurusd_data['POS_STD1_20'], mre_mav_eurusd_data['NEG_STD1_20'], color='C2', lw=2.0, label='$Stdv. \\sigma = 1$ (red)', alpha=0.3)\n\n# plot adjusted closing price\nax[0].plot(mre_mav_eurusd_data['Adj Close'], lw=1.0, color='C3', label='Adj. Closing Prices (purple)')\n\n# rotate x-tick labels\nfor tick in ax[0].get_xticklabels():\n    tick.set_rotation(45)\n\n# set axis labels\nax[0].set_xlabel('[time]', fontsize=10)\nax[0].set_xlim([start_date, end_date])\nax[0].set_ylabel('[market price]', fontsize=10)\n\n# set plot legend\nax[0].legend(loc=\"upper left\", numpoints=1, fancybox=True)\n\n#### plot the standard deviation of 2\n\n# plot moving average adjusted closing price standard deviation of 2\nax[1].fill_between(mre_mav_eurusd_data.index, mre_mav_eurusd_data['POS_STD2_20'], mre_mav_eurusd_data['NEG_STD2_20'], color='C2', lw=2.0, label='$Stdv. \\sigma = 2$ (red)', alpha=0.3)\n\n# plot adjusted closing price\nax[1].plot(mre_mav_eurusd_data['Adj Close'], lw=1.0, color='C3', label='Adj. Closing Prices (purple)')\n\n# rotate x-tick labels\nfor tick in ax[1].get_xticklabels():\n    tick.set_rotation(45)\n\n# set axis labels\nax[1].set_xlabel('[time]', fontsize=10)\nax[1].set_xlim([start_date, end_date])\nax[1].set_ylabel('[market price]', fontsize=10)\n\n# set plot legend\nax[1].legend(loc=\"upper left\", numpoints=1, fancybox=True)\n\n#### plot the standard deviation of 3\n\n# plot moving average adjusted closing price standard deviation of 3\nax[2].fill_between(mre_mav_eurusd_data.index, mre_mav_eurusd_data['POS_STD3_20'], mre_mav_eurusd_data['NEG_STD3_20'], color='C2', lw=2.0, label='$Stdv. \\sigma = 3$ (red)', alpha=0.3)\n\n# plot adjusted closing price\nax[2].plot(mre_mav_eurusd_data['Adj Close'], lw=1.0, color='C3', label='Adj. Closing Prices (purple)')\n\n# rotate x-tick labels\nfor tick in ax[2].get_xticklabels():\n    tick.set_rotation(45)\n\n# set axis labels\nax[2].set_xlabel('[time]', fontsize=10)\nax[2].set_xlim([start_date, end_date])\nax[2].set_ylabel('[market price]', fontsize=10)\n\n# set plot legend\nax[2].legend(loc=\"upper left\", numpoints=1, fancybox=True)\n\n# set plot title\nax[0].set_title('Euro vs. US-Dollar Exchange Rate - Historical Prices, $\\sigma=1$', fontsize=10)\nax[1].set_title('Euro vs. US-Dollar Exchange Rate - Historical Prices, $\\sigma=2$', fontsize=10)\nax[2].set_title('Euro vs. US-Dollar Exchange Rate - Historical Prices, $\\sigma=3$', fontsize=10)\n\n# reset plot layout\nplt.tight_layout()\n</pre> plt.rcParams['figure.figsize'] = [15, 15] fig, ax = plt.subplots(ncols=1, nrows=3)  #### plot the standard deviation of 1  # plot moving average adjusted closing price standard deviation of 1 ax[0].fill_between(mre_mav_eurusd_data.index, mre_mav_eurusd_data['POS_STD1_20'], mre_mav_eurusd_data['NEG_STD1_20'], color='C2', lw=2.0, label='$Stdv. \\sigma = 1$ (red)', alpha=0.3)  # plot adjusted closing price ax[0].plot(mre_mav_eurusd_data['Adj Close'], lw=1.0, color='C3', label='Adj. Closing Prices (purple)')  # rotate x-tick labels for tick in ax[0].get_xticklabels():     tick.set_rotation(45)  # set axis labels ax[0].set_xlabel('[time]', fontsize=10) ax[0].set_xlim([start_date, end_date]) ax[0].set_ylabel('[market price]', fontsize=10)  # set plot legend ax[0].legend(loc=\"upper left\", numpoints=1, fancybox=True)  #### plot the standard deviation of 2  # plot moving average adjusted closing price standard deviation of 2 ax[1].fill_between(mre_mav_eurusd_data.index, mre_mav_eurusd_data['POS_STD2_20'], mre_mav_eurusd_data['NEG_STD2_20'], color='C2', lw=2.0, label='$Stdv. \\sigma = 2$ (red)', alpha=0.3)  # plot adjusted closing price ax[1].plot(mre_mav_eurusd_data['Adj Close'], lw=1.0, color='C3', label='Adj. Closing Prices (purple)')  # rotate x-tick labels for tick in ax[1].get_xticklabels():     tick.set_rotation(45)  # set axis labels ax[1].set_xlabel('[time]', fontsize=10) ax[1].set_xlim([start_date, end_date]) ax[1].set_ylabel('[market price]', fontsize=10)  # set plot legend ax[1].legend(loc=\"upper left\", numpoints=1, fancybox=True)  #### plot the standard deviation of 3  # plot moving average adjusted closing price standard deviation of 3 ax[2].fill_between(mre_mav_eurusd_data.index, mre_mav_eurusd_data['POS_STD3_20'], mre_mav_eurusd_data['NEG_STD3_20'], color='C2', lw=2.0, label='$Stdv. \\sigma = 3$ (red)', alpha=0.3)  # plot adjusted closing price ax[2].plot(mre_mav_eurusd_data['Adj Close'], lw=1.0, color='C3', label='Adj. Closing Prices (purple)')  # rotate x-tick labels for tick in ax[2].get_xticklabels():     tick.set_rotation(45)  # set axis labels ax[2].set_xlabel('[time]', fontsize=10) ax[2].set_xlim([start_date, end_date]) ax[2].set_ylabel('[market price]', fontsize=10)  # set plot legend ax[2].legend(loc=\"upper left\", numpoints=1, fancybox=True)  # set plot title ax[0].set_title('Euro vs. US-Dollar Exchange Rate - Historical Prices, $\\sigma=1$', fontsize=10) ax[1].set_title('Euro vs. US-Dollar Exchange Rate - Historical Prices, $\\sigma=2$', fontsize=10) ax[2].set_title('Euro vs. US-Dollar Exchange Rate - Historical Prices, $\\sigma=3$', fontsize=10)  # reset plot layout plt.tight_layout() <p>We will make use of the \"Standard-Score\" or \"Z-Score\" to convert the Bollinger Band information into a series of binary long- and short-trading-signals of a mean reversion trading strategy. The \"Z-Score\" is the signed number of standard deviations by which the actual price $p_{i}(t)$ of a financial instrument $i$ falls above or below the moving average price, formally denoted by:</p> <p>$$ z_{i}(t) = \\frac{p_{i}(t)-SMA_{i}(t,n)}{\\sigma_{i}(t, n)}$$</p> <p>where $t$ denotes the current point in time and the distinct elements of the Z-Score are defined by:</p> <ul> <li>$SMA(t, n)$ denotes a simple moving average with a lookback window of $n$ historical prices $p_i$ defined by $SMA(t, n)=\\frac{1}{n} \\sum_{k=0}^{n-1} p_{i}(t-k)$, e.g. a 20, 50, or 100-days moving average lookback window; and,</li> <li>$\\sigma_{i}(t, n)$ denotes the moving average strandard deviation with a lookback window of $n$ historical prices $p_i$, e.g. a 20, 50, or 100-days moving average lookback window.</li> </ul> <p>Let's now determine the Z-Score at distinct time steps of the EUR vs. US-Dollar foreign exchange rate:</p> In\u00a0[23]: Copied! <pre>mre_mav_eurusd_data['Z_SCORE'] = (mre_mav_eurusd_data['Adj Close'] - mre_mav_eurusd_data['SMAV_20']) / mre_mav_eurusd_data['STD_20']\n</pre> mre_mav_eurusd_data['Z_SCORE'] = (mre_mav_eurusd_data['Adj Close'] - mre_mav_eurusd_data['SMAV_20']) / mre_mav_eurusd_data['STD_20'] <p>Inspect and validate the different rolling Z scores obtained, starting from the first obtained 20-day moving average price:</p> In\u00a0[24]: Copied! <pre>mre_mav_eurusd_data[['Adj Close', 'SMAV_20', 'STD_20', 'Z_SCORE']].iloc[20:30]\n</pre> mre_mav_eurusd_data[['Adj Close', 'SMAV_20', 'STD_20', 'Z_SCORE']].iloc[20:30] Out[24]: Adj Close SMAV_20 STD_20 Z_SCORE Date 2004-01-28 1.244199 1.262504 0.013107 -1.396632 2004-01-29 1.241003 1.261645 0.013941 -1.480620 2004-01-30 1.246805 1.261075 0.014317 -0.996705 2004-02-02 1.243302 1.259805 0.014726 -1.120719 2004-02-03 1.253494 1.258875 0.014494 -0.371248 2004-02-04 1.253997 1.258370 0.014479 -0.302031 2004-02-05 1.257197 1.257355 0.013761 -0.011451 2004-02-06 1.270002 1.256561 0.012420 1.082279 2004-02-09 1.269406 1.256326 0.012112 1.079927 2004-02-10 1.268504 1.255822 0.011320 1.120372 <p>Let's now derive a mean-reversion trading signal from the calculated rolling Z-Score of the EUR vs. US-Dollar foreign exchange rate. In order to derive such a signal we first specify an upper Z-Score threshold $\\alpha$ and a lower Z-Score threshold $\\beta$, where $\\alpha &gt; \\beta$. Afterwards, we are able to derive a mean-reversion trading signal according to the following rules:</p> <ul> <li>\"Long-signal\" (+1.0) signal if $z_{i}(t) \\leq -\\; \\alpha \\cdot z_{i}(t)$;</li> <li>\"Close Long-signal\" (0.0) signal if $z_{i}(t) \\leq -\\; \\beta \\cdot z_{i}(t)$;</li> <li>\"Short-signal\" (+1.0) signal if $z_{i}(t) \\geq +\\; \\alpha \\cdot z_{i}(t)$; and,</li> <li>\"Close Short-signal\" (0.0) signal if $z_{i}(t) \\geq +\\; \\beta \\cdot z_{i}(t)$.</li> </ul> <p>Let's now start to determine the mean-reversion trading signals by setting the Z-Score thresholds. Thereby, we will set both Z-Score thresholds $\\alpha = 1.0$ and $\\beta = 0.5$ respectively, as done in the following:</p> In\u00a0[25]: Copied! <pre>z_score_alpha_threshold = 1.0\nz_score_beta_threshold = 0.5\n</pre> z_score_alpha_threshold = 1.0 z_score_beta_threshold = 0.5 <p>Subsequently we implement and derive the mean-reversion trading signals of the EUR vs. US-Dollar foreign exchange rate using both Z-Score thresholds as defined above:</p> In\u00a0[26]: Copied! <pre># determine the distinct z-scores\nz_scores = mre_mav_eurusd_data['Z_SCORE']\n\n# init mean reversion signal\nmre_trade_signal = np.zeros(len(z_scores))\n\n# iterate over z-scores\nfor i in range(20, len(z_scores)):\n\n    # determine actual z-score\n    z_score = z_scores[i]\n\n    # case: active trading signal\n    if mre_trade_signal[i-1] == 0.0:\n\n        # case: z-score exceeds positive threshold\n        if z_score &gt; z_score_alpha_threshold:\n\n            # set 'short' signal\n            mre_trade_signal[i] = -1.0\n\n        # case: z-score exceeds negative threshold\n        elif z_score &lt; (z_score_alpha_threshold * -1.0):\n\n            # set 'long' signal\n            mre_trade_signal[i] = 1.0\n\n        # case: z-score doesn't exceed thresholds\n        else:\n\n            # keep prior signal\n            mre_trade_signal[i] = mre_trade_signal[i-1]\n\n    # case: inactive trading signal\n    elif mre_trade_signal[i-1] != 0.0:\n\n        # z-score reverted back to moving average\n        if abs(z_score) &lt; z_score_beta_threshold:\n\n            # set 'neutral' signal\n            mre_trade_signal[i] = 0.0\n\n        # z-score not yer reverted back to moving average\n        elif abs(z_score) &gt; z_score_beta_threshold:\n\n            # keep prior signal\n            mre_trade_signal[i] = mre_trade_signal[i-1]\n</pre> # determine the distinct z-scores z_scores = mre_mav_eurusd_data['Z_SCORE']  # init mean reversion signal mre_trade_signal = np.zeros(len(z_scores))  # iterate over z-scores for i in range(20, len(z_scores)):      # determine actual z-score     z_score = z_scores[i]      # case: active trading signal     if mre_trade_signal[i-1] == 0.0:          # case: z-score exceeds positive threshold         if z_score &gt; z_score_alpha_threshold:              # set 'short' signal             mre_trade_signal[i] = -1.0          # case: z-score exceeds negative threshold         elif z_score &lt; (z_score_alpha_threshold * -1.0):              # set 'long' signal             mre_trade_signal[i] = 1.0          # case: z-score doesn't exceed thresholds         else:              # keep prior signal             mre_trade_signal[i] = mre_trade_signal[i-1]      # case: inactive trading signal     elif mre_trade_signal[i-1] != 0.0:          # z-score reverted back to moving average         if abs(z_score) &lt; z_score_beta_threshold:              # set 'neutral' signal             mre_trade_signal[i] = 0.0          # z-score not yer reverted back to moving average         elif abs(z_score) &gt; z_score_beta_threshold:              # keep prior signal             mre_trade_signal[i] = mre_trade_signal[i-1] <p>Convert the obtained trading signals into a Pandas DataFrame and merge it with the market price data:</p> In\u00a0[27]: Copied! <pre># convert signals to Pandas DataFrame\nmre_mav_eurusd_data_signal = pd.DataFrame(mre_trade_signal, columns=['SIGNAL_20'], index=mre_mav_eurusd_data.index)\n\n# convert pandas DataFrame index to datatype: datetime\nmre_mav_eurusd_data['SIGNAL_20'] = mre_mav_eurusd_data_signal\n</pre> # convert signals to Pandas DataFrame mre_mav_eurusd_data_signal = pd.DataFrame(mre_trade_signal, columns=['SIGNAL_20'], index=mre_mav_eurusd_data.index)  # convert pandas DataFrame index to datatype: datetime mre_mav_eurusd_data['SIGNAL_20'] = mre_mav_eurusd_data_signal <p>Inspect and validate the different Z scores and mean-reversion trading strategy signals obtained, starting from the first obtained 20-day moving average price:</p> In\u00a0[28]: Copied! <pre>mre_mav_eurusd_data[['Adj Close', 'SMAV_20', 'STD_20', 'Z_SCORE', 'SIGNAL_20']].iloc[20:30]\n</pre> mre_mav_eurusd_data[['Adj Close', 'SMAV_20', 'STD_20', 'Z_SCORE', 'SIGNAL_20']].iloc[20:30] Out[28]: Adj Close SMAV_20 STD_20 Z_SCORE SIGNAL_20 Date 2004-01-28 1.244199 1.262504 0.013107 -1.396632 1.0 2004-01-29 1.241003 1.261645 0.013941 -1.480620 1.0 2004-01-30 1.246805 1.261075 0.014317 -0.996705 1.0 2004-02-02 1.243302 1.259805 0.014726 -1.120719 1.0 2004-02-03 1.253494 1.258875 0.014494 -0.371248 0.0 2004-02-04 1.253997 1.258370 0.014479 -0.302031 0.0 2004-02-05 1.257197 1.257355 0.013761 -0.011451 0.0 2004-02-06 1.270002 1.256561 0.012420 1.082279 -1.0 2004-02-09 1.269406 1.256326 0.012112 1.079927 -1.0 2004-02-10 1.268504 1.255822 0.011320 1.120372 -1.0 <p>In addition, let's also prepare a backtest of a \"baseline\" in terms of a simple buy-and-hold trading strategy for comparison purposes. Our buy-and-hold strategy sends a \"long\" (+1.0) signal for each time step:</p> In\u00a0[29]: Copied! <pre>mre_mav_eurusd_data['SIGNAL_BASE'] = 1.0\n</pre> mre_mav_eurusd_data['SIGNAL_BASE'] = 1.0 <p>Prepare the trading signal data to be utilized in backtesting the mean-reversion trading strategy:</p> In\u00a0[30]: Copied! <pre># convert signals to Pandas DataFrame\nmre_mav_eurusd_signal_data = pd.DataFrame(mre_mav_eurusd_data[['SIGNAL_20', 'SIGNAL_BASE']], columns=['SIGNAL_20', 'SIGNAL_BASE'])\n\n# convert pandas DataFrame index to datatype: datetime\nmre_mav_eurusd_signal_data = mre_mav_eurusd_signal_data.set_index(pd.to_datetime(mre_mav_eurusd_signal_data.index))\n</pre> # convert signals to Pandas DataFrame mre_mav_eurusd_signal_data = pd.DataFrame(mre_mav_eurusd_data[['SIGNAL_20', 'SIGNAL_BASE']], columns=['SIGNAL_20', 'SIGNAL_BASE'])  # convert pandas DataFrame index to datatype: datetime mre_mav_eurusd_signal_data = mre_mav_eurusd_signal_data.set_index(pd.to_datetime(mre_mav_eurusd_signal_data.index)) <p>Inspect top 10 rows of the prepared trading signals:</p> In\u00a0[31]: Copied! <pre>mre_mav_eurusd_signal_data.head(10)\n</pre> mre_mav_eurusd_signal_data.head(10) Out[31]: SIGNAL_20 SIGNAL_BASE Date 2003-12-31 0.0 1.0 2004-01-01 0.0 1.0 2004-01-02 0.0 1.0 2004-01-05 0.0 1.0 2004-01-06 0.0 1.0 2004-01-07 0.0 1.0 2004-01-08 0.0 1.0 2004-01-09 0.0 1.0 2004-01-12 0.0 1.0 2004-01-13 0.0 1.0 <p>Inspect some of the exemplary signal deviations between the mean-reversion and our baseline buy and hold trading strategies:</p> In\u00a0[32]: Copied! <pre>mre_mav_eurusd_signal_data[mre_mav_eurusd_signal_data['SIGNAL_20'] != mre_mav_eurusd_signal_data['SIGNAL_BASE']].head(10)\n</pre> mre_mav_eurusd_signal_data[mre_mav_eurusd_signal_data['SIGNAL_20'] != mre_mav_eurusd_signal_data['SIGNAL_BASE']].head(10) Out[32]: SIGNAL_20 SIGNAL_BASE Date 2003-12-31 0.0 1.0 2004-01-01 0.0 1.0 2004-01-02 0.0 1.0 2004-01-05 0.0 1.0 2004-01-06 0.0 1.0 2004-01-07 0.0 1.0 2004-01-08 0.0 1.0 2004-01-09 0.0 1.0 2004-01-12 0.0 1.0 2004-01-13 0.0 1.0 <p>Visualize the prepared trading signals:</p> In\u00a0[33]: Copied! <pre>plt.rcParams['figure.figsize'] = [15, 5]\nfig, ax = plt.subplots(ncols=1, nrows=2)\n\nax[0].plot(mre_mav_eurusd_signal_data['SIGNAL_20'], lw=1.0, color='C2', label='SMAV 16 (red)')\nax[1].plot(mre_mav_eurusd_signal_data['SIGNAL_BASE'], lw=1.0, color='C3', label='BASE (purple)')\n\n# set axis labels\nplt.xlabel('[time]', fontsize=10)\nax[0].set_xlim([start_date, end_date])\nax[0].set_ylabel('[mre 100 signal]', fontsize=10)\nax[1].set_xlim([start_date, end_date])\nax[1].set_ylabel('[base signal]', fontsize=10)\n\n# rotate the x-axis labels\nfor tick in ax[0].get_xticklabels():\n    tick.set_rotation(45)\n\nfor tick in ax[1].get_xticklabels():\n    tick.set_rotation(45)\n\n# set plot title\nax[0].set_title('Euro vs. US-Dollar Exchange Rate - Mean Reversion Trading Signals', fontsize=10)\nax[1].set_title('Euro vs. US-Dollar Exchange Rate - Baseline Buy and Hold Trading Signals', fontsize=10)\n\n# reset plot layout\nplt.tight_layout()\n</pre> plt.rcParams['figure.figsize'] = [15, 5] fig, ax = plt.subplots(ncols=1, nrows=2)  ax[0].plot(mre_mav_eurusd_signal_data['SIGNAL_20'], lw=1.0, color='C2', label='SMAV 16 (red)') ax[1].plot(mre_mav_eurusd_signal_data['SIGNAL_BASE'], lw=1.0, color='C3', label='BASE (purple)')  # set axis labels plt.xlabel('[time]', fontsize=10) ax[0].set_xlim([start_date, end_date]) ax[0].set_ylabel('[mre 100 signal]', fontsize=10) ax[1].set_xlim([start_date, end_date]) ax[1].set_ylabel('[base signal]', fontsize=10)  # rotate the x-axis labels for tick in ax[0].get_xticklabels():     tick.set_rotation(45)  for tick in ax[1].get_xticklabels():     tick.set_rotation(45)  # set plot title ax[0].set_title('Euro vs. US-Dollar Exchange Rate - Mean Reversion Trading Signals', fontsize=10) ax[1].set_title('Euro vs. US-Dollar Exchange Rate - Baseline Buy and Hold Trading Signals', fontsize=10)  # reset plot layout plt.tight_layout() <p>Let's determine the total number of long-short signal changes of the distinct trading strategies:</p> In\u00a0[34]: Copied! <pre># signal changes of the mean-reversion trading strategy\nlen(list(it.groupby(mre_mav_eurusd_signal_data['SIGNAL_20'], lambda x: x &gt; 0)))\n</pre> # signal changes of the mean-reversion trading strategy len(list(it.groupby(mre_mav_eurusd_signal_data['SIGNAL_20'], lambda x: x &gt; 0))) Out[34]: <pre>267</pre> In\u00a0[35]: Copied! <pre># signal changes of the baseline buy and hold trading strategy\nlen(list(it.groupby(mre_mav_eurusd_signal_data['SIGNAL_BASE'], lambda x: x &gt; 0)))\n</pre> # signal changes of the baseline buy and hold trading strategy len(list(it.groupby(mre_mav_eurusd_signal_data['SIGNAL_BASE'], lambda x: x &gt; 0))) Out[35]: <pre>1</pre> <p>Prepare the market data to be utilized in backtesting the mean reversion trading strategy:</p> In\u00a0[36]: Copied! <pre># extract the eurusd exchange rate closing prices\neurusd_market_data = pd.DataFrame(eurusd_data['Adj Close'], columns=['Adj Close'])\n\n# rename the 'close' column to 'eurusd' (since this is the column we want to allocate to in the backtest)\neurusd_market_data = eurusd_market_data.rename(columns={'Adj Close': 'EURUSD'})\n\n# convert pandas DataFrame index to datatype: datetime\neurusd_market_data = eurusd_market_data.set_index(pd.to_datetime(eurusd_data.index))\n</pre> # extract the eurusd exchange rate closing prices eurusd_market_data = pd.DataFrame(eurusd_data['Adj Close'], columns=['Adj Close'])  # rename the 'close' column to 'eurusd' (since this is the column we want to allocate to in the backtest) eurusd_market_data = eurusd_market_data.rename(columns={'Adj Close': 'EURUSD'})  # convert pandas DataFrame index to datatype: datetime eurusd_market_data = eurusd_market_data.set_index(pd.to_datetime(eurusd_data.index)) <p>Inspect top 10 rows of the prepared market data:</p> In\u00a0[37]: Copied! <pre>eurusd_market_data.head(10)\n</pre> eurusd_market_data.head(10) Out[37]: EURUSD Date 2003-12-31 1.259002 2004-01-01 1.258194 2004-01-02 1.258194 2004-01-05 1.268698 2004-01-06 1.272103 2004-01-07 1.264095 2004-01-08 1.277498 2004-01-09 1.285892 2004-01-12 1.274096 2004-01-13 1.278593 <p>Implementing a simple Mean Reversion Trading Strategy by interfacing the Python <code>bt</code>'s Algo class:</p> In\u00a0[38]: Copied! <pre>class MeanReversionStrategy(bt.Algo):\n\n    # inits the strategy\n    def __init__(self, signals):\n\n        # set class signals\n        self.signals = signals\n\n    # calss the trading strategy\n    def __call__(self, target):\n\n        # case: current timestep in signals\n        if target.now in self.signals.index[1:]:\n\n            # get actual signal\n            signal = self.signals[target.now]\n\n            # set target weights according to signal\n            target.temp['weights'] = dict(EURUSD=signal)\n\n        # return 'True' since we want to move on to the next timestep\n        return True\n</pre> class MeanReversionStrategy(bt.Algo):      # inits the strategy     def __init__(self, signals):          # set class signals         self.signals = signals      # calss the trading strategy     def __call__(self, target):          # case: current timestep in signals         if target.now in self.signals.index[1:]:              # get actual signal             signal = self.signals[target.now]              # set target weights according to signal             target.temp['weights'] = dict(EURUSD=signal)          # return 'True' since we want to move on to the next timestep         return True <p>Define the moving average trading strategy backtest algorithm stack.</p> <p>Note: That in the Python <code>bt</code> library a trading strategy usually consists of a so-called stack of algorithms. For each timestep of our backtest timeframe, the <code>bt</code> library executes all algorithm of the stack in sequential order. Each moving average strategy we aim to design and backtest consists in total of three algorithms, briefly described in the following:</p> <ol> <li><code>bt.algos.SelectAll()</code>: Selects all available stocks for trading except stock prices that correspond to NaN or 0.00.</li> <li><code>MovingAverageStrategy()</code>: Assigns the calculated signal in terms of a weight value to the EUR vs. USD exchange rate.</li> <li><code>bt.algos.Rebalance()</code>: Rebalances the available capital based on the weights assigned to each stock.</li> </ol> <p>Define the mean-reversion and buy-and-hold trading strategy backtest algorithm stack:</p> In\u00a0[39]: Copied! <pre>mre_mav_eurusd_strategy_20 = bt.Strategy(name='mre_20', algos=[bt.algos.SelectAll(), MeanReversionStrategy(mre_mav_eurusd_signal_data['SIGNAL_20']), bt.algos.Rebalance()])\nmre_mav_eurusd_strategy_base = bt.Strategy(name='base', algos=[bt.algos.SelectAll(), MeanReversionStrategy(mre_mav_eurusd_signal_data['SIGNAL_BASE']), bt.algos.Rebalance()])\n</pre> mre_mav_eurusd_strategy_20 = bt.Strategy(name='mre_20', algos=[bt.algos.SelectAll(), MeanReversionStrategy(mre_mav_eurusd_signal_data['SIGNAL_20']), bt.algos.Rebalance()]) mre_mav_eurusd_strategy_base = bt.Strategy(name='base', algos=[bt.algos.SelectAll(), MeanReversionStrategy(mre_mav_eurusd_signal_data['SIGNAL_BASE']), bt.algos.Rebalance()]) <p>Let's now define the trading ('fees') commissions used in each rebalancing time-step of a backtest. To achieve this, the <code>bt</code> library expects a callable function that expects the following two parameters as an input:</p> <ul> <li>the 'quantity', denoted by <code>q</code>, of rebalanced assets at a backtest time-step;</li> <li>the 'price', denoted by <code>p</code>, of rebalanced assets at a backtest time-step.</li> </ul> <p>Let's implement such a callable function defining a trading fee of 1% (0.01) per quantity of rebalanced asset (or a flat fee of USD 5.00 per trade):</p> In\u00a0[40]: Copied! <pre># init trading fees function\ndef trading_fees_function(q, p):\n\n    # calcluate trading fees (rebalanced-quantity * trading-fee)\n    fees = 5.00 # flat fee of USD 5.00 per trade\n\n    # return the total trading fees\n    return fees\n</pre> # init trading fees function def trading_fees_function(q, p):      # calcluate trading fees (rebalanced-quantity * trading-fee)     fees = 5.00 # flat fee of USD 5.00 per trade      # return the total trading fees     return fees <p>Upon completion of defining the mean-reversion strategy let's now init the corresponding backtests using (1) both strategies as well as (2) the market data that we aim to evaluate during the backtest:</p> In\u00a0[41]: Copied! <pre>eurusd_backtest_mre_mav_20 = bt.Backtest(strategy=mre_mav_eurusd_strategy_20, data=eurusd_market_data, name='eurusd_backtest_mre_20', commissions=trading_fees_function, progress_bar=True)\neurusd_backtest_mre_mav_base = bt.Backtest(strategy=mre_mav_eurusd_strategy_base, data=eurusd_market_data, name='eurusd_backtest_mre_base', commissions=trading_fees_function, progress_bar=True)\n</pre> eurusd_backtest_mre_mav_20 = bt.Backtest(strategy=mre_mav_eurusd_strategy_20, data=eurusd_market_data, name='eurusd_backtest_mre_20', commissions=trading_fees_function, progress_bar=True) eurusd_backtest_mre_mav_base = bt.Backtest(strategy=mre_mav_eurusd_strategy_base, data=eurusd_market_data, name='eurusd_backtest_mre_base', commissions=trading_fees_function, progress_bar=True) <p>Now, let's run the backtest of the mean-reversion trading strategy configuration as well as the defined baseline:</p> In\u00a0[42]: Copied! <pre>backtest_results_eurusd = bt.run(eurusd_backtest_mre_mav_20, eurusd_backtest_mre_mav_base)\n</pre> backtest_results_eurusd = bt.run(eurusd_backtest_mre_mav_20, eurusd_backtest_mre_mav_base) <pre>eurusd_backtest_mre_20\n0% [############################# ] 100% | ETA: 00:00:00eurusd_backtest_mre_base\n0% [############################# ] 100% | ETA: 00:00:00</pre> <p>Inspect the individual backtest results and performance measures:</p> In\u00a0[43]: Copied! <pre>backtest_results_eurusd.display()\n</pre> backtest_results_eurusd.display() <pre>Stat                 eurusd_backtest_mre_20    eurusd_backtest_mre_base\n-------------------  ------------------------  --------------------------\nStart                2003-12-30                2003-12-30\nEnd                  2017-12-29                2017-12-29\nRisk-free rate       0.00%                     0.00%\n\nTotal Return         31.80%                    -5.09%\nDaily Sharpe         0.24                      0.04\nDaily Sortino        0.39                      0.06\nCAGR                 1.99%                     -0.37%\nMax Drawdown         -19.44%                   -35.01%\nCalmar Ratio         0.10                      -0.01\n\nMTD                  1.44%                     0.76%\n3m                   2.17%                     1.36%\n6m                   -1.08%                    4.91%\nYTD                  -0.81%                    12.92%\n1Y                   0.70%                     14.64%\n3Y (ann.)            5.97%                     -0.63%\n5Y (ann.)            4.50%                     -2.03%\n10Y (ann.)           3.11%                     -1.99%\nSince Incep. (ann.)  1.99%                     -0.37%\n\nDaily Sharpe         0.24                      0.04\nDaily Sortino        0.39                      0.06\nDaily Mean (ann.)    2.46%                     0.45%\nDaily Vol (ann.)     10.42%                    12.83%\nDaily Skew           2.80                      1.66\nDaily Kurt           86.19                     95.43\nBest Day             13.35%                    17.31%\nWorst Day            -9.09%                    -13.35%\n\nMonthly Sharpe       0.26                      0.01\nMonthly Sortino      0.45                      0.02\nMonthly Mean (ann.)  2.37%                     0.11%\nMonthly Vol (ann.)   8.97%                     9.79%\nMonthly Skew         -0.01                     -0.24\nMonthly Kurt         2.88                      1.85\nBest Month           10.79%                    10.10%\nWorst Month          -9.89%                    -9.89%\n\nYearly Sharpe        0.30                      -0.00\nYearly Sortino       0.56                      -0.00\nYearly Mean          2.25%                     -0.02%\nYearly Vol           7.51%                     8.75%\nYearly Skew          -0.19                     0.03\nYearly Kurt          1.48                      -1.29\nBest Year            15.21%                    12.92%\nWorst Year           -14.41%                   -12.71%\n\nAvg. Drawdown        -2.68%                    -2.75%\nAvg. Drawdown Days   117.60                    143.51\nAvg. Up Month        1.85%                     2.05%\nAvg. Down Month      -1.85%                    -2.13%\nWin Year %           64.29%                    50.00%\nWin 12m %            67.09%                    55.06%\n</pre> <p>Collect detailed backtest performance per timestep of the mean-reversion strategy:</p> In\u00a0[44]: Copied! <pre>backtest_mre_20_eurusd_details = eurusd_backtest_mre_mav_20.strategy.prices.to_frame(name='Rel. EQUITY')\nbacktest_mre_20_eurusd_details['Abs. EQUITY'] = eurusd_backtest_mre_mav_20.strategy.values # equity per timestep\nbacktest_mre_20_eurusd_details['CASH'] = eurusd_backtest_mre_mav_20.strategy.cash # cash per timestep\nbacktest_mre_20_eurusd_details['POSITIONS'] = eurusd_backtest_mre_mav_20.strategy.positions # positions per timestep\nbacktest_mre_20_eurusd_details['FEES'] = eurusd_backtest_mre_mav_20.strategy.fees # trading fees per timestep\n</pre> backtest_mre_20_eurusd_details = eurusd_backtest_mre_mav_20.strategy.prices.to_frame(name='Rel. EQUITY') backtest_mre_20_eurusd_details['Abs. EQUITY'] = eurusd_backtest_mre_mav_20.strategy.values # equity per timestep backtest_mre_20_eurusd_details['CASH'] = eurusd_backtest_mre_mav_20.strategy.cash # cash per timestep backtest_mre_20_eurusd_details['POSITIONS'] = eurusd_backtest_mre_mav_20.strategy.positions # positions per timestep backtest_mre_20_eurusd_details['FEES'] = eurusd_backtest_mre_mav_20.strategy.fees # trading fees per timestep <p>Inspect detailed backtest results per timestep:</p> In\u00a0[45]: Copied! <pre>backtest_mre_20_eurusd_details.head(10)\n</pre> backtest_mre_20_eurusd_details.head(10) Out[45]: Rel. EQUITY Abs. EQUITY CASH POSITIONS FEES 2003-12-30 100.0 1000000.0 1000000.0 0.0 0.0 2003-12-31 100.0 1000000.0 1000000.0 0.0 0.0 2004-01-01 100.0 1000000.0 1000000.0 0.0 0.0 2004-01-02 100.0 1000000.0 1000000.0 0.0 0.0 2004-01-05 100.0 1000000.0 1000000.0 0.0 0.0 2004-01-06 100.0 1000000.0 1000000.0 0.0 0.0 2004-01-07 100.0 1000000.0 1000000.0 0.0 0.0 2004-01-08 100.0 1000000.0 1000000.0 0.0 0.0 2004-01-09 100.0 1000000.0 1000000.0 0.0 0.0 2004-01-12 100.0 1000000.0 1000000.0 0.0 0.0 <p>Visualize the monthly returns obtained by the mean-reversion trading strategy:</p> In\u00a0[46]: Copied! <pre>plt.rcParams['figure.figsize'] = [15, 10]\nfig = plt.figure()\nax = fig.add_subplot(111)\n\n# plot heatmap of monthly returns generated by the strategy\nax = sns.heatmap(eurusd_backtest_mre_mav_20.stats.return_table, annot=True, cbar=True, vmin=-0.5, vmax=0.5)\n\n# set axis labels\nax.set_xlabel('[month]', fontsize=10)\nax.set_ylabel('[year]', fontsize=10)\n\n# set plot title\nax.set_title('Euro vs. US-Dollar Exchange Rate - Monthly Returns Mean Reversion Trading Strategy', fontsize=10);\n</pre> plt.rcParams['figure.figsize'] = [15, 10] fig = plt.figure() ax = fig.add_subplot(111)  # plot heatmap of monthly returns generated by the strategy ax = sns.heatmap(eurusd_backtest_mre_mav_20.stats.return_table, annot=True, cbar=True, vmin=-0.5, vmax=0.5)  # set axis labels ax.set_xlabel('[month]', fontsize=10) ax.set_ylabel('[year]', fontsize=10)  # set plot title ax.set_title('Euro vs. US-Dollar Exchange Rate - Monthly Returns Mean Reversion Trading Strategy', fontsize=10); <p>Collect detailed backtest performance per timestep of the \"long only\" baseline strategy:</p> In\u00a0[47]: Copied! <pre>backtest_mre_base_eurusd_details = eurusd_backtest_mre_mav_base.strategy.prices.to_frame(name='Rel. EQUITY')\nbacktest_mre_base_eurusd_details['Abs. EQUITY'] = eurusd_backtest_mre_mav_base.strategy.values # equity per timestep\nbacktest_mre_base_eurusd_details['CASH'] = eurusd_backtest_mre_mav_base.strategy.cash # cash per timestep\nbacktest_mre_base_eurusd_details['POSITIONS'] = eurusd_backtest_mre_mav_base.strategy.positions # positions per timestep\nbacktest_mre_base_eurusd_details['FEES'] = eurusd_backtest_mre_mav_base.strategy.fees # fees per timestep\n</pre> backtest_mre_base_eurusd_details = eurusd_backtest_mre_mav_base.strategy.prices.to_frame(name='Rel. EQUITY') backtest_mre_base_eurusd_details['Abs. EQUITY'] = eurusd_backtest_mre_mav_base.strategy.values # equity per timestep backtest_mre_base_eurusd_details['CASH'] = eurusd_backtest_mre_mav_base.strategy.cash # cash per timestep backtest_mre_base_eurusd_details['POSITIONS'] = eurusd_backtest_mre_mav_base.strategy.positions # positions per timestep backtest_mre_base_eurusd_details['FEES'] = eurusd_backtest_mre_mav_base.strategy.fees # fees per timestep <p>Inspect detailed backtest results per timestep:</p> In\u00a0[48]: Copied! <pre>backtest_mre_base_eurusd_details.head(10)\n</pre> backtest_mre_base_eurusd_details.head(10) Out[48]: Rel. EQUITY Abs. EQUITY CASH POSITIONS FEES 2003-12-30 100.000000 1.000000e+06 1000000.000000 0.0 0.0 2003-12-31 100.000000 1.000000e+06 1000000.000000 0.0 0.0 2004-01-01 99.999500 9.999950e+05 0.047577 794786.0 5.0 2004-01-02 99.999500 9.999950e+05 0.047577 794786.0 0.0 2004-01-05 100.834306 1.008343e+06 0.047577 794786.0 0.0 2004-01-06 101.104957 1.011050e+06 0.047577 794786.0 0.0 2004-01-07 100.468483 1.004685e+06 0.047577 794786.0 0.0 2004-01-08 101.533767 1.015338e+06 0.047577 794786.0 0.0 2004-01-09 102.200939 1.022009e+06 0.047577 794786.0 0.0 2004-01-12 101.263391 1.012634e+06 0.047577 794786.0 0.0 In\u00a0[49]: Copied! <pre>plt.rcParams['figure.figsize'] = [15, 10]\nfig = plt.figure()\nax = fig.add_subplot(111)\n\n# plot heatmap of monthly returns generated by the strategy\nax = sns.heatmap(eurusd_backtest_mre_mav_base.stats.return_table, annot=True, cbar=True, vmin=-0.5, vmax=0.5)\n\n# set axis labels\nax.set_xlabel('[month]', fontsize=10)\nax.set_ylabel('[year]', fontsize=10)\n\n# set plot title\nax.set_title('Euro vs. US-Dollar Exchange Rate - Monthly Returns Buy and Hold Baseline', fontsize=10);\n</pre> plt.rcParams['figure.figsize'] = [15, 10] fig = plt.figure() ax = fig.add_subplot(111)  # plot heatmap of monthly returns generated by the strategy ax = sns.heatmap(eurusd_backtest_mre_mav_base.stats.return_table, annot=True, cbar=True, vmin=-0.5, vmax=0.5)  # set axis labels ax.set_xlabel('[month]', fontsize=10) ax.set_ylabel('[year]', fontsize=10)  # set plot title ax.set_title('Euro vs. US-Dollar Exchange Rate - Monthly Returns Buy and Hold Baseline', fontsize=10); <p>Visualize each strategie's backtest equity progression over time:</p> In\u00a0[50]: Copied! <pre>plt.rcParams['figure.figsize'] = [15, 5]\nfig = plt.figure()\nax = fig.add_subplot(111)\n\n# plot equity progression of the distinct trading strategies\nax.plot(backtest_mre_20_eurusd_details['Rel. EQUITY'], color='C2',lw=1.0, label='20-day mean-reversion strategy (red)')\nax.plot(backtest_mre_base_eurusd_details['Rel. EQUITY'], color='C3',lw=1.0, label='Base buy-and-hold strategy (purple)')\n\n# rotate x-tick labels\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(45)\n\n# set axis labels\nax.set_xlabel('[time]', fontsize=10)\nax.set_xlim([start_date, end_date])\nax.set_ylabel('[rel. equity]', fontsize=10)\n\n# set plot legend\nplt.legend(loc=\"upper left\", numpoints=1, fancybox=True)\n\n# set plot title\nplt.title('Euro vs. US-Dollar Exchange Rate - Backtest % Equity Progression', fontsize=10);\n</pre> plt.rcParams['figure.figsize'] = [15, 5] fig = plt.figure() ax = fig.add_subplot(111)  # plot equity progression of the distinct trading strategies ax.plot(backtest_mre_20_eurusd_details['Rel. EQUITY'], color='C2',lw=1.0, label='20-day mean-reversion strategy (red)') ax.plot(backtest_mre_base_eurusd_details['Rel. EQUITY'], color='C3',lw=1.0, label='Base buy-and-hold strategy (purple)')  # rotate x-tick labels for tick in ax.get_xticklabels():     tick.set_rotation(45)  # set axis labels ax.set_xlabel('[time]', fontsize=10) ax.set_xlim([start_date, end_date]) ax.set_ylabel('[rel. equity]', fontsize=10)  # set plot legend plt.legend(loc=\"upper left\", numpoints=1, fancybox=True)  # set plot title plt.title('Euro vs. US-Dollar Exchange Rate - Backtest % Equity Progression', fontsize=10); <p>We recommend you to try the following exercises as part of the lab:</p> <p>1. Evaluation of distinct daily moving average parameters.</p> <p>Evaluate the mean-reversion trading strategy using distinct moving average look-backs, e.g., 10 days, 30 days, 50 days, 200 days and 300 days. Compare the performance of the lookback parametrizations in terms of total-return, equity progression and yearly sharpe-ratio. Gain an intuition about the years in which the strategy didn't perform well and the potential reason for the poor performance.</p> In\u00a0[\u00a0]: Copied! <pre># ***************************************************\n# INSERT YOUR CODE HERE\n# ***************************************************\n</pre> # *************************************************** # INSERT YOUR CODE HERE # *************************************************** <p>2. Evaluation of distinct Z-Score threshold parameters.</p> <p>Evaluate the simple mean-reversion trading strategy using distinct Z-Score thresholds, e.g., $Z$ = 2, 3, 5, 10. Calculate the trading frequency of each parametrization and compare their performance in terms of total-return, equity progression and yearly-sharpe-ratio. Gain an intuition about the years in which the strategy didn't perform well and the potential reason of the poor performance.</p> In\u00a0[\u00a0]: Copied! <pre># ***************************************************\n# INSERT YOUR CODE HERE\n# ***************************************************\n</pre> # *************************************************** # INSERT YOUR CODE HERE # *************************************************** <p>3. Consideration of trading commissions and impact on strategy performance.</p> <p>Run the backtest of the lookback parametrizations evaluated in exercise 1. but change the commission per trade. Set the trading commission of each backtest to (i) 10 USD per trade and (ii) 1% of the price per share. Determine the impact of such a trading fee on the performance of each strategy in terms of total-return, equity progression and yearly sharpe-ratio.</p> In\u00a0[\u00a0]: Copied! <pre># ***************************************************\n# INSERT YOUR CODE HERE\n# ***************************************************\n</pre> # *************************************************** # INSERT YOUR CODE HERE # *************************************************** <p>4. Optimization of the mean-reversion trading strategy parameters.</p> <p>The lab notebook backtest results obtained for the mean-reversion trading strategy reveals that we did identify a well-performing strategy parametrization. However, there is definitely a \"room for improvement\". Grid search the parameter space of the strategy to determine a parametrization that results in a positive total-return within the time interval 31.12.2003 until 31.12.2017.</p> In\u00a0[\u00a0]: Copied! <pre># ***************************************************\n# INSERT YOUR CODE HERE\n# ***************************************************\n</pre> # *************************************************** # INSERT YOUR CODE HERE # *************************************************** <p>In this lab, a step by step implementation and backtest of a basic mean-reversion trading strategy using the Python programming language is presented. The implemented strategy trades a specific foreign exchange rate based on its adjusted closing price trend. The degree of success of the implemented strategy is evaluated based os its backtest results with particular focus on (1) the strategy's total return as well as (2) its equity progression over time. The code provided in this lab provides a blueprint to develop and backtest more complex trading strategies. It furthermore can be tailored to be applied for momentum trading of other financial instruments.</p> <p>You may want to execute the content of your lab outside of the Jupyter notebook environment, e.g. on a compute node or a server. The cell below converts the lab notebook into a standalone and executable python script. Pls. note that to convert the notebook, you need to install Python's nbconvert library and its extensions:</p> In\u00a0[\u00a0]: Copied! <pre># installing the nbconvert library\n!pip3 install nbconvert\n!pip3 install jupyter_contrib_nbextensions\n</pre> # installing the nbconvert library !pip3 install nbconvert !pip3 install jupyter_contrib_nbextensions <p>Let's now convert the Jupyter notebook into a plain Python script:</p> In\u00a0[\u00a0]: Copied! <pre>!jupyter nbconvert --to script cfds_colab_04.ipynb\n</pre> !jupyter nbconvert --to script cfds_colab_04.ipynb"},{"location":"lectures/trading/trading_02/#lab-04-mean-reversion-trading-strategies","title":"Lab 04 - \"Mean Reversion Trading Strategies\"\u00b6","text":"<p>Chartered Financial Data Scientist (CFDS), Spring Term 2020</p>"},{"location":"lectures/trading/trading_02/#lab-objectives","title":"Lab Objectives:\u00b6","text":""},{"location":"lectures/trading/trading_02/#setup-of-the-analysis-environment","title":"Setup of the Analysis Environment\u00b6","text":""},{"location":"lectures/trading/trading_02/#1-acquire-the-financial-data","title":"1. Acquire the Financial Data\u00b6","text":""},{"location":"lectures/trading/trading_02/#2-pre-process-the-financial-data","title":"2. Pre-Process the Financial Data\u00b6","text":""},{"location":"lectures/trading/trading_02/#3-data-analysis-mean-reversion-strategy-implementation","title":"3. Data Analysis - Mean Reversion Strategy Implementation\u00b6","text":""},{"location":"lectures/trading/trading_02/#4-mean-reversion-trading-signal-generation","title":"4. Mean Reversion Trading Signal Generation\u00b6","text":""},{"location":"lectures/trading/trading_02/#5-mean-reversion-signal-backtest","title":"5. Mean Reversion Signal Backtest\u00b6","text":""},{"location":"lectures/trading/trading_02/#lab-exercises","title":"Lab Exercises:\u00b6","text":""},{"location":"lectures/trading/trading_02/#lab-summary","title":"Lab Summary:\u00b6","text":""},{"location":"lectures/trading/trading_intro/","title":"Introducci\u00f3n","text":"<p>El trading es una actividad financiera que consiste en comprar y vender instrumentos financieros, como acciones, divisas, materias primas o criptomonedas, con el objetivo de obtener ganancias a partir de las fluctuaciones de precios en los mercados financieros.</p>"},{"location":"lectures/trading/trading_intro/#tipos-de-trading","title":"Tipos de Trading","text":"<p>Existen diferentes estilos de trading, cada uno con sus propias caracter\u00edsticas y estrategias:</p> <ul> <li> <p>Day Trading: Los day traders compran y venden activos dentro del mismo d\u00eda de negociaci\u00f3n, cerrando todas sus posiciones antes del cierre del mercado. Este estilo de trading requiere estar atento a los movimientos del mercado durante todo el d\u00eda.</p> </li> <li> <p>Swing Trading: Los swing traders mantienen sus posiciones abiertas durante varios d\u00edas o semanas, aprovechando las tendencias a corto y medio plazo en los precios de los activos.</p> </li> <li> <p>Scalping: Los scalpers realizan operaciones r\u00e1pidas y frecuentes, buscando aprovechar peque\u00f1os movimientos de precios en periodos de tiempo muy cortos, como segundos o minutos.</p> </li> </ul>"},{"location":"lectures/trading/trading_intro/#principios-basicos-del-trading","title":"Principios B\u00e1sicos del Trading","text":"<p>Para tener \u00e9xito en el trading, es importante tener en cuenta algunos principios b\u00e1sicos:</p> <ul> <li> <p>Gesti\u00f3n del riesgo: Es fundamental gestionar el riesgo de manera adecuada, limitando las p\u00e9rdidas y protegiendo el capital de inversi\u00f3n. Esto se puede lograr estableciendo stop-loss y gestionando el tama\u00f1o de las posiciones.</p> </li> <li> <p>An\u00e1lisis del mercado: Los traders utilizan an\u00e1lisis t\u00e9cnico, an\u00e1lisis fundamental y an\u00e1lisis del sentimiento del mercado para tomar decisiones de trading informadas. El an\u00e1lisis t\u00e9cnico se centra en el estudio de los gr\u00e1ficos de precios y los indicadores t\u00e9cnicos, mientras que el an\u00e1lisis fundamental se basa en el estudio de los fundamentos econ\u00f3micos y financieros de los activos.</p> </li> <li> <p>Plan de Trading: Un plan de trading bien definido establece los objetivos de inversi\u00f3n, las estrategias de entrada y salida, los criterios de gesti\u00f3n del riesgo y otros aspectos importantes para guiar las decisiones de trading.</p> </li> </ul>"},{"location":"lectures/trading/trading_intro/#plataformas-de-trading","title":"Plataformas de Trading","text":"<p>Existen numerosas plataformas de trading disponibles que ofrecen acceso a los mercados financieros y herramientas de an\u00e1lisis avanzadas. Algunas de las plataformas m\u00e1s populares incluyen:</p> <ul> <li> <p>MetaTrader 4 (MT4): Una plataforma de trading ampliamente utilizada que ofrece funciones avanzadas de an\u00e1lisis t\u00e9cnico, ejecuci\u00f3n de operaciones y gesti\u00f3n de cuentas.</p> </li> <li> <p>Thinkorswim: Una plataforma de trading de TD Ameritrade que ofrece herramientas de an\u00e1lisis t\u00e9cnico y fundamental, as\u00ed como acceso a una amplia gama de productos financieros.</p> </li> <li> <p>TradingView: Una plataforma basada en la web que ofrece gr\u00e1ficos interactivos, herramientas de an\u00e1lisis t\u00e9cnico y la capacidad de compartir ideas de trading con otros usuarios.</p> </li> </ul>"},{"location":"lectures/visualization/vis_01/","title":"Tipos de Gr\u00e1ficos","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n</pre> import matplotlib.pyplot as plt import numpy as np In\u00a0[2]: Copied! <pre># Definir los datos para los ejes x e y\nx = [1, 2, 3, 4, 5] \ny = [10, 12, 15, 20, 22]  \n\n# Crear gr\u00e1fico\nplt.plot(x, y) \nplt.xlabel('Eje x')  \nplt.ylabel('Eje y')  \nplt.title('Gr\u00e1fico de l\u00edneas')  \n\n# Mostrar el gr\u00e1fico \nplt.show()\n</pre> # Definir los datos para los ejes x e y x = [1, 2, 3, 4, 5]  y = [10, 12, 15, 20, 22]    # Crear gr\u00e1fico plt.plot(x, y)  plt.xlabel('Eje x')   plt.ylabel('Eje y')   plt.title('Gr\u00e1fico de l\u00edneas')    # Mostrar el gr\u00e1fico  plt.show()   In\u00a0[3]: Copied! <pre># Definir los datos para los ejes x e y\nx = [1, 2, 3, 4, 5]\ny = [10, 12, 15, 20, 22]\n\n# Crear gr\u00e1fico\nplt.scatter(x, y)\nplt.xlabel('Eje x')\nplt.ylabel('Eje y')\nplt.title('Diagrama de dispersi\u00f3n')\n\n# Mostrar el gr\u00e1fico \nplt.show()\n</pre> # Definir los datos para los ejes x e y x = [1, 2, 3, 4, 5] y = [10, 12, 15, 20, 22]  # Crear gr\u00e1fico plt.scatter(x, y) plt.xlabel('Eje x') plt.ylabel('Eje y') plt.title('Diagrama de dispersi\u00f3n')  # Mostrar el gr\u00e1fico  plt.show() In\u00a0[4]: Copied! <pre># Definir los datos para los ejes x e y\nx = ['A', 'B', 'C', 'D', 'E']\ny = [100, 200, 150, 300, 250]\n\n# Crear gr\u00e1fico\nplt.bar(x, y)\nplt.xlabel('Productos')\nplt.ylabel('Cantidad vendida')\nplt.title('Ventas por producto')\n\n# Mostrar el gr\u00e1fico \nplt.show()\n</pre> # Definir los datos para los ejes x e y x = ['A', 'B', 'C', 'D', 'E'] y = [100, 200, 150, 300, 250]  # Crear gr\u00e1fico plt.bar(x, y) plt.xlabel('Productos') plt.ylabel('Cantidad vendida') plt.title('Ventas por producto')  # Mostrar el gr\u00e1fico  plt.show() In\u00a0[5]: Copied! <pre># Crear un conjunto de datos aleatorios con una distribuci\u00f3n normal\ndata = np.random.randn(10000)\n\n# Crear el histograma con 20 bins y color verde\nplt.hist(data, bins=20,edgecolor='black')\n\n# Agregar etiquetas y t\u00edtulo\nplt.xlabel('Valores')\nplt.ylabel('Frecuencia')\nplt.title('Histograma de valores aleatorios')\n\n# Mostrar el histograma\nplt.show()\n</pre> # Crear un conjunto de datos aleatorios con una distribuci\u00f3n normal data = np.random.randn(10000)  # Crear el histograma con 20 bins y color verde plt.hist(data, bins=20,edgecolor='black')  # Agregar etiquetas y t\u00edtulo plt.xlabel('Valores') plt.ylabel('Frecuencia') plt.title('Histograma de valores aleatorios')  # Mostrar el histograma plt.show() In\u00a0[6]: Copied! <pre># Crear una matriz aleatoria de 5x5\ndata = np.random.rand(5, 5)\n\n# Crear el heatmap con la funci\u00f3n imshow\nplt.imshow(data, cmap='Blues')\n\n# Agregar etiquetas y t\u00edtulo\nplt.colorbar()\nplt.xticks(range(5), ['A', 'B', 'C', 'D', 'E'])\nplt.yticks(range(5), ['1', '2', '3', '4', '5'])\nplt.xlabel('Columnas')\nplt.ylabel('Filas')\nplt.title('Heatmap de una matriz aleatoria')\n\n# Mostrar el heatmap\nplt.show()\n</pre> # Crear una matriz aleatoria de 5x5 data = np.random.rand(5, 5)  # Crear el heatmap con la funci\u00f3n imshow plt.imshow(data, cmap='Blues')  # Agregar etiquetas y t\u00edtulo plt.colorbar() plt.xticks(range(5), ['A', 'B', 'C', 'D', 'E']) plt.yticks(range(5), ['1', '2', '3', '4', '5']) plt.xlabel('Columnas') plt.ylabel('Filas') plt.title('Heatmap de una matriz aleatoria')  # Mostrar el heatmap plt.show() In\u00a0[7]: Copied! <pre># Crear un conjunto de datos aleatorios\ndata = np.random.normal(0, 1, 1000)\n\n# Crear el boxplot\nplt.boxplot(data)\n\n# Agregar etiquetas y t\u00edtulo\nplt.xlabel('Variable X')\nplt.ylabel('Valores')\nplt.title('Boxplot de un conjunto de datos aleatorios')\n\n# Mostrar el boxplot\nplt.show()\n</pre> # Crear un conjunto de datos aleatorios data = np.random.normal(0, 1, 1000)  # Crear el boxplot plt.boxplot(data)  # Agregar etiquetas y t\u00edtulo plt.xlabel('Variable X') plt.ylabel('Valores') plt.title('Boxplot de un conjunto de datos aleatorios')  # Mostrar el boxplot plt.show() <p>Con el diagrama de boxplot y el histograma, podemos analizar de mejor manera la distribuci\u00f3n de nuestra variable.</p> In\u00a0[8]: Copied! <pre># Crear un conjunto de datos aleatorios\ndata = np.random.normal(0, 1, 1000)\n\n# Crear una figura con dos subplots\nfig, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw={\"height_ratios\": (.15, .85)})\n\n# Crear el boxplot\nax_box.boxplot(data, vert=False)\n\n# Crear el histograma\nax_hist.hist(data, bins=10, edgecolor='black')\n\n# Agregar etiquetas y t\u00edtulo\nax_box.set(xlabel='Variable X', ylabel='Valores')\nax_hist.set(xlabel='Variable X', ylabel='Frecuencia')\nplt.suptitle('Boxplot con histograma')\n\n# Mostrar el gr\u00e1fico\nplt.show()\n</pre> # Crear un conjunto de datos aleatorios data = np.random.normal(0, 1, 1000)  # Crear una figura con dos subplots fig, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw={\"height_ratios\": (.15, .85)})  # Crear el boxplot ax_box.boxplot(data, vert=False)  # Crear el histograma ax_hist.hist(data, bins=10, edgecolor='black')  # Agregar etiquetas y t\u00edtulo ax_box.set(xlabel='Variable X', ylabel='Valores') ax_hist.set(xlabel='Variable X', ylabel='Frecuencia') plt.suptitle('Boxplot con histograma')  # Mostrar el gr\u00e1fico plt.show()"},{"location":"lectures/visualization/vis_01/#tipos-de-graficos","title":"Tipos de Gr\u00e1ficos\u00b6","text":""},{"location":"lectures/visualization/vis_01/#introduccion","title":"Introducci\u00f3n\u00b6","text":"<p>En estad\u00edstica, un gr\u00e1fico es una representaci\u00f3n visual de datos que permite resumir, analizar y comunicar informaci\u00f3n de manera efectiva. Los gr\u00e1ficos se utilizan ampliamente en estad\u00edstica para mostrar patrones, tendencias, distribuciones y relaciones entre variables.</p> <p>Algunos de los tipos de gr\u00e1ficos m\u00e1s comunes utilizados en estad\u00edstica incluyen:</p>"},{"location":"lectures/visualization/vis_01/#anatomia-de-un-grafico","title":"Anatom\u00eda de un Gr\u00e1fico\u00b6","text":"<p>La anatom\u00eda de un gr\u00e1fico se refiere a las diferentes partes o componentes que lo componen. En el contexto de la visualizaci\u00f3n de datos, la anatom\u00eda de un gr\u00e1fico incluir\u00eda elementos como:</p> <ol> <li><p>Ejes (Axes): Los ejes son los marcos rectangulares donde se representan los datos. Pueden tener etiquetas en los ejes x (horizontal) y y (vertical).</p> </li> <li><p>T\u00edtulo (Title): Es el texto que describe la informaci\u00f3n que se muestra en el gr\u00e1fico.</p> </li> <li><p>Etiquetas de los ejes (Axis Labels): Son textos que describen qu\u00e9 se est\u00e1 representando en cada eje. Por ejemplo, en un gr\u00e1fico de dispersi\u00f3n, se puede etiquetar el eje x como \"Variable independiente\" y el eje y como \"Variable dependiente\".</p> </li> <li><p>Leyenda (Legend): Es una caja o \u00e1rea que explica el significado de los diferentes elementos en el gr\u00e1fico, como l\u00edneas o colores, especialmente cuando hay m\u00faltiples conjuntos de datos representados.</p> </li> <li><p>Puntos de datos (Data Points): Son los puntos individuales que representan observaciones o valores en el gr\u00e1fico.</p> </li> <li><p>L\u00edneas (Lines): Se utilizan para conectar puntos de datos y representar relaciones entre ellos, como en gr\u00e1ficos de l\u00edneas o de series temporales.</p> </li> <li><p>Barras (Bars): Se utilizan en gr\u00e1ficos de barras para representar la magnitud de diferentes categor\u00edas o variables.</p> </li> <li><p>Cuadr\u00edcula (Grid): Las l\u00edneas de cuadr\u00edcula ayudan a visualizar y comparar los valores en el gr\u00e1fico.</p> </li> <li><p>Fondo (Background): El \u00e1rea que rodea el gr\u00e1fico, generalmente blanco o de otro color s\u00f3lido, que proporciona contraste con los elementos del gr\u00e1fico.</p> </li> </ol>"},{"location":"lectures/visualization/vis_01/#grafico-de-linea","title":"Gr\u00e1fico de L\u00ednea\u00b6","text":"<p>Un gr\u00e1fico de l\u00edneas es un tipo de gr\u00e1fico que utiliza una o varias l\u00edneas para mostrar la relaci\u00f3n entre dos o m\u00e1s variables. Es com\u00fanmente utilizado en estad\u00edstica para representar datos num\u00e9ricos a lo largo de un eje horizontal y vertical, y para observar tendencias o cambios en los datos a lo largo del tiempo o de una escala continua.</p> <p>En un gr\u00e1fico de l\u00edneas, cada punto representa el valor de una variable en un momento determinado. Estos puntos se unen mediante una l\u00ednea, lo que permite visualizar la tendencia general de los datos. Adem\u00e1s, se pueden a\u00f1adir m\u00faltiples l\u00edneas en un mismo gr\u00e1fico para comparar diferentes series de datos y observar las diferencias en su comportamiento a lo largo del tiempo o de una escala continua.</p>"},{"location":"lectures/visualization/vis_01/#grafico-de-dispersion","title":"Gr\u00e1fico de Dispersi\u00f3n\u00b6","text":"<p>Un gr\u00e1fico de dispersi\u00f3n es un tipo de gr\u00e1fico que muestra la relaci\u00f3n entre dos variables num\u00e9ricas. En este tipo de gr\u00e1fico, cada punto en el eje horizontal representa un valor de la primera variable, y cada punto en el eje vertical representa un valor de la segunda variable. Los puntos se distribuyen en el gr\u00e1fico seg\u00fan los valores que tienen en ambas variables.</p> <p>Los gr\u00e1fico de dispersi\u00f3n son \u00fatiles para observar si existe una relaci\u00f3n entre dos variables y c\u00f3mo se relacionan entre s\u00ed. Si los puntos est\u00e1n agrupados de manera que forman una l\u00ednea o una curva, esto sugiere una relaci\u00f3n entre las dos variables. Si los puntos est\u00e1n dispersos aleatoriamente, esto sugiere que no hay una relaci\u00f3n clara entre las dos variables.</p>"},{"location":"lectures/visualization/vis_01/#grafico-de-barras","title":"Gr\u00e1fico de Barras\u00b6","text":"<p>Un gr\u00e1fico de barras es un tipo de gr\u00e1fico utilizado para representar datos num\u00e9ricos mediante barras rectangulares, donde la longitud de cada barra representa la magnitud de la cantidad correspondiente. Este tipo de gr\u00e1fico es \u00fatil para comparar datos entre diferentes categor\u00edas o grupos.</p> <p>En un gr\u00e1fico de barras, cada barra representa una categor\u00eda o grupo, y la altura o longitud de la barra representa el valor num\u00e9rico de la cantidad que se est\u00e1 midiendo. Las barras pueden ser horizontales o verticales, dependiendo de la preferencia del usuario o de la forma en que se ajuste mejor a los datos.</p>"},{"location":"lectures/visualization/vis_01/#grafico-histograma","title":"Gr\u00e1fico: Histograma\u00b6","text":"<p>Un histograma es un tipo de gr\u00e1fico utilizado en estad\u00edstica para representar la distribuci\u00f3n de frecuencia de un conjunto de datos num\u00e9ricos. En un histograma, los datos se dividen en rangos o intervalos (tambi\u00e9n conocidos como \"bins\" o \"clases\"), y se cuenta cu\u00e1ntas veces aparece un valor dentro de cada rango. Estas frecuencias se representan mediante barras rectangulares, donde la altura de cada barra representa la frecuencia de los valores en ese rango.</p> <p>Los histogramas son \u00fatiles para visualizar la distribuci\u00f3n de frecuencias de un conjunto de datos, lo que permite identificar patrones, tendencias y valores at\u00edpicos. Tambi\u00e9n se pueden utilizar para comparar la distribuci\u00f3n de diferentes conjuntos de datos.</p>"},{"location":"lectures/visualization/vis_01/#grafico-heatmap","title":"Gr\u00e1fico: Heatmap\u00b6","text":"<p>Un heatmap (mapa de calor en espa\u00f1ol) es un tipo de gr\u00e1fico que utiliza colores para representar la magnitud de una variable en una matriz o tabla de datos. En un heatmap, cada celda de la matriz se colorea en funci\u00f3n de su valor, de tal manera que los valores m\u00e1s altos se representan con colores m\u00e1s intensos y los valores m\u00e1s bajos con colores m\u00e1s suaves.</p> <p>Los heatmaps son especialmente \u00fatiles para visualizar grandes cantidades de datos en forma de matrices, ya que permiten identificar patrones, tendencias y valores at\u00edpicos de manera m\u00e1s eficiente que si se utilizaran tablas num\u00e9ricas. Por ejemplo, se pueden utilizar para visualizar patrones de tr\u00e1fico en una red de computadoras, o para mostrar la correlaci\u00f3n entre distintas variables en un conjunto de datos.</p>"},{"location":"lectures/visualization/vis_01/#grafico-boxplot","title":"Gr\u00e1fico: Boxplot\u00b6","text":"<p>Un boxplot (tambi\u00e9n conocido como diagrama de caja y bigotes) es una herramienta gr\u00e1fica utilizada para representar la distribuci\u00f3n de un conjunto de datos num\u00e9ricos a trav\u00e9s de su cuartil, valores m\u00ednimos y m\u00e1ximos, y outliers (datos extremos).</p> <p>El boxplot se dibuja a partir de cinco estad\u00edsticas descriptivas de los datos:</p> <ul> <li>El m\u00ednimo (valor m\u00e1s peque\u00f1o)</li> <li>El primer cuartil (Q1) que representa el valor donde el 25% de los datos son menores y el 75% son mayores.</li> <li>La mediana (Q2), que representa el valor donde el 50% de los datos son menores y el 50% son mayores.</li> <li>El tercer cuartil (Q3) que representa el valor donde el 75% de los datos son menores y el 25% son mayores.</li> <li>El m\u00e1ximo (valor m\u00e1s grande).</li> </ul> <p>El boxplot consta de una caja que se extiende desde el primer al tercer cuartil, con una l\u00ednea en la caja que representa la mediana. Los \"bigotes\" se extienden desde la caja hasta los valores m\u00ednimo y m\u00e1ximo, excluyendo los outliers (datos at\u00edpicos), que se grafican como puntos fuera de los bigotes.</p>"},{"location":"lectures/visualization/vis_02/","title":"Visualizaci\u00f3n: Est\u00e1tica","text":"<p>Estructura del gr\u00e1fico</p> <pre>import matplotlib.pyplot as plt\n\nx = [...]  # Definir los valores del eje x\ny = [...]  # Definir los valores del eje y\n\nplt.[tipo_de_grafico](x, y)  # Utilizar la funci\u00f3n correspondiente para crear el gr\u00e1fico deseado\n\nplt.[funcion_de_personalizacion]()  # Utilizar funciones de personalizaci\u00f3n como plt.xlabel(), plt.ylabel(), plt.title(), etc.\n\nplt.show()  # Mostrar el gr\u00e1fico en pantalla\n</pre> <p>Estructura del gr\u00e1fico</p> <pre>import seaborn as sns\n\ndata = [...]  # Definir los datos que se utilizar\u00e1n para trazar el gr\u00e1fico\n\nsns.set_style(\"[estilo]\")  # Configurar el estilo de Seaborn, si se desea (por ejemplo: \"whitegrid\", \"darkgrid\", \"ticks\", etc.)\n\nsns.[funcion_de_grafico](data=data, ...)  # Utilizar la funci\u00f3n correspondiente para crear el gr\u00e1fico deseado y pasar los datos\n\nsns.[funcion_de_personalizacion]()  # Utilizar funciones de personalizaci\u00f3n como sns.xlabel(), sns.ylabel(), sns.title(), etc.\n\nplt.show()  # Mostrar el gr\u00e1fico en pantalla\n</pre> In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nsns.set_style(\"whitegrid\") # Establecer el estilo de Seaborn\n</pre> import numpy as np import pandas as pd  import matplotlib.pyplot as plt import seaborn as sns   sns.set_style(\"whitegrid\") # Establecer el estilo de Seaborn In\u00a0[2]: Copied! <pre># cargar datos\npath = 'data/supermarket_sales.csv'\ndf = pd.read_csv(path, sep=\",\" ).dropna()\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# agrupar los datos para los graficos\ndf_plots = df.groupby(['Date','Product line'])['Quantity'].sum().reset_index().rename(columns = {'Product line':'Product'})\ndf_plots.head()\n</pre> # cargar datos path = 'data/supermarket_sales.csv' df = pd.read_csv(path, sep=\",\" ).dropna() df['Date'] = pd.to_datetime(df['Date'])  # agrupar los datos para los graficos df_plots = df.groupby(['Date','Product line'])['Quantity'].sum().reset_index().rename(columns = {'Product line':'Product'}) df_plots.head() Out[2]: Date Product Quantity 0 2019-01-01 Electronic accessories 18 1 2019-01-01 Fashion accessories 9 2 2019-01-01 Food and beverages 18 3 2019-01-01 Health and beauty 2 4 2019-01-01 Home and lifestyle 8 In\u00a0[3]: Copied! <pre>df_plots.info()\n</pre> df_plots.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 454 entries, 0 to 453\nData columns (total 3 columns):\n #   Column    Non-Null Count  Dtype         \n---  ------    --------------  -----         \n 0   Date      454 non-null    datetime64[ns]\n 1   Product   454 non-null    object        \n 2   Quantity  454 non-null    int64         \ndtypes: datetime64[ns](1), int64(1), object(1)\nmemory usage: 10.8+ KB\n</pre> In\u00a0[4]: Copied! <pre># seleccionar conjunto de datos\ndf_new1 = df_plots.loc[lambda x: (x['Product'] == 'Sports and travel')] \n\n# Crear el gr\u00e1fico de l\u00ednea\nplt.figure(figsize=(12, 4))  # Definir el tama\u00f1o de la figura\nsns.lineplot(\n    data = df_new1,\n    x='Date', \n    y='Quantity', \n    color = 'skyblue'\n)\n\n# Personalizar el gr\u00e1fico\nplt.xlabel('Fecha')\nplt.ylabel('Unidades')\nplt.title('Unidades Totales a lo largo del Tiempo')\n\n# Mostrar el gr\u00e1fico\nplt.show()\n</pre> # seleccionar conjunto de datos df_new1 = df_plots.loc[lambda x: (x['Product'] == 'Sports and travel')]   # Crear el gr\u00e1fico de l\u00ednea plt.figure(figsize=(12, 4))  # Definir el tama\u00f1o de la figura sns.lineplot(     data = df_new1,     x='Date',      y='Quantity',      color = 'skyblue' )  # Personalizar el gr\u00e1fico plt.xlabel('Fecha') plt.ylabel('Unidades') plt.title('Unidades Totales a lo largo del Tiempo')  # Mostrar el gr\u00e1fico plt.show() In\u00a0[5]: Copied! <pre># seleccionar conjunto de datos\ndf_new2 = df_plots.copy()\n\n# Crear el gr\u00e1fico de l\u00ednea\nplt.figure(figsize=(12, 5))  # Definir el tama\u00f1o de la figura\nsns.lineplot(\n    data = df_new2,\n    x='Date', \n    y='Quantity',\n    hue='Product',\n    palette = \"mako\"\n)\n\n# Personalizar el gr\u00e1fico\nplt.xlabel('Fecha')\nplt.ylabel('Unidades')\nplt.title('Unidades Totales a lo largo del Tiempo (por Producto)')\n\n# Mover la leyenda fuera del gr\u00e1fico y colocarla a la derecha\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n\n# Mostrar el gr\u00e1fico\nplt.show()\n</pre> # seleccionar conjunto de datos df_new2 = df_plots.copy()  # Crear el gr\u00e1fico de l\u00ednea plt.figure(figsize=(12, 5))  # Definir el tama\u00f1o de la figura sns.lineplot(     data = df_new2,     x='Date',      y='Quantity',     hue='Product',     palette = \"mako\" )  # Personalizar el gr\u00e1fico plt.xlabel('Fecha') plt.ylabel('Unidades') plt.title('Unidades Totales a lo largo del Tiempo (por Producto)')  # Mover la leyenda fuera del gr\u00e1fico y colocarla a la derecha plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')  # Mostrar el gr\u00e1fico plt.show() In\u00a0[6]: Copied! <pre># Crear el gr\u00e1fico de l\u00ednea\nplt.figure(figsize=(12, 4))  # Definir el tama\u00f1o de la figura\nsns.scatterplot(\n    data = df_new1,\n    x='Date', \n    y='Quantity', \n    color = 'skyblue'\n)\n\n# Personalizar el gr\u00e1fico\nplt.xlabel('Fecha')\nplt.ylabel('Unidades')\nplt.title('Unidades Totales a lo largo del Tiempo')\n\n# Mostrar el gr\u00e1fico\nplt.show()\n</pre> # Crear el gr\u00e1fico de l\u00ednea plt.figure(figsize=(12, 4))  # Definir el tama\u00f1o de la figura sns.scatterplot(     data = df_new1,     x='Date',      y='Quantity',      color = 'skyblue' )  # Personalizar el gr\u00e1fico plt.xlabel('Fecha') plt.ylabel('Unidades') plt.title('Unidades Totales a lo largo del Tiempo')  # Mostrar el gr\u00e1fico plt.show() In\u00a0[7]: Copied! <pre># Crear el gr\u00e1fico de l\u00ednea\nplt.figure(figsize=(12, 5))  # Definir el tama\u00f1o de la figura\nsns.scatterplot(\n    data = df_new2,\n    x='Date', \n    y='Quantity',\n    hue='Product',\n    palette = \"mako\"\n)\n\n# Personalizar el gr\u00e1fico\nplt.xlabel('Fecha')\nplt.ylabel('Unidades')\nplt.title('Unidades Totales a lo largo del Tiempo (por Producto)')\n\n# Mover la leyenda fuera del gr\u00e1fico y colocarla a la derecha\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n\n# Mostrar el gr\u00e1fico\nplt.show()\n</pre> # Crear el gr\u00e1fico de l\u00ednea plt.figure(figsize=(12, 5))  # Definir el tama\u00f1o de la figura sns.scatterplot(     data = df_new2,     x='Date',      y='Quantity',     hue='Product',     palette = \"mako\" )  # Personalizar el gr\u00e1fico plt.xlabel('Fecha') plt.ylabel('Unidades') plt.title('Unidades Totales a lo largo del Tiempo (por Producto)')  # Mover la leyenda fuera del gr\u00e1fico y colocarla a la derecha plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')  # Mostrar el gr\u00e1fico plt.show() In\u00a0[8]: Copied! <pre># seleccionar conjunto de datos\ndf_plots['year'] = df_plots['Date'].dt.year\ndf_plots['month'] = df_plots['Date'].dt.month\ndf_plots['day'] =df_plots['Date'].dt.day\n\n# seleccionar conjunto de datos\ndf_bar = df_plots.groupby(['month','Product'])['Quantity'].sum().reset_index()\n\ndf_bar.head()\n</pre> # seleccionar conjunto de datos df_plots['year'] = df_plots['Date'].dt.year df_plots['month'] = df_plots['Date'].dt.month df_plots['day'] =df_plots['Date'].dt.day  # seleccionar conjunto de datos df_bar = df_plots.groupby(['month','Product'])['Quantity'].sum().reset_index()  df_bar.head() Out[8]: month Product Quantity 0 1 Electronic accessories 333 1 1 Fashion accessories 336 2 1 Food and beverages 325 3 1 Health and beauty 254 4 1 Home and lifestyle 342 In\u00a0[9]: Copied! <pre># seleccionar conjunto de datos\ndf_new1 = df_bar.loc[lambda x: (x['Product'] == 'Sports and travel')] \ndf_new2 = df_bar.copy()\n</pre> # seleccionar conjunto de datos df_new1 = df_bar.loc[lambda x: (x['Product'] == 'Sports and travel')]  df_new2 = df_bar.copy() In\u00a0[10]: Copied! <pre># Crear el gr\u00e1fico de barras\nplt.figure(figsize=(6, 4))\nsns.barplot(x='month', y='Quantity', data=df_new1, color = 'skyblue')\n\n# A\u00f1adir t\u00edtulo y etiquetas de los ejes\nplt.xlabel('Fecha')\nplt.ylabel('Unidades')\nplt.title('Unidades Totales por Mes')\n\n# Rotar los ticks del eje x para mejorar la legibilidad\nplt.xticks(rotation=0)\n\n# Mostrar el gr\u00e1fico\nplt.tight_layout()\nplt.show()\n</pre> # Crear el gr\u00e1fico de barras plt.figure(figsize=(6, 4)) sns.barplot(x='month', y='Quantity', data=df_new1, color = 'skyblue')  # A\u00f1adir t\u00edtulo y etiquetas de los ejes plt.xlabel('Fecha') plt.ylabel('Unidades') plt.title('Unidades Totales por Mes')  # Rotar los ticks del eje x para mejorar la legibilidad plt.xticks(rotation=0)  # Mostrar el gr\u00e1fico plt.tight_layout() plt.show() In\u00a0[11]: Copied! <pre># Crear el gr\u00e1fico de barras\nplt.figure(figsize=(6, 4))\nsns.barplot(x='Quantity', y='month', data=df_new1, color = 'skyblue',orient='h')\n\n# A\u00f1adir t\u00edtulo y etiquetas de los ejes\nplt.xlabel('Fecha')\nplt.ylabel('Unidades')\nplt.title('Unidades Totales por Mes')\n\n\n# Rotar los ticks del eje x para mejorar la legibilidad\nplt.xticks(rotation=0)\n\n# Mostrar el gr\u00e1fico\nplt.tight_layout()\nplt.show()\n</pre> # Crear el gr\u00e1fico de barras plt.figure(figsize=(6, 4)) sns.barplot(x='Quantity', y='month', data=df_new1, color = 'skyblue',orient='h')  # A\u00f1adir t\u00edtulo y etiquetas de los ejes plt.xlabel('Fecha') plt.ylabel('Unidades') plt.title('Unidades Totales por Mes')   # Rotar los ticks del eje x para mejorar la legibilidad plt.xticks(rotation=0)  # Mostrar el gr\u00e1fico plt.tight_layout() plt.show() In\u00a0[12]: Copied! <pre># Crear el gr\u00e1fico de barras\nplt.figure(figsize=(10, 4))\nsns.barplot(x='month', y='Quantity', data=df_new2, hue = 'Product', palette = \"Blues\")\n\n# A\u00f1adir t\u00edtulo y etiquetas de los ejes\nplt.title('Unidades Totales por Mes (por Producto)')\nplt.xlabel('Fecha')\nplt.ylabel('Unidades')\n\n# Rotar los ticks del eje x para mejorar la legibilidad\nplt.xticks(rotation=0)\n\n# Mover la leyenda fuera del gr\u00e1fico y colocarla a la derecha\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n\n# Mostrar el gr\u00e1fico\nplt.tight_layout()\nplt.show()\n</pre> # Crear el gr\u00e1fico de barras plt.figure(figsize=(10, 4)) sns.barplot(x='month', y='Quantity', data=df_new2, hue = 'Product', palette = \"Blues\")  # A\u00f1adir t\u00edtulo y etiquetas de los ejes plt.title('Unidades Totales por Mes (por Producto)') plt.xlabel('Fecha') plt.ylabel('Unidades')  # Rotar los ticks del eje x para mejorar la legibilidad plt.xticks(rotation=0)  # Mover la leyenda fuera del gr\u00e1fico y colocarla a la derecha plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')  # Mostrar el gr\u00e1fico plt.tight_layout() plt.show() In\u00a0[13]: Copied! <pre># Crear el gr\u00e1fico de barras\nplt.figure(figsize=(12, 6))\nsns.barplot(x='Quantity', y='month', data=df_new2, hue = 'Product', palette = \"Blues\", orient = 'h')\n\n# A\u00f1adir t\u00edtulo y etiquetas de los ejes\nplt.title('Unidades Totales por Mes (por Producto)')\nplt.xlabel('Unidades')\nplt.ylabel('Fecha')\n\n# Rotar los ticks del eje x para mejorar la legibilidad\nplt.xticks(rotation=0)\n\n# Mover la leyenda fuera del gr\u00e1fico y colocarla a la derecha\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n\n# Mostrar el gr\u00e1fico\nplt.tight_layout()\nplt.show()\n</pre> # Crear el gr\u00e1fico de barras plt.figure(figsize=(12, 6)) sns.barplot(x='Quantity', y='month', data=df_new2, hue = 'Product', palette = \"Blues\", orient = 'h')  # A\u00f1adir t\u00edtulo y etiquetas de los ejes plt.title('Unidades Totales por Mes (por Producto)') plt.xlabel('Unidades') plt.ylabel('Fecha')  # Rotar los ticks del eje x para mejorar la legibilidad plt.xticks(rotation=0)  # Mover la leyenda fuera del gr\u00e1fico y colocarla a la derecha plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')  # Mostrar el gr\u00e1fico plt.tight_layout() plt.show() In\u00a0[14]: Copied! <pre># seleccionar conjunto de datos\ndf_new1 = df_plots.loc[lambda x: (x['Product'] == 'Sports and travel')] \n\n# Crear el gr\u00e1fico de l\u00ednea\nplt.figure(figsize=(12, 4))  # Definir el tama\u00f1o de la figura\nsns.histplot(\n    data = df_new1,\n    x='Quantity', \n    color = 'skyblue'\n)\n\n# Personalizar el gr\u00e1fico\nplt.xlabel('Unidades')\nplt.ylabel('')\nplt.title('Unidades Totales')\n\n# Mostrar el gr\u00e1fico\nplt.show()\n</pre> # seleccionar conjunto de datos df_new1 = df_plots.loc[lambda x: (x['Product'] == 'Sports and travel')]   # Crear el gr\u00e1fico de l\u00ednea plt.figure(figsize=(12, 4))  # Definir el tama\u00f1o de la figura sns.histplot(     data = df_new1,     x='Quantity',      color = 'skyblue' )  # Personalizar el gr\u00e1fico plt.xlabel('Unidades') plt.ylabel('') plt.title('Unidades Totales')  # Mostrar el gr\u00e1fico plt.show() In\u00a0[15]: Copied! <pre># seleccionar conjunto de datos\ndf_new2 = df_plots.copy()\n\n# Crear el gr\u00e1fico de l\u00ednea\nplt.figure(figsize=(12, 4))  # Definir el tama\u00f1o de la figura\nsns.histplot(\n    data = df_new2,\n    x='Quantity', \n    color = 'skyblue',\n    hue='Product',\n    palette = \"mako\"\n)\n\n# Personalizar el gr\u00e1fico\nplt.xlabel('Unidades')\nplt.ylabel('')\nplt.title('Unidades Totales por Producto')\n\n# Mostrar el gr\u00e1fico\nplt.show()\n</pre> # seleccionar conjunto de datos df_new2 = df_plots.copy()  # Crear el gr\u00e1fico de l\u00ednea plt.figure(figsize=(12, 4))  # Definir el tama\u00f1o de la figura sns.histplot(     data = df_new2,     x='Quantity',      color = 'skyblue',     hue='Product',     palette = \"mako\" )  # Personalizar el gr\u00e1fico plt.xlabel('Unidades') plt.ylabel('') plt.title('Unidades Totales por Producto')  # Mostrar el gr\u00e1fico plt.show() In\u00a0[16]: Copied! <pre># seleccionar conjunto de datos\npivot_df = df_plots.pivot(\n    index='Date',\n    columns='Product',\n    values='Quantity'\n    ).fillna(0).reset_index()\n\npivot_df.head()\n</pre> # seleccionar conjunto de datos pivot_df = df_plots.pivot(     index='Date',     columns='Product',     values='Quantity'     ).fillna(0).reset_index()  pivot_df.head() Out[16]: Product Date Electronic accessories Fashion accessories Food and beverages Health and beauty Home and lifestyle Sports and travel 0 2019-01-01 18.0 9.0 18.0 2.0 8.0 26.0 1 2019-01-02 6.0 11.0 6.0 10.0 0.0 15.0 2 2019-01-03 0.0 0.0 0.0 25.0 11.0 1.0 3 2019-01-04 10.0 4.0 0.0 5.0 10.0 3.0 4 2019-01-05 7.0 11.0 1.0 14.0 22.0 0.0 In\u00a0[17]: Copied! <pre># Calcular la matriz de correlaci\u00f3n\ndf_new = pivot_df.drop(['Date'],axis =1).dropna()\ncorrelation_matrix = df_new.corr()\ncorrelation_matrix\n</pre> # Calcular la matriz de correlaci\u00f3n df_new = pivot_df.drop(['Date'],axis =1).dropna() correlation_matrix = df_new.corr() correlation_matrix Out[17]: Product Electronic accessories Fashion accessories Food and beverages Health and beauty Home and lifestyle Sports and travel Product Electronic accessories 1.000000 0.205911 0.019437 0.062587 -0.003436 0.066047 Fashion accessories 0.205911 1.000000 0.030850 -0.035541 -0.033227 0.144063 Food and beverages 0.019437 0.030850 1.000000 0.058185 -0.016639 0.017786 Health and beauty 0.062587 -0.035541 0.058185 1.000000 0.048463 -0.287534 Home and lifestyle -0.003436 -0.033227 -0.016639 0.048463 1.000000 0.152806 Sports and travel 0.066047 0.144063 0.017786 -0.287534 0.152806 1.000000 In\u00a0[18]: Copied! <pre># Crear el heatmap de la matriz de correlaci\u00f3n\nplt.figure(figsize=(8, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='Blues', fmt=\".2f\")\n\n# A\u00f1adir t\u00edtulo\nplt.title('Matriz de Correlaci\u00f3n')\n\n# Rotar las etiquetas del eje x para una mejor legibilidad\nplt.xticks(rotation=45)\n\n# Mostrar el heatmap\nplt.show()\n</pre> # Crear el heatmap de la matriz de correlaci\u00f3n plt.figure(figsize=(8, 6)) sns.heatmap(correlation_matrix, annot=True, cmap='Blues', fmt=\".2f\")  # A\u00f1adir t\u00edtulo plt.title('Matriz de Correlaci\u00f3n')  # Rotar las etiquetas del eje x para una mejor legibilidad plt.xticks(rotation=45)  # Mostrar el heatmap plt.show() In\u00a0[19]: Copied! <pre># Crear el gr\u00e1fico de diagrama de caja\nplt.figure(figsize=(13, 5))  # Definir el tama\u00f1o de la figura\nsns.boxplot(data=df_new, color='skyblue')\n\n# Personalizar el gr\u00e1fico\nplt.xlabel('')\nplt.ylabel('Valores')\nplt.title('Diagrama de Caja para Todas las Columnas')\n\n# Rotar las etiquetas del eje x para una mejor legibilidad\nplt.xticks(rotation=45)\n\n# Mostrar el gr\u00e1fico\nplt.show()\n</pre> # Crear el gr\u00e1fico de diagrama de caja plt.figure(figsize=(13, 5))  # Definir el tama\u00f1o de la figura sns.boxplot(data=df_new, color='skyblue')  # Personalizar el gr\u00e1fico plt.xlabel('') plt.ylabel('Valores') plt.title('Diagrama de Caja para Todas las Columnas')  # Rotar las etiquetas del eje x para una mejor legibilidad plt.xticks(rotation=45)  # Mostrar el gr\u00e1fico plt.show() In\u00a0[20]: Copied! <pre># Crear el gr\u00e1fico de diagrama de caja\nplt.figure(figsize=(10, 5))  # Definir el tama\u00f1o de la figura\nsns.boxplot(data=df_new, color='skyblue',orient='h')\n\n# Personalizar el gr\u00e1fico\nplt.xlabel('')\nplt.ylabel('Valores')\nplt.title('Diagrama de Caja para Todas las Columnas')\n\n# Rotar las etiquetas del eje x para una mejor legibilidad\nplt.xticks(rotation=0)\n\n# Mostrar el gr\u00e1fico\nplt.show()\n</pre> # Crear el gr\u00e1fico de diagrama de caja plt.figure(figsize=(10, 5))  # Definir el tama\u00f1o de la figura sns.boxplot(data=df_new, color='skyblue',orient='h')  # Personalizar el gr\u00e1fico plt.xlabel('') plt.ylabel('Valores') plt.title('Diagrama de Caja para Todas las Columnas')  # Rotar las etiquetas del eje x para una mejor legibilidad plt.xticks(rotation=0)  # Mostrar el gr\u00e1fico plt.show()"},{"location":"lectures/visualization/vis_02/#visualizacion-estatica","title":"Visualizaci\u00f3n: Est\u00e1tica\u00b6","text":""},{"location":"lectures/visualization/vis_02/#introduccion","title":"Introducci\u00f3n\u00b6","text":""},{"location":"lectures/visualization/vis_02/#acerca-de-matplotlib","title":"Acerca de Matplotlib\u00b6","text":"<p>Matplotlib es una biblioteca para la generaci\u00f3n de gr\u00e1ficos a partir de datos contenidos en listas o arrays en el lenguaje de programaci\u00f3n Python y su extensi\u00f3n matem\u00e1tica NumPy. Proporciona una API, pylab, dise\u00f1ada para recordar a la de MATLAB.</p>"},{"location":"lectures/visualization/vis_02/#acerca-de-seaborn","title":"Acerca de Seaborn\u00b6","text":"<p>Seaborn proporciona una API sobre Matplotlib que ofrece opciones sensatas para el estilo de trazado y los valores predeterminados de color, define funciones simples de alto nivel para tipos de trazado estad\u00edsticos comunes, y se integra con la funcionalidad proporcionada por Pandas DataFrames.</p>"},{"location":"lectures/visualization/vis_02/#diferencias","title":"Diferencias\u00b6","text":"Aspecto Matplotlib Seaborn Facilidad de uso Bajo Alto Estilo y est\u00e9tica Requiere ajustes manuales Ofrece estilos predefinidos Tipos de gr\u00e1ficos Amplia variedad, incluyendo 3D Especializado en gr\u00e1ficos estad\u00edsticos Interactividad Est\u00e1ticos por defecto Est\u00e1ticos por defecto"},{"location":"lectures/visualization/vis_02/#grafico-a-grafico","title":"Gr\u00e1fico a Gr\u00e1fico\u00b6","text":"<p>\ud83d\udcc8 Ejemplo: El conjunto de datos es uno de los registros de ventas hist\u00f3ricas de una empresa de supermercados que se ha registrado en 3 sucursales diferentes durante 3 meses de datos. Los m\u00e9todos de an\u00e1lisis predictivo de datos son f\u00e1ciles de aplicar con este conjunto de datos.</p> <p></p> <p>\ud83d\udccbDescripci\u00f3n de las columnas</p> <p>\u00a1Entendido! Aqu\u00ed tienes el nombre de las columnas en ingl\u00e9s con las descripciones en espa\u00f1ol:</p> Columna Descripci\u00f3n Invoice ID N\u00famero de identificaci\u00f3n generado por computadora para el recibo de venta. Branch Sucursal del supercentro (hay 3 sucursales disponibles identificadas por A, B y C). City Ubicaci\u00f3n de los supercentros. Customer type Tipo de cliente, registrado por Miembros para clientes que usan tarjeta de membres\u00eda y Normal para los que no la usan. Gender G\u00e9nero del cliente. Product line Grupos de categorizaci\u00f3n general de art\u00edculos: Accesorios electr\u00f3nicos, Accesorios de moda, Alimentos y bebidas, Belleza y salud, Hogar y estilo de vida, Deportes y viajes. Unit price Precio de cada producto en $. Quantity N\u00famero de productos comprados por el cliente. Tax Tarifa de impuesto del 5% para los clientes que compran. Total Precio total incluyendo impuestos. Date Fecha de compra (registros disponibles de enero de 2019 a marzo de 2019). Time Hora de compra (de 10 a.m. a 9 p.m.). Payment M\u00e9todo de pago utilizado por el cliente para la compra (hay 3 m\u00e9todos disponibles: Efectivo, Tarjeta de cr\u00e9dito y Billetera electr\u00f3nica). COGS Costo de bienes vendidos. Gross margin percentage Porcentaje de margen bruto. Gross income Ingreso bruto. Rating Calificaci\u00f3n de estratificaci\u00f3n del cliente sobre su experiencia de compra en general (en una escala de 1 a 10)."},{"location":"lectures/visualization/vis_02/#graficos-de-lineas","title":"Gr\u00e1ficos de L\u00edneas\u00b6","text":""},{"location":"lectures/visualization/vis_02/#graficos-de-dispersion","title":"Gr\u00e1ficos de Dispersi\u00f3n\u00b6","text":""},{"location":"lectures/visualization/vis_02/#grafico-de-barras","title":"Gr\u00e1fico de Barras\u00b6","text":""},{"location":"lectures/visualization/vis_02/#grafico-histograma","title":"Gr\u00e1fico: Histograma\u00b6","text":""},{"location":"lectures/visualization/vis_02/#grafico-heatmap","title":"Gr\u00e1fico: Heatmap\u00b6","text":""},{"location":"lectures/visualization/vis_02/#grafico-boxplot","title":"Gr\u00e1fico: Boxplot\u00b6","text":""},{"location":"lectures/visualization/vis_intro/","title":"Introducci\u00f3n","text":"<p>La visualizaci\u00f3n de datos es una herramienta poderosa en el campo de la  ciencia de datos y el an\u00e1lisis de datos que permite representar informaci\u00f3n de manera gr\u00e1fica y comprensible. A trav\u00e9s de gr\u00e1ficos, diagramas  y otras representaciones visuales, la visualizaci\u00f3n de datos ayuda a identificar patrones, tendencias y relaciones en los datos, facilitando la interpretaci\u00f3n y  la toma de decisiones basadas en evidencia.</p>"},{"location":"lectures/visualization/vis_intro/#motivacion","title":"Motivaci\u00f3n","text":"<p>Aprender sobre visualizaci\u00f3n es importante por varias razones:</p> <p>Comunicar informaci\u00f3n compleja de manera clara y efectiva</p> <p>Al presentar datos de una manera visual, es m\u00e1s f\u00e1cil identificar patrones y tendencias, as\u00ed como tambi\u00e9n hacer comparaciones y contrastes. Esto es especialmente importante cuando se trabaja con grandes conjuntos de datos o cuando se trata de presentar informaci\u00f3n a un p\u00fablico diverso.</p> <p>Descubrir informaci\u00f3n oculta o desconocida</p> <p>A menudo, los datos pueden contener patrones o relaciones que no son obvios a simple vista, pero que pueden ser descubiertos mediante la exploraci\u00f3n y la visualizaci\u00f3n. La visualizaci\u00f3n tambi\u00e9n puede ayudar a identificar errores y anomal\u00edas en los datos, lo que puede ser importante para la toma de decisiones y la planificaci\u00f3n.</p> <p>Mejorar la capacidad de an\u00e1lisis de datos</p> <p>Al comprender c\u00f3mo presentar datos de manera efectiva, se puede desarrollar una mejor comprensi\u00f3n de los datos y las relaciones que existen entre ellos. Esto puede ayudar a tomar decisiones informadas basadas en datos y a identificar tendencias y oportunidades que de otra manera podr\u00edan haber pasado desapercibidas.</p>"},{"location":"lectures/visualization/vis_intro/#malos-graficos","title":"Malos Gr\u00e1ficos","text":""},{"location":"lectures/visualization/vis_intro/#buenos-graficos","title":"Buenos Gr\u00e1ficos","text":""},{"location":"lectures/visualization/vis_intro/#primeras-visualizaciones","title":"Primeras visualizaciones","text":"<p>Campa\u00f1a de Napole\u00f3n a Mosc\u00fa (Charles Minard, 1889)</p> <p>Gr\u00e1fico que muestra el n\u00famero de las fuerzas francesas en su marcha hacia Mosc\u00fa y durante la retirada, por Charles Minard. Tambi\u00e9n contiene informaci\u00f3n ambiental como la temperatura por fecha.</p> <p></p> <p>Mapa del c\u00f3lera (John Snow, 1855)</p> <p>Gr\u00e1fico que muestra los casos de c\u00f3lera durante la epidemia en Londres de 1854 y Las cruces la ubicaci\u00f3n de las bombas de agua.</p> <p></p>"},{"location":"lectures/visualization/vis_intro/#por-que-utilizar-graficos","title":"\u00bfPor qu\u00e9 utilizar gr\u00e1ficos?","text":"<ul> <li>El 70 % de los receptores sensoriales del cuerpo humano est\u00e1 dedicado a la visi\u00f3n.</li> <li> <p>Cerebro ha sido entrenado evolutivamente para interpretar la informaci\u00f3n visual de manera masiva.</p> <p>\u201cThe eye and the visual cortex of the brain form a massively parallel processor that provides the highest bandwidth channel into human cognitive centers\u201d \u2014 Colin Ware, Information Visualization, 2004.</p> </li> </ul>"},{"location":"lectures/visualization/vis_intro/#cuarteto-de-anscombe","title":"Cuarteto de ANSCOMBE","text":"<p>El Cuarteto de Anscombe es un conjunto de cuatro conjuntos de datos que tienen las mismas estad\u00edsticas descriptivas (medias, varianzas, correlaciones y regresiones), pero que se ven muy diferentes cuando se visualizan. Fueron presentados por el estad\u00edstico Francis Anscombe en 1973 para demostrar la importancia de la visualizaci\u00f3n en el an\u00e1lisis de datos.</p> <p>Los cuatro conjuntos de datos consisten en pares de variables x e y, y cada conjunto representa un tipo diferente de relaci\u00f3n entre las variables. A simple vista, los cuatro conjuntos parecen tener distribuciones y relaciones completamente diferentes entre s\u00ed, pero cuando se analizan las estad\u00edsticas descriptivas, todas son id\u00e9nticas.</p> <p>Estad\u00edsticos</p> <p></p> <p>Gr\u00e1ficos</p> <p></p>"},{"location":"lectures/visualization/vis_intro/#teoria-de-visualizacion","title":"Teor\u00eda de visualizaci\u00f3n","text":"<p>La teor\u00eda de visualizaci\u00f3n se refiere a la investigaci\u00f3n y el estudio de c\u00f3mo las personas procesan, interpretan y comprenden informaci\u00f3n visual. La visualizaci\u00f3n puede involucrar cualquier tipo de informaci\u00f3n que pueda ser representada visualmente, incluyendo gr\u00e1ficos, diagramas, mapas, fotograf\u00edas y videos.</p> <p>Algunos de los conceptos y principios importantes en la teor\u00eda de visualizaci\u00f3n incluyen:</p> <ul> <li> <p>Percepci\u00f3n visual: C\u00f3mo procesamos y entendemos la informaci\u00f3n visual a trav\u00e9s de nuestros sentidos.</p> </li> <li> <p>Cognici\u00f3n visual: C\u00f3mo procesamos y entendemos la informaci\u00f3n visual a trav\u00e9s de nuestros procesos mentales, como la atenci\u00f3n, la memoria y la toma de decisiones.</p> </li> <li> <p>Dise\u00f1o visual: C\u00f3mo se pueden crear visualizaciones efectivas y atractivas para comunicar informaci\u00f3n de manera clara y efectiva.</p> </li> <li> <p>Interactividad visual: C\u00f3mo las visualizaciones interactivas pueden ayudar a los usuarios a explorar y comprender mejor la informaci\u00f3n visual.</p> </li> </ul>"},{"location":"lectures/visualization/vis_intro/#consejos-generales","title":"Consejos generales","text":"<p>Noah Iliinsky es un experto en visualizaci\u00f3n de datos y ha identificado cuatro pilares fundamentales de la visualizaci\u00f3n. </p> <p>Estos pilares son:</p> <ul> <li> <p>Contenido: El contenido se refiere a la informaci\u00f3n que se est\u00e1 visualizando. Para que la visualizaci\u00f3n sea efectiva, es importante tener una comprensi\u00f3n clara del contenido y c\u00f3mo se relaciona con el objetivo de la visualizaci\u00f3n.</p> </li> <li> <p>Funci\u00f3n: La funci\u00f3n se refiere al prop\u00f3sito de la visualizaci\u00f3n. \u00bfQu\u00e9 se espera que haga la visualizaci\u00f3n? \u00bfDebe mostrar una tendencia, comparar datos o explorar patrones? Es importante tener en cuenta la funci\u00f3n de la visualizaci\u00f3n para asegurarse de que se est\u00e1 dise\u00f1ando de manera efectiva.</p> </li> <li> <p>Forma: La forma se refiere a la apariencia visual de la visualizaci\u00f3n. Esto incluye cosas como el tipo de gr\u00e1fico o diagrama utilizado, la paleta de colores y la tipograf\u00eda. La forma debe ser coherente y legible para que la visualizaci\u00f3n sea f\u00e1cil de entender.</p> </li> <li> <p>Audiencia: La audiencia se refiere a las personas que ver\u00e1n la visualizaci\u00f3n. La comprensi\u00f3n de la audiencia es esencial para determinar el nivel de detalle y complejidad adecuados para la visualizaci\u00f3n. La visualizaci\u00f3n debe ser accesible y comprensible para su audiencia objetivo.</p> </li> </ul> <p>\ud83d\udd11 Nota: Se recomienda ver el siguiente video para profundizar estos conceptos</p>"},{"location":"lectures/visualization/vis_intro/#honestidad","title":"Honestidad","text":"<p>El ojo humano no tiene la misma precisi\u00f3n al estimar distintas atribuciones:</p> <ul> <li>Largo: Bien estimado y sin sesgo, con un factor multiplicativo de 0.9 a 1.1.</li> <li>\u00c1rea: Subestimado y con sesgo, con un factor multiplicativo de 0.6 a 0.9.</li> <li>Volumen: Muy subestimado y con sesgo, con un factor multiplicativo de 0.5 a 0.8.</li> </ul> <p>Resulta inadecuado realizar gr\u00e1ficos de datos utilizando \u00e1reas o vol\u00famenes si no queda claro la atribuci\u00f3n utilizada.</p> <p></p> <p>Una pseudo-excepci\u00f3n la constituyen los pie-chart o gr\u00e1ficos circulares, porque el ojo humano distingue bien \u00e1ngulos y segmentos de c\u00edrculo, y porque es posible indicar los porcentajes respectivos.</p> <p></p>"},{"location":"lectures/visualization/vis_intro/#percepcion","title":"Percepci\u00f3n","text":"<p>No todos los elementos tienen la misma percepci\u00f3n a nivel del sistema visual. En particular, el color y la forma son elementos preatentivos: un color distinto o una forma distinta se reconocen de manera no conciente.</p> <p></p> <p>El sistema visual humano puede estimar con precisi\u00f3n siguientes atributos visuales:</p> <ol> <li>Posici\u00f3n</li> <li>Largo</li> <li>Pendiente</li> <li>\u00c1ngulo</li> <li>\u00c1rea</li> <li>Volumen</li> <li>Color</li> </ol>"},{"location":"lectures/visualization/vis_intro/#colormaps","title":"Colormaps","text":"<p>Puesto que la percepci\u00f3n del color tiene muy baja precisi\u00f3n, resulta inadecuado tratar de representar un valor num\u00e9rico con colores. * \u00bfQu\u00e9 diferencia num\u00e9rica existe entre el verde y el rojo? * \u00bfQue asociaci\u00f3n preexistente posee el color rojo, el amarillo y el verde? * \u00bfCon cu\u00e1nta precisi\u00f3n podemos distinguir valores en una escala de grises?</p> <p></p>"},{"location":"lectures/visualization/vis_intro/#python-landscape","title":"Python Landscape","text":"<p>Para empezar, PyViz es un sitio web que se dedica a ayudar a los usuarios a decidir dentro de las mejores herramientas de visualizaci\u00f3n open-source implementadas en Python, dependiendo de sus necesidades y objetivos. Mucho de lo que se menciona en esta secci\u00f3n est\u00e1 en detalle en la p\u00e1gina web del proyecto PyViz.</p> <p>Algunas de las librer\u00edas de visualizaci\u00f3n de Python m\u00e1s conocidas son:</p> <p></p> <p>Este esquema es una adaptaci\u00f3n de uno presentado en la charla The Python Visualization Landscape realizada por Jake VanderPlas en la PyCon 2017.</p> <p>Cada una de estas librer\u00edas fue creada para satisfacer diferentes necesidades, algunas han ganado m\u00e1s adeptos que otras por uno u otro motivo. Tal como avanza la tecnolog\u00eda, estas librer\u00edas se actualizan o se crean nuevas, la importancia no recae en ser un experto en una, si no en saber adaptarse a las situaciones, tomar la mejor decicisi\u00f3n y escoger seg\u00fan nuestras necesidades y preferencias. Por ejemplo, <code>matplotlib</code> naci\u00f3 como una soluci\u00f3n para imitar los gr\u00e1ficos de <code>MATLAB</code> (puedes ver la historia completa aqu\u00ed), manteniendo una sintaxis similar y con ello poder crear gr\u00e1ficos est\u00e1ticos de muy buen nivel.</p> <p>Debido al \u00e9xito de <code>matplotlib</code> en la comunidad, nacen librer\u00edas basadas ella. Algunos ejemplos son:</p> <ul> <li><code>seaborn</code> se basa en <code>matp\u013aotlib</code> pero su nicho corresponde a las visualizaciones estad\u00edsticas.</li> <li><code>ggpy</code> una suerte de copia a <code>ggplot2</code> perteneciente al lenguaje de programaci\u00f3n <code>R</code>.</li> <li><code>networkx</code> visualizaciones de grafos.</li> <li><code>pandas</code> no es una librer\u00eda de visualizaci\u00f3n propiamente tal, pero utiliza a <code>matplotplib</code> como bakcned en los m\u00e9todos con tal de crear gr\u00e1ficos de manera muy r\u00e1pida, e.g. <code>pandas.DataFrame.plot.bar()</code></li> </ul> <p>Por otro lado, con tal de crear visualizaciones interactivas aparecen librer\u00edas basadas en <code>javascript</code>, algunas de las m\u00e1s conocidas en Python son:</p> <ul> <li><code>bokeh</code> tiene como objetivo proporcionar gr\u00e1ficos vers\u00e1tiles, elegantes e incluso interactivos, teniendo una gran performance con grandes datasets o incluso streaming de datos.</li> <li><code>plotly</code> visualizaciones interactivas que en conjunto a <code>Dash</code> (de la misma empresa) permite crear aplicaciones webs, similar a <code>shiny</code> de <code>R</code>.</li> <li><code>D3.js</code> a pesar de estar basado en <code>javascript</code> se ha ganado un lugar en el coraz\u00f3n de toda la comunidad, debido a la ilimitada cantidad de visualizaciones que son posibles de hacer, por ejemplo, la malla interactiva que hizo un estudiante de la UTFSM est\u00e1 hecha en <code>D3.js</code>. </li> </ul>"},{"location":"projects/project_definition/","title":"Proyecto Final","text":""},{"location":"projects/project_definition/#introduccion","title":"Introducci\u00f3n","text":"<p>La finalidad de este proyecto es enfrentar a los estudiantes a problemas de Machine Learning con todas las etapas (t\u00edpicas) que eso implica, bas\u00e1ndose en cada uno de los m\u00f3dulos aprendidos a lo largo del curso, dando pie a la investigaci\u00f3n y a la soluci\u00f3n de problemas operacionales del mundo real.</p>"},{"location":"projects/project_definition/#descripcion-del-proyecto","title":"Descripci\u00f3n del Proyecto","text":"<p>Bienvenido al a\u00f1o 2912, donde se necesitan tus habilidades de ciencia de datos  para resolver un misterio c\u00f3smico. Hemos recibido una transmisi\u00f3n desde cuatro a\u00f1os luz de distancia y las cosas no pintan bien.</p> <p>La nave espacial Titanic fue un transatl\u00e1ntico de  pasajeros interestelar lanzado hace un mes.  Con casi 13.000 pasajeros a bordo, la nave emprendi\u00f3  su viaje inaugural transportando emigrantes de nuestro sistema solar a tres exoplanetas recientemente habitables  que orbitan estrellas cercanas.</p> <p>Mientras rodeaba Alpha Centauri en ruta hacia su primer destino,  el t\u00f3rrido 55 Cancri E, la desprevenida nave espacial Titanic choc\u00f3 con una anomal\u00eda del espacio-tiempo escondida dentro de una nube de polvo. Lamentablemente, tuvo un destino similar al de su hom\u00f3nimo de 1000 a\u00f1os antes. Aunque la nave permaneci\u00f3 intacta, \u00a1casi la mitad de los pasajeros fueron transportados a una dimensi\u00f3n alternativa!</p> <p></p>"},{"location":"projects/project_definition/#evaluacion","title":"Evaluaci\u00f3n","text":"<p>El proyecto final consta de dos parte:</p> <ul> <li>Parte T\u00e9cnica: Desarrollar una soluci\u00f3n end to end del proyecto utilizando Jupyter Notebook.</li> <li>Presentaci\u00f3n de Resultados: Realizar una presentaci\u00f3n de 10-20 minutos de sus hallazgos.  </li> </ul>"},{"location":"projects/project_definition/#parte-tecnica","title":"Parte T\u00e9cnica","text":"<p>La Parte t\u00e9cnica debe cumplir con la siguiente r\u00fabrica de trabajo:</p> <ol> <li>Definici\u00f3n del problema </li> <li>Estad\u00edstica descriptiva </li> <li>Visualizaci\u00f3n descriptiva </li> <li>Preprocesamiento</li> <li>Selecci\u00f3n de modelo<ol> <li>Por lo menos debe comparar cuatro modelos</li> <li>Al menos tres de estos modelos tienen que tener hiperpar\u00e1metros.</li> <li>Realizar optimizaci\u00f3n de hiperpar\u00e1metros.</li> </ol> </li> <li>M\u00e9tricas y an\u00e1lisis de resultados</li> <li>Visualizaciones del modelo </li> <li>Conclusiones </li> </ol> <p>Observaci\u00f3n: Tendr\u00e1 una mejor puntaci\u00f3n si tambi\u00e9n incorpora modelos de redes neuronales. Se deja un tutorial de tensorflow a modo de ejemplo.</p> <p>La soluci\u00f3n debe alojarse en su Portafolio Personal del curso (<code>.ipynb</code>).</p>"},{"location":"projects/project_definition/#presentacion-de-resultados","title":"Presentaci\u00f3n de Resultados","text":"<ul> <li>La presentaci\u00f3n consta de 10-20 minutos. </li> <li>Utilizar diapositivas con BEAMER. Se deja el siguiente el siguiente tutorial a modo de ejemplo.</li> <li>La presentaci\u00f3n debe alojarse en su Portafolio Personal del curso (<code>.pdf</code>).</li> </ul>"},{"location":"projects/project_definition/#informacion-importante","title":"Informaci\u00f3n Importante","text":"<ul> <li>Plazo: 05 de Diciembre del 2024 (hasta las 11:59 PM) </li> <li>Esto corresponde a un desafio de Kaggle (link).</li> <li>La informaci\u00f3n respecto a los datos, lo pueden encontrar en el siguiente link.</li> <li>A modo de inspiraci\u00f3n, pueden ocupar algunos gr\u00e1ficos de otros participantes del desaf\u00edo (link).</li> </ul>"}]}